Paper Id,Paper Title,Key Words,Abstract,Conclusion,Document,Paper Type,Summary,Topic,OCR,labels
1,"Multi-document Summarization via Deep Learning
Techniques: A Survey","Multi-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregation","Multi-document summarization (MDS) is an effective tool for information aggregation that generates an informative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,
systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to summarize the design strategies of neural networks and conduct a comprehensive summary of the state of the
art. We highlight the differences between various objective functions that are rarely discussed in the existing
literature. Finally, we propose several future directions pertaining to this new and exciting field.","In this article, we have presented the first comprehensive review of the most notable works to date
on deep-learning-based multi-document summarization (MDS). We propose a taxonomy for organizing and clustering existing publications and devise the network design strategies based on the
state-of-the-art methods. We also provide an overview of the existing multi-document objective
functions, evaluation metrics, and datasets and discuss some of the most pressing open problems
and promising future extensions in MDS research. We hope this survey provides readers with a
comprehensive understanding of the key aspects of MDS tasks, clarifies the most notable advances,
and sheds light on future studies","Multi-document Summarization via Deep Learning
Techniques: A SurveyMulti-document summarization (MDS), Deep learning models, Objective functions, Taxonomy, Evaluation metrics, Future directions, Information aggregationMulti-document summarization (MDS) is an effective tool for information aggregation that generates an informative and concise summary from a cluster of topic-related documents. Our survey, the first of its kind,
systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to summarize the design strategies of neural networks and conduct a comprehensive summary of the state of the
art. We highlight the differences between various objective functions that are rarely discussed in the existing
literature. Finally, we propose several future directions pertaining to this new and exciting field.In this article, we have presented the first comprehensive review of the most notable works to date
on deep-learning-based multi-document summarization (MDS). We propose a taxonomy for organizing and clustering existing publications and devise the network design strategies based on the
state-of-the-art methods. We also provide an overview of the existing multi-document objective
functions, evaluation metrics, and datasets and discuss some of the most pressing open problems
and promising future extensions in MDS research. We hope this survey provides readers with a
comprehensive understanding of the key aspects of MDS tasks, clarifies the most notable advances,
and sheds light on future studieslla i aye RR | poe [Sena Sena | Sena, —+ ar ea) } (b) Word/Sentence-level Concatenation MethodsInput Extractive Summarization Hybrid Summarization Selective Selective | {Generative| Extractor; —e| Selective || input —e) Extractor —e| Sects [Abstractor —+| Generative Extractive Abstractive Model Abstractive Summarization —= Sree Seat avast —+{ Generative Summary. ak Generative) (Generative) sector —{ Sena [avstraror} (Ser } Abstractive - Abstractive Model /‘Dec {a) Naive Networks (c} Auxiliary Task Networks Processing earner} (b) Ensemble Networks =a ao (6) Reconstruction Networks elton Groh === {e) Fusion Networks Encoder oecoser Processing (f) Graph Neural Networks Processing |! {suman (g) Encoder-Decoder Structure a ocesine a a (h) Pre-trained Language Models",Text summarization,"This article presents a systematic overview of recent deep-learning-based models for multi-document summarization (MDS). It proposes a new taxonomy to categorize network design strategies and provides an overview of objective functions, evaluation metrics, and datasets. The article also discusses open problems and future research directions in the field. The survey aims to provide a comprehensive understanding of MDS tasks and highlight notable advances while shedding light on potential areas for further study.",Natural Language Processing,"lla i aye RR | poe [Sena Sena | Sena, —+ ar ea) } (b) Word/Sentence-level Concatenation MethodsInput Extractive Summarization Hybrid Summarization Selective Selective | {Generative| Extractor; —e| Selective || input —e) Extractor —e| Sects [Abstractor —+| Generative Extractive Abstractive Model Abstractive Summarization —= Sree Seat avast —+{ Generative Summary. ak Generative) (Generative) sector —{ Sena [avstraror} (Ser } Abstractive - Abstractive Model /‘Dec {a) Naive Networks (c} Auxiliary Task Networks Processing earner} (b) Ensemble Networks =a ao (6) Reconstruction Networks elton Groh === {e) Fusion Networks Encoder oecoser Processing (f) Graph Neural Networks Processing |! {suman (g) Encoder-Decoder Structure a ocesine a a (h) Pre-trained Language Models",Deep Learning and Machine Learning
2,"NLP based Machine Learning Approaches for Text 
Summarization","Text summarization, Abstractive and extractive summaries, Query-based summarization, Structured-based and semantic-based approaches, Evaluation metrics (ROGUE score and TF_IDF score), Future directions (GANs and transfer learning),Data abundance and information overload","Due to the plethora of data available today, text 
summarization has become very essential to gain just the right 
amount of information from huge texts. We see long articles in 
news websites, blogs, customers’ review websites, and so on. This 
review paper presents various approaches to generate summary 
of huge texts. Various papers have been studied for different 
methods that have been used so far for text summarization. 
Mostly, the methods described in this paper produce Abstractive 
(ABS) or Extractive (EXT) summaries of text documents. Querybased summarization techniques are also discussed. The paper 
mostly discusses about the structured based and semantic based 
approaches for summarization of the text documents. Various 
datasets were used to test the summaries produced by these 
models, such as the CNN corpus, DUC2000, single and multiple 
text documents etc. We have studied these methods and also the 
tendencies, achievements, past work and future scope of them in 
text summarization as well as other fields","We have seen that due to abundant availability of data, text 
summarization has a very vital role in saving user’s time, as 
well as resources. Text summarization is indeed an important 
tool for today. We have seen the use of various algorithms and 
methods for this purpose. These methods, in individual and 
together give different types of summaries. Their accuracy 
score can be compared to find the better and more concise 
summaries. For this purpose, ROGUE score has been used 
more frequently. Similarly, in some cases TF_IDF scores have 
been used too. 
The summaries generated using these methods are not 
always up to the mark. Sometimes, it’s also irrelevant to the 
original document. Therefore, this topic is ongoing and people 
have done various works on this. There isn’t any specific 
model that generates best summaries. So, for future, the models 
discussed can be modified for more accurate summaries. For 
e.g., we could use GAN’s and transfer learning. For future, this 
can give a way to develop and enhance further ideas for text 
summarization. ","NLP based Machine Learning Approaches for Text 
SummarizationText summarization, Abstractive and extractive summaries, Query-based summarization, Structured-based and semantic-based approaches, Evaluation metrics (ROGUE score and TF_IDF score), Future directions (GANs and transfer learning),Data abundance and information overloadDue to the plethora of data available today, text 
summarization has become very essential to gain just the right 
amount of information from huge texts. We see long articles in 
news websites, blogs, customers’ review websites, and so on. This 
review paper presents various approaches to generate summary 
of huge texts. Various papers have been studied for different 
methods that have been used so far for text summarization. 
Mostly, the methods described in this paper produce Abstractive 
(ABS) or Extractive (EXT) summaries of text documents. Querybased summarization techniques are also discussed. The paper 
mostly discusses about the structured based and semantic based 
approaches for summarization of the text documents. Various 
datasets were used to test the summaries produced by these 
models, such as the CNN corpus, DUC2000, single and multiple 
text documents etc. We have studied these methods and also the 
tendencies, achievements, past work and future scope of them in 
text summarization as well as other fieldsWe have seen that due to abundant availability of data, text 
summarization has a very vital role in saving user’s time, as 
well as resources. Text summarization is indeed an important 
tool for today. We have seen the use of various algorithms and 
methods for this purpose. These methods, in individual and 
together give different types of summaries. Their accuracy 
score can be compared to find the better and more concise 
summaries. For this purpose, ROGUE score has been used 
more frequently. Similarly, in some cases TF_IDF scores have 
been used too. 
The summaries generated using these methods are not 
always up to the mark. Sometimes, it’s also irrelevant to the 
original document. Therefore, this topic is ongoing and people 
have done various works on this. There isn’t any specific 
model that generates best summaries. So, for future, the models 
discussed can be modified for more accurate summaries. For 
e.g., we could use GAN’s and transfer learning. For future, this 
can give a way to develop and enhance further ideas for text 
summarization. @STOM © Word Vector Embedding kenearest neighbor algorithm © Differential Evolution Algorithm Newtonian Method Artificial Bee Colony Human Learning Algorithm",Natural Language Processing,"The article discusses the importance of text summarization due to the abundance of data available today. Various approaches, including abstractive and extractive methods, as well as query-based techniques, are presented. The paper focuses on structured and semantic-based approaches and discusses the datasets used for testing these methods. While the accuracy of the summaries generated using these methods can be compared, there is no specific model that generates the best summaries. The article suggests that GANs and transfer learning could be used to improve future text summarization models.",Natural Language Processing,@STOM © Word Vector Embedding kenearest neighbor algorithm © Differential Evolution Algorithm Newtonian Method Artificial Bee Colony Human Learning Algorithm,Deep Learning and Machine Learning
3,"Abstractive text summarization using LSTM-CNN based
deep learning","Text mining . Abstractive text summarization . Relation extraction, Natural Language Processing"," Abstractive Text Summarization (ATS), which is the task of constructing summary
sentences by merging facts from different source sentences and condensing them into a shorter
representation while preserving information content and overall meaning. It is very difficult
and time consuming for human beings to manually summarize large documents of text. In this
paper, we propose an LSTM-CNN based ATS framework (ATSDL) that can construct new
sentences by exploring more fine-grained fragments than sentences, namely, semantic phrases.
Different from existing abstraction based approaches, ATSDL is composed of two main stages,
the first of which extracts phrases from source sentences and the second generates text
summaries using deep learning. Experimental results on the datasets CNN and DailyMail
show that our ATSDL framework outperforms the state-of-the-art models in terms of both
semantics and syntactic structure, and achieves competitive results on manual linguistic quality
evaluation","In this paper, we develop a novel LSTM-CNN based ATSDL model that overcomes several
key problems in the field of TS. The present ETS models are concerned with syntactic
structure, while present ATS models are concerned with semantics. Our model draws on the
strengths of both of summarization models. The new ATSDL model firstly uses phrase
extraction method called MOSP to extract key phrases from the original text, and then learns
the collocation of phrases. After training, the model will generate a phrase sequence that meets
the requirement of syntactic structure. In addition, we use phrase location information to solve
the rare words problem that almost all ATS models would encounter. Finally, we conduct
extensive experiments on two different datasets and the result shows that our model outperforms the state-of-the-art approaches in terms of both semantics and syntactic structure.","Abstractive text summarization using LSTM-CNN based
deep learningText mining . Abstractive text summarization . Relation extraction, Natural Language Processing Abstractive Text Summarization (ATS), which is the task of constructing summary
sentences by merging facts from different source sentences and condensing them into a shorter
representation while preserving information content and overall meaning. It is very difficult
and time consuming for human beings to manually summarize large documents of text. In this
paper, we propose an LSTM-CNN based ATS framework (ATSDL) that can construct new
sentences by exploring more fine-grained fragments than sentences, namely, semantic phrases.
Different from existing abstraction based approaches, ATSDL is composed of two main stages,
the first of which extracts phrases from source sentences and the second generates text
summaries using deep learning. Experimental results on the datasets CNN and DailyMail
show that our ATSDL framework outperforms the state-of-the-art models in terms of both
semantics and syntactic structure, and achieves competitive results on manual linguistic quality
evaluationIn this paper, we develop a novel LSTM-CNN based ATSDL model that overcomes several
key problems in the field of TS. The present ETS models are concerned with syntactic
structure, while present ATS models are concerned with semantics. Our model draws on the
strengths of both of summarization models. The new ATSDL model firstly uses phrase
extraction method called MOSP to extract key phrases from the original text, and then learns
the collocation of phrases. After training, the model will generate a phrase sequence that meets
the requirement of syntactic structure. In addition, we use phrase location information to solve
the rare words problem that almost all ATS models would encounter. Finally, we conduct
extensive experiments on two different datasets and the result shows that our model outperforms the state-of-the-art approaches in terms of both semantics and syntactic structure.encoder decoderWord Merpholosical Coreterence segmentation =O reduction Sesion Phrase process Phrase Phrase Phrasewary ndin wiry UoPPLH, wary snd is input Text",Text summarization,The article presents a new framework for abstractive text summarization (ATS) called the LSTM-CNN based ATSDL model. This model uses a phrase extraction method called MOSP to extract key phrases from the original text and learns the collocation of phrases to generate a phrase sequence that meets the requirement of syntactic structure. The model also uses phrase location information to solve the rare words problem. Experimental results show that the ATSDL model outperforms existing state-of-the-art approaches in terms of both semantics and syntactic structure on two different datasets.,Natural Language Processing,"encoder decoderWord Merpholosical Coreterence segmentation =O reduction Sesion Phrase process Phrase Phrase Phrasewary ndin wiry UoPPLH, wary snd is input Text",Deep Learning and Machine Learning
4,"DEXPERTS: Decoding-Time Controlled Text Generation
with Experts and Anti-Experts","Natural language generation, Controlled text generation, Decoding-time Experts (DEXPERTS), Language detoxification, Sentiment-controlled generation, Pretrained language model, Ensemble learning, Small language models","Despite recent advances in natural language
generation, it remains challenging to control
attributes of generated text. We propose DEXPERTS: Decoding-time Experts, a decodingtime method for controlled text generation
that combines a pretrained language model
with “expert” LMs and/or “anti-expert” LMs
in a product of experts. Intuitively, under
the ensemble, tokens only get high probability if they are considered likely by the experts and unlikely by the anti-experts. We apply DEXPERTS to language detoxification and
sentiment-controlled generation, where we
outperform existing controllable generation
methods on both automatic and human evaluations. Moreover, because DEXPERTS operates
only on the output of the pretrained LM, it is
effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work
highlights the promise of tuning small LMs on
text with (un)desirable attributes for efficient
decoding-time steering","We present DEXPERTS, a method for controlled
text generation that reweights the predictions of
language models based on expert (and anti-expert)
opinions. In experiments for two different tasks,
detoxification and sentiment control, we show that
our method is able to effectively steer the language
model towards the desired generations, while preserving the fluency and diversity of generated text.
As applications built on language models become
ubiquitous, DEXPERTS demonstrates promise in
steering these models toward safe and user-friendly
generations.","DEXPERTS: Decoding-Time Controlled Text Generation
with Experts and Anti-ExpertsNatural language generation, Controlled text generation, Decoding-time Experts (DEXPERTS), Language detoxification, Sentiment-controlled generation, Pretrained language model, Ensemble learning, Small language modelsDespite recent advances in natural language
generation, it remains challenging to control
attributes of generated text. We propose DEXPERTS: Decoding-time Experts, a decodingtime method for controlled text generation
that combines a pretrained language model
with “expert” LMs and/or “anti-expert” LMs
in a product of experts. Intuitively, under
the ensemble, tokens only get high probability if they are considered likely by the experts and unlikely by the anti-experts. We apply DEXPERTS to language detoxification and
sentiment-controlled generation, where we
outperform existing controllable generation
methods on both automatic and human evaluations. Moreover, because DEXPERTS operates
only on the output of the pretrained LM, it is
effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work
highlights the promise of tuning small LMs on
text with (un)desirable attributes for efficient
decoding-time steeringWe present DEXPERTS, a method for controlled
text generation that reweights the predictions of
language models based on expert (and anti-expert)
opinions. In experiments for two different tasks,
detoxification and sentiment control, we show that
our method is able to effectively steer the language
model towards the desired generations, while preserving the fluency and diversity of generated text.
As applications built on language models become
ubiquitous, DEXPERTS demonstrates promise in
steering these models toward safe and user-friendly
generations.reatva star on negative proms oe TT os ee oe SOT © < T TESH NT ot a oe A TaN 2 a ve 2c NOS 237 ee oc: A TL rr oe ST ovr oe SL ns ver EE SSO @ tage searing on postive mets TT 0180 ee a rT ve te ee ET See 0 ee) a rr) eet Ee 1 TT 1655901 ee: 030 08 coe oe ES 205 934 o_o rr eet TE 07 oe oe TT eS 028 ere A SS 937 ao 025 05 078 wo ul uy i ul He Despers Despens eonaan E) 8 ts non-toxic LM toxic LM base LM (expert) {anti-expert) ‘ - z her hs some -—_g@— Mee dp",Text generation,"The paper proposes a method called DEXPERTS for controlled text generation by combining a pretrained language model with expert and anti-expert language models in a product of experts. The approach is applied to language detoxification and sentiment-controlled generation and outperforms existing controllable generation methods. The method is effective with small expert and anti-expert language models, and highlights the promise of tuning language models for efficient decoding-time steering towards safe and user-friendly generations.",Natural Language Processing,reatva star on negative proms oe TT os ee oe SOT © < T TESH NT ot a oe A TaN 2 a ve 2c NOS 237 ee oc: A TL rr oe ST ovr oe SL ns ver EE SSO @ tage searing on postive mets TT 0180 ee a rT ve te ee ET See 0 ee) a rr) eet Ee 1 TT 1655901 ee: 030 08 coe oe ES 205 934 o_o rr eet TE 07 oe oe TT eS 028 ere A SS 937 ao 025 05 078 wo ul uy i ul He Despers Despens eonaan E) 8 ts non-toxic LM toxic LM base LM (expert) {anti-expert) ‘ - z her hs some -—_g@— Mee dp,Deep Learning and Machine Learning
5,A Survey of Knowledge-enhanced Text Generation,"text-to-text generation, natural language processing, knowledge-enhanced text generation, neural encoder-decoder models, internal knowledge, external knowledge, knowledge acquisition, text generation applications","The goal of text-to-text generation is to make machines express like a human in many applications such as
conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural
language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal
by learning to map input text to output text. However, the input text alone often provides limited knowledge
to generate the desired output, so the performance of text generation is still far from satisfaction in many
real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge
embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and
knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text
generation. In this survey, we present a comprehensive review of the research on this topic over the past five
years. The main content includes two parts: (i) general methods and architectures for integrating knowledge
into text generation; (ii) specific techniques and applications according to different forms of knowledge data.
This survey can have broad audiences, researchers and practitioners, in academia and industry.","In this survey, we present a comprehensive review of current representative research efforts and
trends on knowledge-enhanced text generation and expect it can facilitate future research. To summarize, this survey aims to answer two questions that commonly appear in knowledge-enhanced
text generation: how to acquire knowledge and how to incorporate knowledge to facilitate text generation. Based on knowledge acquisition, the main content of our survey is divided into three sections
according to different sources of knowledge enhancement. Based on knowledge incorporation, we
first present general methods of incorporating knowledge into text generation and further discuss
a number of specific ideas and technical solutions that incorporate the knowledge to enhance the
text generation systems in each section. Besides, we review a variety of text generation applications in each section to help practitioners learn to choose and employ the methods.","A Survey of Knowledge-enhanced Text Generationtext-to-text generation, natural language processing, knowledge-enhanced text generation, neural encoder-decoder models, internal knowledge, external knowledge, knowledge acquisition, text generation applicationsThe goal of text-to-text generation is to make machines express like a human in many applications such as
conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural
language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal
by learning to map input text to output text. However, the input text alone often provides limited knowledge
to generate the desired output, so the performance of text generation is still far from satisfaction in many
real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge
embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and
knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text
generation. In this survey, we present a comprehensive review of the research on this topic over the past five
years. The main content includes two parts: (i) general methods and architectures for integrating knowledge
into text generation; (ii) specific techniques and applications according to different forms of knowledge data.
This survey can have broad audiences, researchers and practitioners, in academia and industry.In this survey, we present a comprehensive review of current representative research efforts and
trends on knowledge-enhanced text generation and expect it can facilitate future research. To summarize, this survey aims to answer two questions that commonly appear in knowledge-enhanced
text generation: how to acquire knowledge and how to incorporate knowledge to facilitate text generation. Based on knowledge acquisition, the main content of our survey is divided into three sections
according to different sources of knowledge enhancement. Based on knowledge incorporation, we
first present general methods of incorporating knowledge into text generation and further discuss
a number of specific ideas and technical solutions that incorporate the knowledge to enhance the
text generation systems in each section. Besides, we review a variety of text generation applications in each section to help practitioners learn to choose and employ the methods.(ira => Generation model => foam] | Generation model > [Fone]",Text generation,"The paper discusses the challenges in text-to-text generation and how incorporating internal and external knowledge can improve performance. The authors present a comprehensive survey of the research on knowledge-enhanced text generation in the past five years, covering general methods and specific techniques according to different forms of knowledge data. The survey aims to facilitate future research and help practitioners choose and employ methods for various text generation applications.",Natural Language Processing,(ira => Generation model => foam] | Generation model > [Fone],Deep Learning and Machine Learning
6,"Automatic Text Summarization of COVID-19 Medical
Research Articles using BERT and GPT-2","COVID-19, text summarization, NLP, BERT, OpenAI GPT-2, deep learning, abstractive summarization, medical community","With the COVID-19 pandemic, there is a growing urgency for medical community
to keep up with the accelerating growth in the new coronavirus-related literature.
As a result, the COVID-19 Open Research Dataset Challenge has released a corpus
of scholarly articles and is calling for machine learning approaches to help bridging
the gap between the researchers and the rapidly growing publications. Here, we
take advantage of the recent advances in pre-trained NLP models, BERT and
OpenAI GPT-2, to solve this challenge by performing text summarization on this
dataset. We evaluate the results using ROUGE scores and visual inspection. Our
model provides abstractive and comprehensive information based on keywords
extracted from the original articles. Our work can help the the medical community,
by providing succinct summaries of articles for which the abstract are not already
available.
","Abstractive summarization still represents a standing challenge for deep-learning NLP. Even more
so when this task is applied to a domain-specific corpus that are different from the pre-training, are
highly technical, or contains low amount of training materials. COVID-19 Open Research Dataset
Challenge exemplify all the abovementioned difficulty. Nevertheless We have here illustrated that
the text-to-text, multi-loss training strategy could be used to fine-tune a pre-trained language model
such as GPT-2 to perform abstractive summarization. The result is interpretable and reasonable, even
though it is not near human-level performance.
First of all, We think that our model could benefit from further training as the new coronavirus-related
research publication are becoming available. This should make the model more accurate in its ability
to infer conclusion from the keywords.","Automatic Text Summarization of COVID-19 Medical
Research Articles using BERT and GPT-2COVID-19, text summarization, NLP, BERT, OpenAI GPT-2, deep learning, abstractive summarization, medical communityWith the COVID-19 pandemic, there is a growing urgency for medical community
to keep up with the accelerating growth in the new coronavirus-related literature.
As a result, the COVID-19 Open Research Dataset Challenge has released a corpus
of scholarly articles and is calling for machine learning approaches to help bridging
the gap between the researchers and the rapidly growing publications. Here, we
take advantage of the recent advances in pre-trained NLP models, BERT and
OpenAI GPT-2, to solve this challenge by performing text summarization on this
dataset. We evaluate the results using ROUGE scores and visual inspection. Our
model provides abstractive and comprehensive information based on keywords
extracted from the original articles. Our work can help the the medical community,
by providing succinct summaries of articles for which the abstract are not already
available.
Abstractive summarization still represents a standing challenge for deep-learning NLP. Even more
so when this task is applied to a domain-specific corpus that are different from the pre-training, are
highly technical, or contains low amount of training materials. COVID-19 Open Research Dataset
Challenge exemplify all the abovementioned difficulty. Nevertheless We have here illustrated that
the text-to-text, multi-loss training strategy could be used to fine-tune a pre-trained language model
such as GPT-2 to perform abstractive summarization. The result is interpretable and reasonable, even
though it is not near human-level performance.
First of all, We think that our model could benefit from further training as the new coronavirus-related
research publication are becoming available. This should make the model more accurate in its ability
to infer conclusion from the keywords.Gpr2 Language ‘Model <BOS> | KiKi KsonKs [<sum> | Abstract. | <£0S> | <pad> [eooss | kiki, <sum> |_istactor... | <€0s> | <page 608 | KKK | sums | pistactor,. | <£08> [pads id.",Natural Language Processing,"We used BERT and OpenAI GPT-2 pre-trained NLP models to perform text summarization on the COVID-19 Open Research Dataset Challenge corpus. Our model provides comprehensive information based on extracted keywords and can help the medical community by providing succinct summaries of articles for which abstracts are not available. Abstractive summarization is a challenging task, especially for technical, domain-specific corpora with limited training materials. However, we showed that a multi-loss training strategy could fine-tune a pre-trained language model such as GPT-2 to perform abstractive summarization, though still not at human-level performance. Further training could improve the model's accuracy with new publications becoming available.",Natural Language Processing,"Gpr2 Language ‘Model <BOS> | KiKi KsonKs [<sum> | Abstract. | <£0S> | <pad> [eooss | kiki, <sum> |_istactor... | <€0s> | <page 608 | KKK | sums | pistactor,. | <£08> [pads id.",Deep Learning and Machine Learning
7,A DATA MINING APPROACH FOR PREDICTION OF HEART DISEASE USING NEURAL NETWORKS,"Backpropagation, Data mining, Heart disease, Multilayer perceptron neural network, Neural Network.","Heart disease diagnosis is a complex task which requires much experience and knowledge. Traditional way of predicting Heart disease is doctor’s examination or number of medical tests such as ECG, Stress Test, and Heart MRI etc. Nowadays, Health care industry contains huge amount of heath care data, which contains hidden information. This hidden information is useful for making effective decisions. Computer based information along with advanced Data mining techniques are used for appropriate results. Neural network is widely used tool for predicting Heart disease diagnosis. In this research paper, a Heart Disease Prediction system (HDPS) is developed using Neural network. The HDPS system predicts the likelihood of patient getting a Heart disease. For prediction, the system uses sex, blood pressure, cholesterol like 13 medical parameters. Here two more parameters are added i.e. obesity and smoking for better accuracy. From the results, it has been seen that neural network predict heart disease with nearly 100% accuracy.","In this research paper, we have presented Heart disease prediction system (HDPS) using data mining and artificial neural network (ANN) techniques. From the ANN, a multilayer perceptron neural network along with back propagation algorithm is used to develop the system. Because MLPNN model proves the better results and helps the domain experts and even person related with the field to plan for a better diagnose and provide the patient with early diagnosis results as it performs realistically well even without retraining. The experimental result shows that using neural networks the system predicts Heart disease with nearly 100% accuracy.","A DATA MINING APPROACH FOR PREDICTION OF HEART DISEASE USING NEURAL NETWORKSBackpropagation, Data mining, Heart disease, Multilayer perceptron neural network, Neural Network.Heart disease diagnosis is a complex task which requires much experience and knowledge. Traditional way of predicting Heart disease is doctor’s examination or number of medical tests such as ECG, Stress Test, and Heart MRI etc. Nowadays, Health care industry contains huge amount of heath care data, which contains hidden information. This hidden information is useful for making effective decisions. Computer based information along with advanced Data mining techniques are used for appropriate results. Neural network is widely used tool for predicting Heart disease diagnosis. In this research paper, a Heart Disease Prediction system (HDPS) is developed using Neural network. The HDPS system predicts the likelihood of patient getting a Heart disease. For prediction, the system uses sex, blood pressure, cholesterol like 13 medical parameters. Here two more parameters are added i.e. obesity and smoking for better accuracy. From the results, it has been seen that neural network predict heart disease with nearly 100% accuracy.In this research paper, we have presented Heart disease prediction system (HDPS) using data mining and artificial neural network (ANN) techniques. From the ANN, a multilayer perceptron neural network along with back propagation algorithm is used to develop the system. Because MLPNN model proves the better results and helps the domain experts and even person related with the field to plan for a better diagnose and provide the patient with early diagnosis results as it performs realistically well even without retraining. The experimental result shows that using neural networks the system predicts Heart disease with nearly 100% accuracy.Feed forward r———>) training pattern ——————_Y¥—_. Calculate error y Compute differences ———___¥—______ Propagate error backwards yv Update Weights Figure 3: Back-propagation Neural network100.20% 100.00% 99.80% 99.60% Accuracy Godby, 99.20% 99.00% 98.80% accuracy 13 parameters 15 parameters Number of parameters used for prediction Figure 4: Graph shows accuracy for 13 and 15 parametersime dependent Complication in performance diagnosis Replication of experts not Doctors possible — Knowledge up gradation Adveanteage is difficult Active intelligent diagnosis Difficult to establish multi variables relation = Figure 1: Complexity in diagnosis with doctor [4].output layer x1 —o1 x2 —o2z x3 —ad Figure 2: Multilayer Perceptron Neural Network (MLPNN)",Artificial Neural Network,"This research paper presents a Heart Disease Prediction System (HDPS) that uses data mining and artificial neural network techniques to predict the likelihood of a patient getting a heart disease. The system uses thirteen medical parameters including blood pressure and cholesterol, and two additional parameters, obesity and smoking, for better accuracy. The multilayer perceptron neural network model along with the back propagation algorithm is used for system development. The experimental results show that the system predicts heart disease with nearly 100% accuracy. This system can be a valuable tool for domain experts and healthcare providers to plan for better diagnoses and provide patients with early diagnosis results.",Medical Data Analysis,"Feed forward r———>) training pattern ——————_Y¥—_. Calculate error y Compute differences ———___¥—______ Propagate error backwards yv Update Weights Figure 3: Back-propagation Neural network100.20% 100.00% 99.80% 99.60% Accuracy Godby, 99.20% 99.00% 98.80% accuracy 13 parameters 15 parameters Number of parameters used for prediction Figure 4: Graph shows accuracy for 13 and 15 parametersime dependent Complication in performance diagnosis Replication of experts not Doctors possible — Knowledge up gradation Adveanteage is difficult Active intelligent diagnosis Difficult to establish multi variables relation = Figure 1: Complexity in diagnosis with doctor [4].output layer x1 —o1 x2 —o2z x3 —ad Figure 2: Multilayer Perceptron Neural Network (MLPNN)",Medical Data Analysis
8,"SOCIAL MEDIA  SENTIMENT 
ANALYSIS BASED ON COVID 19","NLP,RNN,
sentiment 
analysis,
social media,
visualization","In today's world, the social media is everywhere, and everybody come in contact with it every day. With social media datas, we are able to do a lot of analysis and statistics nowdays. Within this scope of article, we conclude and analyse the sentiments and manifestations (comments, hastags, posts, tweets) of the users of the Twitter social media platform, based on the main trends (by keyword, which is mostly the ‘covid’ and coronavirus theme in this article) with Natural Language Processing and with Sentiment Classification using Recurrent Neural Network. Where we analyse, compile, visualize statistics, and summarize for further processing. The trained model works much more accurately, with a smaller margin of error, in determining emotional polarity in today's ‘modern’ often with ambiguous tweets. Especially with RNN. We use this fresh scraped data collections (by the keyword's theme) with our RNN model what we have created and trained to determine what emotional manifestations occurred on a given topic in a given time interval.","In this work, we use a Recurrent Neural Network (RNN) to classify emotions on tweets. We developed a model to analyse the emotional nature of various tweets, using the recurrent neural network for emotional prediction, searching for connections between words, and marking them with positive or negative emotions. Where instead of simple positive and negative extremes, we have classified the various texts into a much more articulated class of emotional strength (weakly positive/negative, strongly positive/negative). This has been combined with a keyword-based special data scraper, so we can apply our taught RNN model with these specific freshly scraped datasets. As a result, we get an emotional classification related to specific topics. What kind of tweets they were and what emotional class they belong to, what is the distribution on that topic at the emotional level within the given start interval. In the article, we focused most on the coronavirus and related emotional changes and fluctuations, and it was shown that the overall positive manifestation and presence on the social platform remained on social media surfaces during this pandemic. Of course, in addition to negative and other manifestations. Over time, positivity has strengthened, but there is also a stronger negative array that is natural. According to our expectations this topic remain positive manifestations, sometimes with a higher and sometimes with a smaller percentage. It can be seen that the recurrent neural network provides good performance and prediction in text classification. Where the RNN model brought a smaller amount of data in neutral result or completely reduced to zero that. Which proves that our model is ‘able to make’ a decision and categorize in some direction even on the basis of small details. Our comparisons were made mainly against TextBlob, which also worked very well and delivered stable results, but there were many times when the neutral results were above 30% compared to our RNN model, which we cannot use as usefully for further evaluations as for our RNN model. The classification of emotions for both models (TextBlob, RNN) was properly segmented.","SOCIAL MEDIA  SENTIMENT 
ANALYSIS BASED ON COVID 19NLP,RNN,
sentiment 
analysis,
social media,
visualizationIn today's world, the social media is everywhere, and everybody come in contact with it every day. With social media datas, we are able to do a lot of analysis and statistics nowdays. Within this scope of article, we conclude and analyse the sentiments and manifestations (comments, hastags, posts, tweets) of the users of the Twitter social media platform, based on the main trends (by keyword, which is mostly the ‘covid’ and coronavirus theme in this article) with Natural Language Processing and with Sentiment Classification using Recurrent Neural Network. Where we analyse, compile, visualize statistics, and summarize for further processing. The trained model works much more accurately, with a smaller margin of error, in determining emotional polarity in today's ‘modern’ often with ambiguous tweets. Especially with RNN. We use this fresh scraped data collections (by the keyword's theme) with our RNN model what we have created and trained to determine what emotional manifestations occurred on a given topic in a given time interval.In this work, we use a Recurrent Neural Network (RNN) to classify emotions on tweets. We developed a model to analyse the emotional nature of various tweets, using the recurrent neural network for emotional prediction, searching for connections between words, and marking them with positive or negative emotions. Where instead of simple positive and negative extremes, we have classified the various texts into a much more articulated class of emotional strength (weakly positive/negative, strongly positive/negative). This has been combined with a keyword-based special data scraper, so we can apply our taught RNN model with these specific freshly scraped datasets. As a result, we get an emotional classification related to specific topics. What kind of tweets they were and what emotional class they belong to, what is the distribution on that topic at the emotional level within the given start interval. In the article, we focused most on the coronavirus and related emotional changes and fluctuations, and it was shown that the overall positive manifestation and presence on the social platform remained on social media surfaces during this pandemic. Of course, in addition to negative and other manifestations. Over time, positivity has strengthened, but there is also a stronger negative array that is natural. According to our expectations this topic remain positive manifestations, sometimes with a higher and sometimes with a smaller percentage. It can be seen that the recurrent neural network provides good performance and prediction in text classification. Where the RNN model brought a smaller amount of data in neutral result or completely reduced to zero that. Which proves that our model is ‘able to make’ a decision and categorize in some direction even on the basis of small details. Our comparisons were made mainly against TextBlob, which also worked very well and delivered stable results, but there were many times when the neutral results were above 30% compared to our RNN model, which we cannot use as usefully for further evaluations as for our RNN model. The classification of emotions for both models (TextBlob, RNN) was properly segmented.",Natural Language Processing,"The article discusses the use of natural language processing and sentiment classification using recurrent neural network to analyze sentiments and manifestations of Twitter users on the topic of COVID-19. The RNN model was able to accurately classify emotional polarity in ambiguous tweets and classify emotions into more articulated classes of emotional strength. The analysis showed that despite negative manifestations, overall positivity remained on social media during the pandemic. Comparisons were made against TextBlob, but the RNN model showed better results with less neutral results. The RNN model proved to be effective in categorizing emotions and making decisions even with small details.",Natural Language Processing,,Deep Learning and Machine Learning
9,Explaining Recurrent Neural Network Predictions in Sentiment Analysis,"Layer-wise Relevance Propagation, recurrent neural networks, multiplicative connections, LSTMs, GRUs, sentiment prediction, word relevances, explanation methods","Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work","onclusion In this work we have introduced a simple yet effective strategy for extending the LRP procedure to recurrent architectures, such as LSTMs, by proposing a rule to backpropagate the relevance through multiplicative interactions. We applied the extended LRP version to a bi-directional LSTM model for the sentiment prediction of sentences, demonstrating that the resulting word relevances trustworthy reveal words supporting the classifier’s decision for or against a specific class, and perform better than those obtained by a gradient-based decomposition. Our technique helps understanding and verifying the correct behavior of recurrent classifiers, and can detect important patterns in text datasets. Compared to other non-gradient based explanation methods, which rely e.g. on random sampling or on iterative representation occlusion, our technique is deterministic, and can be computed in one pass through the network. Moreover, our method is self-contained, in that it does not require to train an external classifier to deliver the explanations, these are obtained directly via the original classifier. Future work would include applying the proposed technique to other recurrent architectures such as character-level models or GRUs, as well as to extractive summarization. Besides, our method is not restricted to the NLP domain, and might also be useful to other applications relying on recurrent architectures. Ack","Explaining Recurrent Neural Network Predictions in Sentiment AnalysisLayer-wise Relevance Propagation, recurrent neural networks, multiplicative connections, LSTMs, GRUs, sentiment prediction, word relevances, explanation methodsRecently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous workonclusion In this work we have introduced a simple yet effective strategy for extending the LRP procedure to recurrent architectures, such as LSTMs, by proposing a rule to backpropagate the relevance through multiplicative interactions. We applied the extended LRP version to a bi-directional LSTM model for the sentiment prediction of sentences, demonstrating that the resulting word relevances trustworthy reveal words supporting the classifier’s decision for or against a specific class, and perform better than those obtained by a gradient-based decomposition. Our technique helps understanding and verifying the correct behavior of recurrent classifiers, and can detect important patterns in text datasets. Compared to other non-gradient based explanation methods, which rely e.g. on random sampling or on iterative representation occlusion, our technique is deterministic, and can be computed in one pass through the network. Moreover, our method is self-contained, in that it does not require to train an external classifier to deliver the explanations, these are obtained directly via the original classifier. Future work would include applying the proposed technique to other recurrent architectures such as character-level models or GRUs, as well as to extractive summarization. Besides, our method is not restricted to the NLP domain, and might also be useful to other applications relying on recurrent architectures. Ack",Artificial Neural Network,"The article discusses the extension of the Layer-wise Relevance Propagation (LRP) technique to recurrent neural networks, specifically those with multiplicative connections like LSTMs and GRUs. The extended LRP version was applied to a bi-directional LSTM model for sentiment prediction in sentences and produced better results than a gradient-based method. The technique is deterministic and self-contained, and can detect important patterns in text datasets. Future work includes applying the technique to other recurrent architectures and non-NLP applications.",Deep Learning and Machine Learning,,Deep Learning and Machine Learning
10,A review of feature selection methods in medical applications,"Feature selection, medical applications, dimensionality reduction, machine learning, medical imaging, biomedical signal processing, DNA microarray data analysis, Big Data","Feature selection is a preprocessing technique that identifies the key features of a given problem. It has traditionally been applied in a wide range of problems that include biological data processing, finance, and intrusion detection systems. In particular, feature selection has been successfully used in medical applications, where it can not only reduce dimensionality but also help us understand the causes of a disease. We describe some basic concepts related to medical applications and provide some necessary background information on feature selection. We review the most recent feature selection methods developed for and applied in medical problems, covering prolific research fields such as medical imaging, biomedical signal processing, and DNA microarray data analysis. A case study of two medical applications that includes actual patient data is used to demonstrate the suitability of applying feature selection methods in medical problems and to illustrate how these methods work in real-world scenarios.","In this study, we reviewed recent approaches to medical applications of feature selection; we showed that feature selection is a useful preprocessing tool that not only reduces the number of input features, thereby saving on future data collection but also helps practitioners in understanding the underlying causes of certain diseases. We considered three main types of medical applications, namely medical imaging, biomedical signal processing, and DNA microarray data, wherein feature selection methods are commonly used to solve problems associated with these types of medical applications. For each category, we provided brief background information, described early initiatives, listed widely used repositories of medical data, and examined recent studies on applications. We also demonstrated the suitability of applying feature selection in two real-world ophthalmology problems. In one case, feature selection outperformed previous classification results; in the second case, feature selection reduced the computation time required to extract the image features that had previously prevented the real-time use of a computer-aided system. With regard to future scope, it is important not to overlook the emerging Big Data scenario wherein millions of samples and features need to be simultaneously addressed. The state-of-the-art feature selection methods were not developed under that assumption; thus, most of them are unable to handle vast datasets. Therefore, it is imperative to develop more sophisticated feature selection methods (for example, parallel programming) that can tackle Big Data. Another issue to be considered is that real-time feedback is of paramount importance; this suggests that online feature selection methods are required, which is still a challenge for researchers.","A review of feature selection methods in medical applicationsFeature selection, medical applications, dimensionality reduction, machine learning, medical imaging, biomedical signal processing, DNA microarray data analysis, Big DataFeature selection is a preprocessing technique that identifies the key features of a given problem. It has traditionally been applied in a wide range of problems that include biological data processing, finance, and intrusion detection systems. In particular, feature selection has been successfully used in medical applications, where it can not only reduce dimensionality but also help us understand the causes of a disease. We describe some basic concepts related to medical applications and provide some necessary background information on feature selection. We review the most recent feature selection methods developed for and applied in medical problems, covering prolific research fields such as medical imaging, biomedical signal processing, and DNA microarray data analysis. A case study of two medical applications that includes actual patient data is used to demonstrate the suitability of applying feature selection methods in medical problems and to illustrate how these methods work in real-world scenarios.In this study, we reviewed recent approaches to medical applications of feature selection; we showed that feature selection is a useful preprocessing tool that not only reduces the number of input features, thereby saving on future data collection but also helps practitioners in understanding the underlying causes of certain diseases. We considered three main types of medical applications, namely medical imaging, biomedical signal processing, and DNA microarray data, wherein feature selection methods are commonly used to solve problems associated with these types of medical applications. For each category, we provided brief background information, described early initiatives, listed widely used repositories of medical data, and examined recent studies on applications. We also demonstrated the suitability of applying feature selection in two real-world ophthalmology problems. In one case, feature selection outperformed previous classification results; in the second case, feature selection reduced the computation time required to extract the image features that had previously prevented the real-time use of a computer-aided system. With regard to future scope, it is important not to overlook the emerging Big Data scenario wherein millions of samples and features need to be simultaneously addressed. The state-of-the-art feature selection methods were not developed under that assumption; thus, most of them are unable to handle vast datasets. Therefore, it is imperative to develop more sophisticated feature selection methods (for example, parallel programming) that can tackle Big Data. Another issue to be considered is that real-time feedback is of paramount importance; this suggests that online feature selection methods are required, which is still a challenge for researchers.oA < +1 -fEr}- oS @* e ok % < +9} ye @* @ Fig. 4. Illustrative example of feature extraction versus feature selection.Features ” 8 a € 5 oa DNA microarray dataset DNA microarray image Feature selection process Features Samples Fig. 3. Typical feature selection process applied to microarray data. Source Wikimedia Commons.950. 950. z 900. 900. = 850 850. 3 ‘= 800. 800. a 750. 750. 3 = 700. 700. 650. T r 1 650+ T T 1 0 ‘4 2 3 oO 1 2 3 Fig. 2. Example traces of heart rate variability: healthy heart (left), and myocardial infarction (right). Source Wikimedia Commons.nN (a) Fig. 1. Examples of different medical images: (a) chest X-ray [8], (b) colon CT section [9], (c) brain MRI section [10], and (d) retinographic image [11].",Deep Learning and Machine Learning,"This study reviews the recent approaches to the medical applications of feature selection, which is a useful preprocessing tool that reduces the number of input features and helps practitioners in understanding the underlying causes of certain diseases. The study covers three main types of medical applications, including medical imaging, biomedical signal processing, and DNA microarray data analysis, and examines recent studies on applications. The suitability of applying feature selection in two real-world ophthalmology problems is also demonstrated. The study highlights the need for developing more sophisticated feature selection methods to handle Big Data and online feature selection methods to provide real-time feedback, which is still a challenge for researchers.",Medical Data Analysis,"oA < +1 -fEr}- oS @* e ok % < +9} ye @* @ Fig. 4. Illustrative example of feature extraction versus feature selection.Features ” 8 a € 5 oa DNA microarray dataset DNA microarray image Feature selection process Features Samples Fig. 3. Typical feature selection process applied to microarray data. Source Wikimedia Commons.950. 950. z 900. 900. = 850 850. 3 ‘= 800. 800. a 750. 750. 3 = 700. 700. 650. T r 1 650+ T T 1 0 ‘4 2 3 oO 1 2 3 Fig. 2. Example traces of heart rate variability: healthy heart (left), and myocardial infarction (right). Source Wikimedia Commons.nN (a) Fig. 1. Examples of different medical images: (a) chest X-ray [8], (b) colon CT section [9], (c) brain MRI section [10], and (d) retinographic image [11].",Medical Data Analysis
11,A review of the affects of worry and generalized anxiety disorder upon cardiovascular health and coronary heart disease,anxiety disorders; heart diseases; worry; depression; review,"The aims of this review article are to present psychophysiological and behavioral pathways for the involvement of worry and generalized anxiety disorder (GAD) upon cardiovascular function. The review will focus on persons with and without coronary heart disease (CHD), and encompass etiological and prognostic studies. Articles (1975–2011) reporting on GAD or worry affecting CHD prognosis or cardiovascular function were found using MEDLINE, EMBASE, SCOPUS and PsychINFO database searches, and extracted to form a narrative review. Results: Available evidence in experimental and observational studies in CHD free samples consistently showed that worry was associated with diminished heart rate variability (HRV) and elevated heart rate. Worry and GAD were commonly associated with blood pressure and diagnosed hypertension or medication use in both disease-free and established CHD populations. No evidence was found to support worry being beneficial to cardiovascular function or conducive to health promoting behaviors. The literature indicated that measures of worry were associated with fatal and nonfatal CHD in seven etiological studies of initially disease-free individuals; however, females were underrepresented. Three studies reported that GAD was associated with poorer prognosis in establish CHD, independent of depression. The median GAD prevalence was 10.4% in 3266 patients across 15 studies, suggesting that GAD is marginally less common in CHD samples than is depression.","A growing literature highlights the association between worry and development of CHD. The association between worry, GAD and CHD risk factors (e.g. blood pressure), and HRV are leading mechanisms of cardiopathogenesis that may affect cardiovascular function. Findings regarding worry and GAD in established CHD are less clear. conclusion : A growing body of literature documents the role of GAD and worry in CHD and cardiovascular risk. In established CHD, GAD increased the risk for major cardiac events in three data-sets (Frasure-Smith & Lesperance, 2008; Martens et al., 2010; Tully et al., 2011) and lowered risk in one data-set (Parker et al., 2011). Importantly, there was insufficient evidence to suggest that GAD is merely a marker for other known CVD mechanisms in persons with established CHD. With respect to etiological follow up of initially disease-free individuals, data showed that worry was associated with subsequent CHD and fatal and nonfatal cardiovascular outcomes (Hamer et al., 2011; Holman et al., 2008; Kawachi et al., 1994; Kubzansky et al., 1997; Phillips et al., 2009; Scherrer et al., 2010; Vogt et al., 1994). There was insufficient evidence to suggest a dose–response relationship whereby GAD might exert greater cardiotoxic effects than worry. Limitations of interpreting the literature to date include that findings are predominantly among male samples and generalization to females is dually problematic, for GAD appears to disproportionately affect females (Andrews et al., 2010). The most prominent biological mechanisms of cardiopathogenesis in worry were diminished HRV in nonCHD samples, a finding yet to be consistently demonstrated in persons with documented CHD. Clarification of the impact of worry and GAD upon cardiovascular function in CHD will be informative to the design of psychological treatment strategies and to rule out possible adverse cardiotoxic effects. For example, in vivo exposure to worry imagery, an accepted psychotherapeutic treatment modality for GAD in nonCHD patients (Borkovec & Ruscio, 2001), is potentially associated with greater autonomic response and change in HRV that may have adverse cardiovascular implications for a person with GAD and comorbid CHD. Associations between GAD and CHD risk factors such as smoking, excess alcohol use, and hypertension belie the suggestion that excessive and uncontrollable worry (e.g. clinical GAD) might be beneficial to promote cardiovascular health. In fact, worry was reportedly not related to CVD reducing behavior changes (e.g. physical activity). Given that GAD research has lagged behind other anxiety disorder research in the past decade (Dugas, Anderson, Deschenes, & Donegan, 2010), the recommendation for further GAD research in CHD cannot be understated. A likely explanation for the limited GAD research in CHD samples is that studies, published only since 2008, were preceded by an extensive literature documenting the depression-CHD association. Frasure-Smith and Lespérance (2008), whom led the way in depression research post-MI, concluded that the effects of depression and anxiety on CHD might promote research into common genetic, environmental, and pathophysiological pathways and treatments. Broader cognitive processes, such as rumination, observed across a range of affective disorders may underlie associations with CHD (Larsen & Christenfield, 2009) and medical illnesses generally (Brosschot, Gerin, & Thayer, 2006).","A review of the affects of worry and generalized anxiety disorder upon cardiovascular health and coronary heart diseaseanxiety disorders; heart diseases; worry; depression; reviewThe aims of this review article are to present psychophysiological and behavioral pathways for the involvement of worry and generalized anxiety disorder (GAD) upon cardiovascular function. The review will focus on persons with and without coronary heart disease (CHD), and encompass etiological and prognostic studies. Articles (1975–2011) reporting on GAD or worry affecting CHD prognosis or cardiovascular function were found using MEDLINE, EMBASE, SCOPUS and PsychINFO database searches, and extracted to form a narrative review. Results: Available evidence in experimental and observational studies in CHD free samples consistently showed that worry was associated with diminished heart rate variability (HRV) and elevated heart rate. Worry and GAD were commonly associated with blood pressure and diagnosed hypertension or medication use in both disease-free and established CHD populations. No evidence was found to support worry being beneficial to cardiovascular function or conducive to health promoting behaviors. The literature indicated that measures of worry were associated with fatal and nonfatal CHD in seven etiological studies of initially disease-free individuals; however, females were underrepresented. Three studies reported that GAD was associated with poorer prognosis in establish CHD, independent of depression. The median GAD prevalence was 10.4% in 3266 patients across 15 studies, suggesting that GAD is marginally less common in CHD samples than is depression.A growing literature highlights the association between worry and development of CHD. The association between worry, GAD and CHD risk factors (e.g. blood pressure), and HRV are leading mechanisms of cardiopathogenesis that may affect cardiovascular function. Findings regarding worry and GAD in established CHD are less clear. conclusion : A growing body of literature documents the role of GAD and worry in CHD and cardiovascular risk. In established CHD, GAD increased the risk for major cardiac events in three data-sets (Frasure-Smith & Lesperance, 2008; Martens et al., 2010; Tully et al., 2011) and lowered risk in one data-set (Parker et al., 2011). Importantly, there was insufficient evidence to suggest that GAD is merely a marker for other known CVD mechanisms in persons with established CHD. With respect to etiological follow up of initially disease-free individuals, data showed that worry was associated with subsequent CHD and fatal and nonfatal cardiovascular outcomes (Hamer et al., 2011; Holman et al., 2008; Kawachi et al., 1994; Kubzansky et al., 1997; Phillips et al., 2009; Scherrer et al., 2010; Vogt et al., 1994). There was insufficient evidence to suggest a dose–response relationship whereby GAD might exert greater cardiotoxic effects than worry. Limitations of interpreting the literature to date include that findings are predominantly among male samples and generalization to females is dually problematic, for GAD appears to disproportionately affect females (Andrews et al., 2010). The most prominent biological mechanisms of cardiopathogenesis in worry were diminished HRV in nonCHD samples, a finding yet to be consistently demonstrated in persons with documented CHD. Clarification of the impact of worry and GAD upon cardiovascular function in CHD will be informative to the design of psychological treatment strategies and to rule out possible adverse cardiotoxic effects. For example, in vivo exposure to worry imagery, an accepted psychotherapeutic treatment modality for GAD in nonCHD patients (Borkovec & Ruscio, 2001), is potentially associated with greater autonomic response and change in HRV that may have adverse cardiovascular implications for a person with GAD and comorbid CHD. Associations between GAD and CHD risk factors such as smoking, excess alcohol use, and hypertension belie the suggestion that excessive and uncontrollable worry (e.g. clinical GAD) might be beneficial to promote cardiovascular health. In fact, worry was reportedly not related to CVD reducing behavior changes (e.g. physical activity). Given that GAD research has lagged behind other anxiety disorder research in the past decade (Dugas, Anderson, Deschenes, & Donegan, 2010), the recommendation for further GAD research in CHD cannot be understated. A likely explanation for the limited GAD research in CHD samples is that studies, published only since 2008, were preceded by an extensive literature documenting the depression-CHD association. Frasure-Smith and Lespérance (2008), whom led the way in depression research post-MI, concluded that the effects of depression and anxiety on CHD might promote research into common genetic, environmental, and pathophysiological pathways and treatments. Broader cognitive processes, such as rumination, observed across a range of affective disorders may underlie associations with CHD (Larsen & Christenfield, 2009) and medical illnesses generally (Brosschot, Gerin, & Thayer, 2006).673 articles retrieved from primary literature search {Cs} «505 articles were excluded after a review of abstracts v 168 articles subjected to full text review + 7 identified by manual searching 114 articles were excluded >| based on criteria of quality and salience v 61 articles retained for this review Figure 1. Study inclusion flow chart.",Medical Data Analysis,"This review article examines the relationship between worry, generalized anxiety disorder (GAD), and cardiovascular function in both disease-free individuals and those with coronary heart disease (CHD). The study draws upon experimental and observational studies and investigates etiological and prognostic factors. The evidence suggests that worry and GAD are associated with diminished heart rate variability (HRV), elevated heart rate, blood pressure, diagnosed hypertension, and medication use. Measures of worry are also linked to fatal and nonfatal CHD. However, the association between worry, GAD, and established CHD is less clear. The research highlights the need for further investigation into the effects of GAD on CHD and cardiovascular risk, particularly as it disproportionately affects females. The study recommends greater attention to GAD research in CHD and its underlying cognitive processes.",Medical Data Analysis,673 articles retrieved from primary literature search {Cs} «505 articles were excluded after a review of abstracts v 168 articles subjected to full text review + 7 identified by manual searching 114 articles were excluded >| based on criteria of quality and salience v 61 articles retained for this review Figure 1. Study inclusion flow chart.,Medical Data Analysis
12,Bedside Screening to Detect Oropharyngeal Dysphagia in Patients with Neurological Disorders: An Updated Systematic Review,"Bedside screening , Videofluoroscopy , Fiberoptic endoscopy , Psychometrics , Deglutition , Deglutition disorders.","Oropharyngeal dysphagia is a highly prevalent comorbidity in neurological patients and presents a serious health threat, which may le to outcomes of aspiration pneumonia ranging from hospitalization to death. Therefore, an early identification of risk followed by an accurate diagnosis of oropharyngeal dysphagia is fundamental. This systematic review provides an update of currently available bedside screenings to identify oropharyngeal dysphagia in neurological patients. An electronic search was carried out in the databases PubMed, Embase, CINAHL, and PsychInfo (formerly PsychLit), and all hits from 2008 up to December 2012 were included in the review. Only studies with sufficient methodological quality were considered, after which the psychometric characteristics of the screening tools were determined. Two relevant bedside screenings were identified, with a minimum sensitivity and specificity of C70 and C60 %, respectively.","New bedside screening tools have recently become available. To determine which type of bedside screening is most suitable to use, issues like methodological study quality, the psychometric screening characteristics, and workplace related criteria need to be addressed. Therefore, no single bedside screening can be regarded as superior unless all the above-mentioned criteria are taken into account. ","Bedside Screening to Detect Oropharyngeal Dysphagia in Patients with Neurological Disorders: An Updated Systematic ReviewBedside screening , Videofluoroscopy , Fiberoptic endoscopy , Psychometrics , Deglutition , Deglutition disorders.Oropharyngeal dysphagia is a highly prevalent comorbidity in neurological patients and presents a serious health threat, which may le to outcomes of aspiration pneumonia ranging from hospitalization to death. Therefore, an early identification of risk followed by an accurate diagnosis of oropharyngeal dysphagia is fundamental. This systematic review provides an update of currently available bedside screenings to identify oropharyngeal dysphagia in neurological patients. An electronic search was carried out in the databases PubMed, Embase, CINAHL, and PsychInfo (formerly PsychLit), and all hits from 2008 up to December 2012 were included in the review. Only studies with sufficient methodological quality were considered, after which the psychometric characteristics of the screening tools were determined. Two relevant bedside screenings were identified, with a minimum sensitivity and specificity of C70 and C60 %, respectively.New bedside screening tools have recently become available. To determine which type of bedside screening is most suitable to use, issues like methodological study quality, the psychometric screening characteristics, and workplace related criteria need to be addressed. Therefore, no single bedside screening can be regarded as superior unless all the above-mentioned criteria are taken into account. ",Medical Data Analysis,"This systematic review discusses the prevalence of oropharyngeal dysphagia in neurological patients and its serious health threats, including aspiration pneumonia. Early identification and accurate diagnosis of this condition are crucial. The review focuses on available bedside screenings for oropharyngeal dysphagia in neurological patients and identifies two relevant screenings with minimum sensitivity and specificity of 70% and 60%, respectively. The review stresses the importance of considering methodological study quality, psychometric screening characteristics, and workplace-related criteria to determine the most suitable screening tool. No single bedside screening can be considered superior without taking all these factors into account.",Medical Data Analysis,,Medical Data Analysis
13,"Birth prevalence of congenital heart disease in China, 1980–2019: a systematic review and meta‑analysis of 617 studies","Congenital heart diseases , Prevalence , Chinese , Spatial analysis , Meta-analysis , Systematic review","To assess the birth prevalence and spatial distribution of congenital heart disease (CHD) in China by conducting a complete overview and using spatial epidemiological methods. Unrestricted searches were conducted on seven electronic databases, with an end-date parameter of May 2019. Data on the birth prevalence of CHD and its subtypes were collected and combined using either the random-effect model or fixed-effect model. Subgroup sensitivity analyses were performed to explore potential heterogeneity moderators. The three-dimensional trend analysis and a visualization of CHD birth prevalence among different provinces were performed to describe the spatial distribution characteristics. Total 617 studies involving 76,961,354 births and 201,934 CHD individuals were included. Overall, total CHD birth prevalence increased continuously over time, from 0.201‰ in 1980–1984 to 4.905‰ in 2015–2019. The study on the high-income provinces, population-based monitoring model, male births, and urban regions reported a significantly higher prevalence of total CHD compared with upper-middle-income provinces, hospital-based monitoring model, female births, and rural regions, respectively. National CHD birth prevalence increased gradually from western region to eastern region, but decreased gradually from southern to northern region. Relevant heterogeneity moderators including gender, geographic region, income levels, and monitoring models have been identified by subgroup analyses. Sensitivity analysis yielded consistent results. Total CHD birth prevalence in China increases continuously in the past 40 years. Significant diffrences in gender, geographical regions, income levels, and monitoring models were found. In the future, population wide prospective birth defect registries covering the entire Chinese population need to determine the exact birth prevalence.","Total CHD birth prevalence in China increases continuously in the past forty years. Significant differences in gender, geographical regions, income levels, and monitoring models were found for birth prevalence of CHD. It remains uncertain whether detected diferences in birth prevalence of CHD represent true or merely methodological diferences. In the future, population wide prospective birth defect registries covering the entire Chinese population need to determine the exact birth prevalence.","Birth prevalence of congenital heart disease in China, 1980–2019: a systematic review and meta‑analysis of 617 studiesCongenital heart diseases , Prevalence , Chinese , Spatial analysis , Meta-analysis , Systematic reviewTo assess the birth prevalence and spatial distribution of congenital heart disease (CHD) in China by conducting a complete overview and using spatial epidemiological methods. Unrestricted searches were conducted on seven electronic databases, with an end-date parameter of May 2019. Data on the birth prevalence of CHD and its subtypes were collected and combined using either the random-effect model or fixed-effect model. Subgroup sensitivity analyses were performed to explore potential heterogeneity moderators. The three-dimensional trend analysis and a visualization of CHD birth prevalence among different provinces were performed to describe the spatial distribution characteristics. Total 617 studies involving 76,961,354 births and 201,934 CHD individuals were included. Overall, total CHD birth prevalence increased continuously over time, from 0.201‰ in 1980–1984 to 4.905‰ in 2015–2019. The study on the high-income provinces, population-based monitoring model, male births, and urban regions reported a significantly higher prevalence of total CHD compared with upper-middle-income provinces, hospital-based monitoring model, female births, and rural regions, respectively. National CHD birth prevalence increased gradually from western region to eastern region, but decreased gradually from southern to northern region. Relevant heterogeneity moderators including gender, geographic region, income levels, and monitoring models have been identified by subgroup analyses. Sensitivity analysis yielded consistent results. Total CHD birth prevalence in China increases continuously in the past 40 years. Significant diffrences in gender, geographical regions, income levels, and monitoring models were found. In the future, population wide prospective birth defect registries covering the entire Chinese population need to determine the exact birth prevalence.Total CHD birth prevalence in China increases continuously in the past forty years. Significant differences in gender, geographical regions, income levels, and monitoring models were found for birth prevalence of CHD. It remains uncertain whether detected diferences in birth prevalence of CHD represent true or merely methodological diferences. In the future, population wide prospective birth defect registries covering the entire Chinese population need to determine the exact birth prevalence.35 —e=VSD 2 —eASD 2 3 ——PDA s —ps 82s -+ TOF ~ meme TGA & 2 g ais ao & Ss 1 g 50S é Eo rs Ss se & s = se = SF SF KF FS SF SF Fig.6 Birth prevalence of six specific subtypes of CHD over time in China> » Ss oO » S <== Cyanotic lesions ee Mild lesions . 9 mee Acyanotic lesions , w 2 mem Severe lesions ‘ 2 Es , = ° = 2 ° g7 3’ s a = 6 ~ 6 a Q 2, 5 5 3 6 6 S 4 So 4 g 2 § 3 33 $ 2 ze ? g 2 a a 1 0 Se Re Fig.7 Birth prevalence of mild lesions, severe lesions, cyanotic lesions, and acyanotic lesions over time in China. The solid line is the estimated birth prevalence, and dotted lines represent the 95% confidence intervalFig.4 Total CHD birth preva- Weight lence per province in China Geographic region Proportion [95%_CI] (casas) China, Anhui (18 studies) 0.864 [0.662, 1.065 — 2.9% China, Beijing (38 studies) 5.648 (4.949, 6.347 — 5.0% China, Chongqing (6 studies) 1.617 (0.978, 2.256 Jig 0.9% China, Fujian (17 studies) 2.261 [1.767, 2.756 —— 2.6% China, Gansu (23 studies) 1.250 (0.863, 1.637 o 3.6% China, Guangdong (75 studies) 3.892 [3.300, 4.483 —s 11.5% China, Guangxi (37 studies) 1.945 (1.539, 2.351 a 5.1% China, Guizhou (13 studies) 1.236 [0.883, 1.590) 2 2.1% China, Hainan (3 studies) 1.652 [0.486, 2.818 <_—__.+—_- 0.5% China, Hebei (15 studies) 0.806 (0.533, 1.079 ff 24% China, Heilongjiang (7 studies) 0.579 [0.354, 0.805 = 1.1% China, Henan (25 studies) 0.994 [0.789, 1.198) ss 4.0% China, Hongkong (1 study) 6.355 [$.279, 7.432 = 0.19% China, Hubei (25 studies) 1.115 [0.851, 1.379) — 3.9% China, Hunan (21 studies) 3.128 (2.077, 4.179 _—e 3.3% China, Jiangsu (49 studies) 1.085 [0.917, 1.252 so 7.9% China, Jiangxi (7 studies) 1.693 (0.976, 2.411 —<$$—— 1.1% China, Jilin (18 studies) 1.482 [1.166, 1.798 — 2.9% China, Liaoning (25 studies) 1.652 (1.284, 2.021 —— 3.9% China, Neimenggu (8 studies) 1.266 [0.228, 2.304) <{_————_—s—__ 1.3% China, Ningxia (12 studies) 1.781 (1.260, 2.302 —— 1.9% China, Qinghai (2 studies) 0.220 (0.000, 0.574) <= 0.3% China, Shandong (40 studies) 1.251 (1.091, 1.412 —f 6.3% China, Shanghai (13 studies) 2.565 [1.965, 3.166 a 2.0% China, Shanxi (8 studies) 2.585 (1.820, 3.350 —— 1.3% China, Shanxi (20 studies) 1.132 (0.892, 1.373 —1— 3.2% China, Sichuan (14 studies) 0.952 (0.574. 1.331 —— 2.2% China, Tianjin (3 studies) 3.270 [1.490, 5.050 at 0.5%, China, Xinjiang (16 studies) 0.739 [0.576, 0.902 —i- 2.5% China, Xizang (2 studies) 0.106 [0.000, 0.256) < 0.3% China, Yunnan (13 studies) 1.846 (1.140, 2.552 . 2.1% China, Zhejiang (71 studies) $.977 (5.172. 6.781 = 0.4% Random effects model 2.502 (2.397, 2.607] 4 100.0% Heterogeneity: P=100%, 1? < 0.0001, P=0 050 071 10 141 20 6.0Fig.5 Three-dimensional trend analysis of total CHD birth preva- lence in ChinaFig. 3 Geographical distribution of total CHD birth prevalence at the province level in China based on the average level dur- ing 1980-2019 Prevalence of CHD(1/1000) L 0.000 - 0.220 B0.221 - 0.994 0.995 - 1.482 1.483 - 7.565 we 2.566 - 3.892 ™ 3.893 6.355Prevalence of CHD / 1000 births wo o oe > 9 r FF PF FS SF SF US SF SF K SF SK SK Fig. 2 Total CHD birth prevalence over time in China. The solid line is the estimated birth prevalence, and dotted lines represent the 95% confidence intervalRecords identified through database searching (n=6,819) Additional records identified through other sources (n=5) Records screened (n=4,551) Full-text articles assessed for eligibility (n=985) Studies included in qualitative synthesis (n=617) Studies included in qualitative synthesis (meta-analysis) (n=617) Fig. 1 Flow chart of study identification and selection Records after duplicates removed Records excluded (n=3,566) Review or comment papers (n=46) Unrelated to the topics (n=3,477) Population is not Chinese (n=43) Full-text studies excluded (n=368) Not include CHD (n=237) From the same database (n=63) Data could not be extracted (n=41) Population is not Chinese (n=3) Quality score<4 (n=24)",Medical Data Analysis,"This study aimed to assess the birth prevalence and spatial distribution of congenital heart disease (CHD) in China using spatial epidemiological methods. The researchers conducted unrestricted searches on seven electronic databases, collected data on the birth prevalence of CHD and its subtypes, and performed subgroup sensitivity analyses. The study found that the total CHD birth prevalence in China had increased continuously over the past 40 years, with significant differences in gender, geographic regions, income levels, and monitoring models. The researchers suggested the need for population-wide prospective birth defect registries covering the entire Chinese population to determine the exact birth prevalence of CHD.",Medical Data Analysis,"35 —e=VSD 2 —eASD 2 3 ——PDA s —ps 82s -+ TOF ~ meme TGA & 2 g ais ao & Ss 1 g 50S é Eo rs Ss se & s = se = SF SF KF FS SF SF Fig.6 Birth prevalence of six specific subtypes of CHD over time in China> » Ss oO » S <== Cyanotic lesions ee Mild lesions . 9 mee Acyanotic lesions , w 2 mem Severe lesions ‘ 2 Es , = ° = 2 ° g7 3’ s a = 6 ~ 6 a Q 2, 5 5 3 6 6 S 4 So 4 g 2 § 3 33 $ 2 ze ? g 2 a a 1 0 Se Re Fig.7 Birth prevalence of mild lesions, severe lesions, cyanotic lesions, and acyanotic lesions over time in China. The solid line is the estimated birth prevalence, and dotted lines represent the 95% confidence intervalFig.4 Total CHD birth preva- Weight lence per province in China Geographic region Proportion [95%_CI] (casas) China, Anhui (18 studies) 0.864 [0.662, 1.065 — 2.9% China, Beijing (38 studies) 5.648 (4.949, 6.347 — 5.0% China, Chongqing (6 studies) 1.617 (0.978, 2.256 Jig 0.9% China, Fujian (17 studies) 2.261 [1.767, 2.756 —— 2.6% China, Gansu (23 studies) 1.250 (0.863, 1.637 o 3.6% China, Guangdong (75 studies) 3.892 [3.300, 4.483 —s 11.5% China, Guangxi (37 studies) 1.945 (1.539, 2.351 a 5.1% China, Guizhou (13 studies) 1.236 [0.883, 1.590) 2 2.1% China, Hainan (3 studies) 1.652 [0.486, 2.818 <_—__.+—_- 0.5% China, Hebei (15 studies) 0.806 (0.533, 1.079 ff 24% China, Heilongjiang (7 studies) 0.579 [0.354, 0.805 = 1.1% China, Henan (25 studies) 0.994 [0.789, 1.198) ss 4.0% China, Hongkong (1 study) 6.355 [$.279, 7.432 = 0.19% China, Hubei (25 studies) 1.115 [0.851, 1.379) — 3.9% China, Hunan (21 studies) 3.128 (2.077, 4.179 _—e 3.3% China, Jiangsu (49 studies) 1.085 [0.917, 1.252 so 7.9% China, Jiangxi (7 studies) 1.693 (0.976, 2.411 —<$$—— 1.1% China, Jilin (18 studies) 1.482 [1.166, 1.798 — 2.9% China, Liaoning (25 studies) 1.652 (1.284, 2.021 —— 3.9% China, Neimenggu (8 studies) 1.266 [0.228, 2.304) <{_————_—s—__ 1.3% China, Ningxia (12 studies) 1.781 (1.260, 2.302 —— 1.9% China, Qinghai (2 studies) 0.220 (0.000, 0.574) <= 0.3% China, Shandong (40 studies) 1.251 (1.091, 1.412 —f 6.3% China, Shanghai (13 studies) 2.565 [1.965, 3.166 a 2.0% China, Shanxi (8 studies) 2.585 (1.820, 3.350 —— 1.3% China, Shanxi (20 studies) 1.132 (0.892, 1.373 —1— 3.2% China, Sichuan (14 studies) 0.952 (0.574. 1.331 —— 2.2% China, Tianjin (3 studies) 3.270 [1.490, 5.050 at 0.5%, China, Xinjiang (16 studies) 0.739 [0.576, 0.902 —i- 2.5% China, Xizang (2 studies) 0.106 [0.000, 0.256) < 0.3% China, Yunnan (13 studies) 1.846 (1.140, 2.552 . 2.1% China, Zhejiang (71 studies) $.977 (5.172. 6.781 = 0.4% Random effects model 2.502 (2.397, 2.607] 4 100.0% Heterogeneity: P=100%, 1? < 0.0001, P=0 050 071 10 141 20 6.0Fig.5 Three-dimensional trend analysis of total CHD birth preva- lence in ChinaFig. 3 Geographical distribution of total CHD birth prevalence at the province level in China based on the average level dur- ing 1980-2019 Prevalence of CHD(1/1000) L 0.000 - 0.220 B0.221 - 0.994 0.995 - 1.482 1.483 - 7.565 we 2.566 - 3.892 ™ 3.893 6.355Prevalence of CHD / 1000 births wo o oe > 9 r FF PF FS SF SF US SF SF K SF SK SK Fig. 2 Total CHD birth prevalence over time in China. The solid line is the estimated birth prevalence, and dotted lines represent the 95% confidence intervalRecords identified through database searching (n=6,819) Additional records identified through other sources (n=5) Records screened (n=4,551) Full-text articles assessed for eligibility (n=985) Studies included in qualitative synthesis (n=617) Studies included in qualitative synthesis (meta-analysis) (n=617) Fig. 1 Flow chart of study identification and selection Records after duplicates removed Records excluded (n=3,566) Review or comment papers (n=46) Unrelated to the topics (n=3,477) Population is not Chinese (n=43) Full-text studies excluded (n=368) Not include CHD (n=237) From the same database (n=63) Data could not be extracted (n=41) Population is not Chinese (n=3) Quality score<4 (n=24)",Medical Data Analysis
14,Cardiovascular Disease and Risk Factors in Asia A Selected Review,"Cardiovascular disease prevention, Asia, stroke, coronary heart disease, risk factors, hypertension, salt intake, smoking.","Cardiovascular disease (CVD) prevention in Asia is an important issue for world health, because half of the world’s population lives in Asia. Asian countries and regions such as Japan, the Republic of Korea, the People’s Republic of China, Hong Kong, Taiwan, and the Kingdom of Thailand have greater mortality and morbidity from stroke than from coronary heart disease (CHD), whereas the opposite is true in Western countries.1 The reasons why this specific situation is observed in countries with rapid and early-phase westernization, such as Japan and South Korea, are very interesting. The Seven Countries Study conducted by Keys et al2 in 1957 found that Japanese populations had lower fat intake, lower serum total cholesterol, and lower CHD than populations in the United States and Scandinavia, in spite of higher smoking rates. The serum total cholesterol level in Japan has increased rapidly since World War II in accordance with an increase in dietary fat intake from 10% of total energy intake per capita per day to 25%.1,2 Despite this increase, the specific characteristic of lower CHD incidence and mortality than that in Western countries has persisted.3,4 Whether Japanese people and certain other Asian populations have different risk factors for CHD than Western populations has been a subject of discussion for quite some time.","In Asian countries, stroke is more prominent than CHD. This is most likely due to a higher prevalence of hypertension and a lower level of serum total cholesterol in Asian countries. The population-attributable fraction of hypertension for CVD is as high as 60% in Asian countries. High blood pressure accompanies high salt intake in East Asia, whereas low serum total cholesterol accompanies lower fat intake. Reduction in salt consumption in East Asian countries, including Japan, is important for the reduction of CVD, especially stroke. Prevention of smoking is also an important strategy for reducing CVD in most Asian countries, especially for men. The population-attributable fraction of smoking for CVD is 30%. Recent westernization in Asian countries has increased fat consumption, which has been followed by an increase in serum total cholesterol. This may have caused the increase in CHD in Asian countries. The prevalence of obesity is also increasing, and this may also increase the prevalence of DM, glucose intolerance, and the metabolic syndrome. Management of these traditional risk factors for CVD is important for the prevention of CVD in Asian and Western countries.","Cardiovascular Disease and Risk Factors in Asia A Selected ReviewCardiovascular disease prevention, Asia, stroke, coronary heart disease, risk factors, hypertension, salt intake, smoking.Cardiovascular disease (CVD) prevention in Asia is an important issue for world health, because half of the world’s population lives in Asia. Asian countries and regions such as Japan, the Republic of Korea, the People’s Republic of China, Hong Kong, Taiwan, and the Kingdom of Thailand have greater mortality and morbidity from stroke than from coronary heart disease (CHD), whereas the opposite is true in Western countries.1 The reasons why this specific situation is observed in countries with rapid and early-phase westernization, such as Japan and South Korea, are very interesting. The Seven Countries Study conducted by Keys et al2 in 1957 found that Japanese populations had lower fat intake, lower serum total cholesterol, and lower CHD than populations in the United States and Scandinavia, in spite of higher smoking rates. The serum total cholesterol level in Japan has increased rapidly since World War II in accordance with an increase in dietary fat intake from 10% of total energy intake per capita per day to 25%.1,2 Despite this increase, the specific characteristic of lower CHD incidence and mortality than that in Western countries has persisted.3,4 Whether Japanese people and certain other Asian populations have different risk factors for CHD than Western populations has been a subject of discussion for quite some time.In Asian countries, stroke is more prominent than CHD. This is most likely due to a higher prevalence of hypertension and a lower level of serum total cholesterol in Asian countries. The population-attributable fraction of hypertension for CVD is as high as 60% in Asian countries. High blood pressure accompanies high salt intake in East Asia, whereas low serum total cholesterol accompanies lower fat intake. Reduction in salt consumption in East Asian countries, including Japan, is important for the reduction of CVD, especially stroke. Prevention of smoking is also an important strategy for reducing CVD in most Asian countries, especially for men. The population-attributable fraction of smoking for CVD is 30%. Recent westernization in Asian countries has increased fat consumption, which has been followed by an increase in serum total cholesterol. This may have caused the increase in CHD in Asian countries. The prevalence of obesity is also increasing, and this may also increase the prevalence of DM, glucose intolerance, and the metabolic syndrome. Management of these traditional risk factors for CVD is important for the prevention of CVD in Asian and Western countries.6.20 5.69 S17 4.66 se AE fusgos inf China’ Bejing residents 1902-1984 Cchina:Urbant 903-1 nina: Reding formerstoea. tao CChina:Urbant803-1808 Wi ea | \ cae Rte 10s Cchina:muratses-i00s ® Koreat900 150 | 4 china: Jiangsu farmerst982-1004 (mmol) 30 40 50 60 70 0 isteae Figure 5. Serum total cholesterol levels (Serum T-CHOL) for men among Asian populations in 1980 to 2000 and in the United States in 1960 to 1991. Serum total cholesterol in Singa- pore was the highest among the selected Asian populations, followed by Hong Kong and Japan. That of Japanese people increased from 1980 to 2000, and currently, young people in Japan have levels similar to their American counterparts; how- ever, there remains a 20-mg/dL difference in serum total choles- terol levels between elderly Japanese and Americans. The levels of Taiwanese and Korean populations were intermediate between those of the Japanese and Chinese. Chinese and some Korean populations have lower serum total cholesterol levels than other industrialized Asian populations. Reprinted with permission from Oxford University Press.*(%) Smoking rate 80 A Men 4 c Vietnay Japan Korea m © Wore 70 60 a. Singapore Hong Kong 30 20 Figure 4. Trends in smoking rate by sex and by selected Asian countries. The smoking rate for women in Asian countries was lower (<10%) than that for men. Among Asian countries, Singapore had the lowest smoking rate for men and women, with a declining smoking rate for men. The smoking rate for men in Japan, Korea, China, and Vietnam remains high (over 45%), although declining trends are clear. Data were obtained from previous studies.48.50-54FIN-KUO RUS-NOL FIN-NKA FIN-KUO RUS-NOIL LTU-KAU LTU-KAU CHN-BHL FIN-TUL FIN-NKA YUG-NOS JPN-SHIGA JPN-HOKKAIDO JPN-HOKKAIDO JPN-SHIGA SWENSW CHN-BH YUG-NOS SWENSW FIN-TUL RUS-MOC JPN-AKITA JPN-AKITA RUS-MOC RUS-MOI JPN-NAGANO JPN-OKINAWA. JPN-OKINAWA DEN-GLO GER-KMS JPN-NAGANO RUS-MOI GER-KMS DEN-GLO GER-HAC GER-HAC POL-WAR JPN-OSAKA JPN-OSAKA SWE-GOT SWE-GOT POL-WAR GER-RDM ITA-FRI GER-RHN GER-RDM ITA-FRI GER-RHN 0 50 100 150-200 250 ©3000 (Stroke Incidence Rate, per 100000/year) Figure 2. Age-adjusted (35 to 64 years old) stroke incidence from a MONICA study in 1985 to 1987 and a Japanese study ((]) in 1980 to 1993. The diagnostic criteria of the MONICA study were used for the Japanese study. Male stroke incidence (left) for the 6 Japanese populations and 1 Chinese population (in Beijing, China) showed that rates were intermediate for these populations and much lower than that of Finland. The female Beijing population was in the higher group (right). The world standard population was used for the cal- culation of age-adjusted rates for 35- to 64-year-olds. Reprinted with permission from the Japan Atherosclerosis Society.1 FIN-KUO indicates Kuopio Province, Finland; FIN-NKA, North Karelia, Finland; RUS-NOI, Novosibirsk Intervention, Russia; LTU-KAU, Kaunas, Lithuania; FIN-TUL, Turku/Loimaa, Finland; YUG-NOS, Novi Sad, Yugoslavia; JPN-HOKKAIDO, Hokkaido, Japan; JPN-SHIGA, Shiga, Japan; CHN-BEI, Beijing, China; SWE-NSW, Northern Sweden, Sweden; RUS-MOC, Moscow Control, Russia; JPN-AKITA, Akita, Japan; RUS-MOI, Moscow Intervention, Russia; JPN-OKINAWA, Okinawa, Japan; DEN-GLO, Glostrup, Denmark; JPN-NAGANO, Nagano, Japan; GER-KMS, Karl-Marx-Stadt County, Germany; GER-HAC, Halle County, Germany; POL-WAR, Warsaw, Poland; JPN- OSAKA, Osaka, Japan; SWE-GOT, Gothenburg, Sweden; GER-RDM, Rest of DDR (East Germany)-MONICA; GER-RHN, Rhein-Neckar Region, Germany; and ITA-FRI, Friuli, Italy.Saudi Arabia Saudi Arabia UT UAE vAE En Oman oman eet Jordan Jordan Toe Tar Uzbekistan Uzbekistan Tajikistan Tajikistan Kazakhstan Kazakhstan Iran Iran Pakistan Pakistan Bangladesh Bangladesh India India Malaysia Malaysia Indonesia Stroke Indonesia CHD Thailand Thailand Singapore Singapore Korea Korea Japan Japan China China USA USA UK UK Spain Spain 1 New Zealand NewZealand France France [— Australia Australia /-—-— 0 50 100 «150 200 «250 «300 «350 «400-450 A a ee ee ee ee ee ee ee (Stroke Death Rate per 100 000 Person-year) (CHD Death Rate per 100 000 Person-year) Figure 1. Age-standardized death rates per 100 000 for stroke (left) and CHD (right) across countries in different regions of Asia in 2002. Data from the World Health Organization, Department of Measurement and Health Information. Patterns on the bars represent different regions: Middle Eastern countries ({), Central Asian countries (§), South Asian countries (™), Southeast Asian countries (Z), East Asian countries (1), and non-Asian countries (open bars). UAE indicates United Arab Emirates; USA, United States of Americ: and UK, United Kingdom.FIN-KUO UNCREL, UNK-BEL CAN-HAL RoEwar SwENeer DEN-GLO FOL Wall USA-STA ICBICE DEN-GLO USA-STA ICEICE YUG-NOS SWEGOT AUS-PER BEL-GHE GER-EGE swreTic FRA-LIL TTA-FRI SWLVAF SPA-CAT CHN-BEL CHN-BEL SPA-CAT JPN-HOKKAIDO JPN-OKINAW A JPN-OKINAW A JPN-OSAKA JPN-SHIGA JPN-HOKKAIDO JPN-OSAKA JPN-NAGANO JPN-NAGANO JPN-SHIGA JPN-AKITA JPN-AKITA fs | iy 0 150 300 450 600 750 900 0 150 300 450 600 750 900 (Acute Myocardial Infarction, Incidence per 100000/year) Figure 3. Age-adjusted (35 to 64 years old) incidence of acute myocardial infarction from a MONICA study in 1985 to 1987 and a Japanese study in (MM) 1989 to 1993. The incidence rates of 6 Japanese populations and a single Chinese population were far lower than those of selected Western countries. The world standard population was used for the calculation of age-adjusted rates for 35- to 64-year-olds. Reprinted with permission from the Japan Atherosclerosis Society.1 UNK-BEL indicates Belfast, United Kingdom; CAN-HAL, Halifax County, Canada; ICE-ICE, Iceland; USA-STA, Stanford (California), United States; CZE-CZE, Czech Republic; NEZ-AUC, Auckland, New Zealand; AUS-PER, Perth, Australia; BEL-GHE, Ghent, Belgium; GER-EGE, East Germany; SWI-TIC, Ticino, Switzerland; FRA-LIL, Lille, France; SWI- VAF, Vaud/Fribourg, Switzerland; SPA-CAT, Catalonia, Spain; JPN-SHIGA, Shiga, Japan; other abbreviations as in Figure 2.",Medical Data Analysis,"The prevalence of stroke is higher than that of coronary heart disease (CHD) in Asian countries, possibly due to a higher prevalence of hypertension and lower serum total cholesterol levels. The population-attributable fraction of hypertension for CVD is high in Asian countries, and reduction in salt consumption is important for reducing CVD, especially stroke. Smoking is also a significant risk factor for CVD in most Asian countries, especially for men. Recent westernization in Asian countries has increased fat consumption, which may have caused an increase in CHD. Management of traditional risk factors for CVD, including hypertension, smoking, and obesity, is essential for the prevention of CVD in Asian and Western countries.",Medical Data Analysis,"6.20 5.69 S17 4.66 se AE fusgos inf China’ Bejing residents 1902-1984 Cchina:Urbant 903-1 nina: Reding formerstoea. tao CChina:Urbant803-1808 Wi ea | \ cae Rte 10s Cchina:muratses-i00s ® Koreat900 150 | 4 china: Jiangsu farmerst982-1004 (mmol) 30 40 50 60 70 0 isteae Figure 5. Serum total cholesterol levels (Serum T-CHOL) for men among Asian populations in 1980 to 2000 and in the United States in 1960 to 1991. Serum total cholesterol in Singa- pore was the highest among the selected Asian populations, followed by Hong Kong and Japan. That of Japanese people increased from 1980 to 2000, and currently, young people in Japan have levels similar to their American counterparts; how- ever, there remains a 20-mg/dL difference in serum total choles- terol levels between elderly Japanese and Americans. The levels of Taiwanese and Korean populations were intermediate between those of the Japanese and Chinese. Chinese and some Korean populations have lower serum total cholesterol levels than other industrialized Asian populations. Reprinted with permission from Oxford University Press.*(%) Smoking rate 80 A Men 4 c Vietnay Japan Korea m © Wore 70 60 a. Singapore Hong Kong 30 20 Figure 4. Trends in smoking rate by sex and by selected Asian countries. The smoking rate for women in Asian countries was lower (<10%) than that for men. Among Asian countries, Singapore had the lowest smoking rate for men and women, with a declining smoking rate for men. The smoking rate for men in Japan, Korea, China, and Vietnam remains high (over 45%), although declining trends are clear. Data were obtained from previous studies.48.50-54FIN-KUO RUS-NOL FIN-NKA FIN-KUO RUS-NOIL LTU-KAU LTU-KAU CHN-BHL FIN-TUL FIN-NKA YUG-NOS JPN-SHIGA JPN-HOKKAIDO JPN-HOKKAIDO JPN-SHIGA SWENSW CHN-BH YUG-NOS SWENSW FIN-TUL RUS-MOC JPN-AKITA JPN-AKITA RUS-MOC RUS-MOI JPN-NAGANO JPN-OKINAWA. JPN-OKINAWA DEN-GLO GER-KMS JPN-NAGANO RUS-MOI GER-KMS DEN-GLO GER-HAC GER-HAC POL-WAR JPN-OSAKA JPN-OSAKA SWE-GOT SWE-GOT POL-WAR GER-RDM ITA-FRI GER-RHN GER-RDM ITA-FRI GER-RHN 0 50 100 150-200 250 ©3000 (Stroke Incidence Rate, per 100000/year) Figure 2. Age-adjusted (35 to 64 years old) stroke incidence from a MONICA study in 1985 to 1987 and a Japanese study ((]) in 1980 to 1993. The diagnostic criteria of the MONICA study were used for the Japanese study. Male stroke incidence (left) for the 6 Japanese populations and 1 Chinese population (in Beijing, China) showed that rates were intermediate for these populations and much lower than that of Finland. The female Beijing population was in the higher group (right). The world standard population was used for the cal- culation of age-adjusted rates for 35- to 64-year-olds. Reprinted with permission from the Japan Atherosclerosis Society.1 FIN-KUO indicates Kuopio Province, Finland; FIN-NKA, North Karelia, Finland; RUS-NOI, Novosibirsk Intervention, Russia; LTU-KAU, Kaunas, Lithuania; FIN-TUL, Turku/Loimaa, Finland; YUG-NOS, Novi Sad, Yugoslavia; JPN-HOKKAIDO, Hokkaido, Japan; JPN-SHIGA, Shiga, Japan; CHN-BEI, Beijing, China; SWE-NSW, Northern Sweden, Sweden; RUS-MOC, Moscow Control, Russia; JPN-AKITA, Akita, Japan; RUS-MOI, Moscow Intervention, Russia; JPN-OKINAWA, Okinawa, Japan; DEN-GLO, Glostrup, Denmark; JPN-NAGANO, Nagano, Japan; GER-KMS, Karl-Marx-Stadt County, Germany; GER-HAC, Halle County, Germany; POL-WAR, Warsaw, Poland; JPN- OSAKA, Osaka, Japan; SWE-GOT, Gothenburg, Sweden; GER-RDM, Rest of DDR (East Germany)-MONICA; GER-RHN, Rhein-Neckar Region, Germany; and ITA-FRI, Friuli, Italy.Saudi Arabia Saudi Arabia UT UAE vAE En Oman oman eet Jordan Jordan Toe Tar Uzbekistan Uzbekistan Tajikistan Tajikistan Kazakhstan Kazakhstan Iran Iran Pakistan Pakistan Bangladesh Bangladesh India India Malaysia Malaysia Indonesia Stroke Indonesia CHD Thailand Thailand Singapore Singapore Korea Korea Japan Japan China China USA USA UK UK Spain Spain 1 New Zealand NewZealand France France [— Australia Australia /-—-— 0 50 100 «150 200 «250 «300 «350 «400-450 A a ee ee ee ee ee ee ee (Stroke Death Rate per 100 000 Person-year) (CHD Death Rate per 100 000 Person-year) Figure 1. Age-standardized death rates per 100 000 for stroke (left) and CHD (right) across countries in different regions of Asia in 2002. Data from the World Health Organization, Department of Measurement and Health Information. Patterns on the bars represent different regions: Middle Eastern countries ({), Central Asian countries (§), South Asian countries (™), Southeast Asian countries (Z), East Asian countries (1), and non-Asian countries (open bars). UAE indicates United Arab Emirates; USA, United States of Americ: and UK, United Kingdom.FIN-KUO UNCREL, UNK-BEL CAN-HAL RoEwar SwENeer DEN-GLO FOL Wall USA-STA ICBICE DEN-GLO USA-STA ICEICE YUG-NOS SWEGOT AUS-PER BEL-GHE GER-EGE swreTic FRA-LIL TTA-FRI SWLVAF SPA-CAT CHN-BEL CHN-BEL SPA-CAT JPN-HOKKAIDO JPN-OKINAW A JPN-OKINAW A JPN-OSAKA JPN-SHIGA JPN-HOKKAIDO JPN-OSAKA JPN-NAGANO JPN-NAGANO JPN-SHIGA JPN-AKITA JPN-AKITA fs | iy 0 150 300 450 600 750 900 0 150 300 450 600 750 900 (Acute Myocardial Infarction, Incidence per 100000/year) Figure 3. Age-adjusted (35 to 64 years old) incidence of acute myocardial infarction from a MONICA study in 1985 to 1987 and a Japanese study in (MM) 1989 to 1993. The incidence rates of 6 Japanese populations and a single Chinese population were far lower than those of selected Western countries. The world standard population was used for the calculation of age-adjusted rates for 35- to 64-year-olds. Reprinted with permission from the Japan Atherosclerosis Society.1 UNK-BEL indicates Belfast, United Kingdom; CAN-HAL, Halifax County, Canada; ICE-ICE, Iceland; USA-STA, Stanford (California), United States; CZE-CZE, Czech Republic; NEZ-AUC, Auckland, New Zealand; AUS-PER, Perth, Australia; BEL-GHE, Ghent, Belgium; GER-EGE, East Germany; SWI-TIC, Ticino, Switzerland; FRA-LIL, Lille, France; SWI- VAF, Vaud/Fribourg, Switzerland; SPA-CAT, Catalonia, Spain; JPN-SHIGA, Shiga, Japan; other abbreviations as in Figure 2.",Medical Data Analysis
15,Cardiovascular disease risk in women with pre-eclampsia: systematic review and meta-analysis,"Pre-eclampsia , Cardiovascular disease , Cerebrovascular disease , Hypertension","There is increasing evidence that pre-eclampsia, a principal cause of maternal morbidity, may also be a risk factor for future cardiovascular and cerebrovascular events. This review aimed to assess the current evidence and quantify the risks of cardiovascular disease (CVD), cerebrovascular events and hypertension associated with prior diagnosis of pre-eclampsia. Medline and Embase were searched with no language restrictions, as were core journals and reference lists from reviews up until January 2012. Case–control and cohort studies which reported cardiovascular and cerebrovascular diseases or hypertension diagnosed more than 6 weeks postpartum, in women who had a history of pre-eclampsia relative to women who had unaffected pregnancies, were included. Fifty articles were included in the systematic review and 43 in the meta-analysis. Women with a history of pre-eclampsia or eclampsia were at significantly increased odds of fatal or diagnosed CVD [odds ratio (OR) = 2.28, 95 % confidence interval (CI): 1.87, 2.78], cerebrovascular disease (OR = 1.76, 95 % CI 1.43, 2.21) and hypertension [relative risk (RR) = 3.13, 95 % CI 2.51, 3.89]. Among pre-eclamptic women, pre-term delivery was not associated with an increased risk of a future cardiovascular event (RR = 1.32, 95 % CI 0.79, 2.22). Women diagnosed with pre-eclampsia are at increased risk of future cardiovascular or cerebrovascular events, with an estimated doubling of odds compared to unaffected women. This has implications for the follow-up of all women who experience pre-eclampsia, not just those who deliver preterm. This association may reflect shared common risk factors for both pre-eclampsia and cardiovascular and cerebrovascular disease.","Pre-eclampsia is associated with an approximate twofold increase in odds of CVD and cerebrovascular disease, and a threefold increased risk of hypertension. We found no evidence that the risks of CVD increase when pre-eclampsia is associated with pre-term delivery, although acknowledge there are limitations to our methodology. Women who experience pre-eclampsia should be aware of their increased risk and may benefit from formal postnatal screening for accepted risk factors for CVD.","Cardiovascular disease risk in women with pre-eclampsia: systematic review and meta-analysisPre-eclampsia , Cardiovascular disease , Cerebrovascular disease , HypertensionThere is increasing evidence that pre-eclampsia, a principal cause of maternal morbidity, may also be a risk factor for future cardiovascular and cerebrovascular events. This review aimed to assess the current evidence and quantify the risks of cardiovascular disease (CVD), cerebrovascular events and hypertension associated with prior diagnosis of pre-eclampsia. Medline and Embase were searched with no language restrictions, as were core journals and reference lists from reviews up until January 2012. Case–control and cohort studies which reported cardiovascular and cerebrovascular diseases or hypertension diagnosed more than 6 weeks postpartum, in women who had a history of pre-eclampsia relative to women who had unaffected pregnancies, were included. Fifty articles were included in the systematic review and 43 in the meta-analysis. Women with a history of pre-eclampsia or eclampsia were at significantly increased odds of fatal or diagnosed CVD [odds ratio (OR) = 2.28, 95 % confidence interval (CI): 1.87, 2.78], cerebrovascular disease (OR = 1.76, 95 % CI 1.43, 2.21) and hypertension [relative risk (RR) = 3.13, 95 % CI 2.51, 3.89]. Among pre-eclamptic women, pre-term delivery was not associated with an increased risk of a future cardiovascular event (RR = 1.32, 95 % CI 0.79, 2.22). Women diagnosed with pre-eclampsia are at increased risk of future cardiovascular or cerebrovascular events, with an estimated doubling of odds compared to unaffected women. This has implications for the follow-up of all women who experience pre-eclampsia, not just those who deliver preterm. This association may reflect shared common risk factors for both pre-eclampsia and cardiovascular and cerebrovascular disease.Pre-eclampsia is associated with an approximate twofold increase in odds of CVD and cerebrovascular disease, and a threefold increased risk of hypertension. We found no evidence that the risks of CVD increase when pre-eclampsia is associated with pre-term delivery, although acknowledge there are limitations to our methodology. Women who experience pre-eclampsia should be aware of their increased risk and may benefit from formal postnatal screening for accepted risk factors for CVD.Study Adams et al 1961 Aukes et al 2009 Berends et al 2009 Blaauw et al 2006 Canti et al 2010 Carleton et al 1988 DiehI et al 2008 Edlow et al 2009 Epstein et al 1964 Gaugler-Senden et al Hannaford et al 1997 Haukkamaa et al 2009 Hubel et al 2000 Kaaja et al 2005 Kharazmi et al 2007 Laivouri et al 1996 Lykke et al 2009 Magnussen et al 2009 Manten et al 2009 Marin et al 2000 Melchiorre et al 2011 Nisell et al 1995 North et al 1996 Portelinha er al 2010 Sattar et al 2003 Shahbazian et al 2011 Shammas et al 2000 Sibai et al 1986 Spaan et al 2009 Wilson et al 2003 Overall Hypertension cases/ Pre-eclampsia 79/149 6/39 21/45 4/22 6/40 3/23 50/103 24/79 18/48 11/20 377/3000 15/35 10/30 126/397 30/131 2/22 2919/33826 79/943 56/256 32/94 16/64 9/45 20/50 28/90 7/40 10/35 11/47 60/406 12/22 216/443 2008 NOTE: Weights are from random effects analysis Hypertension cases/ Uncomplicated pregnancies 39/185 4/29 9/106 0/22 114 2/23 22/100 5/140 8/114 2/20 922/18451 137/489 2/30 392/3162 497/2834 0/22 14762/741012 362/14122 0/53 12/86 1/78 1144 1/50 2/60 2/40 1/35 1/46 23/409 2/29 55/206 RR (95% Cl) as 2.52 (1.83, 3.45) —i—, 1.12 (0.35, 3.59) = 5.50 (2.73, 11.05) ———*——> 9.00 (0.51, 157.78) —,— 2.10 (0.28, 15.95) | 1.50 (0.28, 8.16) ES 2.21 (1.45, 3.36) —— 8.51 (3.38, 21.41) Ps 5.34 (2.50, 11.44) _-_ 5.50 (1.39, 21.71) . 2.51 (2.25, 2.82) 1.53 (1.02, 2.30) 5.00 (1.19, 20.92) + 2.56 (2.16, 3.04) 1.31 (0.94, 1.81) 5.00 (0.25, 98.52) . 4.33 (4.17, 4.50) 3.27 (2.59, 4.13) 23.74 (1.49, 378.36) 2.44 (1.34, 4.43) 19.50 (2.66, 143.09) 8.80 (1.16, 66.59) 20.00 (2.79, 143.38) 9.33 (2.31, 37.73) 3.50 (0.77, 15.83) > 40.00 (1.35, 74.00) > 10.77 (1.45, 80.06) 2.63 (1.66, 4.17) 7.91 (1.97, 31.77) =e 1.83 (1.43, 2.33) 3.13 (2.51, 3.89) TT TTrTiiéet 25.5 1 2 4 8 163264 Fig. 4 Forest plot showing the risk of future hypertension following pre-eclampsiaCerebrovascularcases/ © Cerebrovascularcases/ Study Pre-eclampsia Uncomplicated pregnancies OR (95% Cl) Cohort Studies ' ' Hannafordet al1997 25/3000 93/18451 = 1.66 (1.06, 2.58) ' Irgens et al 2001 14/24155 292/602117 1.20 (0.70, 2.04) 1 Lykke et al 2009 600/33826 8240/741012 ° 1.61 (1.48, 1.75) Wilsonet al 2003 34/1043 13/796 —+— 2.03 (1.06, 3.87) ' Subtotal . 1.60 (1.48, 1.74) : : ' i ' ' ' ' ' ' Case-control Studies Ben-Amiet al 2010 20/23 81/179 ——) 807 (2.31, 28.12) Brown et al 2006 40/83 221/599 1.59 (1.00, 2.52) Thorogood et al 1992 57/105 365/1099 / 2.39 (1.59, 3.58) Subtotal <S 2.46 (1.35, 4.49) Overall ro) 1.77 (1.43, 2.21) . ' ' ' NOTE: Weights are from random effects analysis Fig. 3 Forest plot showing the odds of future cerebrovascular disease following pre-eclampsia5189 articles identified from electronic database 239 potentially relevant articles reviewed for second time 5 articles identified from reference lists/journals 136 full text articles reviewed 105 excluded: 25 no relevant CVD outcomes; 42 during pregnancy only; 11 no control group; 17 pre-eclampsia as outcome; 2 pre- eclampsia part of composite group; 8 with blood pressure data only. 31 articles identified 26 articles identified from previous meta-analyses for period prior to. 2006 but 1 only gave data for composite of maternal placental syndrome (Ray 2005); 1 compared pre-eclamptic pregnancies to general population data and not women with uncomplicated pregnancies (Jonsdottir 1995). Therefore 24 were identified. 55 indentified in total 5 duplicate articles excluded (Hubel 2008; Berends 2008; Portelinha |—_+| 2009; Souwer 2012; Lin 2011). y No additional papers identified from supplementary pre-2006 search 50 included in systematic review 7 articles excluded: 2 articles only presented data for outcomes which were a composite of CVD/CVA (Kestanbaum 2003, Iverson &Hannaford 2010); 1 article reported no patients in either group with outcomes (Soonthornpun 2009); Figures from 3 articles could not be deciphered (Garovic 2010, Lin 2011; Saldarriaga 2010); 1 article reported death data on same cohort as diagnoses data available (Lykke 2010) y 43 included in meta-analysis Fig. 1 Process of study selectionFig. 2 Forest plot showing the CVD cases/ odds of future cardiovascular CVD cases/ Uncomplicated disease following pre-eclampsia Study Pre-eclampsia _ pregnancies OR (95% Cl) Cohort Studies Freibert et al 2012 9/222 46/2558 i 2.31 (1.11, 4.78) Funai et al 2005 41/1055 269/36858 i= 5.50 (3.94, 7.68) Hannaford et al 1997 69/3000 216/18451 & 1.99 (1.51, 2.61) Haukkamaa et al 2009 2/35 29/489 —_— 0.96 (0.22, 4.21) Irgens et al 2001 27/24155 325/602117 = 2.07 (1.40, 3.07) Kaaja et al 2005 10/397 25/3162 a 3.24 (1.55, 6.80) Lykke et al 2009 846/33826 9777/741012 2 1.92 (1.79, 2.06) Mongraw-Chaffin et al 2010 24/481 242/13922 = 2.97 (1.93, 4.56) Smith et al 2001 12/22781 31/106509 te 1.81 (0.93, 3.53) Wikstrom et al 2005 176/12533 2306/383081 © 2.35 (2.02, 2.74) Wilson et al 2003 48/1043 38/796 ee: 0.96 (0.62, 1.49) Subtotal 9 2.24 (1.80, 2.80) ' Case-control Studies 1 Croft & Hannaford 1989 39/93 119/513 + 2.39 (1.51, 3.79) Haukkamaa et al 2004 29/32 112/208 _=— 8.29 (2.45, 28.05) Mann et al 1976 21/43 53/227 = 3.13 (1.60, 6.14) Rosenberg et al 1983 26/86 229/971 Le 1.40 (0.87, 2.28) Subtotal < 2.57 (1.49, 4.45) ' Overall 6 2.28 (1.87, 2.77) ' 1 NOTE: Weights are from random effects analysis 2 51 2 4 8 1632CVD cases/ CVD cases/ Pre-eclampsia _ Pre-eclampsia Study pre-term term RR (95% Cl) Lykke et al 2009 62/2662 581/25184 1.01 (0.78, 1.31) Smith et al 2001 3/1498 39/21283 —+— 1.09 (0.34, 3.53) Wikstrom et al 2005 39/1673 136/11110 * 1.90 (1.34, 2.71) Overall '> 1.32 (0.79, 2.22) NOTE: Weights are from random effects analysis Fig. 5 Forest plot showing the risk of future cardiovascular disease following term and ‘Sesdetnt pre-eclamsia",Medical Data Analysis,"Pre-eclampsia, a common complication of pregnancy, is associated with an increased risk of cardiovascular disease (CVD), cerebrovascular events, and hypertension later in life. This systematic review and meta-analysis analyzed 50 articles to quantify the risks associated with pre-eclampsia. Women with a history of pre-eclampsia or eclampsia were at significantly increased odds of fatal or diagnosed CVD, cerebrovascular disease, and hypertension. Among pre-eclamptic women, pre-term delivery was not associated with an increased risk of future cardiovascular events. The increased risk of CVD may reflect shared common risk factors for both pre-eclampsia and cardiovascular and cerebrovascular disease. Women who experience pre-eclampsia should be aware of their increased risk and may benefit from formal postnatal screening for accepted risk factors for CVD.",Medical Data Analysis,"Study Adams et al 1961 Aukes et al 2009 Berends et al 2009 Blaauw et al 2006 Canti et al 2010 Carleton et al 1988 DiehI et al 2008 Edlow et al 2009 Epstein et al 1964 Gaugler-Senden et al Hannaford et al 1997 Haukkamaa et al 2009 Hubel et al 2000 Kaaja et al 2005 Kharazmi et al 2007 Laivouri et al 1996 Lykke et al 2009 Magnussen et al 2009 Manten et al 2009 Marin et al 2000 Melchiorre et al 2011 Nisell et al 1995 North et al 1996 Portelinha er al 2010 Sattar et al 2003 Shahbazian et al 2011 Shammas et al 2000 Sibai et al 1986 Spaan et al 2009 Wilson et al 2003 Overall Hypertension cases/ Pre-eclampsia 79/149 6/39 21/45 4/22 6/40 3/23 50/103 24/79 18/48 11/20 377/3000 15/35 10/30 126/397 30/131 2/22 2919/33826 79/943 56/256 32/94 16/64 9/45 20/50 28/90 7/40 10/35 11/47 60/406 12/22 216/443 2008 NOTE: Weights are from random effects analysis Hypertension cases/ Uncomplicated pregnancies 39/185 4/29 9/106 0/22 114 2/23 22/100 5/140 8/114 2/20 922/18451 137/489 2/30 392/3162 497/2834 0/22 14762/741012 362/14122 0/53 12/86 1/78 1144 1/50 2/60 2/40 1/35 1/46 23/409 2/29 55/206 RR (95% Cl) as 2.52 (1.83, 3.45) —i—, 1.12 (0.35, 3.59) = 5.50 (2.73, 11.05) ———*——> 9.00 (0.51, 157.78) —,— 2.10 (0.28, 15.95) | 1.50 (0.28, 8.16) ES 2.21 (1.45, 3.36) —— 8.51 (3.38, 21.41) Ps 5.34 (2.50, 11.44) _-_ 5.50 (1.39, 21.71) . 2.51 (2.25, 2.82) 1.53 (1.02, 2.30) 5.00 (1.19, 20.92) + 2.56 (2.16, 3.04) 1.31 (0.94, 1.81) 5.00 (0.25, 98.52) . 4.33 (4.17, 4.50) 3.27 (2.59, 4.13) 23.74 (1.49, 378.36) 2.44 (1.34, 4.43) 19.50 (2.66, 143.09) 8.80 (1.16, 66.59) 20.00 (2.79, 143.38) 9.33 (2.31, 37.73) 3.50 (0.77, 15.83) > 40.00 (1.35, 74.00) > 10.77 (1.45, 80.06) 2.63 (1.66, 4.17) 7.91 (1.97, 31.77) =e 1.83 (1.43, 2.33) 3.13 (2.51, 3.89) TT TTrTiiéet 25.5 1 2 4 8 163264 Fig. 4 Forest plot showing the risk of future hypertension following pre-eclampsiaCerebrovascularcases/ © Cerebrovascularcases/ Study Pre-eclampsia Uncomplicated pregnancies OR (95% Cl) Cohort Studies ' ' Hannafordet al1997 25/3000 93/18451 = 1.66 (1.06, 2.58) ' Irgens et al 2001 14/24155 292/602117 1.20 (0.70, 2.04) 1 Lykke et al 2009 600/33826 8240/741012 ° 1.61 (1.48, 1.75) Wilsonet al 2003 34/1043 13/796 —+— 2.03 (1.06, 3.87) ' Subtotal . 1.60 (1.48, 1.74) : : ' i ' ' ' ' ' ' Case-control Studies Ben-Amiet al 2010 20/23 81/179 ——) 807 (2.31, 28.12) Brown et al 2006 40/83 221/599 1.59 (1.00, 2.52) Thorogood et al 1992 57/105 365/1099 / 2.39 (1.59, 3.58) Subtotal <S 2.46 (1.35, 4.49) Overall ro) 1.77 (1.43, 2.21) . ' ' ' NOTE: Weights are from random effects analysis Fig. 3 Forest plot showing the odds of future cerebrovascular disease following pre-eclampsia5189 articles identified from electronic database 239 potentially relevant articles reviewed for second time 5 articles identified from reference lists/journals 136 full text articles reviewed 105 excluded: 25 no relevant CVD outcomes; 42 during pregnancy only; 11 no control group; 17 pre-eclampsia as outcome; 2 pre- eclampsia part of composite group; 8 with blood pressure data only. 31 articles identified 26 articles identified from previous meta-analyses for period prior to. 2006 but 1 only gave data for composite of maternal placental syndrome (Ray 2005); 1 compared pre-eclamptic pregnancies to general population data and not women with uncomplicated pregnancies (Jonsdottir 1995). Therefore 24 were identified. 55 indentified in total 5 duplicate articles excluded (Hubel 2008; Berends 2008; Portelinha |—_+| 2009; Souwer 2012; Lin 2011). y No additional papers identified from supplementary pre-2006 search 50 included in systematic review 7 articles excluded: 2 articles only presented data for outcomes which were a composite of CVD/CVA (Kestanbaum 2003, Iverson &Hannaford 2010); 1 article reported no patients in either group with outcomes (Soonthornpun 2009); Figures from 3 articles could not be deciphered (Garovic 2010, Lin 2011; Saldarriaga 2010); 1 article reported death data on same cohort as diagnoses data available (Lykke 2010) y 43 included in meta-analysis Fig. 1 Process of study selectionFig. 2 Forest plot showing the CVD cases/ odds of future cardiovascular CVD cases/ Uncomplicated disease following pre-eclampsia Study Pre-eclampsia _ pregnancies OR (95% Cl) Cohort Studies Freibert et al 2012 9/222 46/2558 i 2.31 (1.11, 4.78) Funai et al 2005 41/1055 269/36858 i= 5.50 (3.94, 7.68) Hannaford et al 1997 69/3000 216/18451 & 1.99 (1.51, 2.61) Haukkamaa et al 2009 2/35 29/489 —_— 0.96 (0.22, 4.21) Irgens et al 2001 27/24155 325/602117 = 2.07 (1.40, 3.07) Kaaja et al 2005 10/397 25/3162 a 3.24 (1.55, 6.80) Lykke et al 2009 846/33826 9777/741012 2 1.92 (1.79, 2.06) Mongraw-Chaffin et al 2010 24/481 242/13922 = 2.97 (1.93, 4.56) Smith et al 2001 12/22781 31/106509 te 1.81 (0.93, 3.53) Wikstrom et al 2005 176/12533 2306/383081 © 2.35 (2.02, 2.74) Wilson et al 2003 48/1043 38/796 ee: 0.96 (0.62, 1.49) Subtotal 9 2.24 (1.80, 2.80) ' Case-control Studies 1 Croft & Hannaford 1989 39/93 119/513 + 2.39 (1.51, 3.79) Haukkamaa et al 2004 29/32 112/208 _=— 8.29 (2.45, 28.05) Mann et al 1976 21/43 53/227 = 3.13 (1.60, 6.14) Rosenberg et al 1983 26/86 229/971 Le 1.40 (0.87, 2.28) Subtotal < 2.57 (1.49, 4.45) ' Overall 6 2.28 (1.87, 2.77) ' 1 NOTE: Weights are from random effects analysis 2 51 2 4 8 1632CVD cases/ CVD cases/ Pre-eclampsia _ Pre-eclampsia Study pre-term term RR (95% Cl) Lykke et al 2009 62/2662 581/25184 1.01 (0.78, 1.31) Smith et al 2001 3/1498 39/21283 —+— 1.09 (0.34, 3.53) Wikstrom et al 2005 39/1673 136/11110 * 1.90 (1.34, 2.71) Overall '> 1.32 (0.79, 2.22) NOTE: Weights are from random effects analysis Fig. 5 Forest plot showing the risk of future cardiovascular disease following term and ‘Sesdetnt pre-eclamsia",Medical Data Analysis
16,Childhood Interstitial Lung Disease: A Systematic Review,Childhood interstitial lung disease; chILD syndrome; Interstitial lung disease; Diffuse lung disease.,"Childhood interstitial lung disease (chILD) is a group of rare chronic and complex disorders of variable pathology. There has been no systematic review of published chILD research. This study aimed to describe chILD classification systems, epidemiology, morbidity, treatments, outcomes, and the impact of chILD on families and the burden on health services. A systematic literature search for original studies on chILD was undertaken in the major biomedical databases to the end of December 2013. Epidemiological studies, case series and studies describing classification systems were included. Single case studies were excluded. Results: The search yielded 37 publications that met study criteria. Four different chILD classification systems have been proposed in the past decade. The incidence of chILD has been estimated at 0.13–16.2 cases/100,000 children/year. One to five new cases presented to individual hospitals each year. In developed countries, the median mortality was 13% (6–19%). Morbidity and outcomes were highly variable and not systematically reported. Corticosteroids and hydroxychloroquine were the most common treatments. The impact of chILD on families and the burden on health services has not been studied. Conclusions: The heterogeneity of the chILD group of disorders, different determinations of what constitutes a chILD disorder and, a paucity of large epidemiological studies precludes consolidation of results across studies. Consensus on chILD classification is needed to support diagnosis and allow direct comparisons of research evidence. Active disease surveillance and international patient registries are required to advance understanding and management of chILD.","In conclusion, the disorders that together constitute the group of diseases known as chILD are extremely heterogeneous and associated with high morbidity and mortality. Prospective, active surveillance of chILD through strategic international collaboration is needed to provide more accurate estimates of frequency. It is important that a single classification system for chILD is adopted globally to support direct comparisons of research evidence. Patient registries and randomized controlled intervention trials through international collaboration are required to provide an evidence base for improving the lives of children with these rare disorders. Studies that go beyond describing subjective outcomes and describe quality of life (e.g., need for oxygen, exercise capability, school attendance, regularity of symptoms, and duration) will support prognosis. For health services planning and to support families, the impacts of chILD should be addressed in prospective studies. Determining the burden of chILD on health services requires descriptive statistics beyond simply counting the number of cases. Rigorous study of the health service needs for these complex and chronic conditions is needed.","Childhood Interstitial Lung Disease: A Systematic ReviewChildhood interstitial lung disease; chILD syndrome; Interstitial lung disease; Diffuse lung disease.Childhood interstitial lung disease (chILD) is a group of rare chronic and complex disorders of variable pathology. There has been no systematic review of published chILD research. This study aimed to describe chILD classification systems, epidemiology, morbidity, treatments, outcomes, and the impact of chILD on families and the burden on health services. A systematic literature search for original studies on chILD was undertaken in the major biomedical databases to the end of December 2013. Epidemiological studies, case series and studies describing classification systems were included. Single case studies were excluded. Results: The search yielded 37 publications that met study criteria. Four different chILD classification systems have been proposed in the past decade. The incidence of chILD has been estimated at 0.13–16.2 cases/100,000 children/year. One to five new cases presented to individual hospitals each year. In developed countries, the median mortality was 13% (6–19%). Morbidity and outcomes were highly variable and not systematically reported. Corticosteroids and hydroxychloroquine were the most common treatments. The impact of chILD on families and the burden on health services has not been studied. Conclusions: The heterogeneity of the chILD group of disorders, different determinations of what constitutes a chILD disorder and, a paucity of large epidemiological studies precludes consolidation of results across studies. Consensus on chILD classification is needed to support diagnosis and allow direct comparisons of research evidence. Active disease surveillance and international patient registries are required to advance understanding and management of chILD.In conclusion, the disorders that together constitute the group of diseases known as chILD are extremely heterogeneous and associated with high morbidity and mortality. Prospective, active surveillance of chILD through strategic international collaboration is needed to provide more accurate estimates of frequency. It is important that a single classification system for chILD is adopted globally to support direct comparisons of research evidence. Patient registries and randomized controlled intervention trials through international collaboration are required to provide an evidence base for improving the lives of children with these rare disorders. Studies that go beyond describing subjective outcomes and describe quality of life (e.g., need for oxygen, exercise capability, school attendance, regularity of symptoms, and duration) will support prognosis. For health services planning and to support families, the impacts of chILD should be addressed in prospective studies. Determining the burden of chILD on health services requires descriptive statistics beyond simply counting the number of cases. Rigorous study of the health service needs for these complex and chronic conditions is needed.Articles from electronic database search (n=4361) Duplicates and articles excluded as irrelevant after review of title and abstract (n=4253) Reviews and potentially relevant original studies (n=108) Relevant studies (n=24) Studies removed based on inclusion exclusion criteria following an examination of the full text (n=84) Studies identified from reference lists of reviews and relevant original ‘studies and, a recommendation from experts in the field (n=13) Relevant studies included in this systematic review (n=37) classification systems (n=4°) population prevalencefincidence (n=4*) frequency hospital-based incident cases (n=5*) morbidities/treatments/outcomes (n=33"") * These groups are not mutually exclusive, some studies apply to more than one group. Fig. 1. Results of literature search strategy.",Medical Data Analysis,"Childhood interstitial lung disease (chILD) is a rare and complex group of disorders with variable pathology, morbidity, and mortality. There is a lack of consensus on chILD classification, which hinders the consolidation of research evidence. The incidence of chILD is estimated to be 0.13-16.2 cases/100,000 children/year, with a median mortality rate of 13% in developed countries. Corticosteroids and hydroxychloroquine are commonly used treatments. There is a need for active disease surveillance, international patient registries, and randomized controlled intervention trials to advance the understanding and management of chILD. Additionally, studies that go beyond subjective outcomes and describe quality of life, as well as the impacts of chILD on families and health services, are necessary. Determining the burden of chILD on health services requires descriptive statistics beyond simply counting the number of cases.",Medical Data Analysis,"Articles from electronic database search (n=4361) Duplicates and articles excluded as irrelevant after review of title and abstract (n=4253) Reviews and potentially relevant original studies (n=108) Relevant studies (n=24) Studies removed based on inclusion exclusion criteria following an examination of the full text (n=84) Studies identified from reference lists of reviews and relevant original ‘studies and, a recommendation from experts in the field (n=13) Relevant studies included in this systematic review (n=37) classification systems (n=4°) population prevalencefincidence (n=4*) frequency hospital-based incident cases (n=5*) morbidities/treatments/outcomes (n=33"") * These groups are not mutually exclusive, some studies apply to more than one group. Fig. 1. Results of literature search strategy.",Medical Data Analysis
17,Clinical manifestations and evidence of neurological involvement in 2019 novel coronavirus SARS‑CoV‑2: a systematic review and meta‑analysis,"COVID-19 , SARS-CoV-2 , Neurological manifestations , Systematic review , Infammation.","Coronavirus disease 2019 (COVID-19) has become a global pandemic, affecting millions of people. However, clinical research on its neurological manifestations is thus far limited. In this study, we aimed to systematically collect and investigate the clinical manifestations and evidence of neurological involvement in COVID-19. Three medical (Medline, Embase, and Scopus) and two preprints (BioRxiv and MedRxiv) databases were systematically searched for all published articles on neurological involvement in COVID-19 since the outbreak. All included studies were systematically reviewed, and selected clinical data were collected for meta-analysis via random-effects. A total of 41 articles were eligible and included in this review, showing a wide spectrum of neurological manifestations in COVID-19. The meta-analysis for unspecific neurological symptoms revealed that the most common manifestations were fatigue (33.2% [23.1–43.3]), anorexia (30.0% [23.2–36.9]), dyspnea/shortness of breath (26.9% [19.2–34.6]), and malaise (26.7% [13.3–40.1]). The common specific neurological symptoms included olfactory (35.7–85.6%) and gustatory (33.3–88.8%) disorders, especially in mild cases. Guillain–Barré syndrome and acute inflammation of the brain, spinal cord, and meninges were repeatedly reported after COVID-19. Laboratory, electrophysiological, radiological, and pathological evidence supported neurologic involvement of COVID-19. Neurological manifestations are various and prevalent in COVID-19. Emerging clinical evidence suggests neurological involvement is an important aspect of the disease. The underlying mechanisms can include both direct invasion and maladaptive inflammatory responses. More studies should be conducted to explore the role of neurological manifestations in COVID-19 progression and to verify their underlying mechanisms.","The neurological manifestations are various and prevalent in COVID-19, but are usually underestimated. Emerging clinical evidence suggests that neurological involvement is an important aspect of the disease. The multifaceted mechanisms are involved in its neurological impact, which includes both direct invasion and maladaptive inflammatory response. Nevertheless, more clinical and experimental research should be conducted to further explore the role of neurological manifestations on the disease progression and its underlying mechanism.","Clinical manifestations and evidence of neurological involvement in 2019 novel coronavirus SARS‑CoV‑2: a systematic review and meta‑analysisCOVID-19 , SARS-CoV-2 , Neurological manifestations , Systematic review , Infammation.Coronavirus disease 2019 (COVID-19) has become a global pandemic, affecting millions of people. However, clinical research on its neurological manifestations is thus far limited. In this study, we aimed to systematically collect and investigate the clinical manifestations and evidence of neurological involvement in COVID-19. Three medical (Medline, Embase, and Scopus) and two preprints (BioRxiv and MedRxiv) databases were systematically searched for all published articles on neurological involvement in COVID-19 since the outbreak. All included studies were systematically reviewed, and selected clinical data were collected for meta-analysis via random-effects. A total of 41 articles were eligible and included in this review, showing a wide spectrum of neurological manifestations in COVID-19. The meta-analysis for unspecific neurological symptoms revealed that the most common manifestations were fatigue (33.2% [23.1–43.3]), anorexia (30.0% [23.2–36.9]), dyspnea/shortness of breath (26.9% [19.2–34.6]), and malaise (26.7% [13.3–40.1]). The common specific neurological symptoms included olfactory (35.7–85.6%) and gustatory (33.3–88.8%) disorders, especially in mild cases. Guillain–Barré syndrome and acute inflammation of the brain, spinal cord, and meninges were repeatedly reported after COVID-19. Laboratory, electrophysiological, radiological, and pathological evidence supported neurologic involvement of COVID-19. Neurological manifestations are various and prevalent in COVID-19. Emerging clinical evidence suggests neurological involvement is an important aspect of the disease. The underlying mechanisms can include both direct invasion and maladaptive inflammatory responses. More studies should be conducted to explore the role of neurological manifestations in COVID-19 progression and to verify their underlying mechanisms.The neurological manifestations are various and prevalent in COVID-19, but are usually underestimated. Emerging clinical evidence suggests that neurological involvement is an important aspect of the disease. The multifaceted mechanisms are involved in its neurological impact, which includes both direct invasion and maladaptive inflammatory response. Nevertheless, more clinical and experimental research should be conducted to further explore the role of neurological manifestations on the disease progression and its underlying mechanism.Fig.3 The potential mechanism underlying the neurological mani- festation in COVID-19. Based on the current evidence, there are two routes for the neurological involvement of COVID-19. (1) SARS- CoV-2 could direct infect nervous system via hematogenous and neu- °Y mmunoglobutin Cytokines £0 sanscova @ rears @ veate Bio Neutrophis wecrorages ral retrograde pathways; (2) SARS-CoV-2 overactivates the immune system, and secondary cytokines storm and immunoglobins impair the nervous systemStudies — Ev/Total Estimate (95% C.I.) 12(%) Egger’s Test Fatigue -—e—1 13 1112/3138 33.2 (23.1, 43.3) 97 0.760 Dyspnea/shortness of breath eH 16 713/3430 26.9 (19.2, 34.6) 98 0.300 Myalgia reH 17 —-478/3120 16.0 (12.3, 19.8) 87 0.303 Headache i 20 — 408/3837 9.2 (7.2, 11.2) 74 0.183 Nausea/Vomiting 2! 1"" 152/2788 5.1 (3.3, 6.8) 76 0.533 Anorexia a] 4 212/717 30.0 (23.2, 36.9) 75 NA. Malaise |<] 2 39/153 26,7 (13.3, 40.1) 69 NA. Dizziness [-<] 4 T1727 10.0 (5.9, 14.2) 72 NA. Confusion 2 10/149 5.2 (-1.7, 12.2) 76 NA. 8 ° s Prevelance% Fig. 2 Meta-analysis of the prevalence of unspecific neurologic manifestations in COVID-19Fig. 1 Study selection and characteristics medRxiv bioRxiv n=71 Identification Total Analysis n=440 Articles after duplicates removal n=298 Duplicate articles n=142 Screen Selection form abstract, title and article type: -Reviews n=39 -Letters, Notes and comment n=27 -Irrelevant n=164 Other Resources n=11 Relevant articles for Full-Text Selection n=79 2 = 32 i -No clinical data n=31 -Irrelevant n=3 -Specific population n=4 Literature for final analysis n=41",Medical Data Analysis,"This study aimed to investigate the neurological manifestations and evidence of neurological involvement in COVID-19 through a systematic review of published articles on the topic. The study found a wide spectrum of neurological symptoms in COVID-19 patients, with fatigue, anorexia, dyspnea/shortness of breath, and malaise being the most common unspecific symptoms. Olfactory and gustatory disorders were also prevalent, particularly in mild cases. The study also found evidence supporting neurologic involvement of COVID-19 through laboratory, electrophysiological, radiological, and pathological evidence. The study concludes that neurological involvement in COVID-19 is an important aspect of the disease that is often underestimated and calls for more clinical and experimental research to explore its underlying mechanisms and role in disease progression.",Medical Data Analysis,"Fig.3 The potential mechanism underlying the neurological mani- festation in COVID-19. Based on the current evidence, there are two routes for the neurological involvement of COVID-19. (1) SARS- CoV-2 could direct infect nervous system via hematogenous and neu- °Y mmunoglobutin Cytokines £0 sanscova @ rears @ veate Bio Neutrophis wecrorages ral retrograde pathways; (2) SARS-CoV-2 overactivates the immune system, and secondary cytokines storm and immunoglobins impair the nervous systemStudies — Ev/Total Estimate (95% C.I.) 12(%) Egger’s Test Fatigue -—e—1 13 1112/3138 33.2 (23.1, 43.3) 97 0.760 Dyspnea/shortness of breath eH 16 713/3430 26.9 (19.2, 34.6) 98 0.300 Myalgia reH 17 —-478/3120 16.0 (12.3, 19.8) 87 0.303 Headache i 20 — 408/3837 9.2 (7.2, 11.2) 74 0.183 Nausea/Vomiting 2! 1"" 152/2788 5.1 (3.3, 6.8) 76 0.533 Anorexia a] 4 212/717 30.0 (23.2, 36.9) 75 NA. Malaise |<] 2 39/153 26,7 (13.3, 40.1) 69 NA. Dizziness [-<] 4 T1727 10.0 (5.9, 14.2) 72 NA. Confusion 2 10/149 5.2 (-1.7, 12.2) 76 NA. 8 ° s Prevelance% Fig. 2 Meta-analysis of the prevalence of unspecific neurologic manifestations in COVID-19Fig. 1 Study selection and characteristics medRxiv bioRxiv n=71 Identification Total Analysis n=440 Articles after duplicates removal n=298 Duplicate articles n=142 Screen Selection form abstract, title and article type: -Reviews n=39 -Letters, Notes and comment n=27 -Irrelevant n=164 Other Resources n=11 Relevant articles for Full-Text Selection n=79 2 = 32 i -No clinical data n=31 -Irrelevant n=3 -Specific population n=4 Literature for final analysis n=41",Medical Data Analysis
18,Concurrence of big data analytics and healthcare: A systematic review,"Big data Analytics , Healthcare , Predictive analytics , Evidence-based medicine .","The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care. This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges. A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered. Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected. Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare. A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare. This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.","Big Data analytics has emerged as a new frontier for enhancing healthcare delivery. With the opportunities created by digital and information revolution, healthcare industry can exploit the potential benefits of leveraging Big Data technology. Big Data analytics increasingly provides value to healthcare by improving healthcare quality and outcomes and providing cost-effective care. The predictive nature and pattern-recognition aspect of Big Data analytics enable the shift from experience-based medicine to evidence-based medicine. Through its systematic review, the study presents a useful starting point for the application of Big Data analytics in future healthcare research. In addition, the study reflects that once the scope of Big Data analytics is defined; its characteristics and features are understood; and challenges are properly tackled, its application will maximize the healthcare value through promoting the extensive usage of insights.","Concurrence of big data analytics and healthcare: A systematic reviewBig data Analytics , Healthcare , Predictive analytics , Evidence-based medicine .The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care. This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges. A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered. Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected. Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare. A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare. This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.Big Data analytics has emerged as a new frontier for enhancing healthcare delivery. With the opportunities created by digital and information revolution, healthcare industry can exploit the potential benefits of leveraging Big Data technology. Big Data analytics increasingly provides value to healthcare by improving healthcare quality and outcomes and providing cost-effective care. The predictive nature and pattern-recognition aspect of Big Data analytics enable the shift from experience-based medicine to evidence-based medicine. Through its systematic review, the study presents a useful starting point for the application of Big Data analytics in future healthcare research. In addition, the study reflects that once the scope of Big Data analytics is defined; its characteristics and features are understood; and challenges are properly tackled, its application will maximize the healthcare value through promoting the extensive usage of insights.LITERATURE SEARCH Databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis Keywords: “big data” or “big data analytics”, and “healthcare” or “medicine” or “biomedicine” J INITIAL SEARCH RESULT Excluded (n=12268) ScienceDirect 3324 PubMed. 23 Emerald Insight 3167 IEEE Xplore 60 Taylor & Francis 5816 Total articles identified n=12390 y ARTICLES SCREENED ON THE BASIS OF TITLE v Included (n=122) v ARTICLES SCREENED ON THE BASIS OF ABSTACT AND KEYWORDS Excluded (n=35) v Included (n=87) y ARTICLES SCREENED ON THE BASIS OF FULL TEXT Excluded (n=29) v Studies included in review (n=58) Fig. 1. Research Process.",Natural Language Processing,"This systematic review of literature explores the potential and challenges of applying Big Data analytics in healthcare. The review considers articles published in English language literature from January 2013 to January 2018 and identifies the sources and applications of Big Data analytics in healthcare, the techniques used, and the challenges to its adoption. The study finds that Big Data analytics has the potential to improve the quality of care, reduce waste and error, and reduce the cost of care. However, researchers lack consensus on the definition of Big Data in healthcare, and there is a paucity of evidence of real-world use. The review concludes that Big Data analytics has emerged as a new frontier for enhancing healthcare delivery, and its application will maximize healthcare value through promoting the extensive usage of insights.",Medical Data Analysis,"LITERATURE SEARCH Databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis Keywords: “big data” or “big data analytics”, and “healthcare” or “medicine” or “biomedicine” J INITIAL SEARCH RESULT Excluded (n=12268) ScienceDirect 3324 PubMed. 23 Emerald Insight 3167 IEEE Xplore 60 Taylor & Francis 5816 Total articles identified n=12390 y ARTICLES SCREENED ON THE BASIS OF TITLE v Included (n=122) v ARTICLES SCREENED ON THE BASIS OF ABSTACT AND KEYWORDS Excluded (n=35) v Included (n=87) y ARTICLES SCREENED ON THE BASIS OF FULL TEXT Excluded (n=29) v Studies included in review (n=58) Fig. 1. Research Process.",Medical Data Analysis
19,Coronavirus disease (COVID‑19) cases analysis using machine‑learning applications,"COVID-19 , Artifcial intelligence AI , Machine learning , Machine learning tasks , Supervised and unsupervised learning .","Today world thinks about coronavirus disease that which means all even this pandemic disease is not unique. The purpose of this study is to detect the role of machine-learning applications and algorithms in investigating and various purposes that deals with COVID-19. Review of the studies that had been published during 2020 and were related to this topic by seeking in Science Direct, Springer, Hindawi, and MDPI using COVID-19, machine learning, supervised learning, and unsupervised learning as keywords. The total articles obtained were 16,306 overall but after limitation; only 14 researches of these articles were included in this study. Our findings show that machine learning can produce an important role in COVID-19 investigations, prediction, and discrimination. In conclusion, machine learning can be involved in the health provider programs and plans to assess and triage the COVID-19 cases. Supervised learning showed better results than other Unsupervised learning algorithms by having 92.9% testing accuracy. In the future recurrent supervised learning can be utilized for superior accuracy.","This study focused on the articles that applied machine learning applications in COVID-19 disease for various purposes with different algorithms, 14 from 16 articles used supervised learning, and only one among them used unsupervised learning another one used both methods supervised and unsupervised learning and both of them shows accurate results. The studies used different machine-learning algorithms in different countries and by different authors but all of them related to the COVID-19 pandemic, (5) of these articles used Logistic regression algorithm, and all of them showed promising results in the COVID-19 health care applications and involvement. While (3) of the articles used artificial neural network (ANN) which also shows successful results, the rest of the 14 articles used different supervised and unsupervised learning algorithms and all of the models showed accurate results. Our conclusion is ML applications in medicine showed promising results with high accuracy, sensitivity, and specificity using different models and algorithms. In general, the paper results explored the supervised learning is more accurate to detect the COVID19 cases which were above (92%) compare to the unsupervised learning which was mere (7.1%).","Coronavirus disease (COVID‑19) cases analysis using machine‑learning applicationsCOVID-19 , Artifcial intelligence AI , Machine learning , Machine learning tasks , Supervised and unsupervised learning .Today world thinks about coronavirus disease that which means all even this pandemic disease is not unique. The purpose of this study is to detect the role of machine-learning applications and algorithms in investigating and various purposes that deals with COVID-19. Review of the studies that had been published during 2020 and were related to this topic by seeking in Science Direct, Springer, Hindawi, and MDPI using COVID-19, machine learning, supervised learning, and unsupervised learning as keywords. The total articles obtained were 16,306 overall but after limitation; only 14 researches of these articles were included in this study. Our findings show that machine learning can produce an important role in COVID-19 investigations, prediction, and discrimination. In conclusion, machine learning can be involved in the health provider programs and plans to assess and triage the COVID-19 cases. Supervised learning showed better results than other Unsupervised learning algorithms by having 92.9% testing accuracy. In the future recurrent supervised learning can be utilized for superior accuracy.This study focused on the articles that applied machine learning applications in COVID-19 disease for various purposes with different algorithms, 14 from 16 articles used supervised learning, and only one among them used unsupervised learning another one used both methods supervised and unsupervised learning and both of them shows accurate results. The studies used different machine-learning algorithms in different countries and by different authors but all of them related to the COVID-19 pandemic, (5) of these articles used Logistic regression algorithm, and all of them showed promising results in the COVID-19 health care applications and involvement. While (3) of the articles used artificial neural network (ANN) which also shows successful results, the rest of the 14 articles used different supervised and unsupervised learning algorithms and all of the models showed accurate results. Our conclusion is ML applications in medicine showed promising results with high accuracy, sensitivity, and specificity using different models and algorithms. In general, the paper results explored the supervised learning is more accurate to detect the COVID19 cases which were above (92%) compare to the unsupervised learning which was mere (7.1%).90% 86% Percentage of total papers 7% o% | Classifications Regression Fig. 3 Distribution of machine-learning tasks Clustering100% 92.9% 90% 80% 70% 60% 50% 40% 30% 20% Percentage of total papers 10% 7.1% 0% ee Supervised Unsupervised Fig. 2 Distribution of machine-learning typesFig. 1 Overview of machine- learning types and tasks Machine Learning Types and Tasks v v Supervised Learning | Un-Supervised Learning — —y a t Reinforcement Learning — Types = Regression Classification Cluster Analysis Dimensionality reduction Classification | Control eta6 5 . | . o Uj i U i Logistic linear K-Means NN Naive Regression Regression Bayes Number of total papers w Fig. 4 Distribution of machine-learning algorithms",Deep Learning and Machine Learning,"This study reviewed 14 research articles published in 2020 that used machine learning algorithms for investigating and dealing with COVID-19. The study found that machine learning has an important role in COVID-19 investigations, prediction, and discrimination, and can be involved in health provider programs and plans. The review showed that supervised learning algorithms, particularly logistic regression, had better results than unsupervised learning algorithms in detecting COVID-19 cases. The study concludes that machine learning applications in medicine showed promising results with high accuracy, sensitivity, and specificity using different models and algorithms.",Deep Learning and Machine Learning,90% 86% Percentage of total papers 7% o% | Classifications Regression Fig. 3 Distribution of machine-learning tasks Clustering100% 92.9% 90% 80% 70% 60% 50% 40% 30% 20% Percentage of total papers 10% 7.1% 0% ee Supervised Unsupervised Fig. 2 Distribution of machine-learning typesFig. 1 Overview of machine- learning types and tasks Machine Learning Types and Tasks v v Supervised Learning | Un-Supervised Learning — —y a t Reinforcement Learning — Types = Regression Classification Cluster Analysis Dimensionality reduction Classification | Control eta6 5 . | . o Uj i U i Logistic linear K-Means NN Naive Regression Regression Bayes Number of total papers w Fig. 4 Distribution of machine-learning algorithms,Deep Learning and Machine Learning
20,Current Clinical Applications of Diffusion-Tensor Imaging in Neurological Disorders,"diffusion-tensor imaging, diffusion-tensor imaging scalar, postprocessing, neurological disorders.","Diffusion-tensor imaging (DTI) is a noninvasive medical imaging tool used to investigate the structure of white matter. The signal contrast in DTI is generated by differences in the Brownian motion of the water molecules in brain tissue. Postprocessed DTI scalars can be used to evaluate changes in the brain tissue caused by disease, disease progression, and treatment responses, which has led to an enormous amount of interest in DTI in clinical research. This review article provides insights into DTI scalars and the biological background of DTI as a relatively new neuroimaging modality. Further, it summarizes the clinical role of DTI in various disease processes such as amyotrophic lateral sclerosis, multiple sclerosis, Parkinson’s disease, Alzheimer’s dementia, epilepsy, ischemic stroke, stroke with motor or language impairment, traumatic brain injury, spinal cord injury, and depression. Valuable DTI postprocessing tools for clinical research are also introduced.","DTI is an emerging neuroimaging method for identifying microstructural changes. Various DTI scalars—such as FA, AD, RD, MD, and MO—can be correlated with clinical information in order to reveal abnormalities associated with neurological diseases. Apart from selecting appropriate DTI scalars, sophisticated clinical research also requires suitable DTI postprocessing tools. Advanced robust postprocessing techniques have yielded novel anatomical and structural pathway information about the brain. Improvements in DTI acquisition techniques such as shorter scanning times (to reduce the effects of head motion), high spatial and gradient-direction resolutions, higher signal-to-noise ratios, and standardization of postprocessing methods will guarantee the utilization of DTI in clinical research and even as a diagnostic tool.","Current Clinical Applications of Diffusion-Tensor Imaging in Neurological Disordersdiffusion-tensor imaging, diffusion-tensor imaging scalar, postprocessing, neurological disorders.Diffusion-tensor imaging (DTI) is a noninvasive medical imaging tool used to investigate the structure of white matter. The signal contrast in DTI is generated by differences in the Brownian motion of the water molecules in brain tissue. Postprocessed DTI scalars can be used to evaluate changes in the brain tissue caused by disease, disease progression, and treatment responses, which has led to an enormous amount of interest in DTI in clinical research. This review article provides insights into DTI scalars and the biological background of DTI as a relatively new neuroimaging modality. Further, it summarizes the clinical role of DTI in various disease processes such as amyotrophic lateral sclerosis, multiple sclerosis, Parkinson’s disease, Alzheimer’s dementia, epilepsy, ischemic stroke, stroke with motor or language impairment, traumatic brain injury, spinal cord injury, and depression. Valuable DTI postprocessing tools for clinical research are also introduced.DTI is an emerging neuroimaging method for identifying microstructural changes. Various DTI scalars—such as FA, AD, RD, MD, and MO—can be correlated with clinical information in order to reveal abnormalities associated with neurological diseases. Apart from selecting appropriate DTI scalars, sophisticated clinical research also requires suitable DTI postprocessing tools. Advanced robust postprocessing techniques have yielded novel anatomical and structural pathway information about the brain. Improvements in DTI acquisition techniques such as shorter scanning times (to reduce the effects of head motion), high spatial and gradient-direction resolutions, higher signal-to-noise ratios, and standardization of postprocessing methods will guarantee the utilization of DTI in clinical research and even as a diagnostic tool.ee i after suffering an infarction in the right corona Fig. 3. Diffusion-tensor tractography in a patient (a female aged 74 years) with left hemiparesis radiata (arrow in lower left figure) and basal ganglia (arrow in upper right figure). The right CST exhibited marked decreases in the number of fibers [n=234 on the right (blue) and n=876 on the left (red)] and in fractional anisotropy (0.4267 and 0.5483, respectively), but the continuity of the right CST was preserved throughout its course. CST: corticospinal tract, L: left, R: right.Fig. 2. Frontal-habenula-cerebellar and frontal-cerebellar tracts. The long fiber pathway connecting the frontal cortex via the habenula to the cerebellum (left) and the frontal-cerebellar tracts (right) were tracked using 3-1, 64-direction diffusion-tensor imaging data, analyzed using DSI Studio software.Tensor Fig. 1. DTI scalar images derived from diffusion-tensor images with 20 gradient directions. The FA is a DTI scalar that represents axonal integrity and is strongly related to fiber integrity. The AD is related to axonal damage. The RD is probably a DTI marker of myelin, with an increased RD value sugges- tive of myelin damage in white matter tissue. The MD is a measure of the average molecular motion. The size and integrity of cells affects the MD, which is known to be related to necrosis, edema, and cellularity. The MO is a probabilistic tractography measure for crossing white matter fibers. AD: axial diffusivity, DTI: diffusion-tensor imaging, FA: fractional anisotropy, MD: mean diffusivity, MO: mode, RD: radial diffusivity.Fig. 4. Three levels of damage to the left AF. DTI of the AF (left column) and T2-weighted magnetic resonance images (right column) show three types of AF (blue, right AF; red, left AF) categoriz to the severity of da pe A, fibers of the ely damaged and thus are not visualized in DTI reconstructions. In type B, the AF is disrupted between Wernicke's and Broca's areas. In type C, the AF is preserved around the brain lesion. The arrow indicates disruption of the left AF around the stroke lesion. AF: arcuate fasciculus, DTI: diffusion-tensor imaging,",Medical Data Analysis,"This review article provides an overview of diffusion-tensor imaging (DTI) as a noninvasive medical imaging tool used to investigate the structure of white matter. DTI scalars, such as FA, AD, RD, MD, and MO, can be used to evaluate changes in brain tissue caused by various neurological diseases, including amyotrophic lateral sclerosis, multiple sclerosis, Parkinson’s disease, Alzheimer’s dementia, epilepsy, ischemic stroke, traumatic brain injury, spinal cord injury, and depression. The review also highlights the importance of suitable DTI postprocessing tools for clinical research, including advanced robust postprocessing techniques that yield novel anatomical and structural pathway information about the brain. The article concludes that improvements in DTI acquisition techniques and standardization of postprocessing methods will ensure the utilization of DTI in clinical research and even as a diagnostic tool.",Medical Data Analysis,"ee i after suffering an infarction in the right corona Fig. 3. Diffusion-tensor tractography in a patient (a female aged 74 years) with left hemiparesis radiata (arrow in lower left figure) and basal ganglia (arrow in upper right figure). The right CST exhibited marked decreases in the number of fibers [n=234 on the right (blue) and n=876 on the left (red)] and in fractional anisotropy (0.4267 and 0.5483, respectively), but the continuity of the right CST was preserved throughout its course. CST: corticospinal tract, L: left, R: right.Fig. 2. Frontal-habenula-cerebellar and frontal-cerebellar tracts. The long fiber pathway connecting the frontal cortex via the habenula to the cerebellum (left) and the frontal-cerebellar tracts (right) were tracked using 3-1, 64-direction diffusion-tensor imaging data, analyzed using DSI Studio software.Tensor Fig. 1. DTI scalar images derived from diffusion-tensor images with 20 gradient directions. The FA is a DTI scalar that represents axonal integrity and is strongly related to fiber integrity. The AD is related to axonal damage. The RD is probably a DTI marker of myelin, with an increased RD value sugges- tive of myelin damage in white matter tissue. The MD is a measure of the average molecular motion. The size and integrity of cells affects the MD, which is known to be related to necrosis, edema, and cellularity. The MO is a probabilistic tractography measure for crossing white matter fibers. AD: axial diffusivity, DTI: diffusion-tensor imaging, FA: fractional anisotropy, MD: mean diffusivity, MO: mode, RD: radial diffusivity.Fig. 4. Three levels of damage to the left AF. DTI of the AF (left column) and T2-weighted magnetic resonance images (right column) show three types of AF (blue, right AF; red, left AF) categoriz to the severity of da pe A, fibers of the ely damaged and thus are not visualized in DTI reconstructions. In type B, the AF is disrupted between Wernicke's and Broca's areas. In type C, the AF is preserved around the brain lesion. The arrow indicates disruption of the left AF around the stroke lesion. AF: arcuate fasciculus, DTI: diffusion-tensor imaging,",Medical Data Analysis
21,Cyclophosphamide treatment for idiopathic inflammatory myopathies and related interstitial lung disease: a systematic review,"Cyclophosphamide , Idiopathic inflammatory myopathy , Interstitial lung disease .","The purpose of this study is to review and summarize published information on the use, effectiveness, and adverse effects of cyclophosphamide (CYC) in the management of idiopathic inflammatory myopathies (IIM) and IIM-related interstitial lung disease (IIM-ILD). We performed a systematic search on various databases from May 1975 to May 2014 to find articles concerning CYC therapy in IIM and IIM-ILD. The initial search involved 310 articles, and the 12 articles that met the study criteria were analyzed in detail. All studies were non-randomized. Intravenous CYC (IVCYC) was administered as treatment for IIM in 11 of the studies. Additionally, eight of the twelve studies assessed the effect of CYC in developing resistance steroids or in refractory IIM. IVCYC pulses of 0.3–1.0 g/m2 or 10–30 mg/kg were applied at weekly to monthly intervals for 6–12 months together with either glucocorticoids or another immunosuppressive agent. According to a comprehensive analysis of the studies, 80.8 % (42/52) and 73.1 % (38/52) of patients showed improvement in muscle strength and function. The CK levels of 87.5 % (35/ 40) of patients fell. The forced vital capacity (FVC) and diffusing capacity for carbon monoxide (DLCO) improved in 57.6 % (34/59) and 64.3 % (27/42) of patients. The high resolution computed tomography (HRCT) findings improved in 67.3 % (35/52) of patients. IVCYC treatment allowed 58.1 % (25/43) of acute/subacute IIM-ILD patients to survive. However, 18 patients died, and the histopathological findings revealed that the 12 deaths were due to diffuse alveolar damage (DAD). HRCT revealed a ground glass (GrG) pattern in 66.7 % (12/18) of the deaths. Of the patients who died, 70 % (7/10) had pneumomediastinum. IVCYC seems to improve both muscle strength and function and lung function in refractory IIM and IIM-ILD patients, and it appears to be relatively well tolerated and safe.","From the available data, CYC treatment of patients with IIM appears to result in improvements in muscle strength, muscle function, CK levels, pulmonary function, and HRCT lung images. CYC treatment also seems to improve survival rates among patients with acute or subacute ILD. CYC may be an effective immunomodulatory agent in the management of IIM and IIM-related ILD. However, this conclusion needs to be confirmed by large-sample, double-blind, placebo-controlled studies.","Cyclophosphamide treatment for idiopathic inflammatory myopathies and related interstitial lung disease: a systematic reviewCyclophosphamide , Idiopathic inflammatory myopathy , Interstitial lung disease .The purpose of this study is to review and summarize published information on the use, effectiveness, and adverse effects of cyclophosphamide (CYC) in the management of idiopathic inflammatory myopathies (IIM) and IIM-related interstitial lung disease (IIM-ILD). We performed a systematic search on various databases from May 1975 to May 2014 to find articles concerning CYC therapy in IIM and IIM-ILD. The initial search involved 310 articles, and the 12 articles that met the study criteria were analyzed in detail. All studies were non-randomized. Intravenous CYC (IVCYC) was administered as treatment for IIM in 11 of the studies. Additionally, eight of the twelve studies assessed the effect of CYC in developing resistance steroids or in refractory IIM. IVCYC pulses of 0.3–1.0 g/m2 or 10–30 mg/kg were applied at weekly to monthly intervals for 6–12 months together with either glucocorticoids or another immunosuppressive agent. According to a comprehensive analysis of the studies, 80.8 % (42/52) and 73.1 % (38/52) of patients showed improvement in muscle strength and function. The CK levels of 87.5 % (35/ 40) of patients fell. The forced vital capacity (FVC) and diffusing capacity for carbon monoxide (DLCO) improved in 57.6 % (34/59) and 64.3 % (27/42) of patients. The high resolution computed tomography (HRCT) findings improved in 67.3 % (35/52) of patients. IVCYC treatment allowed 58.1 % (25/43) of acute/subacute IIM-ILD patients to survive. However, 18 patients died, and the histopathological findings revealed that the 12 deaths were due to diffuse alveolar damage (DAD). HRCT revealed a ground glass (GrG) pattern in 66.7 % (12/18) of the deaths. Of the patients who died, 70 % (7/10) had pneumomediastinum. IVCYC seems to improve both muscle strength and function and lung function in refractory IIM and IIM-ILD patients, and it appears to be relatively well tolerated and safe.From the available data, CYC treatment of patients with IIM appears to result in improvements in muscle strength, muscle function, CK levels, pulmonary function, and HRCT lung images. CYC treatment also seems to improve survival rates among patients with acute or subacute ILD. CYC may be an effective immunomodulatory agent in the management of IIM and IIM-related ILD. However, this conclusion needs to be confirmed by large-sample, double-blind, placebo-controlled studies.310 potentially relevant references were identified and screened for retrieval Excluded 267 on the basis of title, abstract 43 potentially relevant publications ——-| Excluded 6 studies because full text unavailable 37 potentially relevant publications screened for more detailed assessment Excluded 25 studies: 5 reviews, 20 case reports 12 potentially appropriate studies selected for review Fig. 1 Search strategy",Medical Data Analysis,"This study provides a systematic review of published information on the use, effectiveness, and adverse effects of cyclophosphamide (CYC) in the management of idiopathic inflammatory myopathies (IIM) and IIM-related interstitial lung disease (IIM-ILD). The study analyzed 12 non-randomized studies that used intravenous CYC (IVCYC) in 11 of the studies to treat IIM. The analysis showed that IVCYC treatment improved muscle strength and function, CK levels, pulmonary function, and HRCT lung images in refractory IIM and IIM-ILD patients. It also improved survival rates among patients with acute or subacute ILD. However, there were adverse effects, including 18 deaths due to diffuse alveolar damage (DAD), with a ground glass (GrG) pattern found in 66.7% of the deaths. The conclusion suggests that CYC may be an effective immunomodulatory agent in managing IIM and IIM-related ILD, but large-sample, double-blind, placebo-controlled studies are needed to confirm this conclusion.",Medical Data Analysis,"310 potentially relevant references were identified and screened for retrieval Excluded 267 on the basis of title, abstract 43 potentially relevant publications ——-| Excluded 6 studies because full text unavailable 37 potentially relevant publications screened for more detailed assessment Excluded 25 studies: 5 reviews, 20 case reports 12 potentially appropriate studies selected for review Fig. 1 Search strategy",Medical Data Analysis
22,Data Mining for Wearable Sensors in Health Monitoring Systems: A Review of Recent Trends and Challenges,data mining; wearable sensors; healthcare; physiological sensors; health monitoring system; machine learning technique; vital signs; medical informatics,"The past few years have witnessed an increase in the development of wearable sensors for health monitoring systems. This increase has been due to several factors such as development in sensor technology as well as directed efforts on political and stakeholder levels to promote projects which address the need for providing new methods for care given increasing challenges with an aging population. An important aspect of study in such system is how the data is treated and processed. This paper provides a recent review of the latest methods and algorithms used to analyze data from wearable sensors used for physiological monitoring of vital signs in healthcare services. In particular, the paper outlines the more common data mining tasks that have been applied such as anomaly detection, prediction and decision making when considering in particular continuous time series measurements. Moreover, the paper further details the suitability of particular data mining and machine learning methods used to process the physiological data and provides an overview of the properties of the data sets used in experimental validation. Finally, based on this literature review, a number of key challenges have been outlined for data mining methods in health monitoring systems.","The aim of this study was to provide an overview of recent data mining techniques applied to wearable sensor data in the healthcare domain. This article has attempted to clarify how certain data mining methods have been applied in the literature. It also has revealed trends in the selection of the data processing methods in order to monitor health parameters such as ECG, RR, HR, BP and BG. Finally, particular attention was given to elicit the current challenges of using data processing approaches in the health monitoring systems. For this reason, this paper surveys several solutions provided by healthcare services by considering distinguished aspects. Namely this paper includes (1) data mining tasks for wearable sensors (2) data mining approach and (3) data sets and their properties. In particular, the review outlined the more common data mining tasks that have been applied such as anomaly detection, prediction and decision making when considering in particular continuous time series measurements. Moreover, further details of the suitability of particular data mining methods used to process the wearable sensor data such as SVM, NN and RBR has been described. Further study in this review paper focused on the sensors data sets features and properties such as time horizon, scale and labeling. Finally, the paper addressed future challenges of data mining while analyzing the wearable sensors in healthcare.","Data Mining for Wearable Sensors in Health Monitoring Systems: A Review of Recent Trends and Challengesdata mining; wearable sensors; healthcare; physiological sensors; health monitoring system; machine learning technique; vital signs; medical informaticsThe past few years have witnessed an increase in the development of wearable sensors for health monitoring systems. This increase has been due to several factors such as development in sensor technology as well as directed efforts on political and stakeholder levels to promote projects which address the need for providing new methods for care given increasing challenges with an aging population. An important aspect of study in such system is how the data is treated and processed. This paper provides a recent review of the latest methods and algorithms used to analyze data from wearable sensors used for physiological monitoring of vital signs in healthcare services. In particular, the paper outlines the more common data mining tasks that have been applied such as anomaly detection, prediction and decision making when considering in particular continuous time series measurements. Moreover, the paper further details the suitability of particular data mining and machine learning methods used to process the physiological data and provides an overview of the properties of the data sets used in experimental validation. Finally, based on this literature review, a number of key challenges have been outlined for data mining methods in health monitoring systems.The aim of this study was to provide an overview of recent data mining techniques applied to wearable sensor data in the healthcare domain. This article has attempted to clarify how certain data mining methods have been applied in the literature. It also has revealed trends in the selection of the data processing methods in order to monitor health parameters such as ECG, RR, HR, BP and BG. Finally, particular attention was given to elicit the current challenges of using data processing approaches in the health monitoring systems. For this reason, this paper surveys several solutions provided by healthcare services by considering distinguished aspects. Namely this paper includes (1) data mining tasks for wearable sensors (2) data mining approach and (3) data sets and their properties. In particular, the review outlined the more common data mining tasks that have been applied such as anomaly detection, prediction and decision making when considering in particular continuous time series measurements. Moreover, further details of the suitability of particular data mining methods used to process the wearable sensor data such as SVM, NN and RBR has been described. Further study in this review paper focused on the sensors data sets features and properties such as time horizon, scale and labeling. Finally, the paper addressed future challenges of data mining while analyzing the wearable sensors in healthcare.Figure 6. The distribution of works on three sensor data properties (continuous/discrete, labeling, single sensor/multi sensors), with the relation to three types of data acquisition. 2 © Simulated data Bi Clinical / online database IB Experimental 2 wearable sensor data 20 Number of Papers continuous unlabeled single sensor discrete annotated multi sensorFigure 5. The distribution of works on two sensor data properties (time horizon and scale), with the relation to three types of data acquisition. a ©) Simulated data 8 Clinical / online database i Experimental wearable sensor data 30 Number of papers long term short term large scale small scaleNumber of Papers a Figure 4. The outline of the application of data mining methods in relation to the vital signs measuring with wearable sensors. Msvm NN © Decision tree cum Hum @ Rule Based i Statistical 1 Frequency / Wavelet EcG PPG BG spo2 HR RR BP Wearable SensorFigure 3. A generic architecture of the main data mining approach for wearable sensor data. - Expert Knowledge = ___ - Metadata : Modeling / U scaicenesnanuish enscetndinned : Learning = *s, Sensor data (train) <_ applying ‘S \model . . . . . “a Detection / a ae Prediction / Decision Making Sensor data (test)Figure 1. A schematic overview of the position of the main data mining tasks (anomaly detection, prediction, and diagnosis/decision making) in relation to the different aspects of wearable sensing in the health monitoring systems. Health Patient Anomaly detection Online Alarm Prediction Diagnosis / Decision making Offline Home / Remote ClinicalWearable sensor data Figure 2. The outline of the distribution of three data mining tasks in relation to the vital signs measured by wearable sensors considering in this review. ECG spo2 HR PPG BG RR BP 6 12 Number of Papers i Anomaly detection i Prediction 1B Diagnosis/ Decision making 24",Deep Learning and Machine Learning,"The paper reviews recent developments in wearable sensors for health monitoring and provides an overview of the latest data mining techniques used to analyze data from such sensors for physiological monitoring. The paper outlines common data mining tasks such as anomaly detection, prediction, and decision making in continuous time series measurements, and describes the suitability of particular data mining and machine learning methods used to process physiological data. The review also highlights key challenges for data mining methods in health monitoring systems, and discusses data sets and their properties, including time horizon, scale, and labeling. The study concludes by addressing future challenges in data mining for wearable sensors in healthcare.",Deep Learning and Machine Learning,"Figure 6. The distribution of works on three sensor data properties (continuous/discrete, labeling, single sensor/multi sensors), with the relation to three types of data acquisition. 2 © Simulated data Bi Clinical / online database IB Experimental 2 wearable sensor data 20 Number of Papers continuous unlabeled single sensor discrete annotated multi sensorFigure 5. The distribution of works on two sensor data properties (time horizon and scale), with the relation to three types of data acquisition. a ©) Simulated data 8 Clinical / online database i Experimental wearable sensor data 30 Number of papers long term short term large scale small scaleNumber of Papers a Figure 4. The outline of the application of data mining methods in relation to the vital signs measuring with wearable sensors. Msvm NN © Decision tree cum Hum @ Rule Based i Statistical 1 Frequency / Wavelet EcG PPG BG spo2 HR RR BP Wearable SensorFigure 3. A generic architecture of the main data mining approach for wearable sensor data. - Expert Knowledge = ___ - Metadata : Modeling / U scaicenesnanuish enscetndinned : Learning = *s, Sensor data (train) <_ applying ‘S \model . . . . . “a Detection / a ae Prediction / Decision Making Sensor data (test)Figure 1. A schematic overview of the position of the main data mining tasks (anomaly detection, prediction, and diagnosis/decision making) in relation to the different aspects of wearable sensing in the health monitoring systems. Health Patient Anomaly detection Online Alarm Prediction Diagnosis / Decision making Offline Home / Remote ClinicalWearable sensor data Figure 2. The outline of the distribution of three data mining tasks in relation to the vital signs measured by wearable sensors considering in this review. ECG spo2 HR PPG BG RR BP 6 12 Number of Papers i Anomaly detection i Prediction 1B Diagnosis/ Decision making 24",Deep Learning and Machine Learning
23,Data mining techniques utilizing latent class models to evaluate emergency department revisits,"Hidden Markov Models , Emergency department revisit , Health information exchange , Electronic health records , Predictive analytics .","The use of machine learning techniques is especially pertinent to the composite and challenging conditions of emergency departments (EDs). Repeat ED visits (i.e. revisits) are an example of potentially inappropriate utilization of resources that can be forecasted by these techniques. To track the ED revisit risk over time using the hidden Markov model (HMM) as a major latent class model. Given the HMM states, we carried out forecasting of future ED revisits with various data mining models. Information integrated from four distributed sources (e.g. electronic health records and health information exchange) was integrated into four HMMs which capture the relationships between an observed and a hidden progression that shift over time through a series of hidden states in an adult patient population. Assimilating a pre-analysis of the various patients by applying latent class models and directing them to well-known classifiers functioned well. The performance was significantly better than without utilizing pre-analysis of HMM for all prediction models (classifiers(. Conclusions: These findings suggest that one prospective approach to advanced risk prediction is to leverage the longitudinal nature of health care data by exploiting patients’ between state variation.","These findings strengthen the claim that one prospective approach to advanced risk prediction is to leverage the longitudinal nature of health care delivery. This study makes several contributions. From a methodological perspective, we suggest assimilating a pre-analysis of patient data by applying latent class models and then applying well-known classifiers machine learning methods. This allowed us to increase the capabilities of our predictors/variables. We showed that performance was significantly better than without utilizing pre-analysis of HMM for all prediction models. We extended previous works on HMM in two ways. First, we conducted a subsequent prediction stage based on the HMM states. Second, we implemented four HMM models, each of which belongs to separate class of medical information. Future research should test more latent class models to generalize our methodological contribution since our work was only associated with one latent model, although it is very well known. From a practical perspective, we showed that leveraging patients' longitudinal data and utilizing the information integrated from many distributed sources (e.g. EHRs and HIE) can lead to conditions that enhance risk prediction at the ED point of care. Future research should study more points of care in addition to the ED.","Data mining techniques utilizing latent class models to evaluate emergency department revisitsHidden Markov Models , Emergency department revisit , Health information exchange , Electronic health records , Predictive analytics .The use of machine learning techniques is especially pertinent to the composite and challenging conditions of emergency departments (EDs). Repeat ED visits (i.e. revisits) are an example of potentially inappropriate utilization of resources that can be forecasted by these techniques. To track the ED revisit risk over time using the hidden Markov model (HMM) as a major latent class model. Given the HMM states, we carried out forecasting of future ED revisits with various data mining models. Information integrated from four distributed sources (e.g. electronic health records and health information exchange) was integrated into four HMMs which capture the relationships between an observed and a hidden progression that shift over time through a series of hidden states in an adult patient population. Assimilating a pre-analysis of the various patients by applying latent class models and directing them to well-known classifiers functioned well. The performance was significantly better than without utilizing pre-analysis of HMM for all prediction models (classifiers(. Conclusions: These findings suggest that one prospective approach to advanced risk prediction is to leverage the longitudinal nature of health care data by exploiting patients’ between state variation.These findings strengthen the claim that one prospective approach to advanced risk prediction is to leverage the longitudinal nature of health care delivery. This study makes several contributions. From a methodological perspective, we suggest assimilating a pre-analysis of patient data by applying latent class models and then applying well-known classifiers machine learning methods. This allowed us to increase the capabilities of our predictors/variables. We showed that performance was significantly better than without utilizing pre-analysis of HMM for all prediction models. We extended previous works on HMM in two ways. First, we conducted a subsequent prediction stage based on the HMM states. Second, we implemented four HMM models, each of which belongs to separate class of medical information. Future research should test more latent class models to generalize our methodological contribution since our work was only associated with one latent model, although it is very well known. From a practical perspective, we showed that leveraging patients' longitudinal data and utilizing the information integrated from many distributed sources (e.g. EHRs and HIE) can lead to conditions that enhance risk prediction at the ED point of care. Future research should study more points of care in addition to the ED.Boosted Decision Tree With vs. Without HMM (AUC Levels) e S 0.00 0.25 0.75 1.00 0.50 1-Specificity —-— BDT Without HMM- AUC: 0.7275 —— BDT With HMM- AUC: 0.7569 Fig. 2. Integrating HMM states as a pre-stage for prediction (AUC Levels).The Hidden States: SHigh- High ED Revisit risk Stow- Low ED Revisit risk PH_RED=1 Pi_rep-o0 n—any period The Observation Variable: ED Revisit Results: 1-Yes =ED Revisit 0- no =no ED Revisit Fig. 1. Illustration of the HMM process and the relationship between the observation, the hidden states and the transition matrix.",Deep Learning and Machine Learning,"This article discusses the use of machine learning techniques to forecast repeat visits to emergency departments (EDs) by utilizing patient data from electronic health records and health information exchange. The study suggests that utilizing patients' longitudinal data and integrating information from distributed sources can enhance risk prediction at the point of care. The study utilizes hidden Markov models (HMMs) to capture the relationships between observed and hidden progressions over time through a series of hidden states, and applies pre-analysis of patient data using latent class models to improve the performance of the prediction models. The article suggests that leveraging patients' longitudinal data is a prospective approach to advanced risk prediction and provides a methodological and practical contribution to the field. Future research should explore the application of these techniques to other points of care besides the ED and test additional latent class models to generalize the findings.",Medical Data Analysis,"Boosted Decision Tree With vs. Without HMM (AUC Levels) e S 0.00 0.25 0.75 1.00 0.50 1-Specificity —-— BDT Without HMM- AUC: 0.7275 —— BDT With HMM- AUC: 0.7569 Fig. 2. Integrating HMM states as a pre-stage for prediction (AUC Levels).The Hidden States: SHigh- High ED Revisit risk Stow- Low ED Revisit risk PH_RED=1 Pi_rep-o0 n—any period The Observation Variable: ED Revisit Results: 1-Yes =ED Revisit 0- no =no ED Revisit Fig. 1. Illustration of the HMM process and the relationship between the observation, the hidden states and the transition matrix.",Medical Data Analysis
24,Deep Learning and Neurology: A Systematic Review,Artificial intelligence; Biomedical informatics; Computer vision; Connectome mapping; Deep learning; Genomics; Machine learning; Neurology; Neuroscience,"Deciphering the massive volume of complex electronic data that has been compiled by hospital systems over the past decades has the potential to revolutionize modern medicine, as well as present significant challenges. Deep learning is uniquely suited to address these challenges, and recent advances in techniques and hardware have poised the field of medical machine learning for transformational growth. The clinical neurosciences are particularly well positioned to benefit from these advances given the subtle presentation of symptoms typical of neurologic disease. Here we review the various domains in which deep learning algorithms have already provided impetus for change—areas such as medical image analysis for the improved diagnosis of Alzheimer’s disease and the early detection of acute neurologic events; medical image segmentation for quantitative evaluation of neuroanatomy and vasculature; connectome mapping for the diagnosis of Alzheimer’s, autism spectrum disorder, and attention deficit hyperactivity disorder; and mining of microscopic electroencephalogram signals and granular genetic signatures. We additionally note important challenges in the integration of deep learning tools in the clinical setting and discuss the barriers to tackling the challenges that currently exist.","Deep learning has the potential to fundamentally alter the practice of medicine. The clinical neurosciences in particular are uniquely situated to benefit given the subtle presentation of symptoms typical of neurologic disease. Here, we reviewed the various domains in which deep learning algorithms have already provided impetus for change—areas such as medical image analysis for improved diagnosis of AD and the early detection of acute neurologic events; medical image segmentation for quantitative evaluation of neuroanatomy and vasculature; connectome mapping for the diagnosis of AD, ASD, and ADHD; and mining of microscopic EEG signals and granular genetic signatures. Amidst these advances, however, important challenges remain a barrier to integration of deep learning tools in the clinical setting. While technical challenges surrounding the generalizability and interpretability of models are active areas of research and progress, more difficult challenges surrounding data privacy, accessibility, and ownership will necessitate conversations in the healthcare environment and society in general to arrive at solutions that benefit all relevant stakeholders. The challenge of data quality, in particular, may prove to be a uniquely suitable target for addressing using deep learning techniques that have already demonstrated efficacy in image analysis and natural language processing. Overcoming these hurdles will require the efforts of interdisciplinary teams of physicians, computer scientists, engineers, legal experts, and ethicists working in concert. It is only in this manner that we will truly realize the potential of deep learning in medicine to augment the capability of physicians and enhance the delivery of care to patients.","Deep Learning and Neurology: A Systematic ReviewArtificial intelligence; Biomedical informatics; Computer vision; Connectome mapping; Deep learning; Genomics; Machine learning; Neurology; NeuroscienceDeciphering the massive volume of complex electronic data that has been compiled by hospital systems over the past decades has the potential to revolutionize modern medicine, as well as present significant challenges. Deep learning is uniquely suited to address these challenges, and recent advances in techniques and hardware have poised the field of medical machine learning for transformational growth. The clinical neurosciences are particularly well positioned to benefit from these advances given the subtle presentation of symptoms typical of neurologic disease. Here we review the various domains in which deep learning algorithms have already provided impetus for change—areas such as medical image analysis for the improved diagnosis of Alzheimer’s disease and the early detection of acute neurologic events; medical image segmentation for quantitative evaluation of neuroanatomy and vasculature; connectome mapping for the diagnosis of Alzheimer’s, autism spectrum disorder, and attention deficit hyperactivity disorder; and mining of microscopic electroencephalogram signals and granular genetic signatures. We additionally note important challenges in the integration of deep learning tools in the clinical setting and discuss the barriers to tackling the challenges that currently exist.Deep learning has the potential to fundamentally alter the practice of medicine. The clinical neurosciences in particular are uniquely situated to benefit given the subtle presentation of symptoms typical of neurologic disease. Here, we reviewed the various domains in which deep learning algorithms have already provided impetus for change—areas such as medical image analysis for improved diagnosis of AD and the early detection of acute neurologic events; medical image segmentation for quantitative evaluation of neuroanatomy and vasculature; connectome mapping for the diagnosis of AD, ASD, and ADHD; and mining of microscopic EEG signals and granular genetic signatures. Amidst these advances, however, important challenges remain a barrier to integration of deep learning tools in the clinical setting. While technical challenges surrounding the generalizability and interpretability of models are active areas of research and progress, more difficult challenges surrounding data privacy, accessibility, and ownership will necessitate conversations in the healthcare environment and society in general to arrive at solutions that benefit all relevant stakeholders. The challenge of data quality, in particular, may prove to be a uniquely suitable target for addressing using deep learning techniques that have already demonstrated efficacy in image analysis and natural language processing. Overcoming these hurdles will require the efforts of interdisciplinary teams of physicians, computer scientists, engineers, legal experts, and ethicists working in concert. It is only in this manner that we will truly realize the potential of deep learning in medicine to augment the capability of physicians and enhance the delivery of care to patients.+ 4 2 Convolutional Neural | Recurrent Neural General Adversarial Networks Networks Networks T oon eeee nnn eeet jooeen ne te--------5 Survival Convolutional Long-Short Term Neural N M Gated Recurrent Units Fig. 2 Breakdown of algorithm types in the machi",Deep Learning and Machine Learning,"The article discusses how deep learning can revolutionize modern medicine by analyzing the massive volume of electronic data compiled by hospital systems. The clinical neurosciences are particularly well positioned to benefit from deep learning algorithms due to the subtle presentation of symptoms typical of neurologic disease. The article reviews the various domains in which deep learning algorithms have already provided impetus for change, including medical image analysis, connectome mapping, and mining of microscopic EEG signals and granular genetic signatures. However, important challenges remain a barrier to integration of deep learning tools in the clinical setting, such as data privacy, accessibility, ownership, and data quality. The article concludes that interdisciplinary teams of physicians, computer scientists, engineers, legal experts, and ethicists working in concert can overcome these hurdles to truly realize the potential of deep learning in medicine to augment the capability of physicians and enhance patient care delivery.",Deep Learning and Machine Learning,+ 4 2 Convolutional Neural | Recurrent Neural General Adversarial Networks Networks Networks T oon eeee nnn eeet jooeen ne te--------5 Survival Convolutional Long-Short Term Neural N M Gated Recurrent Units Fig. 2 Breakdown of algorithm types in the machi,Deep Learning and Machine Learning
25,Deep Learning Applications in Medical Image Analysis,"Convolutional neural networks, medical image analysis, machine learning, deep learning.","The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.","A recurring theme in machine learning is the limit imposed by the lack of labelled datasets, which hampers training and task performance. Conversely, it is acknowledged that more data improves performance, as Sun et al. shows using an internal Google dataset of 300 million images. In general computer vision tasks, attempts have been made to circumvent limited data by using smaller filters on deeper layers, with novel CNN architecture combinations, or hyperparameter optimization. In medical image analysis, the lack of data is two-fold and more acute: there is general lack of publicly available data, and high quality labelled data is even more scarce. Most of the datasets presented in this review involve fewer than 100 patients. Yet the situation may not be as dire as it seems, as despite the small training datasets, the papers in this review report relatively satisfactory performance in the various tasks. The question of how many images are necessary for training in medical image analysis was partially answered by Cho et al. [88]. He ascertained the accuracy of a CNN with GoogLeNet architecture in classifying individual axial CT images into one of 6 body regions: brain, neck, shoulder, chest, abdomen, pelvis. With 200 training images, accuracies of 88-98% were achieved on a test set of 6000 images. While categorization into various body regions is not a realistic medical image analysis task, his report does suggest that the problem may be surmountable. Being able to accomplish classification with a small dataset is possibly due to the general intrinsic image homogeneity across different patients, as opposed to the near-infinite variety of natural images, such as a dog in various breeds, colors and poses. The traditional applications for medical image analysis were discussed in Section 3. New areas of research include prognostication, content-based image retrieval, image report or caption generation and manipulation of physical objects with LSTMs and reinforcement learning involving surgical robots. An interesting application was reported by Nie et al. , in which GANs were used to generate CT brain images from MRI images. This is remarkable, as it means that patients can potentially avoid the ionizing radiation from a CT scanner altogether, lowering cost and improving patient safety. Nie also exploited the ability of GANs to generate improved, higher resolution images from native images [108] and reduced the blurriness in the CT images.","Deep Learning Applications in Medical Image AnalysisConvolutional neural networks, medical image analysis, machine learning, deep learning.The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.A recurring theme in machine learning is the limit imposed by the lack of labelled datasets, which hampers training and task performance. Conversely, it is acknowledged that more data improves performance, as Sun et al. shows using an internal Google dataset of 300 million images. In general computer vision tasks, attempts have been made to circumvent limited data by using smaller filters on deeper layers, with novel CNN architecture combinations, or hyperparameter optimization. In medical image analysis, the lack of data is two-fold and more acute: there is general lack of publicly available data, and high quality labelled data is even more scarce. Most of the datasets presented in this review involve fewer than 100 patients. Yet the situation may not be as dire as it seems, as despite the small training datasets, the papers in this review report relatively satisfactory performance in the various tasks. The question of how many images are necessary for training in medical image analysis was partially answered by Cho et al. [88]. He ascertained the accuracy of a CNN with GoogLeNet architecture in classifying individual axial CT images into one of 6 body regions: brain, neck, shoulder, chest, abdomen, pelvis. With 200 training images, accuracies of 88-98% were achieved on a test set of 6000 images. While categorization into various body regions is not a realistic medical image analysis task, his report does suggest that the problem may be surmountable. Being able to accomplish classification with a small dataset is possibly due to the general intrinsic image homogeneity across different patients, as opposed to the near-infinite variety of natural images, such as a dog in various breeds, colors and poses. The traditional applications for medical image analysis were discussed in Section 3. New areas of research include prognostication, content-based image retrieval, image report or caption generation and manipulation of physical objects with LSTMs and reinforcement learning involving surgical robots. An interesting application was reported by Nie et al. , in which GANs were used to generate CT brain images from MRI images. This is remarkable, as it means that patients can potentially avoid the ionizing radiation from a CT scanner altogether, lowering cost and improving patient safety. Nie also exploited the ability of GANs to generate improved, higher resolution images from native images [108] and reduced the blurriness in the CT images.hon ay D h —» Fake h —> Real ty NV Generator Discriminator my FIGURE 3. Various neural network architectures. A. Recurrent neural network, B. Autoencoder, C. Restricted Boltzmann Machine, D. Deep Belief Network, E. Generative Adversarial Network. x, y, h represent input, output and hidden layers respectively.Input image Convolution RELU Pooling Fully conected Output layer layer layer layer classes FIGURE 2. In this example disease classification task, an input image of an abnormal axial slice of a T2-weighted MRI brain is run through a schematic depiction of a CNN. Feature extraction of the input image is performed via the Convolution, RELU and pooling layers, before classification by the fully connected layer.FIGURE 1. A collage of images depicting medical images, from left to right, top to bottom: CT brain scan with a left-sided hemorrhagic stroke, an axial MRI brain scan wi left-sided brain tumor, a normal chest X-ray, a normal axial CT lung scan, and a histology slide with high grade glioma (a brain tumor).",Deep Learning and Machine Learning,"This review discusses the application of machine learning algorithms, particularly convolutional neural networks, in medical image analysis. It highlights the advantage of machine learning in discovering hierarchal relationships within medical big data without laborious hand-crafting of features. The review covers various research areas and applications of medical image classification, localization, detection, segmentation, and registration. However, the lack of publicly available and high-quality labeled data is a major challenge in medical image analysis. Despite this, satisfactory performance is reported in various tasks with small training datasets. The review concludes by discussing research obstacles, emerging trends, and possible future directions, including new areas of research such as prognostication, content-based image retrieval, and manipulation of physical objects with LSTMs and reinforcement learning. An interesting application involves the use of GANs to generate CT brain images from MRI images, potentially avoiding ionizing radiation from a CT scanner altogether and improving patient safety.",Deep Learning and Machine Learning,"hon ay D h —» Fake h —> Real ty NV Generator Discriminator my FIGURE 3. Various neural network architectures. A. Recurrent neural network, B. Autoencoder, C. Restricted Boltzmann Machine, D. Deep Belief Network, E. Generative Adversarial Network. x, y, h represent input, output and hidden layers respectively.Input image Convolution RELU Pooling Fully conected Output layer layer layer layer classes FIGURE 2. In this example disease classification task, an input image of an abnormal axial slice of a T2-weighted MRI brain is run through a schematic depiction of a CNN. Feature extraction of the input image is performed via the Convolution, RELU and pooling layers, before classification by the fully connected layer.FIGURE 1. A collage of images depicting medical images, from left to right, top to bottom: CT brain scan with a left-sided hemorrhagic stroke, an axial MRI brain scan wi left-sided brain tumor, a normal chest X-ray, a normal axial CT lung scan, and a histology slide with high grade glioma (a brain tumor).",Deep Learning and Machine Learning
26,Deep learning for electronic health records: A comparative review of multiple deep neural architectures,"Deep learning , Representation learning , Neural networks , Electronic health records , CPRD.","Despite the recent developments in deep learning models, their applications in clinical decision-support systems have been very limited. Recent digitalisation of health records, however, has provided a great platform for the assessment of the usability of such techniques in healthcare. As a result, the field is starting to see a growing number of research papers that employ deep learning on electronic health records (EHR) for personalised prediction of risks and health trajectories. While this can be a promising trend, vast paper-to-paper variability (from data sources and models they use to the clinical questions they attempt to answer) have hampered the field’s ability to simply compare and contrast such models for a given application of interest. Thus, in this paper, we aim to provide a comparative review of the key deep learning architectures that have been applied to EHR data. Furthermore, we also aim to: (1) introduce and use one of the world’s largest and most complex linked primary care EHR datasets (i.e., Clinical Practice Research Datalink, or CPRD) as a new asset for training such data-hungry models; (2) provide a guideline for working with EHR data for deep learning; (3) share some of the best practices for assessing the “goodness” of deep-learning models in clinical risk prediction; (4) and propose future research ideas for making deep learning models more suitable for the EHR data. Our results highlight the difficulties of working with highly imbalanced datasets, and show that sequential deep learning architectures such as RNN may be more suitable to deal with the temporal nature of EHR.","The use of DL to analyse EHR data has increased over the past years; a growth that is continuing to be facilitated by the availability of more data (EHR and beyond), developments in DL (specifically, models for sequential data), and innovative ways of combining these two trends. In this work, we implemented key deep learning architectures to learn an efficient patient representation for predicting emergency admission, and heart failure. Our objective here was to help the field have a comparative view of these approaches, and to assess their strengths and weaknesses when it comes to EHR. Along this work, we introduced CPRD, which is one of the world’s largest primary care databases, and showed how data from primary care can provide predictions that can be of value in policy and practice of care. Given the complexity of primary care EHR (heterogeneous events recorded in irregular intervals with varying degree of richness and quality across different individuals), and its importance in provision of care in many parts of the world, we believe that the set of best practices we shared for them (e.g., inclusion criteria, preprocessing, medical codes/grouping, performance metrics, and hyperparameter tuning) will be of great value in helping DL research in EHR. Our work showed the strength of recurrent neural networks in dealing with the temporal nature of the EHR. This was consistent with the developments in modelling EHR-like data from other domains, such as natural language and time series data. Our future research aims to explore techniques and methodologies from such domains, and apply them to other types of data from different healthcare systems.","Deep learning for electronic health records: A comparative review of multiple deep neural architecturesDeep learning , Representation learning , Neural networks , Electronic health records , CPRD.Despite the recent developments in deep learning models, their applications in clinical decision-support systems have been very limited. Recent digitalisation of health records, however, has provided a great platform for the assessment of the usability of such techniques in healthcare. As a result, the field is starting to see a growing number of research papers that employ deep learning on electronic health records (EHR) for personalised prediction of risks and health trajectories. While this can be a promising trend, vast paper-to-paper variability (from data sources and models they use to the clinical questions they attempt to answer) have hampered the field’s ability to simply compare and contrast such models for a given application of interest. Thus, in this paper, we aim to provide a comparative review of the key deep learning architectures that have been applied to EHR data. Furthermore, we also aim to: (1) introduce and use one of the world’s largest and most complex linked primary care EHR datasets (i.e., Clinical Practice Research Datalink, or CPRD) as a new asset for training such data-hungry models; (2) provide a guideline for working with EHR data for deep learning; (3) share some of the best practices for assessing the “goodness” of deep-learning models in clinical risk prediction; (4) and propose future research ideas for making deep learning models more suitable for the EHR data. Our results highlight the difficulties of working with highly imbalanced datasets, and show that sequential deep learning architectures such as RNN may be more suitable to deal with the temporal nature of EHR.The use of DL to analyse EHR data has increased over the past years; a growth that is continuing to be facilitated by the availability of more data (EHR and beyond), developments in DL (specifically, models for sequential data), and innovative ways of combining these two trends. In this work, we implemented key deep learning architectures to learn an efficient patient representation for predicting emergency admission, and heart failure. Our objective here was to help the field have a comparative view of these approaches, and to assess their strengths and weaknesses when it comes to EHR. Along this work, we introduced CPRD, which is one of the world’s largest primary care databases, and showed how data from primary care can provide predictions that can be of value in policy and practice of care. Given the complexity of primary care EHR (heterogeneous events recorded in irregular intervals with varying degree of richness and quality across different individuals), and its importance in provision of care in many parts of the world, we believe that the set of best practices we shared for them (e.g., inclusion criteria, preprocessing, medical codes/grouping, performance metrics, and hyperparameter tuning) will be of great value in helping DL research in EHR. Our work showed the strength of recurrent neural networks in dealing with the temporal nature of the EHR. This was consistent with the developments in modelling EHR-like data from other domains, such as natural language and time series data. Our future research aims to explore techniques and methodologies from such domains, and apply them to other types of data from different healthcare systems.Visit 1 Medical Code Medical Code Code || | Time Gap Fig. 3. Modified Deepr architecture in order to consider demographics and have a fair comparison with the other selected models. EHR are processed through a series of steps that include sequencing, embedding, convolution, pooling and classification. Static information (i.e., demographics) is merged together and passed through a fully connected layer before the final classifi- cation task. Convolution Pooling Dem jographics Concatenation Fully Connected Fully Connected Outcome1985 2012 2013 2014 Training (53%) Training (modular |) models) (7%) Validation (20%) Testing (20%) ww Feature extraction interval Prediction interval (6 months) Fig. 2. Study Design. A total of 4, 272, 833 patients were split into training, validation and test sets. A small portion of the training (7%) was kept apart for the modular deep learning architectures. Red region corresponded to the pre- diction interval, which consisted of the last six months of the respective time window. This region was used to create the two separate outcomes (i.e. whether or not a patient had an emergency admission, or a heart failure during these six months). All data previous to these six months (marked with a dashed line) was used for feature extraction. Patients who died before the corresponding pre- diction interval are excluded from the analysis. (For interpretation of the re- ferences to colour in this figure legend, the reader is referred to the web version of this article.)200 100 Number of Publications 2012 2013 2014 2015 2016 2017 2018 Year Fig. 1. Published studies found on Semantic Scholar (https://www. semanticscholar.org/) through October 2018 using keywords “deep learning” AND (“electronic health records” OR “electronic medical records”) in the title or abstract.Emergency Admission Heart Failure ® ze a a = 9 ® ® 3 DQ = Z = 9 ® Q 3 -20 205 104 P oy = a 04 ¥ ! zs 4104 204 r r t r T 20 10 0 10 20 -20 -10 0 10 20 x Fig. 5. Patient representations obtained by each DL model in the DDM scenario, after projection to 2D using UMAP. Each point corresponds to a patient in the test set; blue and red dots correspond to cases of emergency admission or heart failure, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)Emergency Admission Heart Failure 20 10 ° zZ 0 & = ~10 20 10 5 S 8 3 8 2 i 3 10 > 20 10 S ° 3 -10 p [te 20 10 a fil 0 ~ = 2 -10 “ -20 -10 0 10 20 -20 -10 0 10 20 x Fig. 4. Patient representations obtained by each DL model in the DD scenario, after projection to 2D using UMAP. Each point corresponds to a patient in the test set; blue and red dots correspond to cases of emergency admission or heart failure, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)",Deep Learning and Machine Learning,"The paper discusses the limited applications of deep learning models in clinical decision-support systems despite recent developments in the field. It highlights the growing use of electronic health records (EHR) for personalised prediction of risks and health trajectories and provides a comparative review of key deep learning architectures that have been applied to EHR data. The paper introduces the Clinical Practice Research Datalink (CPRD) as a new asset for training data-hungry models, provides a guideline for working with EHR data for deep learning, and shares best practices for assessing the goodness of deep-learning models in clinical risk prediction. The paper concludes that recurrent neural networks are better suited to deal with the temporal nature of EHR data and proposes future research ideas for making deep learning models more suitable for EHR data from different healthcare systems.",Deep Learning and Machine Learning,"Visit 1 Medical Code Medical Code Code || | Time Gap Fig. 3. Modified Deepr architecture in order to consider demographics and have a fair comparison with the other selected models. EHR are processed through a series of steps that include sequencing, embedding, convolution, pooling and classification. Static information (i.e., demographics) is merged together and passed through a fully connected layer before the final classifi- cation task. Convolution Pooling Dem jographics Concatenation Fully Connected Fully Connected Outcome1985 2012 2013 2014 Training (53%) Training (modular |) models) (7%) Validation (20%) Testing (20%) ww Feature extraction interval Prediction interval (6 months) Fig. 2. Study Design. A total of 4, 272, 833 patients were split into training, validation and test sets. A small portion of the training (7%) was kept apart for the modular deep learning architectures. Red region corresponded to the pre- diction interval, which consisted of the last six months of the respective time window. This region was used to create the two separate outcomes (i.e. whether or not a patient had an emergency admission, or a heart failure during these six months). All data previous to these six months (marked with a dashed line) was used for feature extraction. Patients who died before the corresponding pre- diction interval are excluded from the analysis. (For interpretation of the re- ferences to colour in this figure legend, the reader is referred to the web version of this article.)200 100 Number of Publications 2012 2013 2014 2015 2016 2017 2018 Year Fig. 1. Published studies found on Semantic Scholar (https://www. semanticscholar.org/) through October 2018 using keywords “deep learning” AND (“electronic health records” OR “electronic medical records”) in the title or abstract.Emergency Admission Heart Failure ® ze a a = 9 ® ® 3 DQ = Z = 9 ® Q 3 -20 205 104 P oy = a 04 ¥ ! zs 4104 204 r r t r T 20 10 0 10 20 -20 -10 0 10 20 x Fig. 5. Patient representations obtained by each DL model in the DDM scenario, after projection to 2D using UMAP. Each point corresponds to a patient in the test set; blue and red dots correspond to cases of emergency admission or heart failure, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)Emergency Admission Heart Failure 20 10 ° zZ 0 & = ~10 20 10 5 S 8 3 8 2 i 3 10 > 20 10 S ° 3 -10 p [te 20 10 a fil 0 ~ = 2 -10 “ -20 -10 0 10 20 -20 -10 0 10 20 x Fig. 4. Patient representations obtained by each DL model in the DD scenario, after projection to 2D using UMAP. Each point corresponds to a patient in the test set; blue and red dots correspond to cases of emergency admission or heart failure, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)",Deep Learning and Machine Learning
27,Depression among Patients with HIV/AIDS: Research Development and Effective Interventions,HIV; AIDS; depression; scientometrics; bibliometric,"Depression in people living with HIV (PLWH) has become an urgent issue and has attracted the attention of both physicians and epidemiologists. Currently, 39% of HIV patients are reported to suffer from depression. This population is more likely to experience worsening disease states and, thus, poorer health outcomes. In this study, we analyzed research growth and current understandings of depression among HIV-infected individuals. The number of papers and their impacts have been considerably grown in recent years, and a total of 4872 publications published from 1990–2017 were retrieved from the Web of Science database. Research landscapes related to this research field include risk behaviors and attributable causes of depression in HIV population, effects of depression on health outcomes of PLWH, and interventions and health services for these particular subjects. We identified a lack of empirical studies in countries where PLWH face a high risk of depression, and a modest level of interest in biomedical research. By demonstrating these research patterns, highlighting the research gaps and putting forward implications, this study provides a basis for future studies and interventions in addressing the critical issue of HIV epidemics.","In conclusion, by using bibliometric and scientometric analysis, this study presented the global research trends and interests, pointed out the research gaps of available publications, and suggested several implications for depression of HIV-positive individuals. In spite of the fact that this field has attracted a great deal of attention and been extensively studied, more efforts should be made to fulfill the lack of empirical study in developing countries and biomedical investigation on the correlation between HIV and depression.","Depression among Patients with HIV/AIDS: Research Development and Effective InterventionsHIV; AIDS; depression; scientometrics; bibliometricDepression in people living with HIV (PLWH) has become an urgent issue and has attracted the attention of both physicians and epidemiologists. Currently, 39% of HIV patients are reported to suffer from depression. This population is more likely to experience worsening disease states and, thus, poorer health outcomes. In this study, we analyzed research growth and current understandings of depression among HIV-infected individuals. The number of papers and their impacts have been considerably grown in recent years, and a total of 4872 publications published from 1990–2017 were retrieved from the Web of Science database. Research landscapes related to this research field include risk behaviors and attributable causes of depression in HIV population, effects of depression on health outcomes of PLWH, and interventions and health services for these particular subjects. We identified a lack of empirical studies in countries where PLWH face a high risk of depression, and a modest level of interest in biomedical research. By demonstrating these research patterns, highlighting the research gaps and putting forward implications, this study provides a basis for future studies and interventions in addressing the critical issue of HIV epidemics.In conclusion, by using bibliometric and scientometric analysis, this study presented the global research trends and interests, pointed out the research gaps of available publications, and suggested several implications for depression of HIV-positive individuals. In spite of the fact that this field has attracted a great deal of attention and been extensively studied, more efforts should be made to fulfill the lack of empirical study in developing countries and biomedical investigation on the correlation between HIV and depression.CRUTEREES UERYOR TH SCORE NCAPIBFERMINE py fOLLOW DIAGNGRER SOE ORP oof IMMUNODEFICIENCY INVENTORY: SCORES TLLweseHth OR COMMBLERENCE — ATHIRETROVARRNA SPAT MODERAT DIAGNOSIS DISORDERS NDIVIOUALS SEVERE GENERAL EDican AEE QUALITY INFECTEDINICADISEASK crease SEVERITY IV TEREBMONNALRSORDER LE, ie INFECTION a POOR DEPRESSIVE STREATMENT Bee a COPING GREATERSYMPTOMS putts SPF DISTRESS .PSYCHOLEMLEHTAL OUTCOMERY DREVALENCE RORTAORAL FACTOR VARIABLES : CARE “ASSES ARRAS DEMOGRAPHSSOCIATION ANXIETY. PPR NAGE FeAiINEDSUPPORT HICIPANTS NUMBEREENTIAL CHARACTERES#FES 110! MENTAL FAGTORBOPULATION. «9 — y RELATIONSHTP S\NALYSES™ SAMPLE _ TMFoRt ay One on ‘SIMILAR TP! EMQHONADS YCHOSOCIAL POCiAL RATES e —— nom, = SURURROBLENs cSMBETANCE WoHRARVENTIONS | curune oa a oe PRESENCE BETA SERVIC ABUSBLTIPLE §= “MsTORS TEST oo — RANGE ay a pee = PROVIDE DISEASES conmeiaten TEST@GIRRONE LIMITED ~~ PREDICTPRS. ADPTRR EL opment oS oe ae eregOOEL STATES. 5 post RE IN HRQOL . STRESS or = PI B ey e TESTING SNQIENS STIGMA’ RECENT soap INTER’ —_ INJECTION -. ION (CHILDREN ps nemo Figure Al. Co-occurrence of most frequent topics emerging from exploratory factor analysis of abstracts’ contents.Proximity plot Figure 4. Cont.Figure 2. The global network among 68 countries having co-authorships of selected papers. The size of nodes shows the proportional contribution to the number of papers and the thickness of lines indicates the percentage of the number of collaborations.bs c hiv/aids — africa ) pregnancy j2itica) ""mentathealth} sub-saharan africa Figure 3. Co-occurrence of most frequent author’s keywords. Note: the colors of the nodes refer to principle components of data structure; the nodes size was scaled to the keywords’ occurrences; and the thickness of the lines was drawn based on the strength of the association between two keywords.Proximity plot (b) Figure 4. Proximity plots of the terms (a) “Intervention(s)” and (b) “Trial(s)” with the top 50 most frequent concurrence terms in all abstracts. The x-axis refers to the Jaccard coefficient that measures the similarity between finite sample sets and is defined as the size of the intersection divided by the size of the union of the sample sets.Records downloaded from Web of Science search (n = 371,656) Excluded (n = 120,187) Year 2018 ( = 8492) Year 1907 and not about HIV/AIDS (n = 1) Not articles and reviews (n = 101,863) Author anonymous (1 = 392) Languages not English (1 = 9439) Total papers related to HIV/AIDS (n = 250,270) Total papers selected by “depression”, “depressed”, “anxiety”, “mood” in Title or Abstract (n = 4872) Figure 1. Selection of papers.",Medical Data Analysis,"This study analyzes the research growth and current understandings of depression among HIV-infected individuals, which has become an urgent issue. The study shows that depression is common among HIV patients, and it is associated with poor health outcomes. The research landscape in this field includes risk behaviors, causes of depression, effects of depression on health outcomes, and interventions for PLWH. The study also identifies a lack of empirical studies in countries where PLWH face a high risk of depression and a modest level of interest in biomedical research. The study suggests that more efforts should be made to fulfill the research gaps, especially in developing countries and biomedical investigation on the correlation between HIV and depression.",Medical Data Analysis,"CRUTEREES UERYOR TH SCORE NCAPIBFERMINE py fOLLOW DIAGNGRER SOE ORP oof IMMUNODEFICIENCY INVENTORY: SCORES TLLweseHth OR COMMBLERENCE — ATHIRETROVARRNA SPAT MODERAT DIAGNOSIS DISORDERS NDIVIOUALS SEVERE GENERAL EDican AEE QUALITY INFECTEDINICADISEASK crease SEVERITY IV TEREBMONNALRSORDER LE, ie INFECTION a POOR DEPRESSIVE STREATMENT Bee a COPING GREATERSYMPTOMS putts SPF DISTRESS .PSYCHOLEMLEHTAL OUTCOMERY DREVALENCE RORTAORAL FACTOR VARIABLES : CARE “ASSES ARRAS DEMOGRAPHSSOCIATION ANXIETY. PPR NAGE FeAiINEDSUPPORT HICIPANTS NUMBEREENTIAL CHARACTERES#FES 110! MENTAL FAGTORBOPULATION. «9 — y RELATIONSHTP S\NALYSES™ SAMPLE _ TMFoRt ay One on ‘SIMILAR TP! EMQHONADS YCHOSOCIAL POCiAL RATES e —— nom, = SURURROBLENs cSMBETANCE WoHRARVENTIONS | curune oa a oe PRESENCE BETA SERVIC ABUSBLTIPLE §= “MsTORS TEST oo — RANGE ay a pee = PROVIDE DISEASES conmeiaten TEST@GIRRONE LIMITED ~~ PREDICTPRS. ADPTRR EL opment oS oe ae eregOOEL STATES. 5 post RE IN HRQOL . STRESS or = PI B ey e TESTING SNQIENS STIGMA’ RECENT soap INTER’ —_ INJECTION -. ION (CHILDREN ps nemo Figure Al. Co-occurrence of most frequent topics emerging from exploratory factor analysis of abstracts’ contents.Proximity plot Figure 4. Cont.Figure 2. The global network among 68 countries having co-authorships of selected papers. The size of nodes shows the proportional contribution to the number of papers and the thickness of lines indicates the percentage of the number of collaborations.bs c hiv/aids — africa ) pregnancy j2itica) ""mentathealth} sub-saharan africa Figure 3. Co-occurrence of most frequent author’s keywords. Note: the colors of the nodes refer to principle components of data structure; the nodes size was scaled to the keywords’ occurrences; and the thickness of the lines was drawn based on the strength of the association between two keywords.Proximity plot (b) Figure 4. Proximity plots of the terms (a) “Intervention(s)” and (b) “Trial(s)” with the top 50 most frequent concurrence terms in all abstracts. The x-axis refers to the Jaccard coefficient that measures the similarity between finite sample sets and is defined as the size of the intersection divided by the size of the union of the sample sets.Records downloaded from Web of Science search (n = 371,656) Excluded (n = 120,187) Year 2018 ( = 8492) Year 1907 and not about HIV/AIDS (n = 1) Not articles and reviews (n = 101,863) Author anonymous (1 = 392) Languages not English (1 = 9439) Total papers related to HIV/AIDS (n = 250,270) Total papers selected by “depression”, “depressed”, “anxiety”, “mood” in Title or Abstract (n = 4872) Figure 1. Selection of papers.",Medical Data Analysis
28,Detection and diagnosis of chronic kidney disease using deep learning-based heterogeneous modified artificial neural network,Chronic renal failure Kidney disease Artificial neural network Deep learning Support vector machine Segmentation,"The prevalence of chronic kidney disease (CKD) increases annually in the present scenario of research. One of the sources for further therapy is the CKD prediction where the Machine learning techniques become more important in medical diagnosis due to their high accuracy classification ability. In the recent past, the accuracy of classification algorithms depends on the proper use of algorithms for feature selection to reduce the data size. In this paper, Heterogeneous Modified Artificial Neural Network (HMANN) has been proposed for the early detection, segmentation, and diagnosis of chronic renal failure on the Internet of Medical Things (IoMT) platform. Furthermore, the proposed HMANN is classified as a Support Vector Machine and Multilayer Perceptron (MLP) with a Backpropagation (BP) algorithm. The proposed algorithm works based on an ultrasound image which is denoted as a preprocessing step and the region of kidney interest is segmented in the ultrasound image. In kidney segmentation, the proposed HMANN method achieves high accuracy and significantly reducing the time to delineate the contour.","This paper presents a deep learning-based Heterogeneous Modified Artifical Neural Network (HMANN) method for the detection of chronic renal disease. There are some noisy and complexity during image segmentation. Therefore, it needs an algorithm to manage missing and noisy values with a classification capability. The Proposed HMANN method reduces the noise and helps to segment the kidney image for the clear identification of kidney stone location. To find a good solution to this problem tested three classifiers: support vector machine, artificial neural networks, and Multilayer perceptron. In this paper, the performed feature reduction with the help of significant results from this study, both to reduce excess fitness and to identify the most significant predictive attributes for CKD. Besides, the new factors have found that classifiers use for more accurate detection of CKD than modern formulation.’","Detection and diagnosis of chronic kidney disease using deep learning-based heterogeneous modified artificial neural networkChronic renal failure Kidney disease Artificial neural network Deep learning Support vector machine SegmentationThe prevalence of chronic kidney disease (CKD) increases annually in the present scenario of research. One of the sources for further therapy is the CKD prediction where the Machine learning techniques become more important in medical diagnosis due to their high accuracy classification ability. In the recent past, the accuracy of classification algorithms depends on the proper use of algorithms for feature selection to reduce the data size. In this paper, Heterogeneous Modified Artificial Neural Network (HMANN) has been proposed for the early detection, segmentation, and diagnosis of chronic renal failure on the Internet of Medical Things (IoMT) platform. Furthermore, the proposed HMANN is classified as a Support Vector Machine and Multilayer Perceptron (MLP) with a Backpropagation (BP) algorithm. The proposed algorithm works based on an ultrasound image which is denoted as a preprocessing step and the region of kidney interest is segmented in the ultrasound image. In kidney segmentation, the proposed HMANN method achieves high accuracy and significantly reducing the time to delineate the contour.This paper presents a deep learning-based Heterogeneous Modified Artifical Neural Network (HMANN) method for the detection of chronic renal disease. There are some noisy and complexity during image segmentation. Therefore, it needs an algorithm to manage missing and noisy values with a classification capability. The Proposed HMANN method reduces the noise and helps to segment the kidney image for the clear identification of kidney stone location. To find a good solution to this problem tested three classifiers: support vector machine, artificial neural networks, and Multilayer perceptron. In this paper, the performed feature reduction with the help of significant results from this study, both to reduce excess fitness and to identify the most significant predictive attributes for CKD. Besides, the new factors have found that classifiers use for more accurate detection of CKD than modern formulation.’(a) ‘Accuracy of CT scan lower right lobe nodule, bronchoscopy of the left main bronchial mass (b) Accuracy of CT scan isolated ref Maron metastasis Fig. 10. CT Scan image for kidney metastasis.Performance Ratio 0%) 125 100 © sBp-Pcn @ Anris @ PDA-ADMI @ HMANN c= —= — —— 10 20 30 40 50 Available Dataset (a) Performance Ratio 1.0 Cancer-specific survival Overall survival 08 Overall survival probabilities 04 06 02 0.0 Time (years) (b) Survival Rate Fig. 11. Performance ratio Vs Survival rate.0 ~® SBD-PCN @ ANFIS -@ PDA-ADMI o® PERS ® ANN-SVM. @ HMANN Prediction Ratio @6) Available Dataset (a) Prediction Ratio Sensitivity 1.0 08 0.6 0.4 0.2 0.0 Specificity (b) Sensitivity and Specificity AUC Fig. 12. Prediction Vs Sensitivity and Specificity AUC analyses.100 80 60 40 20 True Positive rate (Sensitivity) oO 20 40 60 80 100 False Positive rate (100-Specificity) Fig. 14. Receiver Operating Characteristic Curve.120 -@ SBD-PCN @ ANFIS ~> PDA-ADMI © PEFS ® ANN-SVM ® HMANN Overall Classification Accuracy ©) 20 T 10 20 30 40 50 Number of Available Dataset (a) Overall Classification accuracy (00 095 g Dice Score g 0.75 —Training 070 —Testing 0 50 100 150 200 250 30 Epoch (b) Dice Accuracy Fig. 9. Accuracy analysis.Fig. 8. Chronic Kidney Disease Detection using deep learning on IoMT Platform.Computational Time 60 50 40 30 20 10 1 2 3 — Regression Line @ Observations Fig. 13. Computational Time.Fig. 7. Image Classification process.iture Extracted Image | Fig. 6. Feature extraction process using GLCM.(d) Segmentation -4 Fig. 5. (a-d). Four Segmentation of Chronic Kidney Disease (CKD) from CT ac- quisition of different Patients taken from (tt hive.ics.uci.edu/m e 5(a) the actual image (b) Denoised image by Gabor filter Fig. 4. Image analysis using filter.Feature Image Wavelet Extraction z Fig. 2. The Proposed HMANN Block Diagram.Fig. 3. Image Preprocessing.Fig. 1. (a) Stent, Stone, Calcification and bone on CT Image (b) CT Slic",Artificial Neural Network,"This paper discusses the use of machine learning techniques, specifically the Heterogeneous Modified Artificial Neural Network (HMANN), for the early detection and diagnosis of chronic kidney disease (CKD) using ultrasound images on the Internet of Medical Things (IoMT) platform. The proposed algorithm achieves high accuracy in kidney segmentation and significantly reduces the time for delineating the contour. The paper also includes a discussion of the importance of feature selection in classification algorithms and the use of support vector machine, artificial neural networks, and multilayer perceptron classifiers. The paper concludes that the proposed HMANN method helps to reduce noise and segment kidney images for clear identification of kidney stone location and improves the accuracy of CKD detection.",Deep Learning and Machine Learning,"(a) ‘Accuracy of CT scan lower right lobe nodule, bronchoscopy of the left main bronchial mass (b) Accuracy of CT scan isolated ref Maron metastasis Fig. 10. CT Scan image for kidney metastasis.Performance Ratio 0%) 125 100 © sBp-Pcn @ Anris @ PDA-ADMI @ HMANN c= —= — —— 10 20 30 40 50 Available Dataset (a) Performance Ratio 1.0 Cancer-specific survival Overall survival 08 Overall survival probabilities 04 06 02 0.0 Time (years) (b) Survival Rate Fig. 11. Performance ratio Vs Survival rate.0 ~® SBD-PCN @ ANFIS -@ PDA-ADMI o® PERS ® ANN-SVM. @ HMANN Prediction Ratio @6) Available Dataset (a) Prediction Ratio Sensitivity 1.0 08 0.6 0.4 0.2 0.0 Specificity (b) Sensitivity and Specificity AUC Fig. 12. Prediction Vs Sensitivity and Specificity AUC analyses.100 80 60 40 20 True Positive rate (Sensitivity) oO 20 40 60 80 100 False Positive rate (100-Specificity) Fig. 14. Receiver Operating Characteristic Curve.120 -@ SBD-PCN @ ANFIS ~> PDA-ADMI © PEFS ® ANN-SVM ® HMANN Overall Classification Accuracy ©) 20 T 10 20 30 40 50 Number of Available Dataset (a) Overall Classification accuracy (00 095 g Dice Score g 0.75 —Training 070 —Testing 0 50 100 150 200 250 30 Epoch (b) Dice Accuracy Fig. 9. Accuracy analysis.Fig. 8. Chronic Kidney Disease Detection using deep learning on IoMT Platform.Computational Time 60 50 40 30 20 10 1 2 3 — Regression Line @ Observations Fig. 13. Computational Time.Fig. 7. Image Classification process.iture Extracted Image | Fig. 6. Feature extraction process using GLCM.(d) Segmentation -4 Fig. 5. (a-d). Four Segmentation of Chronic Kidney Disease (CKD) from CT ac- quisition of different Patients taken from (tt hive.ics.uci.edu/m e 5(a) the actual image (b) Denoised image by Gabor filter Fig. 4. Image analysis using filter.Feature Image Wavelet Extraction z Fig. 2. The Proposed HMANN Block Diagram.Fig. 3. Image Preprocessing.Fig. 1. (a) Stent, Stone, Calcification and bone on CT Image (b) CT Slic",Deep Learning and Machine Learning
29,Economic impact of HIV/AIDS: a systematic review in five European countries,Economic impact; Costs; VIH/AIDS,"The HIV/AIDS disease represent a priority for all health authorities in all countries and it also represents serious added socioeconomic problems for societies over the world. The aim of this paper is to analize the economic impact associated to the HIV/AIDS in an European context. We conducted a systematic literature review for five different countries (France, Germany, Italy, Spain and United Kingdom) and searched five databases. Three types of analyses were undertaken: descriptive statistics; quantitative analysis to calculate mean costs; and comparison across countries. 26 papers were included in this study containing seventy-six cost estimates. Most of the studies analyzed the health care cost of treatment of HIV/AIDS. Only 50% of the cost estimates provided mean lymphocyte count describing the patients’ disease stage. Approximately thirty percent of cost estimates did not indicate the developmental stage of the illness in the patients included. There is a high degree of variability in the estimated annual cost per patient of the treatments across countries. There is also a great disparity in total healh care costs for patients with lymphocyte counts between 200CD4+/mm3 and 500 CD4/mm3, although the reason of variation is unclear. In spite of the potential economic impact in terms of productivity losses and cost of formal and informal care, few studies have set out to estimate the non-medical costs of HIV/AIDS in the countries selected. Another important result is that, despite the low HIV/AIDS prevalence, its economic burden is very relevant in terms of the total health care costs in this five countries. This study also shows that there are relatively few studies of HIV costs in European countries compared to other diseases. Finally, we conclude that the methodology used in many of the studies carried out leaves ample room for improvement and that there is a need for these studies to reflect the economic impact of HIV/AIDS beyond health care including other components of social burden.","In summary, this review shows the profound impact that HIV/AIDS-treatment costs have in the countries studied. Our study also shows that the methodology used in many of the studies carried out in this field leaves ample room for improvement. Lastly, although there has been a marked increase in the number of investigations on medical-care costs in recent years, there is still a great amount of work to be done toward applying the social perspective to these cost analyses. In addition to calculating the medical and non-medical costs associated with HIV/AIDS, studies of this nature should reflect the social burden that lies outside the healthcare realm but which nonetheless is borne by persons who are not carriers of the disease. The impact of this disease may fall on society as a whole.","Economic impact of HIV/AIDS: a systematic review in five European countriesEconomic impact; Costs; VIH/AIDSThe HIV/AIDS disease represent a priority for all health authorities in all countries and it also represents serious added socioeconomic problems for societies over the world. The aim of this paper is to analize the economic impact associated to the HIV/AIDS in an European context. We conducted a systematic literature review for five different countries (France, Germany, Italy, Spain and United Kingdom) and searched five databases. Three types of analyses were undertaken: descriptive statistics; quantitative analysis to calculate mean costs; and comparison across countries. 26 papers were included in this study containing seventy-six cost estimates. Most of the studies analyzed the health care cost of treatment of HIV/AIDS. Only 50% of the cost estimates provided mean lymphocyte count describing the patients’ disease stage. Approximately thirty percent of cost estimates did not indicate the developmental stage of the illness in the patients included. There is a high degree of variability in the estimated annual cost per patient of the treatments across countries. There is also a great disparity in total healh care costs for patients with lymphocyte counts between 200CD4+/mm3 and 500 CD4/mm3, although the reason of variation is unclear. In spite of the potential economic impact in terms of productivity losses and cost of formal and informal care, few studies have set out to estimate the non-medical costs of HIV/AIDS in the countries selected. Another important result is that, despite the low HIV/AIDS prevalence, its economic burden is very relevant in terms of the total health care costs in this five countries. This study also shows that there are relatively few studies of HIV costs in European countries compared to other diseases. Finally, we conclude that the methodology used in many of the studies carried out leaves ample room for improvement and that there is a need for these studies to reflect the economic impact of HIV/AIDS beyond health care including other components of social burden.In summary, this review shows the profound impact that HIV/AIDS-treatment costs have in the countries studied. Our study also shows that the methodology used in many of the studies carried out in this field leaves ample room for improvement. Lastly, although there has been a marked increase in the number of investigations on medical-care costs in recent years, there is still a great amount of work to be done toward applying the social perspective to these cost analyses. In addition to calculating the medical and non-medical costs associated with HIV/AIDS, studies of this nature should reflect the social burden that lies outside the healthcare realm but which nonetheless is borne by persons who are not carriers of the disease. The impact of this disease may fall on society as a whole.NHS EED HEED HTA 1,180 titles and abstracts identified 285 762 133 121 excluded (duplicates): |_____p| NHS HEED 4 HEED 117 v 1,059 titles and abstracts identified ¥. 878 excluded because: being an economic evaluation 352 being a review 8 >>| not being a country of interest 201 not related to HIV/AIDS 61 not related to costs 191 being published before 1996 65 evaluation 181 papers selected for v 155 excluded because: being an economic evaluation 16 being a review i not being a country of interest 100 not related to HIV/AIDS 6 not related to costs 9 being published in german, italian »| and french 5 data before 1996 7 same study 26 papers included Figure 1 Flowchart of study identification and selection.= Economic/disease burden ™ Cost-effectiveness of a treatment | Treatment cost ™ Cost of adverse effects ™ Resources use and cost Figure 2 Distribution of study types.",Medical Data Analysis,"This paper analyzes the economic impact associated with HIV/AIDS in a European context by conducting a systematic literature review for five different countries. The study includes 26 papers containing 76 cost estimates, most of which analyze the health care costs of treating HIV/AIDS. The study finds a high degree of variability in estimated annual costs per patient across countries, and a great disparity in total health care costs for patients with different disease stages. Few studies have estimated the non-medical costs of HIV/AIDS, despite its potential impact on productivity losses and cost of care. The study concludes that there is a need for improvement in the methodology used in many of the studies carried out and for these studies to reflect the economic impact of HIV/AIDS beyond health care. Lastly, the paper emphasizes the importance of reflecting the social burden of the disease beyond the healthcare realm.",Medical Data Analysis,"NHS EED HEED HTA 1,180 titles and abstracts identified 285 762 133 121 excluded (duplicates): |_____p| NHS HEED 4 HEED 117 v 1,059 titles and abstracts identified ¥. 878 excluded because: being an economic evaluation 352 being a review 8 >>| not being a country of interest 201 not related to HIV/AIDS 61 not related to costs 191 being published before 1996 65 evaluation 181 papers selected for v 155 excluded because: being an economic evaluation 16 being a review i not being a country of interest 100 not related to HIV/AIDS 6 not related to costs 9 being published in german, italian »| and french 5 data before 1996 7 same study 26 papers included Figure 1 Flowchart of study identification and selection.= Economic/disease burden ™ Cost-effectiveness of a treatment | Treatment cost ™ Cost of adverse effects ™ Resources use and cost Figure 2 Distribution of study types.",Medical Data Analysis
30,A comprehensive review of deep learning in colon cancer,"Deep learning , Medical image analysis , Colon cancer , Colorectal cancer , Rectal cancer , Inflammatory bowel diseases , Convolutional neural networks.","Deep learning has emerged as a leading machine learning tool in object detection and has attracted attention with its achievements in progressing medical image analysis. Convolutional Neural Networks (CNNs) are the most preferred method of deep learning algorithms for this purpose and they have an essential role in the detection and potential early diagnosis of colon cancer. In this article, we hope to bring a perspective to progress in this area by reviewing deep learning practices for colon cancer analysis. This study first presents an overview of popular deep learning architectures used in colon cancer analysis. After that, all studies related to colon cancer analysis are collected under the field of colon cancer and deep learning, then they are divided into five categories that are detection, classification, segmentation, survival prediction, and inflammatory bowel diseases. Then, the studies collected under each category are summarized in detail and listed. We conclude our work with a summary of recent deep learning practices for colon cancer analysis, a critical discussion of the challenges faced, and suggestions for future research. This study differs from other studies by including 135 recent academic papers, separating colon cancer into five different classes, and providing a comprehensive structure. We hope that this study is beneficial to researchers interested in using deep learning techniques for the diagnosis of colon cancer.","Colon cancer ranks in the top three among the most severe and deadly cancers in the world. As with any cancer, early diagnosis is the most important stage. Deep learning applications have recently become very popular in medical image analysis due to the effects and successes it has achieved in early detection and screening of a cancerous tissue or organ. In this article, we have reviewed the latest studies on the application of deep learning methods used only in the detection and diagnosis of colon cancer. To make the review more comprehensible, we gathered all the works together and organized them into five main categories. These categories are listed as follows according to the number of studies conducted: detection, classification, segmentation, survival prediction, and inflammatory bowel diseases. We present the summaries of the studies in each category with different aspects. We also listed the works in tables to make a more detailed comparison. These tables are composed of different tables including datasets, imaging techniques, and results of each study. After expressing the successes of deep learning for colon cancer analysis, we also revealed the difficulties experienced. Finally, we made some suggestions such as increasing the number of public datasets and determining common experimental setup and evaluation criteria for future studies and researchers.","A comprehensive review of deep learning in colon cancerDeep learning , Medical image analysis , Colon cancer , Colorectal cancer , Rectal cancer , Inflammatory bowel diseases , Convolutional neural networks.Deep learning has emerged as a leading machine learning tool in object detection and has attracted attention with its achievements in progressing medical image analysis. Convolutional Neural Networks (CNNs) are the most preferred method of deep learning algorithms for this purpose and they have an essential role in the detection and potential early diagnosis of colon cancer. In this article, we hope to bring a perspective to progress in this area by reviewing deep learning practices for colon cancer analysis. This study first presents an overview of popular deep learning architectures used in colon cancer analysis. After that, all studies related to colon cancer analysis are collected under the field of colon cancer and deep learning, then they are divided into five categories that are detection, classification, segmentation, survival prediction, and inflammatory bowel diseases. Then, the studies collected under each category are summarized in detail and listed. We conclude our work with a summary of recent deep learning practices for colon cancer analysis, a critical discussion of the challenges faced, and suggestions for future research. This study differs from other studies by including 135 recent academic papers, separating colon cancer into five different classes, and providing a comprehensive structure. We hope that this study is beneficial to researchers interested in using deep learning techniques for the diagnosis of colon cancer.Colon cancer ranks in the top three among the most severe and deadly cancers in the world. As with any cancer, early diagnosis is the most important stage. Deep learning applications have recently become very popular in medical image analysis due to the effects and successes it has achieved in early detection and screening of a cancerous tissue or organ. In this article, we have reviewed the latest studies on the application of deep learning methods used only in the detection and diagnosis of colon cancer. To make the review more comprehensible, we gathered all the works together and organized them into five main categories. These categories are listed as follows according to the number of studies conducted: detection, classification, segmentation, survival prediction, and inflammatory bowel diseases. We present the summaries of the studies in each category with different aspects. We also listed the works in tables to make a more detailed comparison. These tables are composed of different tables including datasets, imaging techniques, and results of each study. After expressing the successes of deep learning for colon cancer analysis, we also revealed the difficulties experienced. Finally, we made some suggestions such as increasing the number of public datasets and determining common experimental setup and evaluation criteria for future studies and researchers.Input Data a€ * 805 x 80S 20 x pZE x oT 82ZT x ZT x PZT ZS * 82 x BZ bZOT x BZ x 82 9 x 252 x 757 9 x G2 x 292 al 508 “Height x Width x Colour Convolution/ tanh 1S x 99% 09 | Max Pooling 349 ZS x 99 x 19 =e awoexstxst | Ft c E 0 sv0z x 8x8 | Concatenation | Convolution Transpose = 3 | Dropout 5 = 2 Resize eS Group 4 ky : iE Fig. 15. Gland segmentation system (MIMONet) proposed by Raza et al. [176].Image Region Proposal Detector augmentation ose Uiseameion Softmax Crop r= & Resize Coe & Y Box- regression ¢ regression Ss ¢ om Zzun Pre-trained Deep CNN Post-Learning (FP / Off-line learning) Fig. 14. Faster R-CNN based polyp detection system proposed by Shin et al. [97].Number of papers =CNNs mAES m= RNNs = RBMs-DBNs LL... Detection Classification Segmentation Survival Prediction Inflammatory Bowel Diseases Fig. 10. Distribution of categorized colon cancer versus deep learning architectures.Number of papers. mPublic = Private Detection Classification Segmentation Survival Prediction Inflammatory Bowel Diseases Fig. 11. Distribution of categorized colon cancer versus dataset availability.Problem Definition Image Acquisition © Public dataset © Colonoscopy based » CWC ClinicDB = Etis-Lanb = CVC-VideoClinicDB CVC-ColonDB Kvasir-SEG_ © Histology based ‘CRCHistoPhenotypes CRAG dataset The Warwick-QU Dataset NCT-CRC-HE-100K PanNuke Dataset © Private dataret Sil Honea ls oc olfer somes Generative adversarial networks Qs Deep Le ‘Algorithms Problem Application Pi Cuesaas a =a, ‘© Some CNN based structures for colon cancer analysis ‘© Detection algorithms ectron? © Classification, Survival Prediction, IBD and Backbones u x ‘© Segmentation Detectron2 MaskRCNN UNet © Crohn's disease detection © Automatic detection of bowel Fig. 13. Stages of applying deep learning techniques to colon cancer analysis.i input 572x572 570x570 568 x 568 + 128 128 208 2822 Fig. 7. An illustration of UNet architecture. => conv 3x3, ReLU > copy and crop # max pool 2x2 4 up-conv 2x2 > conv ixieo. ©@ ee Se . 3:34 ofr £6 eo. @ e @ ee A oe @e @ 06.08 ee a Ss @ e @ ®@ @ @ @ es... C@ ,e @ e.. Ce o @ Input node ©) Convolution or pool @ Recurrent node @ Output node @ Hidden node @ Probalistic input node @ Kernel @ Match input output node @ Probalistic hidden node Fig. 8. Five deep learning architectures can be divided into two main categories: supervised learning algorithms including CNNs (a) and RNNs(b); and unsupervised learning algorithms including AEs (c), RBMs (d) and DBNs (e) [50].mlnflammatory Bowel Diseases __® Survival Prediction =Segmentation Classification =Detection 2020 2019 2018 2017 2015- 2016 6 8 10 12 14 Number of papers ° nN - Fig. 9. Distribution of categorized colon cancer by years.ulnflammatory Bowel Diseases = Survival Prediction = Segmentation =Classification ™ Detection LMI WCE CT MRI Endoscopy Histopathology Colonoscopy - ° a So 45 20 25 30 35 Number of papers Fig. 12. Distribution of categorized colon cancer versus imaging modality.Po/4 BiFPN Layer EfficientNet backbone Fig. 6. An illustration of EfficientDet architecture.dense jdense 1 TH 128 Max Max pooling “77 2048 pooling Fig. 5. An illustration of the architecture of AlexNet [25].Input Kernel 6 3 7/2 Too Output 04 5/4) « [o of0| = Boe 8 7/3/99 001 8 8 24 8/4 [exi|exonmo2 | [Glomepon [le|onr) 2) [enone fo oo ax0sx0 4 0454 0454 04 5 4 som at) 9||5 7/3/19 |51/7/3 9/|58 7713/9 2|4 8) 4] [2 4)8)4) feiaie) 4) |2 vapors 9 9 12 9 12 9 12 8 8 8 Fig. 2. Example of the convolution operation with an input image (4 x 4), a kernel (3 x 3), and a stride of 1.ee ee propagation — > output > Labels a Back propa 1u- s0he poyoouu09 Aung {= 10k} payoauuoo Aung a sake pajoeu0o Aung Weights: (Guyjood-xew) soke| Busjoor, 7 i (e713) s0heAyuwous- oN 49h euonnIonuod {Buyjood-xen) 19481 Buslood maps. i (97134) s0Ke} Ayse0Uy-UON | s9he| euojnjonuog (Guyoog-xew) sok Buyoog I _ i (ora) 20he) Ayswour- oy 49h euonnyoaueD Wy Input Fig. 1. Typical architecture of a convolutional neural network.Minimum Pooling | yD won a > No «ek Max Pooling 9 9 NyoonM @D rPoow aowun rFoonn Average Pooling Fig. 4. Example of the pooling operation with an input (4 x 4), a filter (2 x 2), and a stride of 2.0.8 0.6 0 5 z : (a) (b) (c) Fig. 3. Most common nonlinear activation functions. (a) sigmoid; (b) tanh; (c) ReLU.",Deep Learning and Machine Learning,"This article provides a comprehensive review of the latest deep learning practices in the detection and diagnosis of colon cancer. It starts with an overview of popular deep learning architectures used in colon cancer analysis, followed by a collection of all studies related to colon cancer analysis and their division into five categories: detection, classification, segmentation, survival prediction, and inflammatory bowel diseases. The article provides detailed summaries of the studies in each category and lists them in tables for a more detailed comparison, including datasets, imaging techniques, and results. The article concludes by discussing the successes and challenges of deep learning in colon cancer analysis and providing suggestions for future research, such as increasing the number of public datasets and establishing common experimental setups and evaluation criteria. Overall, this article is a useful resource for researchers interested in using deep learning techniques for the diagnosis of colon cancer.",Deep Learning and Machine Learning,"Input Data a€ * 805 x 80S 20 x pZE x oT 82ZT x ZT x PZT ZS * 82 x BZ bZOT x BZ x 82 9 x 252 x 757 9 x G2 x 292 al 508 “Height x Width x Colour Convolution/ tanh 1S x 99% 09 | Max Pooling 349 ZS x 99 x 19 =e awoexstxst | Ft c E 0 sv0z x 8x8 | Concatenation | Convolution Transpose = 3 | Dropout 5 = 2 Resize eS Group 4 ky : iE Fig. 15. Gland segmentation system (MIMONet) proposed by Raza et al. [176].Image Region Proposal Detector augmentation ose Uiseameion Softmax Crop r= & Resize Coe & Y Box- regression ¢ regression Ss ¢ om Zzun Pre-trained Deep CNN Post-Learning (FP / Off-line learning) Fig. 14. Faster R-CNN based polyp detection system proposed by Shin et al. [97].Number of papers =CNNs mAES m= RNNs = RBMs-DBNs LL... Detection Classification Segmentation Survival Prediction Inflammatory Bowel Diseases Fig. 10. Distribution of categorized colon cancer versus deep learning architectures.Number of papers. mPublic = Private Detection Classification Segmentation Survival Prediction Inflammatory Bowel Diseases Fig. 11. Distribution of categorized colon cancer versus dataset availability.Problem Definition Image Acquisition © Public dataset © Colonoscopy based » CWC ClinicDB = Etis-Lanb = CVC-VideoClinicDB CVC-ColonDB Kvasir-SEG_ © Histology based ‘CRCHistoPhenotypes CRAG dataset The Warwick-QU Dataset NCT-CRC-HE-100K PanNuke Dataset © Private dataret Sil Honea ls oc olfer somes Generative adversarial networks Qs Deep Le ‘Algorithms Problem Application Pi Cuesaas a =a, ‘© Some CNN based structures for colon cancer analysis ‘© Detection algorithms ectron? © Classification, Survival Prediction, IBD and Backbones u x ‘© Segmentation Detectron2 MaskRCNN UNet © Crohn's disease detection © Automatic detection of bowel Fig. 13. Stages of applying deep learning techniques to colon cancer analysis.i input 572x572 570x570 568 x 568 + 128 128 208 2822 Fig. 7. An illustration of UNet architecture. => conv 3x3, ReLU > copy and crop # max pool 2x2 4 up-conv 2x2 > conv ixieo. ©@ ee Se . 3:34 ofr £6 eo. @ e @ ee A oe @e @ 06.08 ee a Ss @ e @ ®@ @ @ @ es... C@ ,e @ e.. Ce o @ Input node ©) Convolution or pool @ Recurrent node @ Output node @ Hidden node @ Probalistic input node @ Kernel @ Match input output node @ Probalistic hidden node Fig. 8. Five deep learning architectures can be divided into two main categories: supervised learning algorithms including CNNs (a) and RNNs(b); and unsupervised learning algorithms including AEs (c), RBMs (d) and DBNs (e) [50].mlnflammatory Bowel Diseases __® Survival Prediction =Segmentation Classification =Detection 2020 2019 2018 2017 2015- 2016 6 8 10 12 14 Number of papers ° nN - Fig. 9. Distribution of categorized colon cancer by years.ulnflammatory Bowel Diseases = Survival Prediction = Segmentation =Classification ™ Detection LMI WCE CT MRI Endoscopy Histopathology Colonoscopy - ° a So 45 20 25 30 35 Number of papers Fig. 12. Distribution of categorized colon cancer versus imaging modality.Po/4 BiFPN Layer EfficientNet backbone Fig. 6. An illustration of EfficientDet architecture.dense jdense 1 TH 128 Max Max pooling “77 2048 pooling Fig. 5. An illustration of the architecture of AlexNet [25].Input Kernel 6 3 7/2 Too Output 04 5/4) « [o of0| = Boe 8 7/3/99 001 8 8 24 8/4 [exi|exonmo2 | [Glomepon [le|onr) 2) [enone fo oo ax0sx0 4 0454 0454 04 5 4 som at) 9||5 7/3/19 |51/7/3 9/|58 7713/9 2|4 8) 4] [2 4)8)4) feiaie) 4) |2 vapors 9 9 12 9 12 9 12 8 8 8 Fig. 2. Example of the convolution operation with an input image (4 x 4), a kernel (3 x 3), and a stride of 1.ee ee propagation — > output > Labels a Back propa 1u- s0he poyoouu09 Aung {= 10k} payoauuoo Aung a sake pajoeu0o Aung Weights: (Guyjood-xew) soke| Busjoor, 7 i (e713) s0heAyuwous- oN 49h euonnIonuod {Buyjood-xen) 19481 Buslood maps. i (97134) s0Ke} Ayse0Uy-UON | s9he| euojnjonuog (Guyoog-xew) sok Buyoog I _ i (ora) 20he) Ayswour- oy 49h euonnyoaueD Wy Input Fig. 1. Typical architecture of a convolutional neural network.Minimum Pooling | yD won a > No «ek Max Pooling 9 9 NyoonM @D rPoow aowun rFoonn Average Pooling Fig. 4. Example of the pooling operation with an input (4 x 4), a filter (2 x 2), and a stride of 2.0.8 0.6 0 5 z : (a) (b) (c) Fig. 3. Most common nonlinear activation functions. (a) sigmoid; (b) tanh; (c) ReLU.",Deep Learning and Machine Learning
31,A review of statistical and machine learning methods for modeling cancer risk using structured clinical data,"Cancer prediction , Cancer recurrence , Cancer relapse , Data mining  , Electronic health records .","Advancements are constantly being made in oncology, improving prevention and treatment of cancers. To help reduce the impact and deadliness of cancers, they must be detected early. Additionally, there is a risk of cancers recurring after potentially curative treatments are performed. Predictive models can be built using historical patient data to model the characteristics of patients that developed cancer or relapsed. These models can then be deployed into clinical settings to determine if new patients are at high risk for cancer development or recurrence. For large-scale predictive models to be built, structured data must be captured for a wide range of diverse patients. This paper explores current methods for building cancer risk models using structured clinical patient data. Trends in statistical and machine learning techniques are explored, and gaps are identified for future research. The field of cancer risk prediction is a high-impact one, and research must continue for these models to be embraced for clinical decision support of both practitioners and patients.","This paper presents a comprehensive review of literature utilizing data mining techniques to perform cancer risk and recurrence prediction. This field is important, as these models can inform patient screening and treatment patterns, potentially improving patient outcomes and reducing overall healthcare costs. The key impact of these models is reducing costs. Governments spend billions of dollars on chronic conditions and acute end of life care. These models can determine who to spend those resources on, and more importantly, who not to spend those resources on. This both improves patient care and reduces operating costs, allowing funds to be spent advancing cutting-edge developments in cancer care. The data provided to these models must be structured, frequently captured, and clinically relevant as to apply to large populations of patients. Coding standards must be enhanced to allow many different clinics and hospitals to exchange structured clinical data. While many standards exist for financial, laboratory, and prescription data, there are gaps in the transfer of point-of-care data such as outcomes and treatment plans. Trends in statistical and machine learning techniques are presented, and analysis is performed to provide several valuable avenues for future work. Many studies utilize statistical survival analysis techniques, such as the Cox Proportional Hazards Model. Those that do not use survival analysis build predictive models using machine learning techniques such as Decision Trees, Neural Networks, and Support Vector Machines. To propel research in this area, advanced modeling methods using state of the art machine learning techniques must be employed, including time-series analysis, missing data imputation, and feature selection.","A review of statistical and machine learning methods for modeling cancer risk using structured clinical dataCancer prediction , Cancer recurrence , Cancer relapse , Data mining  , Electronic health records .Advancements are constantly being made in oncology, improving prevention and treatment of cancers. To help reduce the impact and deadliness of cancers, they must be detected early. Additionally, there is a risk of cancers recurring after potentially curative treatments are performed. Predictive models can be built using historical patient data to model the characteristics of patients that developed cancer or relapsed. These models can then be deployed into clinical settings to determine if new patients are at high risk for cancer development or recurrence. For large-scale predictive models to be built, structured data must be captured for a wide range of diverse patients. This paper explores current methods for building cancer risk models using structured clinical patient data. Trends in statistical and machine learning techniques are explored, and gaps are identified for future research. The field of cancer risk prediction is a high-impact one, and research must continue for these models to be embraced for clinical decision support of both practitioners and patients.This paper presents a comprehensive review of literature utilizing data mining techniques to perform cancer risk and recurrence prediction. This field is important, as these models can inform patient screening and treatment patterns, potentially improving patient outcomes and reducing overall healthcare costs. The key impact of these models is reducing costs. Governments spend billions of dollars on chronic conditions and acute end of life care. These models can determine who to spend those resources on, and more importantly, who not to spend those resources on. This both improves patient care and reduces operating costs, allowing funds to be spent advancing cutting-edge developments in cancer care. The data provided to these models must be structured, frequently captured, and clinically relevant as to apply to large populations of patients. Coding standards must be enhanced to allow many different clinics and hospitals to exchange structured clinical data. While many standards exist for financial, laboratory, and prescription data, there are gaps in the transfer of point-of-care data such as outcomes and treatment plans. Trends in statistical and machine learning techniques are presented, and analysis is performed to provide several valuable avenues for future work. Many studies utilize statistical survival analysis techniques, such as the Cox Proportional Hazards Model. Those that do not use survival analysis build predictive models using machine learning techniques such as Decision Trees, Neural Networks, and Support Vector Machines. To propel research in this area, advanced modeling methods using state of the art machine learning techniques must be employed, including time-series analysis, missing data imputation, and feature selection.0.2 — BCRSVM (0.85) ---- Adjuvant! Online (0.71) at NPI (0.70) 0.0 0.0 0.2 0.4 0.6 0.8 1.0 1-Specificity eoYu Weiser Singal Shin Rudloff Razavi Radespiel-Troger 3074 e 2574 205 Paper = ry 3 Years a 104 1985 1990 1995 2000 2005 2010 2015 54 Year r 1 <> Published <> Study Period Study Start Study End Fig. 9. Year published vs. study period. Four papers did not disclose study period and are excluded from this figure. Left: the line with arrows indicates the duration of the study period, and the dot indicates the year of publication. Right: boxplot summarizing the distributions of the time between the year published and year of study start or end.Family Tree 4 Survival Analysis 4 Support Vector Machine 4 Regression + Neural Network 7 Bayesian 5 I Number of Models 2468 Algorithm SVM Random Forest Naive Bayes Logistic ELM Decision Tree Cox Competing Risks C5.0 C4.5 Bayesian Tree ANN Fig. 7. Feature selection and model algorithm methods. Studies with more than one method are counted multiple times. Feature selection indicates use of a feature ranker or feature subset-selector. Left: model algorithms are grouped by their algorithm family. Right: each model algorithm is outlined.Separating hyperplane \-> Vector 1 | Recurrence | , |9¢| Non-recurrence wnapnite Complex in low dimensions Simple in higher dimensions Fig. 5. Basic framework of an SVM [4].lymph-nede positive ~ 3 25 22% lymph-nexle positive ~= 3 age~= 38 34 [72% age > 38 196 | 93% Jymmph-nede negative 123 | 96% dymmph-nede positive ER: -.+ Ee ee 73. | 86% 22 [3% [70% [16 | so% | LT Fecal, Promincot % tumor size > 2 tumer size ~= 2 Le | 75% | Fig. 4. Example decision tree [40]. Each node presents the number of training instances in that node, and the percentage of those that did not recur within 5 years.1.0 Low-risk group 0.6 04 F Probability of event 0.2 F High-risk group , p<0.001 0.0 1 1 1 1 1 0 20 40 60 80 100 120 Months Fig. 3. Example Kaplan-Meier curve [4]. The event is recurrence-free survival. The low-risk and high-risk labels are assigned to patients based on the output of the predictive model. This graph shows that the model does indeed discriminate between high-risk and low-risk patients.Number of Articles 0 Li Demographic Lab Histopathologic Clinical Feature Type Lifestyle Problem [i Recurrence risk Fig. 1. Feature types by prediction problem (22 total articles). Many articles ao 10 Points Age Size Margin Grade Histology Atypical Lipoma or Well Diff Total Points 0 50 100 150 250 350 400 450 1 y Predicted LR rate at year 3 a. 01 02 03 04050607 Predicted LR rate at year 5 01 02 03 0405060708 (a) Manual nomogram. Lines are drawn from each feature to a particular score at the top line depending on the value of that feature. These points are then added up to reveal the predicted recurrence probability at either three or five years. The two styles of arrows indicate two different predictions made using the nomogram. Sarcoma Nomogram: Local Recurrence Risk after Limb-Sparing Surgery without text size (FA) (fA) Radiation Nomogram ‘This tool can be used to predict the chance of soft tissue sarcoma returning at the site of intial surgery after the tumor is removed through limb-sparing surgery if the patient does NOT receive radiation. The probabilly of local recurrence is calculated for both three years and five years after surgery. Learn more about your results below. Age 50 or younger B =< mace ee ee Select the tumor size. 5 Recurrence Margin Positive or Close B Moun Grade igh grade B Histology ALT/Well-diff lipo B POEUN tug a, — sais. aan) (b) Online version of the nomogram. Fig. 2. Example nomogram [24].",Deep Learning and Machine Learning,"This paper reviews current methods for building cancer risk models using structured clinical patient data, exploring trends in statistical and machine learning techniques. The importance of cancer risk prediction models is highlighted, as they can inform patient screening and treatment patterns, potentially improving patient outcomes and reducing healthcare costs. The paper identifies gaps in the transfer of point-of-care data and suggests that advanced modeling methods using state-of-the-art machine learning techniques must be employed for future research. The paper concludes that research must continue in this area for these models to be embraced for clinical decision support of both practitioners and patients.",Medical Data Analysis,"0.2 — BCRSVM (0.85) ---- Adjuvant! Online (0.71) at NPI (0.70) 0.0 0.0 0.2 0.4 0.6 0.8 1.0 1-Specificity eoYu Weiser Singal Shin Rudloff Razavi Radespiel-Troger 3074 e 2574 205 Paper = ry 3 Years a 104 1985 1990 1995 2000 2005 2010 2015 54 Year r 1 <> Published <> Study Period Study Start Study End Fig. 9. Year published vs. study period. Four papers did not disclose study period and are excluded from this figure. Left: the line with arrows indicates the duration of the study period, and the dot indicates the year of publication. Right: boxplot summarizing the distributions of the time between the year published and year of study start or end.Family Tree 4 Survival Analysis 4 Support Vector Machine 4 Regression + Neural Network 7 Bayesian 5 I Number of Models 2468 Algorithm SVM Random Forest Naive Bayes Logistic ELM Decision Tree Cox Competing Risks C5.0 C4.5 Bayesian Tree ANN Fig. 7. Feature selection and model algorithm methods. Studies with more than one method are counted multiple times. Feature selection indicates use of a feature ranker or feature subset-selector. Left: model algorithms are grouped by their algorithm family. Right: each model algorithm is outlined.Separating hyperplane \-> Vector 1 | Recurrence | , |9¢| Non-recurrence wnapnite Complex in low dimensions Simple in higher dimensions Fig. 5. Basic framework of an SVM [4].lymph-nede positive ~ 3 25 22% lymph-nexle positive ~= 3 age~= 38 34 [72% age > 38 196 | 93% Jymmph-nede negative 123 | 96% dymmph-nede positive ER: -.+ Ee ee 73. | 86% 22 [3% [70% [16 | so% | LT Fecal, Promincot % tumor size > 2 tumer size ~= 2 Le | 75% | Fig. 4. Example decision tree [40]. Each node presents the number of training instances in that node, and the percentage of those that did not recur within 5 years.1.0 Low-risk group 0.6 04 F Probability of event 0.2 F High-risk group , p<0.001 0.0 1 1 1 1 1 0 20 40 60 80 100 120 Months Fig. 3. Example Kaplan-Meier curve [4]. The event is recurrence-free survival. The low-risk and high-risk labels are assigned to patients based on the output of the predictive model. This graph shows that the model does indeed discriminate between high-risk and low-risk patients.Number of Articles 0 Li Demographic Lab Histopathologic Clinical Feature Type Lifestyle Problem [i Recurrence risk Fig. 1. Feature types by prediction problem (22 total articles). Many articles ao 10 Points Age Size Margin Grade Histology Atypical Lipoma or Well Diff Total Points 0 50 100 150 250 350 400 450 1 y Predicted LR rate at year 3 a. 01 02 03 04050607 Predicted LR rate at year 5 01 02 03 0405060708 (a) Manual nomogram. Lines are drawn from each feature to a particular score at the top line depending on the value of that feature. These points are then added up to reveal the predicted recurrence probability at either three or five years. The two styles of arrows indicate two different predictions made using the nomogram. Sarcoma Nomogram: Local Recurrence Risk after Limb-Sparing Surgery without text size (FA) (fA) Radiation Nomogram ‘This tool can be used to predict the chance of soft tissue sarcoma returning at the site of intial surgery after the tumor is removed through limb-sparing surgery if the patient does NOT receive radiation. The probabilly of local recurrence is calculated for both three years and five years after surgery. Learn more about your results below. Age 50 or younger B =< mace ee ee Select the tumor size. 5 Recurrence Margin Positive or Close B Moun Grade igh grade B Histology ALT/Well-diff lipo B POEUN tug a, — sais. aan) (b) Online version of the nomogram. Fig. 2. Example nomogram [24].",Medical Data Analysis
32,"A review on deep learning approaches in healthcare systems: Taxonomies, challenges, and open issues","Machine learning , Deep neural network , Healthcare applications , Diagnostics tools , Health data analytics.","In the last few years, the application of Machine Learning approaches like Deep Neural Network (DNN) models have become more attractive in the healthcare system given the rising complexity of the healthcare data. Machine Learning (ML) algorithms provide efficient and effective data analysis models to uncover hidden patterns and other meaningful information from the considerable amount of health data that conventional analytics are not able to discover in a reasonable time. In particular, Deep Learning (DL) techniques have been shown as promising methods in pattern recognition in the healthcare systems. Motivated by this consideration, the contribution of this paper is to investigate the deep learning approaches applied to healthcare systems by reviewing the cutting-edge network architectures, applications, and industrial trends. The goal is first to provide extensive insight into the application of deep learning models in healthcare solutions to bridge deep learning techniques and human healthcare interpretability. And then, to present the existing open challenges and future directions.","As an emerging technique, deep learning has demonstrated tremendous potential in tackling challenging problems in healthcare. In this study, we focused on those problems in healthcare that have been addressed using deep learning with promising results. Deep learning based techniques that have been shown as powerful tools in dealing with disease detection in preprocessing, feature extraction, feature selection, classification, and clustering steps. The technical aspects of ML and DL architectures were evaluated in this paper by focusing on disease detection in healthcare systems. The performance of these methods was discussed in terms of the accuracy of disease detection and algorithm parameters. Finally, the top architectures of DL methods applied to healthcare were analyzed and discussed. As a summary, we can conclude that hybrid and ensemble methods based on DL produce better accuracy results compared to single techniques. These methods lead to an improvement of the process by combining two or more methods simultaneously and independent of the type of the datasets. On the negative side, deep learning approaches are memory and time-consuming. Hence, designing and applying optimal methods in healthcare systems is an important challenge. In the future, researchers may also put their efforts into developing and integrating efficient technologies to fulfill the hardware requirements of decision making systems including those with a deep neural network structure. As discussed in this paper, to build more effective systems, it is also necessary to improve the structures of the current neural network models. Therefore, to solve complex problems in the healthcare systems, it is necessary to create well-defined architectures that are general and work with different types of health data. It is also important to implement deep learning models in terms of Explainable Artificial Intelligence (XAI) to apply in distributed systems which can significantly improve the processing time. To conclude, we believe that there is tremendous potential for the deep learning models and their applications in medicine and healthcare systems especially considering the size and complexity of health data.","A review on deep learning approaches in healthcare systems: Taxonomies, challenges, and open issuesMachine learning , Deep neural network , Healthcare applications , Diagnostics tools , Health data analytics.In the last few years, the application of Machine Learning approaches like Deep Neural Network (DNN) models have become more attractive in the healthcare system given the rising complexity of the healthcare data. Machine Learning (ML) algorithms provide efficient and effective data analysis models to uncover hidden patterns and other meaningful information from the considerable amount of health data that conventional analytics are not able to discover in a reasonable time. In particular, Deep Learning (DL) techniques have been shown as promising methods in pattern recognition in the healthcare systems. Motivated by this consideration, the contribution of this paper is to investigate the deep learning approaches applied to healthcare systems by reviewing the cutting-edge network architectures, applications, and industrial trends. The goal is first to provide extensive insight into the application of deep learning models in healthcare solutions to bridge deep learning techniques and human healthcare interpretability. And then, to present the existing open challenges and future directions.As an emerging technique, deep learning has demonstrated tremendous potential in tackling challenging problems in healthcare. In this study, we focused on those problems in healthcare that have been addressed using deep learning with promising results. Deep learning based techniques that have been shown as powerful tools in dealing with disease detection in preprocessing, feature extraction, feature selection, classification, and clustering steps. The technical aspects of ML and DL architectures were evaluated in this paper by focusing on disease detection in healthcare systems. The performance of these methods was discussed in terms of the accuracy of disease detection and algorithm parameters. Finally, the top architectures of DL methods applied to healthcare were analyzed and discussed. As a summary, we can conclude that hybrid and ensemble methods based on DL produce better accuracy results compared to single techniques. These methods lead to an improvement of the process by combining two or more methods simultaneously and independent of the type of the datasets. On the negative side, deep learning approaches are memory and time-consuming. Hence, designing and applying optimal methods in healthcare systems is an important challenge. In the future, researchers may also put their efforts into developing and integrating efficient technologies to fulfill the hardware requirements of decision making systems including those with a deep neural network structure. As discussed in this paper, to build more effective systems, it is also necessary to improve the structures of the current neural network models. Therefore, to solve complex problems in the healthcare systems, it is necessary to create well-defined architectures that are general and work with different types of health data. It is also important to implement deep learning models in terms of Explainable Artificial Intelligence (XAI) to apply in distributed systems which can significantly improve the processing time. To conclude, we believe that there is tremendous potential for the deep learning models and their applications in medicine and healthcare systems especially considering the size and complexity of health data.No. of studies 70 60 40 30 20 10 IO1LS | 2016 I017 IOI1S&= Sensititivity = Specification = Accuracy 0.85 0.8 0.75 ""iE 0.65 SDAE A-SDAE100 95 90 85 80 75 70 65 = SF-ELM = SF-PCA-ELM = SF-TTEST-ELM = PGBM-ELM = PGBM-RBM-ELM Accuracy = SF-KNN m= SF-PCA-KNN = SF-TTEST-KNN = PGBM-KNN = PGBM-RBM-KNN Sensitivity =SF-SVM = SF-PCA-SVM = SF-TTEST-SVM = PGBM-SVM = PGBM-RBM-SVM SpecificityAUC (%) 100 DBN(PGB. 95 CNN+DBN CNN*DBN*+SDAE — CNN+HCF 90 85 RNN-GRU DCNN 80 CNN 75 70 65 s> o& woe o o * oe oe = os a os Ss 4° 4°= AUC m AccuracyAccuracy (%) 90 85 80 75 70 65 60 55 a CNN mSAE @&CNN-SAE Dataset 1 Dataset 2Sensitivity (%) 120 100 s+ +} y+ »} > Y y y & s s° oe ra x s or LS DBN DBN ‘NN FCNNAUC 0.92 0.9 0.88 0.86 0.84 0.82 S&S CNN with multi channel ROI @ DBN with multi channel ROI = SDAE with multi channel ROI = CNN with one channel ROI = DBN with one channel ROI ROI based-without ROI based with Nodule based Nodule based with ageregation ageregation without aggregation ageregation% 100 90 7 6 5 4 3 2 10 oooocl.c“( SHCF a CNN SHCF+CNN Sensitivitywt (w+17+3n)*(h+17+3n)*3 (wt 346m) *(h+34+6ny*3RawFeat msPCA mGMM oakK-Means gICA mDeepPatient me AUC (ROC) accuracy-Trainsfer learning from handcrafted features to CNN features -Classification using RFBIA? Compressed i ( ) so Reconstructed Input i aExtract normal- appearing 3D patches in the MNI152 space Multimoder deep learning Image-Level =>) feature vector | => construction Supervised classifier learning Missing feature imputation Compute sparse representation Train a random forest classifierInput Convolutional Layer CNN Input layer AE AE AE AE AE AE Output layer SAE1.2 mRaterl mRater2 m Visual+Conventional quantification m= PD-Net Sensitivity Specificity Accuracy_— - t» Architectures —————+» Applications Parkinson Brain Tumor Epilepsy Multiple Sclerosis Breast Cancer Lung Cancer Heart Failure Diabetes30 Share of each method (%) aa _ N N an oo an ° an ° Share of each disease (%) CNN 2018 DBN DL method (a) Application area (b) 2017 02018 2016 2017 2015 02016 20150.8 0.6 0.4 0.2 KE-CNN BOCNN RF handcrafted features m RF-CNN Manual CNN Manual = RF handcrafted features manual AccuracyG 100 100 95 95 kNN 90 SVM 85 CNN 90 SVM 80 CNN Fuzzy sugeno Random Fuzzy sugeno forest 2y sug Random forest (a) (b)0.78 0.77 0.76 0.75 ) 0.74 ») < 0.73 0.72 0.71 0.7 0.69 3 4 5 6 7 8 9 Months mLogistic regression mSVM MLP o«&k.NN’) &RNN",Deep Learning and Machine Learning,"This paper explores the use of deep learning techniques in healthcare systems to analyze and uncover meaningful information from large amounts of data. The focus is on disease detection in preprocessing, feature extraction, feature selection, classification, and clustering steps. The paper evaluates the technical aspects of machine learning and deep learning architectures, including the accuracy of disease detection and algorithm parameters. The top architectures of deep learning methods applied to healthcare are also analyzed and discussed. The potential of hybrid and ensemble methods is highlighted, but the challenges of memory and time consumption are noted. Future research efforts should focus on developing efficient technologies and improving neural network models, as well as implementing explainable artificial intelligence in distributed systems. Overall, the paper concludes that deep learning models have tremendous potential in medicine and healthcare systems, given the complexity of health data.",Deep Learning and Machine Learning,"No. of studies 70 60 40 30 20 10 IO1LS | 2016 I017 IOI1S&= Sensititivity = Specification = Accuracy 0.85 0.8 0.75 ""iE 0.65 SDAE A-SDAE100 95 90 85 80 75 70 65 = SF-ELM = SF-PCA-ELM = SF-TTEST-ELM = PGBM-ELM = PGBM-RBM-ELM Accuracy = SF-KNN m= SF-PCA-KNN = SF-TTEST-KNN = PGBM-KNN = PGBM-RBM-KNN Sensitivity =SF-SVM = SF-PCA-SVM = SF-TTEST-SVM = PGBM-SVM = PGBM-RBM-SVM SpecificityAUC (%) 100 DBN(PGB. 95 CNN+DBN CNN*DBN*+SDAE — CNN+HCF 90 85 RNN-GRU DCNN 80 CNN 75 70 65 s> o& woe o o * oe oe = os a os Ss 4° 4°= AUC m AccuracyAccuracy (%) 90 85 80 75 70 65 60 55 a CNN mSAE @&CNN-SAE Dataset 1 Dataset 2Sensitivity (%) 120 100 s+ +} y+ »} > Y y y & s s° oe ra x s or LS DBN DBN ‘NN FCNNAUC 0.92 0.9 0.88 0.86 0.84 0.82 S&S CNN with multi channel ROI @ DBN with multi channel ROI = SDAE with multi channel ROI = CNN with one channel ROI = DBN with one channel ROI ROI based-without ROI based with Nodule based Nodule based with ageregation ageregation without aggregation ageregation% 100 90 7 6 5 4 3 2 10 oooocl.c“( SHCF a CNN SHCF+CNN Sensitivitywt (w+17+3n)*(h+17+3n)*3 (wt 346m) *(h+34+6ny*3RawFeat msPCA mGMM oakK-Means gICA mDeepPatient me AUC (ROC) accuracy-Trainsfer learning from handcrafted features to CNN features -Classification using RFBIA? Compressed i ( ) so Reconstructed Input i aExtract normal- appearing 3D patches in the MNI152 space Multimoder deep learning Image-Level =>) feature vector | => construction Supervised classifier learning Missing feature imputation Compute sparse representation Train a random forest classifierInput Convolutional Layer CNN Input layer AE AE AE AE AE AE Output layer SAE1.2 mRaterl mRater2 m Visual+Conventional quantification m= PD-Net Sensitivity Specificity Accuracy_— - t» Architectures —————+» Applications Parkinson Brain Tumor Epilepsy Multiple Sclerosis Breast Cancer Lung Cancer Heart Failure Diabetes30 Share of each method (%) aa _ N N an oo an ° an ° Share of each disease (%) CNN 2018 DBN DL method (a) Application area (b) 2017 02018 2016 2017 2015 02016 20150.8 0.6 0.4 0.2 KE-CNN BOCNN RF handcrafted features m RF-CNN Manual CNN Manual = RF handcrafted features manual AccuracyG 100 100 95 95 kNN 90 SVM 85 CNN 90 SVM 80 CNN Fuzzy sugeno Random Fuzzy sugeno forest 2y sug Random forest (a) (b)0.78 0.77 0.76 0.75 ) 0.74 ») < 0.73 0.72 0.71 0.7 0.69 3 4 5 6 7 8 9 Months mLogistic regression mSVM MLP o«&k.NN’) &RNN",Deep Learning and Machine Learning
33,A systematic literature review on obesity: Understanding the causes & consequences of obesity and reviewing various machine learning approaches used to predict obesity,"Obesity Overweight , Risk factors , Diseases.","Obesity is considered a principal public health concern and ranked as the fifth foremost reason for death globally. Overweight and obesity are one of the main lifestyle illnesses that leads to further health concerns and contributes to numerous chronic diseases, including cancers, diabetes, metabolic syndrome, and cardiovascular diseases. The World Health Organization also predicted that 30% of death in the world will be initiated with lifestyle diseases in 2030 and can be stopped through the suitable identification and addressing of associated risk factors and behavioral involvement policies. Thus, detecting and diagnosing obesity as early as possible is crucial. Therefore, the machine learning approach is a promising solution to early predictions of obesity and the risk of overweight because it can offer quick, immediate, and accurate identification of risk factors and condition likelihoods. The present study conducted a systematic literature review to examine obesity research and machine learning techniques for the prevention and treatment of obesity from 2010 to 2020. Accordingly, 93 papers are identified from the review articles as primary studies from an initial pool of over 700 papers addressing obesity. Consequently, this study initially recognized the significant potential factors that influence and cause adult obesity. Next, the main diseases and health consequences of obesity and overweight are investigated. Ultimately, this study recognized the machine learning methods that can be used for the prediction of obesity. Finally, this study seeks to support decision-makers looking to understand the impact of obesity on health in the general population and identify outcomes that can be used to guide health authorities and public health to further mitigate threats and effectively guide obese people globally.","Specific ML approaches can be categorized as either single or hybrid method based on the trends identified in this SLR. As of this writing, the ANN approach is the most often applied ML method in the existing literature. Finally, this SLR also reviewed certain types of diseases related to obesity. The results confirmed that obesity is a healthcare concern and epidemic worldwide and is also well documented as a risk for increasing rates of various other conditions. Therefore, obesity deserves serious consideration and attention from policymakers, healthcare providers, and researchers alike. Obesity prevention must be multifaceted and should actively involve stakeholders at different levels. From potential areas of policy to the development and implementation of these policies, approaches should account for people’s home environments and broad, society-level views of socioeconomic environments. However, many barriers that prevent strategizing on the level of policy alone may be encountered. Instead, the reduction and prevention of obesity will considerably depend on individual lifestyle changes. In this case, further research on motivations for individual and societal behavioral changes is crucial.","A systematic literature review on obesity: Understanding the causes & consequences of obesity and reviewing various machine learning approaches used to predict obesityObesity Overweight , Risk factors , Diseases.Obesity is considered a principal public health concern and ranked as the fifth foremost reason for death globally. Overweight and obesity are one of the main lifestyle illnesses that leads to further health concerns and contributes to numerous chronic diseases, including cancers, diabetes, metabolic syndrome, and cardiovascular diseases. The World Health Organization also predicted that 30% of death in the world will be initiated with lifestyle diseases in 2030 and can be stopped through the suitable identification and addressing of associated risk factors and behavioral involvement policies. Thus, detecting and diagnosing obesity as early as possible is crucial. Therefore, the machine learning approach is a promising solution to early predictions of obesity and the risk of overweight because it can offer quick, immediate, and accurate identification of risk factors and condition likelihoods. The present study conducted a systematic literature review to examine obesity research and machine learning techniques for the prevention and treatment of obesity from 2010 to 2020. Accordingly, 93 papers are identified from the review articles as primary studies from an initial pool of over 700 papers addressing obesity. Consequently, this study initially recognized the significant potential factors that influence and cause adult obesity. Next, the main diseases and health consequences of obesity and overweight are investigated. Ultimately, this study recognized the machine learning methods that can be used for the prediction of obesity. Finally, this study seeks to support decision-makers looking to understand the impact of obesity on health in the general population and identify outcomes that can be used to guide health authorities and public health to further mitigate threats and effectively guide obese people globally.Specific ML approaches can be categorized as either single or hybrid method based on the trends identified in this SLR. As of this writing, the ANN approach is the most often applied ML method in the existing literature. Finally, this SLR also reviewed certain types of diseases related to obesity. The results confirmed that obesity is a healthcare concern and epidemic worldwide and is also well documented as a risk for increasing rates of various other conditions. Therefore, obesity deserves serious consideration and attention from policymakers, healthcare providers, and researchers alike. Obesity prevention must be multifaceted and should actively involve stakeholders at different levels. From potential areas of policy to the development and implementation of these policies, approaches should account for people’s home environments and broad, society-level views of socioeconomic environments. However, many barriers that prevent strategizing on the level of policy alone may be encountered. Instead, the reduction and prevention of obesity will considerably depend on individual lifestyle changes. In this case, further research on motivations for individual and societal behavioral changes is crucial.Machine Learning Techniques for Obesity Prediction Hybrid Method Multilayer Feedforward Neural Networks (MLFFNN), Support Vector Machine Regression Model (SVMs) and Decision Tree Regression (DT) Random Tree, Random Forest, 48, ID3, Naive Bayes, and Bayes trained Rule based and machine learning based algorithms Logistic Regression (LR), K-nearest Neighbours (k-NN), and Support Vector Machines (SVM) Machine learning (ML) and 3D image processing Support Vector Machines, Random Forests and Extreme Gradient Boosting Deep Learning: Recurrent Neural Network (RNN) architecture with Long Short-term Memory (LSTM) Decision Trees (DT) and Logistic Regression (LR) Gradient Boosting, Generalized Linear Model, Classification and Regression Trees, K-Nearest Neighbours, Support Vector Machines, Random Forest and Multilayer Perceptron Neural Network Logistic Regression, Naive Bayes, Decision Tree, KNN, Random Forest, AdaBoost Gradient Boosting Machine Regression and Multivariate linear regression Decision Tree (IDT), Weighted K-Nearest Neighbor (KNN), Artificial Neural Network (ANN) Support Vector Machine (SVM), K-nearest Neighbor, And Decision Tree (DT) Single Method Multi-layer Perceptron Feed Forward Artificial Neural Networks (MLFFNN) Artificial Neural Networks (ANN) Decision Trees (DT) Ranked Guided Iterative Feature Elimination (RGIFE) Deep Convolutional Neural Networks Support Vector Machines (SVM) Random Forest (RF) 448 Lasso Model XGBoost model predictions Deep Learning (DL) Group factor analysis (GFA) Machine Learning PipelineMajor Diseases/ Disorders that are Associated with Obesity— [Ss & aA ¢ » Environmental ‘ factors * Socio-economic status Psychological factorsERVIEW OF RESEARCH METHODOLOGY RQ1: What are the potential factors that influence and cause adult obesity? RQ2: What are the most ; ; important applied machine dent leaming techniques for aa predicting obesity issues? ‘ RQ3: What are the major disorders/diseases that are assodated with obesity based on the previous studies? Planning Queryl: “Obesity” OR “Over weight"" AND” “Factors” OR “Parameters” Quey2: “Obesity” OR “Overweight” AND ""Machine learning techniques” Query3: “Obesity” OR “Overweight” AND “Disorders” OR “Diseases” c Forward and Backward Technology Search on reference of Filtered studies Conducting the Review Reporting the ReviewResearch question formulation Research Process + Digital library selection + Keywords identification Data analysis eiial @ Quality assessment 2 Research Process + Inclu + Scrutiny and data extraction ion/exclusion criteria Aae 80 60 40 20 0 ‘TE-0207 80-0207 90-0207 70-0207 TL-6107 80-6107 90-6107 20-6107 TLE-810z 80-8107 80-8107 20-07, TL-L107 80-L 107 S0-L 107 ZO-L LOZ TL-9107 80-9107 90-9107, 20-9107 TL-st0z 80-S107 $0-SL07, 20-107 UL-L0z 80-PLOZ 80-107 ZO-PLOZ Teo 80-EL07 90-€107 ZO-ELOz TL-ZL0z 80-7107 0-707 ZOO TE-LL0z 80-107 0-107 ZO-LLOZ TL-0107 © Machine learning =e obesity20 18 16 14 12 10 NUMBER OF PUBLISHED ARTICLES 2 0 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 YEAR Figure 3: Temporal view of primary studies",Deep Learning and Machine Learning,"The article highlights the importance of identifying and diagnosing obesity as early as possible due to its negative impact on public health. The use of machine learning techniques for the prevention and treatment of obesity is promising as it can offer quick and accurate identification of risk factors and condition likelihoods. The study reviews 93 papers from 2010 to 2020 and identifies significant potential factors that influence and cause adult obesity, main diseases and health consequences of obesity and overweight, and machine learning methods that can be used for the prediction of obesity. The article concludes that obesity is a healthcare concern and epidemic worldwide and deserves serious consideration from policymakers, healthcare providers, and researchers alike. Obesity prevention must be multifaceted and should actively involve stakeholders at different levels, including individual lifestyle changes.",Medical Data Analysis,"Machine Learning Techniques for Obesity Prediction Hybrid Method Multilayer Feedforward Neural Networks (MLFFNN), Support Vector Machine Regression Model (SVMs) and Decision Tree Regression (DT) Random Tree, Random Forest, 48, ID3, Naive Bayes, and Bayes trained Rule based and machine learning based algorithms Logistic Regression (LR), K-nearest Neighbours (k-NN), and Support Vector Machines (SVM) Machine learning (ML) and 3D image processing Support Vector Machines, Random Forests and Extreme Gradient Boosting Deep Learning: Recurrent Neural Network (RNN) architecture with Long Short-term Memory (LSTM) Decision Trees (DT) and Logistic Regression (LR) Gradient Boosting, Generalized Linear Model, Classification and Regression Trees, K-Nearest Neighbours, Support Vector Machines, Random Forest and Multilayer Perceptron Neural Network Logistic Regression, Naive Bayes, Decision Tree, KNN, Random Forest, AdaBoost Gradient Boosting Machine Regression and Multivariate linear regression Decision Tree (IDT), Weighted K-Nearest Neighbor (KNN), Artificial Neural Network (ANN) Support Vector Machine (SVM), K-nearest Neighbor, And Decision Tree (DT) Single Method Multi-layer Perceptron Feed Forward Artificial Neural Networks (MLFFNN) Artificial Neural Networks (ANN) Decision Trees (DT) Ranked Guided Iterative Feature Elimination (RGIFE) Deep Convolutional Neural Networks Support Vector Machines (SVM) Random Forest (RF) 448 Lasso Model XGBoost model predictions Deep Learning (DL) Group factor analysis (GFA) Machine Learning PipelineMajor Diseases/ Disorders that are Associated with Obesity— [Ss & aA ¢ » Environmental ‘ factors * Socio-economic status Psychological factorsERVIEW OF RESEARCH METHODOLOGY RQ1: What are the potential factors that influence and cause adult obesity? RQ2: What are the most ; ; important applied machine dent leaming techniques for aa predicting obesity issues? ‘ RQ3: What are the major disorders/diseases that are assodated with obesity based on the previous studies? Planning Queryl: “Obesity” OR “Over weight"" AND” “Factors” OR “Parameters” Quey2: “Obesity” OR “Overweight” AND ""Machine learning techniques” Query3: “Obesity” OR “Overweight” AND “Disorders” OR “Diseases” c Forward and Backward Technology Search on reference of Filtered studies Conducting the Review Reporting the ReviewResearch question formulation Research Process + Digital library selection + Keywords identification Data analysis eiial @ Quality assessment 2 Research Process + Inclu + Scrutiny and data extraction ion/exclusion criteria Aae 80 60 40 20 0 ‘TE-0207 80-0207 90-0207 70-0207 TL-6107 80-6107 90-6107 20-6107 TLE-810z 80-8107 80-8107 20-07, TL-L107 80-L 107 S0-L 107 ZO-L LOZ TL-9107 80-9107 90-9107, 20-9107 TL-st0z 80-S107 $0-SL07, 20-107 UL-L0z 80-PLOZ 80-107 ZO-PLOZ Teo 80-EL07 90-€107 ZO-ELOz TL-ZL0z 80-7107 0-707 ZOO TE-LL0z 80-107 0-107 ZO-LLOZ TL-0107 © Machine learning =e obesity20 18 16 14 12 10 NUMBER OF PUBLISHED ARTICLES 2 0 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 YEAR Figure 3: Temporal view of primary studies",Medical Data Analysis
34,An overview of deep learning methods for multimodal medical data mining," Multimodal medical data , Review.","Deep learning methods have achieved significant results in various fields. Due to the success of these methods, many researchers have used deep learning algorithms in medical analyses. Using multimodal data to achieve more accurate results is a successful strategy because multimodal data provide complementary information. This paper first introduces the most popular modalities, fusion strategies, and deep learning architectures. We also explain learning strategies, including transfer learning, end-to-end learning, and multitask learning. Then, we give an overview of deep learning methods for multimodal medical data analysis. We have focused on articles published over the last four years. We end with a summary of the current state-of-the-art, common problems, and directions for future research.","Deep learning is a powerful technique to analyze complex medical data. Recently, deep learning methods have become very popular in medical analyses because they have achieved outstanding results in this field. Multimodal data improve neural networks’ performance as they provide complementary information. This paper presents a comprehensive overview of the latest studies on multimodal medical data analysis using deep learning algorithms. We divided related articles into four main categories, including supervised, semi-supervised, self-supervised, and unsupervised methods. We observed that many articles on COVID-19 had used transfer learning because they did not access large datasets. We conclude transfer learning methods are invaluable in situations such as pandemics when not enough data is available. Different modalities, deep learning architectures, and fusion strategies are also introduced in this paper. Furthermore, we provided links to access some of the most well-known multimodal datasets and identified common problems and open challenges in this field. We believe that deep learning methods in multimodal medical data analysis will remain an active research area in the coming years.","An overview of deep learning methods for multimodal medical data mining Multimodal medical data , Review.Deep learning methods have achieved significant results in various fields. Due to the success of these methods, many researchers have used deep learning algorithms in medical analyses. Using multimodal data to achieve more accurate results is a successful strategy because multimodal data provide complementary information. This paper first introduces the most popular modalities, fusion strategies, and deep learning architectures. We also explain learning strategies, including transfer learning, end-to-end learning, and multitask learning. Then, we give an overview of deep learning methods for multimodal medical data analysis. We have focused on articles published over the last four years. We end with a summary of the current state-of-the-art, common problems, and directions for future research.Deep learning is a powerful technique to analyze complex medical data. Recently, deep learning methods have become very popular in medical analyses because they have achieved outstanding results in this field. Multimodal data improve neural networks’ performance as they provide complementary information. This paper presents a comprehensive overview of the latest studies on multimodal medical data analysis using deep learning algorithms. We divided related articles into four main categories, including supervised, semi-supervised, self-supervised, and unsupervised methods. We observed that many articles on COVID-19 had used transfer learning because they did not access large datasets. We conclude transfer learning methods are invaluable in situations such as pandemics when not enough data is available. Different modalities, deep learning architectures, and fusion strategies are also introduced in this paper. Furthermore, we provided links to access some of the most well-known multimodal datasets and identified common problems and open challenges in this field. We believe that deep learning methods in multimodal medical data analysis will remain an active research area in the coming years.Decoder Encoder Output |_| Hidden state pA Xi X2 X3 XT a InputPrevious layer 1*1 1*1 1*1 3°*3 Channel Concatenation PS Convolution [| Max poolingLayer input Convolution Skip connection ConvolutionConcatenation fer Convolution +ReLU | Max pooling 2 * 2 SS Up convolution 2 * 2yt YuiModelity A Modality B Modality A Modality B Modality A Modality B Modality A Modality B ett Neural Network a v | > > > | Fusion > X. Sire (a) Input-level fusion , ae Ste ae Site 5° SAMS ° at (b) Layer-level fusion 5 > Output ae Site > OutputB (c) Decision-level fusion Output Output Output OutputDNA TACGGCGTTAGACAAGTGCGAGTACAC ATGCCGCAATCTGTTCACGCACTCATG Transcription AUGCCGCAAUCUGUUCACGCACUCAUG RNA Protein Met Pro Gin Ser Val His Ala Leu MetNumber of papers sd) 160 140 120 100 80 60 40 20 2010 2013 2014 2015 2016 2017 2018 2019 2020 2021 Years : eee 0 ; 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 Years2019 2020 2021 Attention U-Net EfficientNet U-Net 3+ UNETR SegAN CE-Net CA-Net TransUNet U-Net++ 2018 Swin-Unet R2U-Net MedT 2017 2016 2015 2014 DenseNet U-Net GAN RAN V-Net ResNet VGG DeepLab Highway network Inception 2012 oO AlexNet Attention mechanism GRU 1986 1989 1997 2006 Autoencoder LeNet LSTM DBN Hopfield Network RBM 5 1982 Self-organized map 1980 1968 1957 1943 Neocognitron Group P ti - 6 Method of ""TecPton McCulloch Pitts Data HandlingCNN 2018 (Y Wang et al., 2018) (Zhiguo Zhou et al., 2018) (Kirienko et al., 2018) (Manhua Liu et al., 2018) (Ma et al., 2018) (Vasquez-Correa et al., 2018) 2019 (Guo, Li, et al., 2019) (McKinley et al., 2019) (Nie et al., 2019) (Peng et al., 2019) (HL. Lictal., 2019) (Feng et al., 2019a) (Mingxia Liu et al., 2019) (F. Zhang et al., 2019b) 2020 (Shikalgar & Sonavane, 2020) (McKinley et al., 2020) (Tang et al., 2020) (Mukherjee et al., 2020) (M. Xuet al., 2020) (7. Zhang & Shi, 2020a) (El-Sappagh et al., 2020b) (Qiu et al., 2020) 2021 CY. D. Zhang et al., 2021) (CNN-based Inception 2020 (El Asnaoui & Chawki, 2020) (Kassani et al., 2020) (Vaghefi et al., 2020) U-Net 2019 (Myronenko, 2019) (Isensee et al., 2019) 2020 (Z. Jiang et al., 2020) (Y. Zhao et al., 2020) Dense-Net 2018 Liang et al. (2018) 2019 Guo, et al. (2019) 2020 Qin et al. (2020) S. Wang et al. (2020) Supervised00 VGG 2018 (Vu et al., 2018) 2019 (Yan et al., 2019) 2020 (Saba et al., 2020) (X. Jiang et al., 2020) ResNet 2018 (Yap et al.,2018) 2019 (Abrol et al., 2019) 2020 (Rehman et al.,2020 (van Sonsbeek & Worring, 2020) RNN-based ey RNN FCN 2019 2019 (G. Lee, Nho, et al., 2019 (Lai et al., 2019) 2020 (H. Chen et al., 2019b P (Soltaninejad et al.2019). (Shukla & Marlin, 2020) (Sun et al..2019) a (Hung et al..2019) LSTM 2020 2019 (Lassau et al., 2020) (Feng et al., 2019b) (Ali et al.,2020) 2020 (Bai et al., 2020) (Bagheri et al., 2020) (D. Zhang et al.,2020) GRU 2019 (G. Lee, Kang, et al.,2019 CNN 2020 (Bagheri et al.,2020) (Hosseini et al..2020) (Ge et al.,2020) (Y. Huang & Chung.2020) Auto-encoder Anto-encoder 2019 2016 (Rubinstein et al.,2019) (Varghese et al.,2016) (Q. Liu & Hu, 2019) 2019 (Khamparia et al.,2019) U-Net 2020 (Z. Wang et al. 2020) (Y. X. Zhao et al..2020) U-Net 2019 (Hervella et al.,2019) 2020 (Hervella et al..2020) CNN 2021 (Taleb et al., 2021) GAN 2020 (X. Liet al.,2020) (B. Cao et al..2020)",Deep Learning and Machine Learning,"The paper provides an overview of deep learning methods for multimodal medical data analysis, which has become increasingly popular due to the success of deep learning algorithms in various fields. The paper explains the popular modalities, fusion strategies, deep learning architectures, and learning strategies such as transfer learning, end-to-end learning, and multitask learning. The authors review articles published in the last four years and divide them into supervised, semi-supervised, self-supervised, and unsupervised methods. They conclude that multimodal data improve neural networks' performance by providing complementary information, and transfer learning is valuable when data is limited, such as during a pandemic. The paper also identifies common problems and open challenges in this field and provides links to some of the most well-known multimodal datasets. The authors believe that deep learning methods in multimodal medical data analysis will remain an active research area in the coming years.",Medical Data Analysis,"Decoder Encoder Output |_| Hidden state pA Xi X2 X3 XT a InputPrevious layer 1*1 1*1 1*1 3°*3 Channel Concatenation PS Convolution [| Max poolingLayer input Convolution Skip connection ConvolutionConcatenation fer Convolution +ReLU | Max pooling 2 * 2 SS Up convolution 2 * 2yt YuiModelity A Modality B Modality A Modality B Modality A Modality B Modality A Modality B ett Neural Network a v | > > > | Fusion > X. Sire (a) Input-level fusion , ae Ste ae Site 5° SAMS ° at (b) Layer-level fusion 5 > Output ae Site > OutputB (c) Decision-level fusion Output Output Output OutputDNA TACGGCGTTAGACAAGTGCGAGTACAC ATGCCGCAATCTGTTCACGCACTCATG Transcription AUGCCGCAAUCUGUUCACGCACUCAUG RNA Protein Met Pro Gin Ser Val His Ala Leu MetNumber of papers sd) 160 140 120 100 80 60 40 20 2010 2013 2014 2015 2016 2017 2018 2019 2020 2021 Years : eee 0 ; 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 Years2019 2020 2021 Attention U-Net EfficientNet U-Net 3+ UNETR SegAN CE-Net CA-Net TransUNet U-Net++ 2018 Swin-Unet R2U-Net MedT 2017 2016 2015 2014 DenseNet U-Net GAN RAN V-Net ResNet VGG DeepLab Highway network Inception 2012 oO AlexNet Attention mechanism GRU 1986 1989 1997 2006 Autoencoder LeNet LSTM DBN Hopfield Network RBM 5 1982 Self-organized map 1980 1968 1957 1943 Neocognitron Group P ti - 6 Method of ""TecPton McCulloch Pitts Data HandlingCNN 2018 (Y Wang et al., 2018) (Zhiguo Zhou et al., 2018) (Kirienko et al., 2018) (Manhua Liu et al., 2018) (Ma et al., 2018) (Vasquez-Correa et al., 2018) 2019 (Guo, Li, et al., 2019) (McKinley et al., 2019) (Nie et al., 2019) (Peng et al., 2019) (HL. Lictal., 2019) (Feng et al., 2019a) (Mingxia Liu et al., 2019) (F. Zhang et al., 2019b) 2020 (Shikalgar & Sonavane, 2020) (McKinley et al., 2020) (Tang et al., 2020) (Mukherjee et al., 2020) (M. Xuet al., 2020) (7. Zhang & Shi, 2020a) (El-Sappagh et al., 2020b) (Qiu et al., 2020) 2021 CY. D. Zhang et al., 2021) (CNN-based Inception 2020 (El Asnaoui & Chawki, 2020) (Kassani et al., 2020) (Vaghefi et al., 2020) U-Net 2019 (Myronenko, 2019) (Isensee et al., 2019) 2020 (Z. Jiang et al., 2020) (Y. Zhao et al., 2020) Dense-Net 2018 Liang et al. (2018) 2019 Guo, et al. (2019) 2020 Qin et al. (2020) S. Wang et al. (2020) Supervised00 VGG 2018 (Vu et al., 2018) 2019 (Yan et al., 2019) 2020 (Saba et al., 2020) (X. Jiang et al., 2020) ResNet 2018 (Yap et al.,2018) 2019 (Abrol et al., 2019) 2020 (Rehman et al.,2020 (van Sonsbeek & Worring, 2020) RNN-based ey RNN FCN 2019 2019 (G. Lee, Nho, et al., 2019 (Lai et al., 2019) 2020 (H. Chen et al., 2019b P (Soltaninejad et al.2019). (Shukla & Marlin, 2020) (Sun et al..2019) a (Hung et al..2019) LSTM 2020 2019 (Lassau et al., 2020) (Feng et al., 2019b) (Ali et al.,2020) 2020 (Bai et al., 2020) (Bagheri et al., 2020) (D. Zhang et al.,2020) GRU 2019 (G. Lee, Kang, et al.,2019 CNN 2020 (Bagheri et al.,2020) (Hosseini et al..2020) (Ge et al.,2020) (Y. Huang & Chung.2020) Auto-encoder Anto-encoder 2019 2016 (Rubinstein et al.,2019) (Varghese et al.,2016) (Q. Liu & Hu, 2019) 2019 (Khamparia et al.,2019) U-Net 2020 (Z. Wang et al. 2020) (Y. X. Zhao et al..2020) U-Net 2019 (Hervella et al.,2019) 2020 (Hervella et al..2020) CNN 2021 (Taleb et al., 2021) GAN 2020 (X. Liet al.,2020) (B. Cao et al..2020)",Medical Data Analysis
35,Application of deep learning for retinal image analysis: A review,"Deep learning , Deep neural network , Convolutional neural network , Auto-encoder , Sparse stacked auto-encoder , De-noised sparse auto-encoder , Softmax , Random forest , Rectified linear unit , Hidden layers.","Retinal image analysis holds an imperative position for the identification and classification of retinal diseases such as Diabetic Retinopathy (DR), Age Related Macular Degeneration (AMD), Macular Bunker, Retinoblastoma, Retinal Detachment, and Retinitis Pigmentosa. Automated identification of retinal diseases is a big step towards early diagnosis and prevention of exacerbation of the disease. A number of state-of-the-art methods have been developed in the past that helped in the automatic segmentation and identification of retinal landmarks and pathologies. However, the current unprecedented advancements in deep learning and modern imaging modalities in ophthalmology have opened a whole new arena for researchers. This paper is a review of deep learning techniques applied to 2-D fundus and 3-D Optical Coherence Tomography (OCT) retinal images for automated classification of retinal landmarks, pathology, and disease classification. The methodologies are analyzed in terms of sensitivity, specificity, Area under ROC curve, accuracy, and F score on publicly available datasets which includes DRIVE, STARE, CHASE_DB1, DRiDB, NIH AREDS, ARIA, MESSIDOR-2, E-OPTHA, EyePACS-1 DIARETDB and OCT image datasets.","Retinal image analysis through DNNs is a nascent field. Although research has been conducted in extraction of retinal landmarks and pathologies but the epitome of this technique is yet to be witnessed. However, unsupervised learning based DNNs have not seen much progress in retinal image analysis. Deep learning techniques can be efficiently applied for segmentation of dot and blot hemorrhages, cotton wool spots, hard exudates, soft exudates, drusen etc. There is no restriction on number of layers and architecture of neural network, network architecture is chosen heuristically in accordance with problem domain. The variants of Deep Neural Networks like AlexNet, LSTM, VggNet, and GoogleNet can be used for extraction of retinal anatomical structures. Although VGG-16 is used by Lee et al. [78] for 3-D OCT retinal image analysis; however, there is no precedence of its use in case of color fundus images. All these networks are very deep and they have capability of extracting much more complex features than those extracted by traditional methods and with better performance measures. This property makes DNN capable of replacing traditional ophthalmologic screening practices.","Application of deep learning for retinal image analysis: A reviewDeep learning , Deep neural network , Convolutional neural network , Auto-encoder , Sparse stacked auto-encoder , De-noised sparse auto-encoder , Softmax , Random forest , Rectified linear unit , Hidden layers.Retinal image analysis holds an imperative position for the identification and classification of retinal diseases such as Diabetic Retinopathy (DR), Age Related Macular Degeneration (AMD), Macular Bunker, Retinoblastoma, Retinal Detachment, and Retinitis Pigmentosa. Automated identification of retinal diseases is a big step towards early diagnosis and prevention of exacerbation of the disease. A number of state-of-the-art methods have been developed in the past that helped in the automatic segmentation and identification of retinal landmarks and pathologies. However, the current unprecedented advancements in deep learning and modern imaging modalities in ophthalmology have opened a whole new arena for researchers. This paper is a review of deep learning techniques applied to 2-D fundus and 3-D Optical Coherence Tomography (OCT) retinal images for automated classification of retinal landmarks, pathology, and disease classification. The methodologies are analyzed in terms of sensitivity, specificity, Area under ROC curve, accuracy, and F score on publicly available datasets which includes DRIVE, STARE, CHASE_DB1, DRiDB, NIH AREDS, ARIA, MESSIDOR-2, E-OPTHA, EyePACS-1 DIARETDB and OCT image datasets.Retinal image analysis through DNNs is a nascent field. Although research has been conducted in extraction of retinal landmarks and pathologies but the epitome of this technique is yet to be witnessed. However, unsupervised learning based DNNs have not seen much progress in retinal image analysis. Deep learning techniques can be efficiently applied for segmentation of dot and blot hemorrhages, cotton wool spots, hard exudates, soft exudates, drusen etc. There is no restriction on number of layers and architecture of neural network, network architecture is chosen heuristically in accordance with problem domain. The variants of Deep Neural Networks like AlexNet, LSTM, VggNet, and GoogleNet can be used for extraction of retinal anatomical structures. Although VGG-16 is used by Lee et al. [78] for 3-D OCT retinal image analysis; however, there is no precedence of its use in case of color fundus images. All these networks are very deep and they have capability of extracting much more complex features than those extracted by traditional methods and with better performance measures. This property makes DNN capable of replacing traditional ophthalmologic screening practices.ae. Drusen(Yéllow Dots)Arterioles Macular Region a NTA (x3 ~ Optic Disk et ECT) Arterioles R fey NST (=r yrSoft Exudates Arterioles — Jf ~s Hemorrhages ~ f a Optic Disk \ Venules Exudates : Hemorrhages he sre Hard Exudates a a Soft Exudatesinput layer hidden layer 1 hidden layer 2 (a) (b) GQ 5 G n; input feature maps feature mapsfeature mapsfeature maps output VOTES STS . my ie # \ “@ ON <[- fate"", \ os \ l “i =F ap a ® y =e. z *. i , ~~ full convolution ™ subsamping — convolution =| a \ nen \ We cted subsampling ee \ feature extraction classification (c)",Deep Learning and Machine Learning,"This paper provides a review of deep learning techniques applied to 2-D fundus and 3-D Optical Coherence Tomography (OCT) retinal images for automated classification of retinal landmarks, pathology, and disease classification. The authors discuss the importance of early detection and prevention of retinal diseases, and how deep learning algorithms can aid in this process. They analyze the methodologies in terms of sensitivity, specificity, accuracy, and F score on publicly available datasets, and conclude that while deep learning has shown promise in retinal image analysis, there is still much progress to be made. They suggest that deep neural networks (DNNs) can be efficiently applied for segmentation of various retinal pathologies, and that DNNs have the potential to replace traditional ophthalmologic screening practices.",Deep Learning and Machine Learning,"ae. Drusen(Yéllow Dots)Arterioles Macular Region a NTA (x3 ~ Optic Disk et ECT) Arterioles R fey NST (=r yrSoft Exudates Arterioles — Jf ~s Hemorrhages ~ f a Optic Disk \ Venules Exudates : Hemorrhages he sre Hard Exudates a a Soft Exudatesinput layer hidden layer 1 hidden layer 2 (a) (b) GQ 5 G n; input feature maps feature mapsfeature mapsfeature maps output VOTES STS . my ie # \ “@ ON <[- fate"", \ os \ l “i =F ap a ® y =e. z *. i , ~~ full convolution ™ subsamping — convolution =| a \ nen \ We cted subsampling ee \ feature extraction classification (c)",Deep Learning and Machine Learning
36,Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: A review,"Covid-19 , Machine learning , Artificial intelligence , Pandemic.","Background and objective: During the recent global urgency, scientists, clinicians, and healthcare experts around the globe keep on searching for a new technology to support in tackling the Covid-19 pandemic. The evidence of Machine Learning (ML) and Artificial Intelligence (AI) application on the previous epidemic encourage researchers by giving a new angle to fight against the novel Coronavirus outbreak. This paper aims to comprehensively review the role of AI and ML as one significant method in the arena of screening, predicting, forecasting, contact tracing, and drug development for SARS-CoV-2 and its related epidemic. Method: A selective assessment of information on the research article was executed on the databases related to the application of ML and AI technology on Covid-19. Rapid and critical analysis of the three crucial parameters, i.e., abstract, methodology, and the conclusion was done to relate to the model’s possibilities for tackling the SARS-CoV-2 epidemic. Result: This paper addresses on recent studies that apply ML and AI technology towards augmenting the researchers on multiple angles. It also addresses a few errors and challenges while using such algorithms in real-world problems. The paper also discusses suggestions conveying researchers on model design, medical experts, and policymakers in the current situation while tackling the Covid-19 pandemic and ahead. Conclusion: The ongoing development in AI and ML has significantly improved treatment, medication, screening, prediction, forecasting, contact tracing, and drug/vaccine development process for the Covid-19 pandemic and reduce the human intervention in medical practice. However, most of the models are not deployed enough to show their real-world operation, but they are still up to the mark to tackle the SARS-CoV-2 epidemic.","Since the outbreak of the novel SARS-CoV-2, scientists and medical industries around the globe ubiquitously urged to fight against the pandemic, searching alternative method of rapid screening and prediction process, contact tracing, forecasting, and development of vaccine or drugs with the more accurate and reliable operation. Machine Learning and Artificial Intelligence are such promising methods employed by various healthcare providers. This paper addresses on recent studies that apply such advance technology in augmenting the researchers in multiple angles, addressing the troubles and challenges while using such algorithm in assisting medical expert in real-world problems. This paper also discusses suggestions conveying researchers on AI/ML based model design, medical experts, and policymakers on few errors encountered in the current situation while tackling the current pandemic. This review shows that the use of modern technology with AI and ML dramatically improves the screening, prediction, contact tracing, forecasting, and drug/vaccine development with extreme reliability. Majority of the paper employed deep learning algorithms and is found to have more potential, robust, and advance among the other learning algorithms. However, the current urgency requires an improved model with high-end performance accuracy in screening and predicting the SARS-CoV-2 with a different kind of related disease by analyzing the clinical, mammographic, and demographic information of the suspects and infected patients. Finally, it is evident that AI and ML can significantly improve treatment, medication, screening & prediction, forecasting, contact tracing, and drug/vaccine development for the Covid-19 pandemic and reduce the human intervention in medical practice. However, most of the models are not deployed enough to show their real-world operation, but they are still up to the mark to tackle the pandemic.","Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: A reviewCovid-19 , Machine learning , Artificial intelligence , Pandemic.Background and objective: During the recent global urgency, scientists, clinicians, and healthcare experts around the globe keep on searching for a new technology to support in tackling the Covid-19 pandemic. The evidence of Machine Learning (ML) and Artificial Intelligence (AI) application on the previous epidemic encourage researchers by giving a new angle to fight against the novel Coronavirus outbreak. This paper aims to comprehensively review the role of AI and ML as one significant method in the arena of screening, predicting, forecasting, contact tracing, and drug development for SARS-CoV-2 and its related epidemic. Method: A selective assessment of information on the research article was executed on the databases related to the application of ML and AI technology on Covid-19. Rapid and critical analysis of the three crucial parameters, i.e., abstract, methodology, and the conclusion was done to relate to the model’s possibilities for tackling the SARS-CoV-2 epidemic. Result: This paper addresses on recent studies that apply ML and AI technology towards augmenting the researchers on multiple angles. It also addresses a few errors and challenges while using such algorithms in real-world problems. The paper also discusses suggestions conveying researchers on model design, medical experts, and policymakers in the current situation while tackling the Covid-19 pandemic and ahead. Conclusion: The ongoing development in AI and ML has significantly improved treatment, medication, screening, prediction, forecasting, contact tracing, and drug/vaccine development process for the Covid-19 pandemic and reduce the human intervention in medical practice. However, most of the models are not deployed enough to show their real-world operation, but they are still up to the mark to tackle the SARS-CoV-2 epidemic.Since the outbreak of the novel SARS-CoV-2, scientists and medical industries around the globe ubiquitously urged to fight against the pandemic, searching alternative method of rapid screening and prediction process, contact tracing, forecasting, and development of vaccine or drugs with the more accurate and reliable operation. Machine Learning and Artificial Intelligence are such promising methods employed by various healthcare providers. This paper addresses on recent studies that apply such advance technology in augmenting the researchers in multiple angles, addressing the troubles and challenges while using such algorithm in assisting medical expert in real-world problems. This paper also discusses suggestions conveying researchers on AI/ML based model design, medical experts, and policymakers on few errors encountered in the current situation while tackling the current pandemic. This review shows that the use of modern technology with AI and ML dramatically improves the screening, prediction, contact tracing, forecasting, and drug/vaccine development with extreme reliability. Majority of the paper employed deep learning algorithms and is found to have more potential, robust, and advance among the other learning algorithms. However, the current urgency requires an improved model with high-end performance accuracy in screening and predicting the SARS-CoV-2 with a different kind of related disease by analyzing the clinical, mammographic, and demographic information of the suspects and infected patients. Finally, it is evident that AI and ML can significantly improve treatment, medication, screening & prediction, forecasting, contact tracing, and drug/vaccine development for the Covid-19 pandemic and reduce the human intervention in medical practice. However, most of the models are not deployed enough to show their real-world operation, but they are still up to the mark to tackle the pandemic.",Deep Learning and Machine Learning,"This paper comprehensively reviews the role of Machine Learning (ML) and Artificial Intelligence (AI) in screening, predicting, forecasting, contact tracing, and drug development for SARS-CoV-2 and its related epidemic. The review shows that the use of modern technology with AI and ML dramatically improves the screening, prediction, contact tracing, forecasting, and drug/vaccine development with extreme reliability. Majority of the paper employed deep learning algorithms and is found to have more potential, robust, and advance among the other learning algorithms. However, the current urgency requires an improved model with high-end performance accuracy in screening and predicting the SARS-CoV-2 with different kinds of related disease by analyzing the clinical, mammographic, and demographic information of the suspects and infected patients. Finally, it is evident that AI and ML can significantly improve treatment, medication, screening & prediction, forecasting, contact tracing, and drug/vaccine development for the Covid-19 pandemic and reduce the human intervention in medical practice.",Deep Learning and Machine Learning,,Deep Learning and Machine Learning
37,Deep Learning and Medical Diagnosis: A Review of Literature,medical diagnosis; segmentation;,"In this review the application of deep learning for medical diagnosis is addressed. A thorough analysis of various scientific articles in the domain of deep neural networks application in the medical field has been conducted. More than 300 research articles were obtained, and after several selection steps, 46 articles were presented in more detail. The results indicate that convolutional neural networks (CNN) are the most widely represented when it comes to deep learning and medical image analysis. Furthermore, based on the findings of this article, it can be noted that the application of deep learning technology is widespread, but the majority of applications are focused on bioinformatics, medical diagnosis and other similar fields.","Deep learning methods have a wide application in the medical field. In this case, medical diagnosis is conducted through use-cases of deep learning networks. As mentioned before, these include detection, segmentation, classification, prediction and other. The results of the reviewed studies indicate that deep learning methods can be far superior in comparison to other high-performing algorithms. Therefore, it is safe to assume that deep learning is and will continue to diversify its uses. The future development of deep learning promises more applications in various fields of medicine, particularly in the domain of medical diagnosis. However, in the current state, it is not evident that deep learning can substitute the role of doctors/clinicians in medical diagnosis. So far, deep learning can provide good support for experts in the medical field. All indicators point towards an even wider use of deep learning in various fields. Deep learning has already found its application in transportation and greenhouse-gas emission control, traffic control, text classification, object detection, speech detection, translation and in other fields. These applications were not so represented in the past. Traditional approaches to various similarity measures are ineffective when compared to deep learning. Based on these findings, it can be suggested that deep learning and deep neural networks will prevail, and that they will find many other uses in the near future.","Deep Learning and Medical Diagnosis: A Review of Literaturemedical diagnosis; segmentation;In this review the application of deep learning for medical diagnosis is addressed. A thorough analysis of various scientific articles in the domain of deep neural networks application in the medical field has been conducted. More than 300 research articles were obtained, and after several selection steps, 46 articles were presented in more detail. The results indicate that convolutional neural networks (CNN) are the most widely represented when it comes to deep learning and medical image analysis. Furthermore, based on the findings of this article, it can be noted that the application of deep learning technology is widespread, but the majority of applications are focused on bioinformatics, medical diagnosis and other similar fields.Deep learning methods have a wide application in the medical field. In this case, medical diagnosis is conducted through use-cases of deep learning networks. As mentioned before, these include detection, segmentation, classification, prediction and other. The results of the reviewed studies indicate that deep learning methods can be far superior in comparison to other high-performing algorithms. Therefore, it is safe to assume that deep learning is and will continue to diversify its uses. The future development of deep learning promises more applications in various fields of medicine, particularly in the domain of medical diagnosis. However, in the current state, it is not evident that deep learning can substitute the role of doctors/clinicians in medical diagnosis. So far, deep learning can provide good support for experts in the medical field. All indicators point towards an even wider use of deep learning in various fields. Deep learning has already found its application in transportation and greenhouse-gas emission control, traffic control, text classification, object detection, speech detection, translation and in other fields. These applications were not so represented in the past. Traditional approaches to various similarity measures are ineffective when compared to deep learning. Based on these findings, it can be suggested that deep learning and deep neural networks will prevail, and that they will find many other uses in the near future.Records identified through Additional records identified & database searching through other sources 3 (n=136) (n=196 ) = 5 = ey Records after duplicates removed _ (n = 263 ) £ t 3 Records screened Records excluded ” (n = 263 ) : (n =120) ¥ ia) Full-text articles assessed Full-text articles excluded, for eligibility -—>) with reasons (n= 143) (n =97) 3 ¥ a 3 Studies included in is qualitative synthesis a (n= 46)",Deep Learning and Machine Learning,"This review analyzes the application of deep learning in medical diagnosis and concludes that deep learning methods have a wide range of applications in the medical field, particularly in bioinformatics, medical diagnosis, and similar fields. The review notes that convolutional neural networks are the most widely represented in deep learning and medical image analysis. While deep learning technology is widespread, the majority of its applications are focused on medical diagnosis. The review suggests that deep learning methods can be superior to other high-performing algorithms and will continue to diversify their uses, particularly in the domain of medical diagnosis. However, deep learning cannot replace the role of doctors/clinicians in medical diagnosis, but can provide good support for experts in the medical field. Additionally, the review suggests that deep learning will find many other uses in various fields in the near future.",Medical Data Analysis,"Records identified through Additional records identified & database searching through other sources 3 (n=136) (n=196 ) = 5 = ey Records after duplicates removed _ (n = 263 ) £ t 3 Records screened Records excluded ” (n = 263 ) : (n =120) ¥ ia) Full-text articles assessed Full-text articles excluded, for eligibility -—>) with reasons (n= 143) (n =97) 3 ¥ a 3 Studies included in is qualitative synthesis a (n= 46)",Medical Data Analysis
38,Deep learning for heterogeneous medical data analysis,"Medical Data Analysis  , Survey.","At present, how to make use of massive medical information resources to provide scientific decision-making for the diagnosis and treatment of diseases, summarize the curative effect of various treatment schemes, and better serve the decision-making management, medical treatment, and scientific research, has drawn more and more attention of researchers. Deep learning, as the focus of most concern by both academia and industry, has been effectively applied in many fields and has outperformed most of the machine learning methods. Under this background, deep learning based medical data analysis emerged. In this survey, we focus on reviewing and then categorizing the current development. Firstly, we fully discuss the scope, characteristic and structure of the heterogeneous medical data. Afterward and primarily, the main deep learning models involved in medical data analysis, including their variants and various hybrid models, as well as main tasks in medical data analysis are all analyzed and reviewed in a series of typical cases respectively. Finally, we provide a brief introduction to certain useful online resources of deep learning development tools.","Deep learning is a powerful technology to realize complex medical data analysis tasks. So far, a large number of deep learning models have appeared, from a pure model perspective, some of which may not be new things. The main advantage of deep model is that when the node size of the deep network remains roughly unchanged, more powerful function expression can be obtained by increasing the number of node layers. This capability is often achieved by adjusting network weights and biases. In the process of solving practical problems, such ability can be used to create abstract level representations of static data, sequence data, and decision-making data, etc. Although deep learning has achieved some satisfying results in medical data analysis, it also has problems from different perspectives. For example, most of the learning content of deep structure is only variations of gradient descent algorithms, and some algorithms lack solid theoretical basis and strict theoretical proofs. In addition, deep learning requires considerable skill and experience in trying out the configurations and structures of various deep networks, which involve many variations and a large number of high-dimensional parameters. In addition, in order to better understand the success and failure of deep learning models, appropriate theory should be established to clearly explain the convergence, speed, approximation and other characteristics of learning algorithms, and even to give strict proofs for some contents. Finally, deep learning technology cannot completely replace other technologies. We should also explore the combination of deep learning and traditional models, establish new models and algorithms, and explore and apply a variety of large-scale problems.","Deep learning for heterogeneous medical data analysisMedical Data Analysis  , Survey.At present, how to make use of massive medical information resources to provide scientific decision-making for the diagnosis and treatment of diseases, summarize the curative effect of various treatment schemes, and better serve the decision-making management, medical treatment, and scientific research, has drawn more and more attention of researchers. Deep learning, as the focus of most concern by both academia and industry, has been effectively applied in many fields and has outperformed most of the machine learning methods. Under this background, deep learning based medical data analysis emerged. In this survey, we focus on reviewing and then categorizing the current development. Firstly, we fully discuss the scope, characteristic and structure of the heterogeneous medical data. Afterward and primarily, the main deep learning models involved in medical data analysis, including their variants and various hybrid models, as well as main tasks in medical data analysis are all analyzed and reviewed in a series of typical cases respectively. Finally, we provide a brief introduction to certain useful online resources of deep learning development tools.Deep learning is a powerful technology to realize complex medical data analysis tasks. So far, a large number of deep learning models have appeared, from a pure model perspective, some of which may not be new things. The main advantage of deep model is that when the node size of the deep network remains roughly unchanged, more powerful function expression can be obtained by increasing the number of node layers. This capability is often achieved by adjusting network weights and biases. In the process of solving practical problems, such ability can be used to create abstract level representations of static data, sequence data, and decision-making data, etc. Although deep learning has achieved some satisfying results in medical data analysis, it also has problems from different perspectives. For example, most of the learning content of deep structure is only variations of gradient descent algorithms, and some algorithms lack solid theoretical basis and strict theoretical proofs. In addition, deep learning requires considerable skill and experience in trying out the configurations and structures of various deep networks, which involve many variations and a large number of high-dimensional parameters. In addition, in order to better understand the success and failure of deep learning models, appropriate theory should be established to clearly explain the convergence, speed, approximation and other characteristics of learning algorithms, and even to give strict proofs for some contents. Finally, deep learning technology cannot completely replace other technologies. We should also explore the combination of deep learning and traditional models, establish new models and algorithms, and explore and apply a variety of large-scale problems.ee Figure 2 Multi-dimensional scheme for classification of deep learning based medical data analysisCardiovascular diseases Cancers Respiratory disease Diabetes, blood and endocrine disease Dementia Lower respiratory infections Neonatal deaths Diartheal diseases Liver disease Tuberculosis Kidney disease Digestive disease HIV/AIDS Malaria Nutritional deficiencies Meningitis Protein-energy malnutrition Matemal deaths Parkinson's disease Intestinal infectious diseases Drug disorder Hepatitis ° a o oooh Ens 3.54 een mee! 3.19 Ea 2.38 Sa 2.38 = 1.73 = 1.66 EERE 1.26 = 1.21 co 1.19 oI 1.09 SS 1.03 o 0.719551 a 0.368107 a 0.3184 o 0.308304 i 0.230615 1 0.211296 1 0.155449 1 0.143775 1 0.134045 Number of people/Milhion 16 18 20 17.65 Figure 1 Deaths related to disease, illness and other health factors worldwide in 2016. Non-communicable diseases (NCDs) are as follows: “Dementia” refers to the number of deaths attributed to Alzheimer’s and other forms of dementia. “Cardiovascular disease” refers to rheumatic and ischemic heart diseases. “Respiratory diseases” include deaths from chronic obstructive pulmonary disease (COPD), and pneumoconiosis diseases. “Cancers” include all forms of cancer, also referred to in the GBD database as “neoplasms”. “Digestive diseases” refer to all deaths resultant from ulcer diseases, pancreatitis, gallbladder, bowel disease, gastritis, and intestinal diseases",Deep Learning and Machine Learning,"This review discusses the application of deep learning in medical data analysis. It highlights the need for scientific decision-making in the diagnosis and treatment of diseases using massive medical information resources. The review explores the scope, characteristics, and structure of heterogeneous medical data and discusses various deep learning models involved in medical data analysis, including their variants and hybrid models. The review also examines different tasks in medical data analysis and provides a brief introduction to useful online resources of deep learning development tools. Deep learning has shown promising results in medical data analysis, but there are challenges such as the lack of solid theoretical basis and the need for considerable skill and experience in configuring deep networks. Furthermore, deep learning technology cannot completely replace other technologies, and exploring the combination of deep learning and traditional models can lead to new models and algorithms for large-scale problems. Overall, deep learning is a powerful technology that can provide abstract level representations of static data, sequence data, and decision-making data.",Medical Data Analysis,"ee Figure 2 Multi-dimensional scheme for classification of deep learning based medical data analysisCardiovascular diseases Cancers Respiratory disease Diabetes, blood and endocrine disease Dementia Lower respiratory infections Neonatal deaths Diartheal diseases Liver disease Tuberculosis Kidney disease Digestive disease HIV/AIDS Malaria Nutritional deficiencies Meningitis Protein-energy malnutrition Matemal deaths Parkinson's disease Intestinal infectious diseases Drug disorder Hepatitis ° a o oooh Ens 3.54 een mee! 3.19 Ea 2.38 Sa 2.38 = 1.73 = 1.66 EERE 1.26 = 1.21 co 1.19 oI 1.09 SS 1.03 o 0.719551 a 0.368107 a 0.3184 o 0.308304 i 0.230615 1 0.211296 1 0.155449 1 0.143775 1 0.134045 Number of people/Milhion 16 18 20 17.65 Figure 1 Deaths related to disease, illness and other health factors worldwide in 2016. Non-communicable diseases (NCDs) are as follows: “Dementia” refers to the number of deaths attributed to Alzheimer’s and other forms of dementia. “Cardiovascular disease” refers to rheumatic and ischemic heart diseases. “Respiratory diseases” include deaths from chronic obstructive pulmonary disease (COPD), and pneumoconiosis diseases. “Cancers” include all forms of cancer, also referred to in the GBD database as “neoplasms”. “Digestive diseases” refer to all deaths resultant from ulcer diseases, pancreatitis, gallbladder, bowel disease, gastritis, and intestinal diseases",Medical Data Analysis
39,DEEP LEARNING TECHNOLOGY FOR IMPROVING CANCER CARE IN SOCIETY: NEW DIRECTIONS IN CANCER IMAGING DRIVEN BY ARTIFICIAL INTELLIGENCE,"cancer imaging, artificial intelligence, lung cancer, breast cancer, technological paradigm, Amara’s law, Gartner hype cycle, emerging technology, new technology.","The goal of this study is to show emerging applications of deep learning technology in cancer imaging. Deep learning technology is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior. Applications of deep learning technology to cancer imaging can assist pathologists in the detection and classification of cancer in the early stages of its development to allow patients to have appropriate treatments that can increase survival or recovery of patients. Statistical analyses and other analytical approaches, based on data of ScienceDirect (a source for scientific research), suggest that, since the late 1990s, the sharp increase of the studies of deep learning technology in cancer imaging seems to be driven by high rates of mortality of some types of cancer (e.g., lung and breast) in order to solve consequential problems of a more accurate detection and characterization of cancer types to apply efficient anti-cancer therapies. Moreover, this study also shows sources of the trajectories of deep learning technology in cancer imaging at level of scientific subject areas, universities and countries with the highest scientific production in these research fields. This new technology, in accordance with Amara’s law, can generate a shift of technological paradigm for diagnostic assessment of any cancer type and disease. This new technology can also generate benefits for poor regions because they can send digital images to labs of other developed regions to have diagnosis of cancer types, reducing as far as possible current gap in healthcare among different regions.","Deep learning technology has the opportunity to assist pathologists and physicians by improving the efficiency of their work, standardizing quality, and providing better prognostication. Although organizational workflow of pathologists and other physicians is likely to change with deep learning technology, the contributions of pathologists, and in general of humans, to patient care is important, very important to treat diseases. Deep learning technology in oncology, to date, has not been vigorously validated for reproducibility and generalizability of applications in clinical practice. Further research has to determine the feasibility of applying deep learning technology in different clinical settings and to determine whether the use of deep learning technology can improve health care and efficiency of hospitals and research labs. The study here suggests that deep learning technology in oncology can pave a revolution in the management of cancer based on new and effective diagnostic approaches in clinical practice. However, the results and arguments of this study are of course tentative. In fact, analyses here are not sufficient to explain the manifold economic, social and organizational pros and cons of the patterns of deep learning technology in cancer studies, since we know that other factors are often not equal over time and 25 space (cf., Sonntag, 2019). In general, this new technology seems to be subjected to the Amara’s law in which we tend to overestimate the effect of deep learning technology in the short run and underestimate the effect in the long run (Amara, 1984).","DEEP LEARNING TECHNOLOGY FOR IMPROVING CANCER CARE IN SOCIETY: NEW DIRECTIONS IN CANCER IMAGING DRIVEN BY ARTIFICIAL INTELLIGENCEcancer imaging, artificial intelligence, lung cancer, breast cancer, technological paradigm, Amara’s law, Gartner hype cycle, emerging technology, new technology.The goal of this study is to show emerging applications of deep learning technology in cancer imaging. Deep learning technology is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior. Applications of deep learning technology to cancer imaging can assist pathologists in the detection and classification of cancer in the early stages of its development to allow patients to have appropriate treatments that can increase survival or recovery of patients. Statistical analyses and other analytical approaches, based on data of ScienceDirect (a source for scientific research), suggest that, since the late 1990s, the sharp increase of the studies of deep learning technology in cancer imaging seems to be driven by high rates of mortality of some types of cancer (e.g., lung and breast) in order to solve consequential problems of a more accurate detection and characterization of cancer types to apply efficient anti-cancer therapies. Moreover, this study also shows sources of the trajectories of deep learning technology in cancer imaging at level of scientific subject areas, universities and countries with the highest scientific production in these research fields. This new technology, in accordance with Amara’s law, can generate a shift of technological paradigm for diagnostic assessment of any cancer type and disease. This new technology can also generate benefits for poor regions because they can send digital images to labs of other developed regions to have diagnosis of cancer types, reducing as far as possible current gap in healthcare among different regions.Deep learning technology has the opportunity to assist pathologists and physicians by improving the efficiency of their work, standardizing quality, and providing better prognostication. Although organizational workflow of pathologists and other physicians is likely to change with deep learning technology, the contributions of pathologists, and in general of humans, to patient care is important, very important to treat diseases. Deep learning technology in oncology, to date, has not been vigorously validated for reproducibility and generalizability of applications in clinical practice. Further research has to determine the feasibility of applying deep learning technology in different clinical settings and to determine whether the use of deep learning technology can improve health care and efficiency of hospitals and research labs. The study here suggests that deep learning technology in oncology can pave a revolution in the management of cancer based on new and effective diagnostic approaches in clinical practice. However, the results and arguments of this study are of course tentative. In fact, analyses here are not sufficient to explain the manifold economic, social and organizational pros and cons of the patterns of deep learning technology in cancer studies, since we know that other factors are often not equal over time and 25 space (cf., Sonntag, 2019). In general, this new technology seems to be subjected to the Amara’s law in which we tend to overestimate the effect of deep learning technology in the short run and underestimate the effect in the long run (Amara, 1984).25 20 15 Figure 8. Main scientific subject areas of the scientific production of deep learning technology in thyroid cancer380 330 280 230 180 130 8I So Figure 6. Main scientific subject areas of the scientific production of deep learning technology in breast cancer140 120 100 80 60 40 20 0Agricultural and Biological Sciences Pharmacology, Toxicology and... Health Professions Materials Science Physics and Astronomy Mathematics Biochemistry, Genetics and Molecular... Engineering Computer Science Medicine ° wu o B So 3 a a o 8Lung cancer deep learning and lung cancer Breast cancer deep learning and breast cancer Thyroid cancer deep learning and thyroid cancer © oneerved © observed © observed —Etsenetat 10004 | oa. ° on 9 col ofo ° ° «0 20 oo % °° 6 = ° Too = 1985 0 ‘bs mio as 20 year Figure 3. Exponential curves of the number of articles about the studies of deep learning in different types of cancer (lung, breast, thyroid), 1996-2018 period“deep learning and lung cancer deep learning and breast cancer —deep learning and thyroid cancer 2019 2018 2017 2016 2015 2014 2013 2012 e010 2009 2008 2007 F206 2005 2004 2003 2002 [2001 2000 1999 H926 P1997 P1996 254 sajomue yoseesed Jo Jequinn, 2254 284 year",Deep Learning and Machine Learning,"This study explores the emerging applications of deep learning technology in cancer imaging, highlighting its potential to improve the detection and classification of cancer in its early stages. The study discusses the trajectory of deep learning technology in cancer imaging research and its potential to revolutionize cancer management through new and effective diagnostic approaches. However, the study also notes that further research is needed to determine the feasibility of applying deep learning technology in different clinical settings and to determine whether its use can improve healthcare and efficiency in hospitals and research labs. The study acknowledges that the results and arguments presented are tentative and subject to the Amara's law, which suggests that the short-term effects of a new technology may be overestimated while the long-term effects may be underestimated.",Medical Data Analysis,"25 20 15 Figure 8. Main scientific subject areas of the scientific production of deep learning technology in thyroid cancer380 330 280 230 180 130 8I So Figure 6. Main scientific subject areas of the scientific production of deep learning technology in breast cancer140 120 100 80 60 40 20 0Agricultural and Biological Sciences Pharmacology, Toxicology and... Health Professions Materials Science Physics and Astronomy Mathematics Biochemistry, Genetics and Molecular... Engineering Computer Science Medicine ° wu o B So 3 a a o 8Lung cancer deep learning and lung cancer Breast cancer deep learning and breast cancer Thyroid cancer deep learning and thyroid cancer © oneerved © observed © observed —Etsenetat 10004 | oa. ° on 9 col ofo ° ° «0 20 oo % °° 6 = ° Too = 1985 0 ‘bs mio as 20 year Figure 3. Exponential curves of the number of articles about the studies of deep learning in different types of cancer (lung, breast, thyroid), 1996-2018 period“deep learning and lung cancer deep learning and breast cancer —deep learning and thyroid cancer 2019 2018 2017 2016 2015 2014 2013 2012 e010 2009 2008 2007 F206 2005 2004 2003 2002 [2001 2000 1999 H926 P1997 P1996 254 sajomue yoseesed Jo Jequinn, 2254 284 year",Medical Data Analysis
40,Exploring HCV genome to construct multi-epitope based subunit vaccine to battle HCV infection: Immunoinformatics based approach,"B cell , T cell epitope , HCV , Immunoinformatics , Molecular docking.","Hepatitis C Virus (HCV) infection is a major cause of chronic liver disease, hepatocellular carcinoma, and the single most common indication for liver transplantation. HCV vaccines eliciting specific T-cell responses, have been considered as potent method to prevent HCV infection. Despite several reports on progress of vaccine, these vaccine failed in mediating clinical relevance activity against HCV in humans. In this study we integrated both immunoinformatic and molecular docking approach to present a multiepitope vaccine against HCV by designating 17 conserved epitopes from eight viral proteins such as Core protein, E1, E2, NS2, NS34A, NS4B, NS5A, and NS5B. The epitopes were prioritized based on conservation among epitopes of T cell, B cell and IFN-γ that were then scanned for non-homologous to host and antigenicity. The prioritized epitopes were then linked together by AAY linker and adjuvant (β-defensin) were attached at N-terminal to enhance immunogenic potential. The construct thus formed were subjected to structural modeling and physiochemical characteristics. The modeled structure were successfully docked to antigenic receptor TLR-3 and In-silico cloning confers the authenticity of its expression efficiency. However, the proposed construct need to be validate experimentally to ensure its safety and immunogenic profile.","This study employs multiple approaches including immunoinformatic to design multiepitope vaccine that could be safe and harbors immunogenic potential which may consequently elicit both sort of responses: innate and adaptive thus contribute for control and prevention of HCV. Despite the computational prediction and validation of epitopes, this study warrants further experimental validation. This approach grant a template for the research of other emerging viruses and their subtype.","Exploring HCV genome to construct multi-epitope based subunit vaccine to battle HCV infection: Immunoinformatics based approachB cell , T cell epitope , HCV , Immunoinformatics , Molecular docking.Hepatitis C Virus (HCV) infection is a major cause of chronic liver disease, hepatocellular carcinoma, and the single most common indication for liver transplantation. HCV vaccines eliciting specific T-cell responses, have been considered as potent method to prevent HCV infection. Despite several reports on progress of vaccine, these vaccine failed in mediating clinical relevance activity against HCV in humans. In this study we integrated both immunoinformatic and molecular docking approach to present a multiepitope vaccine against HCV by designating 17 conserved epitopes from eight viral proteins such as Core protein, E1, E2, NS2, NS34A, NS4B, NS5A, and NS5B. The epitopes were prioritized based on conservation among epitopes of T cell, B cell and IFN-γ that were then scanned for non-homologous to host and antigenicity. The prioritized epitopes were then linked together by AAY linker and adjuvant (β-defensin) were attached at N-terminal to enhance immunogenic potential. The construct thus formed were subjected to structural modeling and physiochemical characteristics. The modeled structure were successfully docked to antigenic receptor TLR-3 and In-silico cloning confers the authenticity of its expression efficiency. However, the proposed construct need to be validate experimentally to ensure its safety and immunogenic profile.This study employs multiple approaches including immunoinformatic to design multiepitope vaccine that could be safe and harbors immunogenic potential which may consequently elicit both sort of responses: innate and adaptive thus contribute for control and prevention of HCV. Despite the computational prediction and validation of epitopes, this study warrants further experimental validation. This approach grant a template for the research of other emerging viruses and their subtype.Psi (degrees) 4 Ss 6 Phi (degrees) Fig. 3. 3D structure prediction and validation of Multi-epitope vaccine (A) 3D structure of final multi-epitope vaccine candidate. (B) The validation via Ramachandran plot of multi-epitope vaccine 3D structure where 87.8% residues reside in most favored region, 10.4% residues were present in additional allowed region, 1.3% and 0.4% in generously allowed and outlier region respectively.w f= 6 > 5 a 3 < NS5B EAAK Linker AAY Linkers Fig. 1. Schematic illustration of 263 amino acid long multi-epitope vaccine construct combined together by linkers and an adjuvant.et ARM rR pr re — A Ry RR yy | ems CCCCCCCCEEEEEECCEEEECCCCCCCEECEECCCCCCCCCCCCHHHHHH a GI I NTLQKYYCRVRGGRCAVLSCLPKEEQI GKCSTRGRKCCRRKKEAAAK 10 20 » “0 60 on ii — — a a a — — aR LI _________ ees HOC CCCCCCEEEEECCCCCCCHHCCCCCCCCCEEEEEECCCCCCcCCcecce ~~ WLSPRSRPWAAYVYLPRRGPLAAYLPRRGPLGVAAYVRTRKSERSAAY WP o 7 2 °0 100 oo Mia fe TS Co — wes CCOCHHCHHHHHHHHHCHHHHHHHHHHHCCCCHHHHHCCCHHHHHHHHHHH rm ROASYGCAAYVRGMCGFRTAAYWGLLVAEPFAAYYYHLPWGLLAAYYQTY 0 120 130 120 160 ees HOCCHHHCCCCCCCCHHHHHHHHCHHHHHHHHHCCCCCHHHHHHHHHHCC Mm YHLPWAAYGKSTKVPVLAAYCHLGI GTVLAAYKKCDETDALAAY! COECH 10 +70 100 190 8 On i il I con i —— eee nes CHHHHHCCCHHHHHHHHHCCCHHHHHHHHHHHHHCCCCCEEHHHHHCCCC wa LGI AAYASSASQLSLAAYGNTTCYKAAAAYYRRCRAGVTAAYLPI LSNRN 220 230 280 » & Cont cmt ew EGEEEEECCCCCCC Mm VAAYVYTSARKKYV Legend: Strand Confidence of prediction Helix ment cartoon — Coil fiction uence Fig. 2. Schematic illustration of secondary structure of the final vaccine candidate. It depicts the arrangement of a-helix (43.35%), B-strands (28.14%) and coils (28.52%).\ fA PY \ Fig. 4. (A) Vaccine construct-TLR3 docked complex: the receptor TLR3 is shown in blue color while green color is representing final vaccine construct. (B) Illustration highlighting Interacting residues of docked vaccine (chain B) with TLR3 (chain A). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)",Medical Data Analysis,"This study presents a novel approach to designing a multiepitope vaccine against Hepatitis C Virus (HCV) using immunoinformatics and molecular docking. The study identified 17 conserved epitopes from eight viral proteins and linked them together using a linker and adjuvant to enhance immunogenic potential. The modeled structure was successfully docked to antigenic receptor TLR-3, and in-silico cloning confirmed the expression efficiency. However, the proposed construct needs further experimental validation to ensure its safety and immunogenic profile. This approach offers a template for research on other emerging viruses and their subtypes. The study concludes that the multiepitope vaccine designed may contribute to the control and prevention of HCV.",Medical Data Analysis,"Psi (degrees) 4 Ss 6 Phi (degrees) Fig. 3. 3D structure prediction and validation of Multi-epitope vaccine (A) 3D structure of final multi-epitope vaccine candidate. (B) The validation via Ramachandran plot of multi-epitope vaccine 3D structure where 87.8% residues reside in most favored region, 10.4% residues were present in additional allowed region, 1.3% and 0.4% in generously allowed and outlier region respectively.w f= 6 > 5 a 3 < NS5B EAAK Linker AAY Linkers Fig. 1. Schematic illustration of 263 amino acid long multi-epitope vaccine construct combined together by linkers and an adjuvant.et ARM rR pr re — A Ry RR yy | ems CCCCCCCCEEEEEECCEEEECCCCCCCEECEECCCCCCCCCCCCHHHHHH a GI I NTLQKYYCRVRGGRCAVLSCLPKEEQI GKCSTRGRKCCRRKKEAAAK 10 20 » “0 60 on ii — — a a a — — aR LI _________ ees HOC CCCCCCEEEEECCCCCCCHHCCCCCCCCCEEEEEECCCCCCcCCcecce ~~ WLSPRSRPWAAYVYLPRRGPLAAYLPRRGPLGVAAYVRTRKSERSAAY WP o 7 2 °0 100 oo Mia fe TS Co — wes CCOCHHCHHHHHHHHHCHHHHHHHHHHHCCCCHHHHHCCCHHHHHHHHHHH rm ROASYGCAAYVRGMCGFRTAAYWGLLVAEPFAAYYYHLPWGLLAAYYQTY 0 120 130 120 160 ees HOCCHHHCCCCCCCCHHHHHHHHCHHHHHHHHHCCCCCHHHHHHHHHHCC Mm YHLPWAAYGKSTKVPVLAAYCHLGI GTVLAAYKKCDETDALAAY! COECH 10 +70 100 190 8 On i il I con i —— eee nes CHHHHHCCCHHHHHHHHHCCCHHHHHHHHHHHHHCCCCCEEHHHHHCCCC wa LGI AAYASSASQLSLAAYGNTTCYKAAAAYYRRCRAGVTAAYLPI LSNRN 220 230 280 » & Cont cmt ew EGEEEEECCCCCCC Mm VAAYVYTSARKKYV Legend: Strand Confidence of prediction Helix ment cartoon — Coil fiction uence Fig. 2. Schematic illustration of secondary structure of the final vaccine candidate. It depicts the arrangement of a-helix (43.35%), B-strands (28.14%) and coils (28.52%).\ fA PY \ Fig. 4. (A) Vaccine construct-TLR3 docked complex: the receptor TLR3 is shown in blue color while green color is representing final vaccine construct. (B) Illustration highlighting Interacting residues of docked vaccine (chain B) with TLR3 (chain A). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)",Medical Data Analysis
41,Extraction of Significant Patterns from Heart Disease Warehouses for Heart Attack Prediction,"Data Mining, Disease Diagnosis, Heart Disease, Pre-processing, Frequent Patterns, MAFIA (MAximal Frequent Itemset Algorithm), Clustering, K-Means, Significant Patterns.","The diagnosis of diseases is a significant and tedious task in medicine. The detection of heart disease from various factors or symptoms is a multi-layered issue which is not free from false presumptions often accompanied by unpredictable effects. Thus the effort to utilize knowledge and experience of numerous specialists and clinical screening data of patients collected in databases to facilitate the diagnosis process is considered a valuable option. The healthcare industry gathers enormous amounts of heart disease data that regrettably, are not “mined” to determine concealed information for effective decision making by healthcare practitioners. In this paper, we have proposed an efficient approach for the extraction of significant patterns from the heart disease warehouses for heart attack prediction. Initially, the data warehouse is preprocessed to make it appropriate for the mining process. After preprocessing, the heart disease warehouse is clustered using the K-means clustering algorithm, which will extract the data relevant to heart attack from the warehouse. Subsequently the frequent patterns are mined from the extracted data, relevant to heart disease, using the MAFIA algorithm. Then the significant weightage of the frequent patterns are calculated. Further, the patterns significant to heart attack prediction are chosen based on the calculated significant weightage. These significant patterns can be used in the development of heart attack prediction system.","Data mining in health care management is unlike the other fields owing to the fact that the data present are heterogeneous and that certain ethical, legal, and social constraints apply to private medical information. Health care related data are voluminous in nature and they arrive from diverse sources all of them not entirely appropriate in structure or quality. These days, the exploitation of knowledge and experience of numerous specialists and clinical screening data of patients gathered in a database during the diagnosis procedure, has been widely recognized. In this paper we have presented an efficient approach for extracting significant patterns from the heart disease data warehouses for the efficient prediction of heart attack. The preprocessed heart disease data warehouse was clustered to extract data most relevant to heart attack using K-means clustering algorithm. The frequent items have been mined effectively using MAFIA algorithm. Based on the calculated significant weightage, the frequent patterns having value greater than a predefined threshold were chosen for the valuable prediction of heart attack. In our future work, we have planned to design and develop an efficient heart attack prediction system with the aid of these selected significant patterns using artificial intelligence techniques.","Extraction of Significant Patterns from Heart Disease Warehouses for Heart Attack PredictionData Mining, Disease Diagnosis, Heart Disease, Pre-processing, Frequent Patterns, MAFIA (MAximal Frequent Itemset Algorithm), Clustering, K-Means, Significant Patterns.The diagnosis of diseases is a significant and tedious task in medicine. The detection of heart disease from various factors or symptoms is a multi-layered issue which is not free from false presumptions often accompanied by unpredictable effects. Thus the effort to utilize knowledge and experience of numerous specialists and clinical screening data of patients collected in databases to facilitate the diagnosis process is considered a valuable option. The healthcare industry gathers enormous amounts of heart disease data that regrettably, are not “mined” to determine concealed information for effective decision making by healthcare practitioners. In this paper, we have proposed an efficient approach for the extraction of significant patterns from the heart disease warehouses for heart attack prediction. Initially, the data warehouse is preprocessed to make it appropriate for the mining process. After preprocessing, the heart disease warehouse is clustered using the K-means clustering algorithm, which will extract the data relevant to heart attack from the warehouse. Subsequently the frequent patterns are mined from the extracted data, relevant to heart disease, using the MAFIA algorithm. Then the significant weightage of the frequent patterns are calculated. Further, the patterns significant to heart attack prediction are chosen based on the calculated significant weightage. These significant patterns can be used in the development of heart attack prediction system.Data mining in health care management is unlike the other fields owing to the fact that the data present are heterogeneous and that certain ethical, legal, and social constraints apply to private medical information. Health care related data are voluminous in nature and they arrive from diverse sources all of them not entirely appropriate in structure or quality. These days, the exploitation of knowledge and experience of numerous specialists and clinical screening data of patients gathered in a database during the diagnosis procedure, has been widely recognized. In this paper we have presented an efficient approach for extracting significant patterns from the heart disease data warehouses for the efficient prediction of heart attack. The preprocessed heart disease data warehouse was clustered to extract data most relevant to heart attack using K-means clustering algorithm. The frequent items have been mined effectively using MAFIA algorithm. Based on the calculated significant weightage, the frequent patterns having value greater than a predefined threshold were chosen for the valuable prediction of heart attack. In our future work, we have planned to design and develop an efficient heart attack prediction system with the aid of these selected significant patterns using artificial intelligence techniques.ne?) 3 Fig. 1 Recording sites selected in the study. Per each side of the chest, two recordings were performed at the lung bases at seven cm below the scapular angle, at both two and five cm from the paravertebral line respectively; another recording was taken from mid chest in correspondence of the fourth or fifth intercostal space, at two cm from the paravertebral line (a). Metallic marks were applied to the posterior chest of the patient and were visible at HRCT (black arrows, (b)",Deep Learning and Machine Learning,"This paper discusses the challenges of diagnosing heart disease and the potential benefits of data mining in healthcare. The authors propose an approach to extract significant patterns from heart disease data warehouses using clustering and frequent pattern mining algorithms. The significant patterns can then be used in the development of a heart attack prediction system. The authors acknowledge the heterogeneous and voluminous nature of healthcare data, as well as ethical, legal, and social constraints related to privacy. They suggest that their approach has the potential to improve healthcare decision-making and call for further development of artificial intelligence techniques in this area.",Medical Data Analysis,"ne?) 3 Fig. 1 Recording sites selected in the study. Per each side of the chest, two recordings were performed at the lung bases at seven cm below the scapular angle, at both two and five cm from the paravertebral line respectively; another recording was taken from mid chest in correspondence of the fourth or fifth intercostal space, at two cm from the paravertebral line (a). Metallic marks were applied to the posterior chest of the patient and were visible at HRCT (black arrows, (b)",Medical Data Analysis
42,“Velcro-type” crackles predict specific radiologic features of fibrotic interstitial lung disease,"Fibrotic interstitial lung disease, Idiopathic pulmonary fibrosis, Velcro crackles, Lung sounds, Breath sounds","Background: “Velcro-type” crackles on chest auscultation are considered a typical acoustic finding of Fibrotic Interstitial Lung Disease (FILD), however whether they may have a role in the early detection of these disorders has been unknown. This study investigated how “Velcro-type” crackles correlate with the presence of distinct patterns of FILD and individual radiologic features of pulmonary fibrosis on High Resolution Computed Tomography (HRCT). Methods: Lung sounds were digitally recorded from subjects immediately prior to undergoing clinically indicated chest HRCT. Audio files were independently assessed by two chest physicians and both full volume and single HRCT sections corresponding to the recording sites were extracted. The relationships between audible “Velcro-type” crackles and radiologic HRCT patterns and individual features of pulmonary fibrosis were investigated using multivariate regression models. Results: 148 subjects were enrolled: bilateral “Velcro-type” crackles predicted the presence of FILD at HRCT (OR 13.46, 95% CI 5.85–30.96, p < 0.001) and most strongly the Usual Interstitial Pneumonia (UIP) pattern (OR 19.8, 95% CI 5.28–74. 25, p < 0.001). Extent of isolated reticulation (OR 2.04, 95% CI 1.62–2.57, p < 0.001), honeycombing (OR 1.88, 95% CI 1. 24–2.83, < 0.01), ground glass opacities (OR 1.74, 95% CI 1.29–2.32, p < 0.001) and traction bronchiectasis (OR 1.55, 95% CI 1.03–2.32, p < 0.05) were all independently associated with the presence of “Velcro-type” crackles. Conclusions : “Velcro-type” crackles predict the presence of FILD and directly correlate with the extent of distinct radiologic features of pulmonary fibrosis. Such evidence provides grounds for further investigation of lung sounds as an early identification tool in FILD.","In conclusion, we identify that “Velcro-type” crackles not only predict the presence of FILD patterns at HRCT, but are also closely associated to the extent of different interstitial abnormalities in the lung parenchyma. Our finding that individual features of pulmonary fibrosis such as ground glass change and reticulation generate “Velcro-type” crackles warrants further investigation of the role of lung sounds as an early identification tool in FILD. The clinical utility of chest auscultation for assisting diagnosis and clinical management of ILD has been historically hampered by the subjectivity of standard chest auscultation and the poor signal transmission of standard stethoscopes [19]; if electronic auscultation were combined with computerized methods for lung sounds analysis and classification, this cost-effective approach might lead to the definition of an “acoustic signature” of FILD for both diagnostic and prognostic purposes, and this should be a focus of future studies.","“Velcro-type” crackles predict specific radiologic features of fibrotic interstitial lung diseaseFibrotic interstitial lung disease, Idiopathic pulmonary fibrosis, Velcro crackles, Lung sounds, Breath soundsBackground: “Velcro-type” crackles on chest auscultation are considered a typical acoustic finding of Fibrotic Interstitial Lung Disease (FILD), however whether they may have a role in the early detection of these disorders has been unknown. This study investigated how “Velcro-type” crackles correlate with the presence of distinct patterns of FILD and individual radiologic features of pulmonary fibrosis on High Resolution Computed Tomography (HRCT). Methods: Lung sounds were digitally recorded from subjects immediately prior to undergoing clinically indicated chest HRCT. Audio files were independently assessed by two chest physicians and both full volume and single HRCT sections corresponding to the recording sites were extracted. The relationships between audible “Velcro-type” crackles and radiologic HRCT patterns and individual features of pulmonary fibrosis were investigated using multivariate regression models. Results: 148 subjects were enrolled: bilateral “Velcro-type” crackles predicted the presence of FILD at HRCT (OR 13.46, 95% CI 5.85–30.96, p < 0.001) and most strongly the Usual Interstitial Pneumonia (UIP) pattern (OR 19.8, 95% CI 5.28–74. 25, p < 0.001). Extent of isolated reticulation (OR 2.04, 95% CI 1.62–2.57, p < 0.001), honeycombing (OR 1.88, 95% CI 1. 24–2.83, < 0.01), ground glass opacities (OR 1.74, 95% CI 1.29–2.32, p < 0.001) and traction bronchiectasis (OR 1.55, 95% CI 1.03–2.32, p < 0.05) were all independently associated with the presence of “Velcro-type” crackles. Conclusions : “Velcro-type” crackles predict the presence of FILD and directly correlate with the extent of distinct radiologic features of pulmonary fibrosis. Such evidence provides grounds for further investigation of lung sounds as an early identification tool in FILD.In conclusion, we identify that “Velcro-type” crackles not only predict the presence of FILD patterns at HRCT, but are also closely associated to the extent of different interstitial abnormalities in the lung parenchyma. Our finding that individual features of pulmonary fibrosis such as ground glass change and reticulation generate “Velcro-type” crackles warrants further investigation of the role of lung sounds as an early identification tool in FILD. The clinical utility of chest auscultation for assisting diagnosis and clinical management of ILD has been historically hampered by the subjectivity of standard chest auscultation and the poor signal transmission of standard stethoscopes [19]; if electronic auscultation were combined with computerized methods for lung sounds analysis and classification, this cost-effective approach might lead to the definition of an “acoustic signature” of FILD for both diagnostic and prognostic purposes, and this should be a focus of future studies.",Medical Data Analysis,"This study investigated the relationship between ""Velcro-type"" crackles heard on chest auscultation and radiologic features of pulmonary fibrosis on High Resolution Computed Tomography (HRCT). The study found that bilateral ""Velcro-type"" crackles predict the presence of Fibrotic Interstitial Lung Disease (FILD) and are closely associated with the extent of different interstitial abnormalities in the lung parenchyma. Individual features of pulmonary fibrosis, such as ground glass change and reticulation, were found to generate ""Velcro-type"" crackles. The study suggests that lung sounds could be a cost-effective tool for early identification of FILD if combined with computerized methods for analysis and classification.",Medical Data Analysis,,Medical Data Analysis
43,Health Utilities Using the EQ-5D in Studies of Cancer,"cancer, EQ-5D, validity, reliability, psychometric properties, patient-reported outcomes, health-related quality of life, utility scores, clinical trials, meta-analysis.","Cancer is one of the most frequent disease-specific applications of the EQ-5D. The objective of this review was to summarize evidence to support the validity and reliability of the EQ-5D in cancer, and to provide a catalogue of utility scores based on the use of the EQ-5D in clinical trials and in studies of patients with cancer. A structured literature search was conducted in EMBASE and MEDLINE to identify papers using key words related to cancer and the EQ-5D. Original research studies of patients with cancer that reported EQ-5D psychometric properties, responses and/or summary scores were included. Of 57 identified articles, 34 were selected for inclusion, where 12 studies reported evidence of validity or reliability and 31 reported EQ-5D responses or summary scores. The majority of investigations using the EQ-5D concerned patients with prostate cancer (n = 4), breast cancer (n = 4), cancers of the digestive system (n = 7) and Hodgkin and/or non-Hodgkin lymphoma (n = 3). Mean index- based scores ranged from 0.33 (SD 0.4) to 0.93 (SD 0.12) and visual analogue scale scores ranged from 43 (SD 13.3) to 84 (SD 12.0) across subtypes of cancer. A substantial and growing body of literature using the EQ-5D in cancer that supports the validity and reliability of EQ-5D in cancer has emerged. This review provides utility estimates for cancer patients across a wide range of cancer subtypes, treatment regimens and tumor stage(s) that may inform the modelling of outcomes in economic evaluations of cancer treatment.","The use of the EQ-5D in cancer has increased in recent years, and published studies provide evidence to support its validity and reliability. The broad range of EQ-5D index-based and VAS scores in these studies likely reflects systematic variance attributable to stage of treatment protocols, progression of disease and type of cancer, in addition to patient characteristics such as age. Consistent reporting of statistics that include both mean (SD) and median (IQR) EQ-5D scores in studies of cancer would improve the informational value of studies and better facilitate meta-analyses when appropriate. Only a handful of studies reported HR-QOL values according to stage of disease and level of toxicity. Such estimates are likely to be important to researchers who seek to model cancer-related endpoints in future studies. Thus, there continues to be much opportunity for research using the EQ-5D in cancer that would fill gaps in knowledge relating to values/utility scores associated with cancer stage by type of cancer, common sites of metastates within various types of cancer and common treatment induced toxicities.","Health Utilities Using the EQ-5D in Studies of Cancercancer, EQ-5D, validity, reliability, psychometric properties, patient-reported outcomes, health-related quality of life, utility scores, clinical trials, meta-analysis.Cancer is one of the most frequent disease-specific applications of the EQ-5D. The objective of this review was to summarize evidence to support the validity and reliability of the EQ-5D in cancer, and to provide a catalogue of utility scores based on the use of the EQ-5D in clinical trials and in studies of patients with cancer. A structured literature search was conducted in EMBASE and MEDLINE to identify papers using key words related to cancer and the EQ-5D. Original research studies of patients with cancer that reported EQ-5D psychometric properties, responses and/or summary scores were included. Of 57 identified articles, 34 were selected for inclusion, where 12 studies reported evidence of validity or reliability and 31 reported EQ-5D responses or summary scores. The majority of investigations using the EQ-5D concerned patients with prostate cancer (n = 4), breast cancer (n = 4), cancers of the digestive system (n = 7) and Hodgkin and/or non-Hodgkin lymphoma (n = 3). Mean index- based scores ranged from 0.33 (SD 0.4) to 0.93 (SD 0.12) and visual analogue scale scores ranged from 43 (SD 13.3) to 84 (SD 12.0) across subtypes of cancer. A substantial and growing body of literature using the EQ-5D in cancer that supports the validity and reliability of EQ-5D in cancer has emerged. This review provides utility estimates for cancer patients across a wide range of cancer subtypes, treatment regimens and tumor stage(s) that may inform the modelling of outcomes in economic evaluations of cancer treatment.The use of the EQ-5D in cancer has increased in recent years, and published studies provide evidence to support its validity and reliability. The broad range of EQ-5D index-based and VAS scores in these studies likely reflects systematic variance attributable to stage of treatment protocols, progression of disease and type of cancer, in addition to patient characteristics such as age. Consistent reporting of statistics that include both mean (SD) and median (IQR) EQ-5D scores in studies of cancer would improve the informational value of studies and better facilitate meta-analyses when appropriate. Only a handful of studies reported HR-QOL values according to stage of disease and level of toxicity. Such estimates are likely to be important to researchers who seek to model cancer-related endpoints in future studies. Thus, there continues to be much opportunity for research using the EQ-5D in cancer that would fill gaps in knowledge relating to values/utility scores associated with cancer stage by type of cancer, common sites of metastates within various types of cancer and common treatment induced toxicities.be he 2 he 8 : 8 Heo ve § te a a o < 2 he 8 a D “4 a a < — < he . a re re 5 oS $ e 2 he % ‘* é . bet bet ° bet he . . 8 e ° ° 8 = 6 é se Ico i hed QD <8 . GO e 23 22 ° e< e —— ° - 29 Q@NH OH tT QA o oo oo ooo LO @109s xepul GS-04 colon and rectum (colorectal) cancer; ESO = ligh-dose chemotherapy; LIV = liver cancer; MA Fig. 1. EQ-5D mean/median index scores of patients with breast, prostate or digestive system cancers. The Y-error bars represent the 95% confidence interval about the mean score. Tsuchiya et al.!*8) and Dolan refer to the algorithms used to convert quality-of-life scores into a single preference-based score. !10-17.1820.22-27.33.34.36] Chemo = chemotherapy; CoRe oesophageal cancer; FAC = fluorouracil, doxorubicin, cyclosphosphamide; HDC megestrol acetate; MEPS Results; tx Surveillance Epidemiology and End Medical Expenditure Panel Survey; PRO = prostate cancer; SEER = treatment.1 0.9 e Mean (95% Cl) 08 . ee + . 207 e t . 8 ° 20.6 } @ ee 205 $ B0.44° ° Gos 0.2 01 0+ I~; I~; - mf ox DK Ss OK 2S GX 2GLRLOO as EP EPITES SSP STS SIE GPE ES FE SF GES FS Se Beret tFIFIZLS & §Ss § os & eEeEgcegaeeE = Sa Sexes EeEKLELES BREEZES SsSsgss s 88 B88 BSE-S SS Ssess acsg0ucecog a § S$ 8&So 8Fosayue ag & o Psfcesé & & & es ses FF F325 FF SEE SsesSse F = FPF PS OLS REISE PEGE s Bseacee & s gs 88 GEZF BFS FEB gegcoorse & $$ EX CoOBE g@éG @¢ sasopeteztar 2 a 2 sy fog & SFT OOFe EC = ss sé & ES EE OS s & &§ 25 ¢€ 223 @ = go o _ a é & = & Tc ae §¥ 2 3s cS x = x = Fig. 2. EQ-5D mean/median index scores of patients with various types of cancer. The Y-error bars represent the 95% confidence interval about the mean score.|""0.15.16.28-32,37-40.42] aqlP| = age-adjusted International Prognostic Index; ABMT = autologous bone marrow transplanta- = tion; BONJNT = bone and joint cancers; BRAIN = brain and nervous system cancers; CHOP = cyclophosphamide, doxorubicin, vincristine, prednisone; CoRe = colon and rectum (colorectal) cancer; GEN = general cancer (no type specified); GI = digestive system; HdNk = head and neck; HOD = Hodgkin disease; MTMY = multiple myeloma; NHL = non-Hodgkin lymphoma; PBSCT (PSCT) = peripheral blood stem- cell transplantation.s ° zee e §DE Zoe . Doe sre ° r Zé ° 22 He € he £8 He 38 Lo . Q2 ° £3 @ te |O2 be tot i. tei 8 no S re ® to s ves 8 at a be . _~ e 5 ° 5 x a 8 ° & § % 8 @ . 2 2 . | ° He °. eeogao ea ea a ea Pe 8 S35 SS SG oO So SG SG2O2R GOH FHA HK Fig. 3. Mean EQ-VAS scores in patients with various types of cancer. The Y-error bars represent the 95% confidence interval about the mean score. |16.18-21,23,25.28,30,31,34,96,37.99,4042] ABMT chemotherapy; CoRe bone and joint cancers; chem autologous bone marrow transplantation; BONJNT = general cancer (no type specified); Gl esophageal cancer; GEN = colon and rectum (colorectal) cancer; ESO peripheral blood stem-cell Hodgkin disease; LIV = liver cancer; PBSCT visual analogue scale. head and neck cancer; HOD transplantation; tx = treatment; VAS digestive system; HdNk",Medical Data Analysis,"This review article provides an overview of the use of EQ-5D in cancer patients and summarizes evidence supporting its validity and reliability. The article presents a catalog of utility scores based on the use of EQ-5D in clinical trials and studies of patients with cancer. A literature search of EMBASE and MEDLINE identified 34 studies reporting EQ-5D responses or summary scores and 12 studies reporting evidence of validity or reliability. The majority of studies using EQ-5D concerned patients with prostate cancer, breast cancer, cancers of the digestive system, and Hodgkin and/or non-Hodgkin lymphoma. Mean index-based scores ranged from 0.33 to 0.93, and visual analogue scale scores ranged from 43 to 84 across subtypes of cancer. The article concludes that the EQ-5D is a valid and reliable tool for assessing health-related quality of life in cancer patients, and that there is much opportunity for future research to fill gaps in knowledge relating to values/utility scores associated with cancer stage, type of cancer, common sites of metastates, and treatment-induced toxicities.",Medical Data Analysis,"be he 2 he 8 : 8 Heo ve § te a a o < 2 he 8 a D “4 a a < — < he . a re re 5 oS $ e 2 he % ‘* é . bet bet ° bet he . . 8 e ° ° 8 = 6 é se Ico i hed QD <8 . GO e 23 22 ° e< e —— ° - 29 Q@NH OH tT QA o oo oo ooo LO @109s xepul GS-04 colon and rectum (colorectal) cancer; ESO = ligh-dose chemotherapy; LIV = liver cancer; MA Fig. 1. EQ-5D mean/median index scores of patients with breast, prostate or digestive system cancers. The Y-error bars represent the 95% confidence interval about the mean score. Tsuchiya et al.!*8) and Dolan refer to the algorithms used to convert quality-of-life scores into a single preference-based score. !10-17.1820.22-27.33.34.36] Chemo = chemotherapy; CoRe oesophageal cancer; FAC = fluorouracil, doxorubicin, cyclosphosphamide; HDC megestrol acetate; MEPS Results; tx Surveillance Epidemiology and End Medical Expenditure Panel Survey; PRO = prostate cancer; SEER = treatment.1 0.9 e Mean (95% Cl) 08 . ee + . 207 e t . 8 ° 20.6 } @ ee 205 $ B0.44° ° Gos 0.2 01 0+ I~; I~; - mf ox DK Ss OK 2S GX 2GLRLOO as EP EPITES SSP STS SIE GPE ES FE SF GES FS Se Beret tFIFIZLS & §Ss § os & eEeEgcegaeeE = Sa Sexes EeEKLELES BREEZES SsSsgss s 88 B88 BSE-S SS Ssess acsg0ucecog a § S$ 8&So 8Fosayue ag & o Psfcesé & & & es ses FF F325 FF SEE SsesSse F = FPF PS OLS REISE PEGE s Bseacee & s gs 88 GEZF BFS FEB gegcoorse & $$ EX CoOBE g@éG @¢ sasopeteztar 2 a 2 sy fog & SFT OOFe EC = ss sé & ES EE OS s & &§ 25 ¢€ 223 @ = go o _ a é & = & Tc ae §¥ 2 3s cS x = x = Fig. 2. EQ-5D mean/median index scores of patients with various types of cancer. The Y-error bars represent the 95% confidence interval about the mean score.|""0.15.16.28-32,37-40.42] aqlP| = age-adjusted International Prognostic Index; ABMT = autologous bone marrow transplanta- = tion; BONJNT = bone and joint cancers; BRAIN = brain and nervous system cancers; CHOP = cyclophosphamide, doxorubicin, vincristine, prednisone; CoRe = colon and rectum (colorectal) cancer; GEN = general cancer (no type specified); GI = digestive system; HdNk = head and neck; HOD = Hodgkin disease; MTMY = multiple myeloma; NHL = non-Hodgkin lymphoma; PBSCT (PSCT) = peripheral blood stem- cell transplantation.s ° zee e §DE Zoe . Doe sre ° r Zé ° 22 He € he £8 He 38 Lo . Q2 ° £3 @ te |O2 be tot i. tei 8 no S re ® to s ves 8 at a be . _~ e 5 ° 5 x a 8 ° & § % 8 @ . 2 2 . | ° He °. eeogao ea ea a ea Pe 8 S35 SS SG oO So SG SG2O2R GOH FHA HK Fig. 3. Mean EQ-VAS scores in patients with various types of cancer. The Y-error bars represent the 95% confidence interval about the mean score. |16.18-21,23,25.28,30,31,34,96,37.99,4042] ABMT chemotherapy; CoRe bone and joint cancers; chem autologous bone marrow transplantation; BONJNT = general cancer (no type specified); Gl esophageal cancer; GEN = colon and rectum (colorectal) cancer; ESO peripheral blood stem-cell Hodgkin disease; LIV = liver cancer; PBSCT visual analogue scale. head and neck cancer; HOD transplantation; tx = treatment; VAS digestive system; HdNk",Medical Data Analysis
44,HIV/AIDS in Nigeria: a bibliometric analysis,"Nigeria, HIV/AIDS, bibliometric analysis, impact factor , health policy.","Background: Nigeria is home to more people living with HIV than any other country in the world, except South Africa and India-where an estimated 2.9 million [1.7 million – 4.2 million] people were living with the virus in 2005. A systematic assessment of recent HIV/AIDS research output from Nigeria is not available. Without objective information about the current deficiencies and strengths in the HIV research output from Nigeria, it is difficult to plan substantial improvements in HIV/AIDS research that could enhance population health. The aim of this study was to analyse the trends in Nigeria's SCI publications in HIV/AIDS from 1980 to 2006. Special attention was paid to internationally collaborated works that were identified based on the countries of the authors' affiliation. Methods: A bibliometric analysis regarding Nigerian HIV/AIDS research was conducted in the ISI databases for the period of 1980 to 2006. An attempt was made to identify the patterns of the growth in HIV/AIDS literature, as well as type of document published, authorship, institutional affiliations of authors, and subject content. International collaboration was deemed to exist in an article if any co-author's affiliation was located outside Nigeria. The impact factors in the 2006 Journal Citations Reports Science Edition was arbitrarily adopted to estimate the quality of articles. Results: Nigeria's ISI publications in HIV/AIDS increased from one articles in 1987 to 33 in 2006, and the articles with international collaboration increased from one articles in 1980 to 16 in 2006. Articles with international collaboration appeared in journals with higher impact factors and received more citations. A high pattern of co-authorship was found. Over 85% of the articles were published in collaboration among two or more authors. The USA, as the most important collaborating partner of Nigeria's HIV/AIDS researchers, contributed 30.8% of articles with international collaboration. Conclusion: Nigeria has achieved a significant increase in the number of SCI publications and collaborations in HIV literature from 1987 to 2005. There is need to challenge the status, scientists from Nigeria should forge multiple collaborations beyond historical, political, and cultural lines to share knowledge and expertise on HIV/AIDS.","In summary, Nigeria achieved a significant increase in the number of SCI publications and collaborations in HIV literature from 1987 to 2005. As noted by Katz et al [21], research collaboration is good and should be encouraged. Yet there exists opportunity for improvement in international collaboration. There is need to challenge the status, scientists from Nigeria should forge multiple collaborations beyond historical, political, and cultural lines to share knowledge and expertise on HIV/AIDS. Research has helped to quantify HIV associated morbidity and mortality, identified strategies to improve health of people living with HIV/AIDS, and shown the effectiveness of HIV/AIDS preventive interventions. Furthermore, comparison analyses of HIV/AIDS literature production in Nigeria with other countries from sub-Saharan need to be conducted to obtain a more complete regional picture of the situation. These analyses will provide further support to AIDS researchers, health policy analyst, and librarians or information officers in the sub-Saharan Africa.","HIV/AIDS in Nigeria: a bibliometric analysisNigeria, HIV/AIDS, bibliometric analysis, impact factor , health policy.Background: Nigeria is home to more people living with HIV than any other country in the world, except South Africa and India-where an estimated 2.9 million [1.7 million – 4.2 million] people were living with the virus in 2005. A systematic assessment of recent HIV/AIDS research output from Nigeria is not available. Without objective information about the current deficiencies and strengths in the HIV research output from Nigeria, it is difficult to plan substantial improvements in HIV/AIDS research that could enhance population health. The aim of this study was to analyse the trends in Nigeria's SCI publications in HIV/AIDS from 1980 to 2006. Special attention was paid to internationally collaborated works that were identified based on the countries of the authors' affiliation. Methods: A bibliometric analysis regarding Nigerian HIV/AIDS research was conducted in the ISI databases for the period of 1980 to 2006. An attempt was made to identify the patterns of the growth in HIV/AIDS literature, as well as type of document published, authorship, institutional affiliations of authors, and subject content. International collaboration was deemed to exist in an article if any co-author's affiliation was located outside Nigeria. The impact factors in the 2006 Journal Citations Reports Science Edition was arbitrarily adopted to estimate the quality of articles. Results: Nigeria's ISI publications in HIV/AIDS increased from one articles in 1987 to 33 in 2006, and the articles with international collaboration increased from one articles in 1980 to 16 in 2006. Articles with international collaboration appeared in journals with higher impact factors and received more citations. A high pattern of co-authorship was found. Over 85% of the articles were published in collaboration among two or more authors. The USA, as the most important collaborating partner of Nigeria's HIV/AIDS researchers, contributed 30.8% of articles with international collaboration. Conclusion: Nigeria has achieved a significant increase in the number of SCI publications and collaborations in HIV literature from 1987 to 2005. There is need to challenge the status, scientists from Nigeria should forge multiple collaborations beyond historical, political, and cultural lines to share knowledge and expertise on HIV/AIDS.In summary, Nigeria achieved a significant increase in the number of SCI publications and collaborations in HIV literature from 1987 to 2005. As noted by Katz et al [21], research collaboration is good and should be encouraged. Yet there exists opportunity for improvement in international collaboration. There is need to challenge the status, scientists from Nigeria should forge multiple collaborations beyond historical, political, and cultural lines to share knowledge and expertise on HIV/AIDS. Research has helped to quantify HIV associated morbidity and mortality, identified strategies to improve health of people living with HIV/AIDS, and shown the effectiveness of HIV/AIDS preventive interventions. Furthermore, comparison analyses of HIV/AIDS literature production in Nigeria with other countries from sub-Saharan need to be conducted to obtain a more complete regional picture of the situation. These analyses will provide further support to AIDS researchers, health policy analyst, and librarians or information officers in the sub-Saharan Africa.40 30 20 ‘Average citatiors per year 19 e hop e . Eo a Ee ~ 38 — = I SD ee ee Pubicaton Year —+— ‘ticles with four or more authors —— Armotes wi less than tour authors Figure 2 Citation performance of articles with four or more authors and articles with less than four authors. The figures for more recent years are lower because papers pub- lished during this period have had less time to accumulate citations.40 30 iors per year Average 19 « = -_ — 7 Publication Year —+— “Aaticles with international coleboration —— Ametos winout international cotaboration Figure 3 Citation performance of articles with or without international collaboration. The figures for more recent years are lower because papers published during this period have had less time to accumulate citations.100 = le with foreign address —+*— Percentage of a [] Number of articles 3 2 8 3 $ & ° e@gasesee3e8#teesgsg:s 833 82388 3 8 888 8 8 $88 8 8 33333888 8 222 22 2 2 2 2 2 8 RRR Publication Year Figure | Trends in Nigeria's HIV/AIDS literature publications in Web of Science. No records were found for the period of 1980 to 1986.",Medical Data Analysis,"The abstract describes a study that analyzed the trends in Nigeria's HIV/AIDS research output between 1980 and 2006 using bibliometric analysis. The study aimed to identify deficiencies and strengths in HIV/AIDS research output from Nigeria and the impact of international collaboration on the quality of publications. The results showed a significant increase in the number of SCI publications and collaborations in HIV literature from Nigeria between 1987 and 2005. However, there is a need for improved international collaboration beyond historical, political, and cultural lines. The study recommends further comparison analyses of HIV/AIDS literature production in Nigeria with other countries in sub-Saharan Africa to obtain a more complete regional picture of the situation.",Medical Data Analysis,40 30 20 ‘Average citatiors per year 19 e hop e . Eo a Ee ~ 38 — = I SD ee ee Pubicaton Year —+— ‘ticles with four or more authors —— Armotes wi less than tour authors Figure 2 Citation performance of articles with four or more authors and articles with less than four authors. The figures for more recent years are lower because papers pub- lished during this period have had less time to accumulate citations.40 30 iors per year Average 19 « = -_ — 7 Publication Year —+— “Aaticles with international coleboration —— Ametos winout international cotaboration Figure 3 Citation performance of articles with or without international collaboration. The figures for more recent years are lower because papers published during this period have had less time to accumulate citations.100 = le with foreign address —+*— Percentage of a [] Number of articles 3 2 8 3 $ & ° e@gasesee3e8#teesgsg:s 833 82388 3 8 888 8 8 $88 8 8 33333888 8 222 22 2 2 2 2 2 8 RRR Publication Year Figure | Trends in Nigeria's HIV/AIDS literature publications in Web of Science. No records were found for the period of 1980 to 1986.,Medical Data Analysis
45,HIV infection and mental health: Suicidal behaviour – Systematic review,suicidal behaviour; HIV; mental health,"Suicide has long been associated with serious illness generally and HIV specifically. New treatments have affected prognosis in HIV positively, but it is unclear how they impact on suicidal burden (thoughts, self-harm and completions). This review examines all published suicide and HIV data for a definitive account of (1) prevalence of HIV-related suicidality, (2) measurement within studies and (3) effectiveness of interventions. Standard systematic research methods were used to gather quality published papers on HIV and suicide, searching published databases according to quality inclusion criteria. From the search, 332 papers were generated and hand searched resulting in 66 studies for analysis. Of these, 75% were American/European, but there was representation from developing countries. The breakdown of papers provided 12, which measured completed suicides (death records), five reporting suicide as a cause of attrition. Deliberate self-harm was measured in 21, using 22 instruments; 16 studies measured suicidal ideation using 14 instruments, suicidal thoughts were measured in 17, using 15 instruments. Navigating the diverse range of studies clearly points to a high-suicidal burden among people with HIV. The overview shows that autopsy studies reveal 9.4% of deceased HIVþ individuals had committed suicide; 2.4% HIVþ study participants commit suicide; approximately 20% of HIVþ people studied had deliberately harmed themselves; 26.9% reported suicidal ideation, 28.5% during the past week and 6.5% reported ideation as a side effect to medication; 22.2% had a suicide plan; 19.7% were generally ‘‘suicidal’’ (11.7% of people with AIDS, 15.3% at other stages of HIV); 23.1% reported thoughts of ending their own life; and 14.4% expressed a desire for death. Only three studies recruited over 70% female participants (39 studies recruited over 70% men), and six focused on injecting drug users. Only three studies looked at interventions – predominantly indirect. Our detailed data suggest that all aspects of suicide are elevated and urgently require routine monitoring and tracking as a standard component of clinical care. There is scant evidence of direct interventions to reduce any aspect of suicidality, which needs urgent redress.","The studies clearly suggest that participant characteristics may also be relevant and there is a need for sub-analysis. Pre-existing literature shows specific elevated rates of suicidal behaviors among a number of groups represented among HIV positive groups. For example, King et al. shows a 1.5-fold elevation of suicidal risk for gay men. Gender differences are well documented in suicidal risk and variables such as age, drug use and mental illness diagnosis are commonly known as cofactors in suicidal risk. The search may have had limitations in the research terms, yet there were clear gaps in the research database revealed. Only three studies recruited over 70% female participants (39 studies recruited over 70% men), and six studies focused exclusively on IDUs. Furthermore, over 75% of the studies had been implemented in North America or Western Europe with only 1% of studies conducted in Africa. Considering the impact of HIV and AIDS across the globe, this research seems to be overwhelmingly disproportionately focused on the West. As the three studies that focus on Africa illustrate, suicidal thoughts and suicidality are not an exclusively Western experience (Olley et al., 2005; Petrushkin et al., 2005; Sebit et al., 2002). Our detailed data suggest that monitoring and tracking of all aspects of suicidality should be a routine component of clinical care. The review shows notable presence of suicidal thoughts, plans and acts resulting on self-harm as well as death. Given this pervasive prevalence, it is clear that there is a gap in both provision and evaluation of interventions. There is scant evidence of direct interventions to reduce any aspect of suicidality and there is some (again scant) evidence of indirect interventions that may affect suicidality outcomes (such as treatment for HIV infection in the first place).","HIV infection and mental health: Suicidal behaviour – Systematic reviewsuicidal behaviour; HIV; mental healthSuicide has long been associated with serious illness generally and HIV specifically. New treatments have affected prognosis in HIV positively, but it is unclear how they impact on suicidal burden (thoughts, self-harm and completions). This review examines all published suicide and HIV data for a definitive account of (1) prevalence of HIV-related suicidality, (2) measurement within studies and (3) effectiveness of interventions. Standard systematic research methods were used to gather quality published papers on HIV and suicide, searching published databases according to quality inclusion criteria. From the search, 332 papers were generated and hand searched resulting in 66 studies for analysis. Of these, 75% were American/European, but there was representation from developing countries. The breakdown of papers provided 12, which measured completed suicides (death records), five reporting suicide as a cause of attrition. Deliberate self-harm was measured in 21, using 22 instruments; 16 studies measured suicidal ideation using 14 instruments, suicidal thoughts were measured in 17, using 15 instruments. Navigating the diverse range of studies clearly points to a high-suicidal burden among people with HIV. The overview shows that autopsy studies reveal 9.4% of deceased HIVþ individuals had committed suicide; 2.4% HIVþ study participants commit suicide; approximately 20% of HIVþ people studied had deliberately harmed themselves; 26.9% reported suicidal ideation, 28.5% during the past week and 6.5% reported ideation as a side effect to medication; 22.2% had a suicide plan; 19.7% were generally ‘‘suicidal’’ (11.7% of people with AIDS, 15.3% at other stages of HIV); 23.1% reported thoughts of ending their own life; and 14.4% expressed a desire for death. Only three studies recruited over 70% female participants (39 studies recruited over 70% men), and six focused on injecting drug users. Only three studies looked at interventions – predominantly indirect. Our detailed data suggest that all aspects of suicide are elevated and urgently require routine monitoring and tracking as a standard component of clinical care. There is scant evidence of direct interventions to reduce any aspect of suicidality, which needs urgent redress.The studies clearly suggest that participant characteristics may also be relevant and there is a need for sub-analysis. Pre-existing literature shows specific elevated rates of suicidal behaviors among a number of groups represented among HIV positive groups. For example, King et al. shows a 1.5-fold elevation of suicidal risk for gay men. Gender differences are well documented in suicidal risk and variables such as age, drug use and mental illness diagnosis are commonly known as cofactors in suicidal risk. The search may have had limitations in the research terms, yet there were clear gaps in the research database revealed. Only three studies recruited over 70% female participants (39 studies recruited over 70% men), and six studies focused exclusively on IDUs. Furthermore, over 75% of the studies had been implemented in North America or Western Europe with only 1% of studies conducted in Africa. Considering the impact of HIV and AIDS across the globe, this research seems to be overwhelmingly disproportionately focused on the West. As the three studies that focus on Africa illustrate, suicidal thoughts and suicidality are not an exclusively Western experience (Olley et al., 2005; Petrushkin et al., 2005; Sebit et al., 2002). Our detailed data suggest that monitoring and tracking of all aspects of suicidality should be a routine component of clinical care. The review shows notable presence of suicidal thoughts, plans and acts resulting on self-harm as well as death. Given this pervasive prevalence, it is clear that there is a gap in both provision and evaluation of interventions. There is scant evidence of direct interventions to reduce any aspect of suicidality and there is some (again scant) evidence of indirect interventions that may affect suicidality outcomes (such as treatment for HIV infection in the first place).332 hits from MedLine and PsycINFO 65 duplicates excluded 267 unique hits to be screened 77 excluded from abstract screening: 28 books 19 non-English language 18 reviews 9 theses 1 newspaper article 2 case studies 95 papers to be reviewed in full- text 21 excluded at full-text screening: 8 inaccessible in full-text 3 qualitative studies 11 had no relevant data 1 non-unique dataset 1 intervention protocol 9 coded from + (Preview abstract + 3 had no relevant measures 1 unrelated to HIV 2 case studies 66 papers in final set Figure 1. Search summary.",Medical Data Analysis,"This review examines the prevalence of HIV-related suicidality, measurement within studies, and effectiveness of interventions. The review shows a high-suicidal burden among people with HIV, with notable presence of suicidal thoughts, plans, and acts resulting in self-harm as well as death. However, there is a gap in both provision and evaluation of interventions, with scant evidence of direct interventions to reduce any aspect of suicidality. The review calls for routine monitoring and tracking of all aspects of suicidality as a standard component of clinical care, and for urgent redress of the gap in interventions. The review also highlights the need for sub-analysis of participant characteristics, and the importance of considering the impact of HIV and AIDS across the globe, not just in the West.",Medical Data Analysis,332 hits from MedLine and PsycINFO 65 duplicates excluded 267 unique hits to be screened 77 excluded from abstract screening: 28 books 19 non-English language 18 reviews 9 theses 1 newspaper article 2 case studies 95 papers to be reviewed in full- text 21 excluded at full-text screening: 8 inaccessible in full-text 3 qualitative studies 11 had no relevant data 1 non-unique dataset 1 intervention protocol 9 coded from + (Preview abstract + 3 had no relevant measures 1 unrelated to HIV 2 case studies 66 papers in final set Figure 1. Search summary.,Medical Data Analysis
46,Implementation of Convolutional Neural Network Approach for COVID-19 Disease Detection,"COVID-19 detection, Convolutional Neural Network, Deep learning, Image classification, Medical image processing.","In this paper two novel, powerful and robust Convolutional Neural Network (CNN) architectures are designed and proposed for two different classification tasks using publicly available datasets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 vs. Normal vs. Pneumonia) with 98.27% average accuracy. The hyper-parameters of the both CNN models are automatically determined using Grid Search. Experimental results on large clinical datasets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome disadvantages mentioned above. Moreover, the proposed CNN models are fully automatic in terms of not requiring the extraction of diseased tissue; which is a great improvement of available automatic methods in the literature. To the best of author’s knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN whose hyper parameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN based COVID-19 chest X-ray image classification study which uses the largest possible clinical dataset. A total of 1524 COVID-19, 1527 pneumonia and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.","Hyper-parameters are indispensable for deep learning algorithms and are highly effective on performance. Deep learning model is almost equivalent to choosing the most suitable hyper parameter group. However, pairs of parameters are not always sought for the best performance. In such cases, how to move, which hyper parameters will be changed, to what extent, which hyper parameters are in correlation with each other should be investigated, trends should be determined and presented in studies. Recently, hyper-parameter groups that give the best performance are given in studies involving deep learning studies. In some studies, even these parameter pairs are not fully given. The choice of hyper-parameters and how they achieve success are not discussed. This weakens the analysis value and importance of the study. In the deep learning studies I proposed the selection of hyper parameters at certain intervals instead of the intuitive hyper-parameter selection method, and detailed analysis of how the model performance and working time are affected by the hyperparameter change in these intervals, and discussion of the hyper-parameter groups that are correlated with each other, are analysed in relation to the hyper-parameters. It is imperative that parameter analysis section be present in all studies. In my opinion, it is essential to establish such a standard in such an area where we can no longer keep up with the pace. In this paper, two novel and fully automatic studies using deep convolutional neural networks are presented for COVID-19 detection and virus classification. Two novel, powerful and robust CNN architectures are designed and proposed for two different classification tasks using publicly available datasets. The hyper-parameters of both CNN architectures are automatically determined using Grid Search Optimizer method. Detection of COVID-19 disease is achieved with a high accuracy such as 98.92%. Moreover, classification of chest X-ray images into normal, pneumonia and COVID-19 is obtained with satisfying accuracy of 98.27%. Experimental results on large clinical datasets show the effectiveness of the proposed architectures. The results of Task 1 and Task 2 indicate that, to the best of the author’s knowledge, state-of-the-art classification performance is achieved using a large clinical dataset without data augmentation. One of the main contributions of this study to the literature is that a wide variety of data sets have been used. It is aimed to create the largest possible number of X-ray images of COVID-19 that exist in the literature until the writing of this research. A total of 1524 COVID-19 images, 1527 pneumonia images and 1524 normal images have been collected and used for this research. It is believed that thanks to their simplicity and flexibility, the models proposed in this paper can be readily used in practice to help the physicians for diagnosing the COVID-19 disease.","Implementation of Convolutional Neural Network Approach for COVID-19 Disease DetectionCOVID-19 detection, Convolutional Neural Network, Deep learning, Image classification, Medical image processing.In this paper two novel, powerful and robust Convolutional Neural Network (CNN) architectures are designed and proposed for two different classification tasks using publicly available datasets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 vs. Normal vs. Pneumonia) with 98.27% average accuracy. The hyper-parameters of the both CNN models are automatically determined using Grid Search. Experimental results on large clinical datasets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome disadvantages mentioned above. Moreover, the proposed CNN models are fully automatic in terms of not requiring the extraction of diseased tissue; which is a great improvement of available automatic methods in the literature. To the best of author’s knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN whose hyper parameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN based COVID-19 chest X-ray image classification study which uses the largest possible clinical dataset. A total of 1524 COVID-19, 1527 pneumonia and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.Hyper-parameters are indispensable for deep learning algorithms and are highly effective on performance. Deep learning model is almost equivalent to choosing the most suitable hyper parameter group. However, pairs of parameters are not always sought for the best performance. In such cases, how to move, which hyper parameters will be changed, to what extent, which hyper parameters are in correlation with each other should be investigated, trends should be determined and presented in studies. Recently, hyper-parameter groups that give the best performance are given in studies involving deep learning studies. In some studies, even these parameter pairs are not fully given. The choice of hyper-parameters and how they achieve success are not discussed. This weakens the analysis value and importance of the study. In the deep learning studies I proposed the selection of hyper parameters at certain intervals instead of the intuitive hyper-parameter selection method, and detailed analysis of how the model performance and working time are affected by the hyperparameter change in these intervals, and discussion of the hyper-parameter groups that are correlated with each other, are analysed in relation to the hyper-parameters. It is imperative that parameter analysis section be present in all studies. In my opinion, it is essential to establish such a standard in such an area where we can no longer keep up with the pace. In this paper, two novel and fully automatic studies using deep convolutional neural networks are presented for COVID-19 detection and virus classification. Two novel, powerful and robust CNN architectures are designed and proposed for two different classification tasks using publicly available datasets. The hyper-parameters of both CNN architectures are automatically determined using Grid Search Optimizer method. Detection of COVID-19 disease is achieved with a high accuracy such as 98.92%. Moreover, classification of chest X-ray images into normal, pneumonia and COVID-19 is obtained with satisfying accuracy of 98.27%. Experimental results on large clinical datasets show the effectiveness of the proposed architectures. The results of Task 1 and Task 2 indicate that, to the best of the author’s knowledge, state-of-the-art classification performance is achieved using a large clinical dataset without data augmentation. One of the main contributions of this study to the literature is that a wide variety of data sets have been used. It is aimed to create the largest possible number of X-ray images of COVID-19 that exist in the literature until the writing of this research. A total of 1524 COVID-19 images, 1527 pneumonia images and 1524 normal images have been collected and used for this research. It is believed that thanks to their simplicity and flexibility, the models proposed in this paper can be readily used in practice to help the physicians for diagnosing the COVID-19 disease.#ERROR!",Deep Learning and Machine Learning,"This paper presents two novel and powerful Convolutional Neural Network (CNN) architectures for COVID-19 detection and virus classification using chest X-ray images. The first architecture is designed to detect COVID-19 with an average accuracy of 98.92%, while the second architecture can classify chest X-ray images into three classes (COVID-19 vs. Normal vs. Pneumonia) with an average accuracy of 98.27%. The hyper-parameters of both CNN architectures are automatically determined using the Grid Search Optimizer method. The proposed CNN models are fully automatic and do not require the extraction of diseased tissue. The study uses a large clinical dataset, including 1524 COVID-19, 1527 pneumonia, and 1524 normal X-ray images. The proposed models show high accuracy and effectiveness on large clinical datasets, which can help physicians in diagnosing COVID-19 disease. The study emphasizes the importance of hyper-parameter analysis and establishing a standard in deep learning studies.",Deep Learning and Machine Learning,#ERROR!,Deep Learning and Machine Learning
47,Improved Study of Heart Disease Prediction System using Data Mining Classification Techniques,"Data Mining, Heart Disease, Neural Networks, Decision Trees, Naive Bayes.","The Healthcare industry is generally “information rich”, but unfortunately not all the data are mined which is required for discovering hidden patterns & effective decision making. Advanced data mining techniques are used to discover knowledge in database and for medical research, particularly in Heart disease prediction. This paper has analysed prediction systems for Heart disease using more number of input attributes. The system uses medical terms such as sex, blood pressure, cholesterol like 13 attributes to predict the likelihood of patient getting a Heart disease. Until now, 13 attributes are used for prediction. This research paper added two more attributes i.e. obesity and smoking. The data mining classification techniques, namely Decision Trees, Naive Bayes, and Neural Networks are analyzed on Heart disease database. The performance of these techniques is compared, based on accuracy. As per our results accuracy of Neural Networks, Decision Trees, and Naive Bayes are 100%, 99.62%, and 90.74% respectively. Our analysis shows that out of these three classification models Neural Networks predicts Heart disease with highest accuracy.","The overall objective of our work is to predict more accurately the presence of heart disease. In this paper, two more input attributes obesity and smoking are used to get more accurate results. Three data mining classification techniques were applied namely Decision trees, Naive Bayes & Neural Networks. From results it has been seen that Neural Networks provides accurate results as compare to Decision trees & Naive Bayes. This system can be further expanded. It can use more number of input attributes listed above in table 1 and 2. Other data mining techniques can also be used for predication e.g. Clustering, Time series, Association rules. The text mining can be used to mine huge amount of unstructured data available in healthcare industry database.","Improved Study of Heart Disease Prediction System using Data Mining Classification TechniquesData Mining, Heart Disease, Neural Networks, Decision Trees, Naive Bayes.The Healthcare industry is generally “information rich”, but unfortunately not all the data are mined which is required for discovering hidden patterns & effective decision making. Advanced data mining techniques are used to discover knowledge in database and for medical research, particularly in Heart disease prediction. This paper has analysed prediction systems for Heart disease using more number of input attributes. The system uses medical terms such as sex, blood pressure, cholesterol like 13 attributes to predict the likelihood of patient getting a Heart disease. Until now, 13 attributes are used for prediction. This research paper added two more attributes i.e. obesity and smoking. The data mining classification techniques, namely Decision Trees, Naive Bayes, and Neural Networks are analyzed on Heart disease database. The performance of these techniques is compared, based on accuracy. As per our results accuracy of Neural Networks, Decision Trees, and Naive Bayes are 100%, 99.62%, and 90.74% respectively. Our analysis shows that out of these three classification models Neural Networks predicts Heart disease with highest accuracy.The overall objective of our work is to predict more accurately the presence of heart disease. In this paper, two more input attributes obesity and smoking are used to get more accurate results. Three data mining classification techniques were applied namely Decision trees, Naive Bayes & Neural Networks. From results it has been seen that Neural Networks provides accurate results as compare to Decision trees & Naive Bayes. This system can be further expanded. It can use more number of input attributes listed above in table 1 and 2. Other data mining techniques can also be used for predication e.g. Clustering, Time series, Association rules. The text mining can be used to mine huge amount of unstructured data available in healthcare industry database.",Artificial Neural Network,"This research paper explores the use of advanced data mining techniques to predict the likelihood of heart disease in patients. It analyzes the performance of three classification models, Decision Trees, Naive Bayes, and Neural Networks, based on 15 input attributes including two newly added attributes, obesity and smoking. The results show that Neural Networks outperforms the other two models in terms of accuracy, achieving 100% accuracy. The paper suggests further expanding the system by incorporating more input attributes and utilizing other data mining techniques, such as clustering and text mining.",Deep Learning and Machine Learning,,Deep Learning and Machine Learning
48,Integrative Data Analysis of Multi-platform Cancer Data with a Multimodal Deep Learning Approach,"Multi-platform cancer data analysis, restricted Boltzmann machine, multimodal deep belief network, identification of cancer subtypes, genomic data, clinical data.","Identification of cancer subtypes plays an important role in revealing useful insights into disease pathogenesis and advancing personalized therapy. The recent development of high-throughput sequencing technologies has enabled the rapid collection of multi-platform genomic data (e.g., gene expression, miRNA expression and DNA methylation) for the same set of tumor samples. Although numerous integrative clustering approaches have been developed to analyze cancer data, few of them are particularly designed to exploit both deep intrinsic statistical properties of each input modality and complex cross-modality correlations among multi-platform input data. In this paper, we propose a new machine learning model, called multimodal deep belief network (DBN), to cluster cancer patients from multi-platform observation data. In our integrative clustering framework, relationships among inherent features of each single modality are first encoded into multiple layers of hidden variables, and then a joint latent model is employed to fuse common features derived from multiple input modalities. A practical learning algorithm, called contrastive divergence (CD), is applied to infer the parameters of our multimodal DBN model in an unsupervised manner. Tests on two available cancer datasets show that our integrative data analysis approach can effectively extract a unified representation of latent features to capture both intra- and cross-modality correlations, and identify meaningful disease subtypes from multi-platform cancer data. In addition, our approach can identify key genes and miRNAs that may play distinct roles in the pathogenesis of different cancer subtypes. Among those key miRNAs, we found that the expression level of miR-29a is highly correlated with survival time in ovarian cancer patients. These results indicate that our multimodal DBN based data analysis approach may have practical applications in cancer pathogenesis studies and provide useful guidelines for personalized cancer therapy.","In this paper, we have mainly applied a multi-modal DBN model to perform integrative clustering on multi-platform cancer data. In principle, our model can be also used to predict missing values based on other variables of a cancer patient after clustering the whole dataset. For example, our model can be used to predict drug use for each patient based on available genetic information. As our modal is a probabilistic framework, each new prediction can be associated with a confidence score. A similar strategy has been used in some references to predict missing drug-target interactions by exploiting the intrinsic correlations of previously known interactions.","Integrative Data Analysis of Multi-platform Cancer Data with a Multimodal Deep Learning ApproachMulti-platform cancer data analysis, restricted Boltzmann machine, multimodal deep belief network, identification of cancer subtypes, genomic data, clinical data.Identification of cancer subtypes plays an important role in revealing useful insights into disease pathogenesis and advancing personalized therapy. The recent development of high-throughput sequencing technologies has enabled the rapid collection of multi-platform genomic data (e.g., gene expression, miRNA expression and DNA methylation) for the same set of tumor samples. Although numerous integrative clustering approaches have been developed to analyze cancer data, few of them are particularly designed to exploit both deep intrinsic statistical properties of each input modality and complex cross-modality correlations among multi-platform input data. In this paper, we propose a new machine learning model, called multimodal deep belief network (DBN), to cluster cancer patients from multi-platform observation data. In our integrative clustering framework, relationships among inherent features of each single modality are first encoded into multiple layers of hidden variables, and then a joint latent model is employed to fuse common features derived from multiple input modalities. A practical learning algorithm, called contrastive divergence (CD), is applied to infer the parameters of our multimodal DBN model in an unsupervised manner. Tests on two available cancer datasets show that our integrative data analysis approach can effectively extract a unified representation of latent features to capture both intra- and cross-modality correlations, and identify meaningful disease subtypes from multi-platform cancer data. In addition, our approach can identify key genes and miRNAs that may play distinct roles in the pathogenesis of different cancer subtypes. Among those key miRNAs, we found that the expression level of miR-29a is highly correlated with survival time in ovarian cancer patients. These results indicate that our multimodal DBN based data analysis approach may have practical applications in cancer pathogenesis studies and provide useful guidelines for personalized cancer therapy.In this paper, we have mainly applied a multi-modal DBN model to perform integrative clustering on multi-platform cancer data. In principle, our model can be also used to predict missing values based on other variables of a cancer patient after clustering the whole dataset. For example, our model can be used to predict drug use for each patient based on available genetic information. As our modal is a probabilistic framework, each new prediction can be associated with a confidence score. A similar strategy has been used in some references to predict missing drug-target interactions by exploiting the intrinsic correlations of previously known interactions.Estimated Survival Function 104 Group 4 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 Group 8 |-+ Group 1-censored + Group 2-censored Group 3-censored |-+- Group 4-censored Group 5-censored -+-Group 6-censored © Group 7-censored Group 8-censored oe 4 — os 4 by on 024 Ky oo} 3 sto 10o0=« 100 »=«2000 «800-3000 Time (Days)Estimated Survival Function os os 024 004 | Group 4. croup 8 Group © Group D + Group A-censored |+-Group B-censored |+ Group C-censored |+- Group D-censored 10 Time (Years)noitonua Isvivue betsmite’ 200 4000 Lie (Dsh2) 4200 000 S200 3000 ov os oe oe |_\-enbet-a.onb p-ceuzoieg |_{-2nbet-a.onb y-ceuea.ed | enbel-d.on | i2nbet-auonb yEstimated Survival Function 104 os os os 004 han |_Meroup 1 |-croup 2 Group 3 ""Group 4 Group 5 |--Group 6 Group 7 Group 8 |+ Group 1-censored ++ Group 2-censored Group 3-censored |-+- Group 4-censored Group 5-censored I+ Group 6-censored + Group 7-censored Group 8-censored 500 1000 1500 Time (Days) 2000 2500 3000Drug use rate 100. 90. 80. 70. 60. 50. 40. 30. 20. 10. . 00% 00% 00% 00% 00% 00% 00% 00% 00% 00% 00% Carboplatin Cisplatin Doxil Gemcitabine Drug Taxol Taxotere Topotecan Group 1 BGroup &",Deep Learning and Machine Learning,This paper proposes a machine learning model called multimodal deep belief network (DBN) to cluster cancer patients using multi-platform observation data. The model encodes relationships among features from each input modality and fuses common features to extract a unified representation of latent features. Tests on two cancer datasets show that the approach can effectively identify meaningful disease subtypes and key genes/miRNAs that may play distinct roles in cancer pathogenesis. The model can also be used to predict missing values and drug use for each patient based on available genetic information. The approach may have practical applications in cancer pathogenesis studies and personalized cancer therapy.,Medical Data Analysis,"Estimated Survival Function 104 Group 4 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 Group 8 |-+ Group 1-censored + Group 2-censored Group 3-censored |-+- Group 4-censored Group 5-censored -+-Group 6-censored © Group 7-censored Group 8-censored oe 4 — os 4 by on 024 Ky oo} 3 sto 10o0=« 100 »=«2000 «800-3000 Time (Days)Estimated Survival Function os os 024 004 | Group 4. croup 8 Group © Group D + Group A-censored |+-Group B-censored |+ Group C-censored |+- Group D-censored 10 Time (Years)noitonua Isvivue betsmite’ 200 4000 Lie (Dsh2) 4200 000 S200 3000 ov os oe oe |_\-enbet-a.onb p-ceuzoieg |_{-2nbet-a.onb y-ceuea.ed | enbel-d.on | i2nbet-auonb yEstimated Survival Function 104 os os os 004 han |_Meroup 1 |-croup 2 Group 3 ""Group 4 Group 5 |--Group 6 Group 7 Group 8 |+ Group 1-censored ++ Group 2-censored Group 3-censored |-+- Group 4-censored Group 5-censored I+ Group 6-censored + Group 7-censored Group 8-censored 500 1000 1500 Time (Days) 2000 2500 3000Drug use rate 100. 90. 80. 70. 60. 50. 40. 30. 20. 10. . 00% 00% 00% 00% 00% 00% 00% 00% 00% 00% 00% Carboplatin Cisplatin Doxil Gemcitabine Drug Taxol Taxotere Topotecan Group 1 BGroup &",Medical Data Analysis
49,Machine Learning and Data Mining Methods in Diabetes Research,"Diabetes mellitus , Diabetic complications , Disease prediction models , Biomarker(s) identification.","The remarkable advances in biotechnology and health sciences have led to a significant production of data, such as high throughput genetic data and clinical information, generated from large Electronic Health Records (EHRs). To this end, application of machine learning and data mining methods in biosciences is presently, more than ever before, vital and indispensable in efforts to transform intelligently all available information into valuable knowledge. Diabetes mellitus (DM) is defined as a group of metabolic disorders exerting significant pressure on human health worldwide. Extensive research in all aspects of diabetes (diagnosis, etiopathophysiology, therapy, etc.) has led to the generation of huge amounts of data. The aim of the present study is to conduct a systematic review of the applications of machine learning, data mining techniques and tools in the field of diabetes research with respect to a) Prediction and Diagnosis, b) Diabetic Complications, c) Genetic Background and Environment, and e) Health Care and Management with the first category appearing to be the most popular. A wide range of machine learning algorithms were employed. In general, 85% of those used were characterized by supervised learning approaches and 15% by unsupervised ones, and more specifically, association rules. Support vector machines (SVM) arise as the most successful and widely used algorithm. Concerning the type of data, clinical datasets were mainly used. The title applications in the selected articles project the usefulness of extracting valuable knowledge leading to new hypotheses targeting deeper understanding and further investigation in DM.","In this study, a systematic effort was made to identify and review machine learning and data mining approaches applied on DM research. DM is rapidly emerging as one of the greatest global health challenges of the 21st century. To date, there is a significant work carried out in almost all aspects of DM research and especially biomarker identification and prediction-diagnosis. The advent of biotechnology, with the vast amount of data produced, along with the increasing amount of EHRs is expected to give rise to further in-depth exploration toward diagnosis, etiopathophysiology and treatment of DM through employment of machine learning and data mining techniques in enriched datasets that include clinical and biological information.","Machine Learning and Data Mining Methods in Diabetes ResearchDiabetes mellitus , Diabetic complications , Disease prediction models , Biomarker(s) identification.The remarkable advances in biotechnology and health sciences have led to a significant production of data, such as high throughput genetic data and clinical information, generated from large Electronic Health Records (EHRs). To this end, application of machine learning and data mining methods in biosciences is presently, more than ever before, vital and indispensable in efforts to transform intelligently all available information into valuable knowledge. Diabetes mellitus (DM) is defined as a group of metabolic disorders exerting significant pressure on human health worldwide. Extensive research in all aspects of diabetes (diagnosis, etiopathophysiology, therapy, etc.) has led to the generation of huge amounts of data. The aim of the present study is to conduct a systematic review of the applications of machine learning, data mining techniques and tools in the field of diabetes research with respect to a) Prediction and Diagnosis, b) Diabetic Complications, c) Genetic Background and Environment, and e) Health Care and Management with the first category appearing to be the most popular. A wide range of machine learning algorithms were employed. In general, 85% of those used were characterized by supervised learning approaches and 15% by unsupervised ones, and more specifically, association rules. Support vector machines (SVM) arise as the most successful and widely used algorithm. Concerning the type of data, clinical datasets were mainly used. The title applications in the selected articles project the usefulness of extracting valuable knowledge leading to new hypotheses targeting deeper understanding and further investigation in DM.In this study, a systematic effort was made to identify and review machine learning and data mining approaches applied on DM research. DM is rapidly emerging as one of the greatest global health challenges of the 21st century. To date, there is a significant work carried out in almost all aspects of DM research and especially biomarker identification and prediction-diagnosis. The advent of biotechnology, with the vast amount of data produced, along with the increasing amount of EHRs is expected to give rise to further in-depth exploration toward diagnosis, etiopathophysiology and treatment of DM through employment of machine learning and data mining techniques in enriched datasets that include clinical and biological information.GS Newel Discd Dag Tsges Mad iage Aas! BMC Bioinformatics InEavecaRasPubie Henle Metedctatasee PLoS One IEEE Eng Med Biol Soc Diabetes 4 Raceislend Capen Eagoverng Atif Intell Med Med Biol Eng Comput Big Data J Diabetes Sci Technol aia Expert Syst Appl BMCSyBiel S assa Dec Makes =-- Comput Biol Med fazer] Biomed Inforin, * AMIA“Annt Syimp Proc Comput Meth Prog Bio™ Bionied Res Int: swéssisyeas IEEE Biomed*Health Inf “Se, IEEE Trans Biomed Eng“, J Dia Coape hiss Mateteited % Jam Med Infers Ace PCOS (Ree Daterieat “Z %, OMpli, Send tng One Goo Drop Dea duvet 7%, Soe Ep ‘Obes Ras Giza Pract“. x ee (0%QUERY 1: PubMed QUERY 2: PubMed “Machine Learning” AND “Data Mining” AND “Diabetes” “Diabetes” 139 p13) <5 years <5 years BSNL) Bao LE) 110 BET Manual Inspection Manual Inspection Manual Inspection Final Collection —__——» Bicmna Drugs and Health Care Identification Therapies Mancecmene and Prediction Pi Seen 438 13 5",Deep Learning and Machine Learning,"This paper discusses the importance of using machine learning and data mining techniques in biosciences, particularly in the field of diabetes research. The study conducted a systematic review of the applications of these techniques in diabetes research, focusing on four main categories: prediction and diagnosis, diabetic complications, genetic background and environment, and health care and management. The results showed that supervised learning approaches, particularly support vector machines, were the most successful and widely used algorithms, with clinical datasets being the most commonly used type of data. The study concludes that the use of machine learning and data mining techniques in diabetes research can lead to the extraction of valuable knowledge and new hypotheses, ultimately contributing to deeper understanding and further investigation in the field.",Medical Data Analysis,"GS Newel Discd Dag Tsges Mad iage Aas! BMC Bioinformatics InEavecaRasPubie Henle Metedctatasee PLoS One IEEE Eng Med Biol Soc Diabetes 4 Raceislend Capen Eagoverng Atif Intell Med Med Biol Eng Comput Big Data J Diabetes Sci Technol aia Expert Syst Appl BMCSyBiel S assa Dec Makes =-- Comput Biol Med fazer] Biomed Inforin, * AMIA“Annt Syimp Proc Comput Meth Prog Bio™ Bionied Res Int: swéssisyeas IEEE Biomed*Health Inf “Se, IEEE Trans Biomed Eng“, J Dia Coape hiss Mateteited % Jam Med Infers Ace PCOS (Ree Daterieat “Z %, OMpli, Send tng One Goo Drop Dea duvet 7%, Soe Ep ‘Obes Ras Giza Pract“. x ee (0%QUERY 1: PubMed QUERY 2: PubMed “Machine Learning” AND “Data Mining” AND “Diabetes” “Diabetes” 139 p13) <5 years <5 years BSNL) Bao LE) 110 BET Manual Inspection Manual Inspection Manual Inspection Final Collection —__——» Bicmna Drugs and Health Care Identification Therapies Mancecmene and Prediction Pi Seen 438 13 5",Medical Data Analysis
50,Machine learning for assisting cervical cancer diagnosis: An ensemble approach,"Cervical Cancer, Machine Learning.","Cervical cancer remains one of the most prevalent gynecologic malignancies, worldwide. As cervical cancer is a highly preventable disease, therefore, early screening represents the most effective strategy to minimize the global burden of cervical cancer. However, due to scarce awareness, lack of access to medical centers, and highly expensive procedures in developing countries, the vulnerable patient populations cannot afford to undergo examination regularly. A novel ensemble approach is presented in this paper to predict the risk of cervical cancer. By adopting a voting strategy, this method addresses the challenges associated with previous studies on cervical cancer. A data correction mechanism is proposed to improve the performance of the prediction. A gene-assistance module is also included as an optional strategy to enhance the robustness of the prediction. Multiple measurements are performed to evaluate the proposed method. The results indicate that the likelihood of developing cervical cancer can be effectively predicted using the voting strategy. Compared with other methods, the proposed method is more scalable and practical.","In this study, we investigated multiple approaches for cervical cancer and proposed a novel and efficient auxiliary model for the prediction of cervical cancer using a gene sequence module. Besides, compared with other methods, the proposed method is more scalable and practical. Although the proposed method is limited by experimental support, the results imply that machine learning has infinite potentials in the field of medical research. In future, our investigations will focus on more distinctive data including colposcopy images.","Machine learning for assisting cervical cancer diagnosis: An ensemble approachCervical Cancer, Machine Learning.Cervical cancer remains one of the most prevalent gynecologic malignancies, worldwide. As cervical cancer is a highly preventable disease, therefore, early screening represents the most effective strategy to minimize the global burden of cervical cancer. However, due to scarce awareness, lack of access to medical centers, and highly expensive procedures in developing countries, the vulnerable patient populations cannot afford to undergo examination regularly. A novel ensemble approach is presented in this paper to predict the risk of cervical cancer. By adopting a voting strategy, this method addresses the challenges associated with previous studies on cervical cancer. A data correction mechanism is proposed to improve the performance of the prediction. A gene-assistance module is also included as an optional strategy to enhance the robustness of the prediction. Multiple measurements are performed to evaluate the proposed method. The results indicate that the likelihood of developing cervical cancer can be effectively predicted using the voting strategy. Compared with other methods, the proposed method is more scalable and practical.In this study, we investigated multiple approaches for cervical cancer and proposed a novel and efficient auxiliary model for the prediction of cervical cancer using a gene sequence module. Besides, compared with other methods, the proposed method is more scalable and practical. Although the proposed method is limited by experimental support, the results imply that machine learning has infinite potentials in the field of medical research. In future, our investigations will focus on more distinctive data including colposcopy images.Metrics 1.04 0.8 5 0.05 = i wer 7 Nin fee yi --- Accuracy ! tee Z \ I ad — |S Recall / sal i J. Me (> ~Precision Sten jl F1-Score Classifier thresholda risk factors fill missing values feature extraction training \ | binary classifier results existing methods ) a risk factors fill missing values feature extration Logistic Regression, SVM, KNN, MLP, Decision Tree results proposed method genomic sequencing dataset--- Accuracy sees Recall —:- Precision .»—— F1-Score T T 40 60 80 100 Number of PCA componentsU.L2U 0.18 0.16 0.14 20.12 0.10 obabil a 0.08 0.06 0.04 0.02 0.00 12345 67 8 9 1011121314151617181920212223 ChromosomeDistribution 100 630 560 NN W Sf BD rF ouUN ooo fo Oo 140 70 12345 67 8 9 1011121314151617181920212223 Chromosome",Deep Learning and Machine Learning,"The paper presents a novel ensemble approach for predicting the risk of cervical cancer, which addresses the challenges associated with previous studies on cervical cancer. Due to scarce awareness, lack of access to medical centers, and highly expensive procedures in developing countries, vulnerable patient populations cannot afford to undergo examination regularly. The proposed method includes a data correction mechanism and a gene-assistance module as optional strategies to enhance the robustness of the prediction. Multiple measurements are performed to evaluate the proposed method, and the results indicate that the likelihood of developing cervical cancer can be effectively predicted using the voting strategy. The proposed method is more scalable and practical compared to other methods. The study implies that machine learning has infinite potentials in the field of medical research, and future investigations will focus on more distinctive data, including colposcopy images.",Deep Learning and Machine Learning,"Metrics 1.04 0.8 5 0.05 = i wer 7 Nin fee yi --- Accuracy ! tee Z \ I ad — |S Recall / sal i J. Me (> ~Precision Sten jl F1-Score Classifier thresholda risk factors fill missing values feature extraction training \ | binary classifier results existing methods ) a risk factors fill missing values feature extration Logistic Regression, SVM, KNN, MLP, Decision Tree results proposed method genomic sequencing dataset--- Accuracy sees Recall —:- Precision .»—— F1-Score T T 40 60 80 100 Number of PCA componentsU.L2U 0.18 0.16 0.14 20.12 0.10 obabil a 0.08 0.06 0.04 0.02 0.00 12345 67 8 9 1011121314151617181920212223 ChromosomeDistribution 100 630 560 NN W Sf BD rF ouUN ooo fo Oo 140 70 12345 67 8 9 1011121314151617181920212223 Chromosome",Deep Learning and Machine Learning
51,Metabolomics and Its Application to Acute Lung Diseases,"metabolites, nuclear magnetic resonance, mass spectroscopy, pneumonia, acute respiratory distress syndrome, environmental exposure, precision medicine, biomarkers","Metabolomics is a rapidly expanding field of systems biology that is gaining significant attention in many areas of biomedical research. Also known as metabonomics, it comprises the analysis of all small molecules or metabolites that are present within an organism or a specific compartment of the body. Metabolite detection and quantification provide a valuable addition to genomics and proteomics and give unique insights into metabolic changes that occur in tangent to alterations in gene and protein activity that are associated with disease. As a novel approach to understanding disease, metabolomics provides a “snapshot” in time of all metabolites present in a biological sample such as whole blood, plasma, serum, urine, and many other specimens that may be obtained from either patients or experimental models. In this article, we review the burgeoning field of metabolomics in its application to acute lung diseases, specifically pneumonia and acute respiratory disease syndrome (ARDS). We also discuss the potential applications of metabolomics for monitoring exposure to aerosolized environmental toxins. Recent reports have suggested that metabolomics analysis using nuclear magnetic resonance (NMR) and mass spectrometry (MS) approaches may provide clinicians with the opportunity to identify new biomarkers that may predict progression to more severe disease, such as sepsis, which kills many patients each year. In addition, metabolomics may provide more detailed phenotyping of patient heterogeneity, which is needed to achieve the goal of precision medicine. However, although several experimental and clinical metabolomics studies have been conducted assessing the application of the science to acute lung diseases, only incremental progress has been made. Specifically, little is known about the metabolic phenotypes of these illnesses. These data are needed to substantiate metabolomics biomarker credentials so that clinicians can employ them for clinical decision-making and investigators can use them to design clinical trials.","In summary, we have reviewed the rapidly expanding field of metabolomics and its application to acute lung diseases. Metabolomics is an important component of systems biology that has enormous clinical potential in the development of biomarkers and as a novel approach to understanding disease mechanisms. Metabolomics allows us to generate a snapshot of all the metabolites present in a biological sample, and to follow rapidly changing trends in metabolites over time in a way that cannot be captured by genomics or proteomics. These changes may be monitored by the application of NMR or MS-based approaches. The challenge for the application of metabolomics to acute lung diseases rests with whether it will be able to identify more precise patient phenotypes that are not presently recognized by currently available clinical tools. The extent of the predictive and prognostic value of a given set of metabolites (e.g., biomarker credentials) will be required for optimal patient selection for clinical trials and ultimately for clinical decision making (14, 15) that will be needed to realize precision medicine. To date, urine metabolomics shows promise for rapidly differentiating pneumonia pathogens that is needed for timely antibiotic selection. However, for ARDS, metabolomics data that enable the distinction of susceptible patients and ARDS severity, are lacking. Analytically, there is a need to improve the sensitivity of NMR analysis and its reproducibility across centers. For MS-based approaches, new developing strategies to address the large number of unknown metabolites are being tested. With these challenges in place, we look forward to a future of increasingly sophisticated analyses of biological samples that will enhance our capability for diagnosing and monitoring human lung diseases.","Metabolomics and Its Application to Acute Lung Diseasesmetabolites, nuclear magnetic resonance, mass spectroscopy, pneumonia, acute respiratory distress syndrome, environmental exposure, precision medicine, biomarkersMetabolomics is a rapidly expanding field of systems biology that is gaining significant attention in many areas of biomedical research. Also known as metabonomics, it comprises the analysis of all small molecules or metabolites that are present within an organism or a specific compartment of the body. Metabolite detection and quantification provide a valuable addition to genomics and proteomics and give unique insights into metabolic changes that occur in tangent to alterations in gene and protein activity that are associated with disease. As a novel approach to understanding disease, metabolomics provides a “snapshot” in time of all metabolites present in a biological sample such as whole blood, plasma, serum, urine, and many other specimens that may be obtained from either patients or experimental models. In this article, we review the burgeoning field of metabolomics in its application to acute lung diseases, specifically pneumonia and acute respiratory disease syndrome (ARDS). We also discuss the potential applications of metabolomics for monitoring exposure to aerosolized environmental toxins. Recent reports have suggested that metabolomics analysis using nuclear magnetic resonance (NMR) and mass spectrometry (MS) approaches may provide clinicians with the opportunity to identify new biomarkers that may predict progression to more severe disease, such as sepsis, which kills many patients each year. In addition, metabolomics may provide more detailed phenotyping of patient heterogeneity, which is needed to achieve the goal of precision medicine. However, although several experimental and clinical metabolomics studies have been conducted assessing the application of the science to acute lung diseases, only incremental progress has been made. Specifically, little is known about the metabolic phenotypes of these illnesses. These data are needed to substantiate metabolomics biomarker credentials so that clinicians can employ them for clinical decision-making and investigators can use them to design clinical trials.In summary, we have reviewed the rapidly expanding field of metabolomics and its application to acute lung diseases. Metabolomics is an important component of systems biology that has enormous clinical potential in the development of biomarkers and as a novel approach to understanding disease mechanisms. Metabolomics allows us to generate a snapshot of all the metabolites present in a biological sample, and to follow rapidly changing trends in metabolites over time in a way that cannot be captured by genomics or proteomics. These changes may be monitored by the application of NMR or MS-based approaches. The challenge for the application of metabolomics to acute lung diseases rests with whether it will be able to identify more precise patient phenotypes that are not presently recognized by currently available clinical tools. The extent of the predictive and prognostic value of a given set of metabolites (e.g., biomarker credentials) will be required for optimal patient selection for clinical trials and ultimately for clinical decision making (14, 15) that will be needed to realize precision medicine. To date, urine metabolomics shows promise for rapidly differentiating pneumonia pathogens that is needed for timely antibiotic selection. However, for ARDS, metabolomics data that enable the distinction of susceptible patients and ARDS severity, are lacking. Analytically, there is a need to improve the sensitivity of NMR analysis and its reproducibility across centers. For MS-based approaches, new developing strategies to address the large number of unknown metabolites are being tested. With these challenges in place, we look forward to a future of increasingly sophisticated analyses of biological samples that will enhance our capability for diagnosing and monitoring human lung diseases.Acute phase Late phase Exudative phase Proliferative phase Fibrotic phase R 7 4-7 days >7-21 days >21 days epresentative * interstitial & alveolar —+_ alveolar & intimal * extensive Initiating events edema fibrosis pulmonary fibrosis * sepsis * neutrophil influx * proliferation of + loss of normal * pneumonia * enhance cytokine type Il cells & alveolar * trauma/shock production fibroblasts architecture + aspiration * loss of coagulation & : ee * blood transfusion fibrinolytic: ene homeostasis Diffuse Alveolar Damage (DAD)> Bh o Ww o BAL cells (x 10°/mL) N Oo Macs PMNs O Sham Oi Infected C infected + 8 “ as | oN | op 4 / e . / s \ { 7 \ Nol | | ° | ee } \ . : 4 & wm (fale = J g \ e oe | | Sham 2 .)~ 8 - &£ oC 8 “42R OH “\ R NH 0 *"" 2 + a ° or NS A R OH MSTFA R~OH-Si- —RONH;Si- 0 R“OH-Si-Absorbance at 220 nm Gradient (% solvent B) (16.773 min} Frags175.0¥ 2013-0219_020.4 753023 5158238 SN m/z > 5 = ela : Qs relative intensity OD 4 2 = 35 g; Z @ 25 ae ne 519.052 A 73980 107615.0 67446.1 171.006 05:. pReumoniaeNMR magnet NMR tube ¥ Signal collection & analysis Patient sample { =) s- i"" ULTRASHIELD™ 800 PLUS ‘ | } = ao CreatinineGenome Transcriptome Proteome RS . || XY Metabolome Lo 0 OH HO It CH, 9° HN OH 9 ° HO. tl oO ‘O—P—On | SS OH HC N —>ee hur aes urine « metabolismDisease ut KL Control | Untargeted Binning bin1 bin2 bin3 bin4 binS bin6 bin7 bin8.. o4 oq b ¥ 10 ==> UU. of_ => creatinine hippurate urea water | \ fumarate | hippurate allantoin NX taurine citrate 2-oxoglutarate| “succinate ppm 7 6 5 4 3 2 1 Targeted Profiling PCA, PLS-DA Plot PC2 25 20 @ 10 ; Disease 5 -19 Control -15 -20 25. PCL -30 -20 -10 0 10# Metabolites or Features detected (Log,,) Technology & Sensitivity M mM uM Sensitivity or nM LDL pM+ °@ .. A rt © => ® e ionization aw pone © °} j& detection & read out @ m/z neutral G) metabolites & negative mode ae detection & chromatography |——» | ionization |——» | mass analyzer |—» iad olit",Medical Data Analysis,"The field of metabolomics, which involves analyzing all small molecules or metabolites present within an organism or specific body compartment, is rapidly expanding and gaining attention in biomedical research. Metabolomics complements genomics and proteomics and provides valuable insights into metabolic changes associated with disease. Metabolomics can provide a snapshot of all metabolites present in a biological sample, and recent reports suggest that metabolomics analysis may identify biomarkers that predict disease progression and severity. Metabolomics has potential clinical applications in the development of biomarkers and precision medicine, but there is still a need to improve sensitivity and reproducibility across centers. While progress has been made in applying metabolomics to acute lung diseases, more data are needed to substantiate metabolomics biomarker credentials for clinical decision-making and trial design.",Medical Data Analysis,"Acute phase Late phase Exudative phase Proliferative phase Fibrotic phase R 7 4-7 days >7-21 days >21 days epresentative * interstitial & alveolar —+_ alveolar & intimal * extensive Initiating events edema fibrosis pulmonary fibrosis * sepsis * neutrophil influx * proliferation of + loss of normal * pneumonia * enhance cytokine type Il cells & alveolar * trauma/shock production fibroblasts architecture + aspiration * loss of coagulation & : ee * blood transfusion fibrinolytic: ene homeostasis Diffuse Alveolar Damage (DAD)> Bh o Ww o BAL cells (x 10°/mL) N Oo Macs PMNs O Sham Oi Infected C infected + 8 “ as | oN | op 4 / e . / s \ { 7 \ Nol | | ° | ee } \ . : 4 & wm (fale = J g \ e oe | | Sham 2 .)~ 8 - &£ oC 8 “42R OH “\ R NH 0 *"" 2 + a ° or NS A R OH MSTFA R~OH-Si- —RONH;Si- 0 R“OH-Si-Absorbance at 220 nm Gradient (% solvent B) (16.773 min} Frags175.0¥ 2013-0219_020.4 753023 5158238 SN m/z > 5 = ela : Qs relative intensity OD 4 2 = 35 g; Z @ 25 ae ne 519.052 A 73980 107615.0 67446.1 171.006 05:. pReumoniaeNMR magnet NMR tube ¥ Signal collection & analysis Patient sample { =) s- i"" ULTRASHIELD™ 800 PLUS ‘ | } = ao CreatinineGenome Transcriptome Proteome RS . || XY Metabolome Lo 0 OH HO It CH, 9° HN OH 9 ° HO. tl oO ‘O—P—On | SS OH HC N —>ee hur aes urine « metabolismDisease ut KL Control | Untargeted Binning bin1 bin2 bin3 bin4 binS bin6 bin7 bin8.. o4 oq b ¥ 10 ==> UU. of_ => creatinine hippurate urea water | \ fumarate | hippurate allantoin NX taurine citrate 2-oxoglutarate| “succinate ppm 7 6 5 4 3 2 1 Targeted Profiling PCA, PLS-DA Plot PC2 25 20 @ 10 ; Disease 5 -19 Control -15 -20 25. PCL -30 -20 -10 0 10# Metabolites or Features detected (Log,,) Technology & Sensitivity M mM uM Sensitivity or nM LDL pM+ °@ .. A rt © => ® e ionization aw pone © °} j& detection & read out @ m/z neutral G) metabolites & negative mode ae detection & chromatography |——» | ionization |——» | mass analyzer |—» iad olit",Medical Data Analysis
52,Natural Language Processing of Clinical Notes on Chronic Diseases: Systematic Review,electronic health records; clinical notes; chronic diseases; natural language processing; machine learning; deep learning; heart disease; stroke; cancer; diabetes; lung disease,"Background: Novel approaches that complement and go beyond evidence-based medicine are required in the domain of chronic diseases, given the growing incidence of such conditions on the worldwide population. A promising avenue is the secondary use of electronic health records (EHRs), where patient data are analyzed to conduct clinical and translational research. Methods based on machine learning to process EHRs are resulting in improved understanding of patient clinical trajectories and chronic disease risk prediction, creating a unique opportunity to derive previously unknown clinical insights. However, a wealth of clinical histories remains locked behind clinical narratives in free-form text. Consequently, unlocking the full potential of EHR data is contingent on the development of natural language processing (NLP) methods to automatically transform clinical text into structured clinical data that can guide clinical decisions and potentially delay or prevent disease onset. Objective: The goal of the research was to provide a comprehensive overview of the development and uptake of NLP methods applied to free-text clinical notes related to chronic diseases, including the investigation of challenges faced by NLP methodologies in understanding clinical narratives. Methods: Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed and searches were conducted in 5 databases using “clinical notes,” “natural language processing,” and “chronic disease” and their variations as keywords to maximize coverage of the articles. Results: Of the 2652 articles considered, 106 met the inclusion criteria. Review of the included papers resulted in identification of 43 chronic diseases, which were then further classified into 10 disease categories using the International Classification of Diseases, 10th Revision. The majority of studies focused on diseases of the circulatory system (n=38) while endocrine and metabolic diseases were fewest (n=14). This was due to the structure of clinical records related to metabolic diseases, which typically contain much more structured data, compared with medical records for diseases of the circulatory system, which focus more on unstructured data and consequently have seen a stronger focus of NLP. The review has shown that there is a significant increase in the use of machine learning methods compared to rule-based approaches; however, deep learning methods remain emergent (n=3). Consequently, the majority of works focus on classification of disease phenotype with only a handful of papers addressing extraction of comorbidities from the free text or integration of clinical notes with structured data. There is a notable use of relatively simple methods, such as shallow classifiers (or combination with rule-based methods), due to the interpretability of predictions, which still represents a significant issue for more complex methods. Finally, scarcity of publicly available data may also have contributed to insufficient development of more advanced methods, such as extraction of word embeddings from clinical notes.","Efforts are still required to improve (1) progression of clinical NLP methods from extraction toward understanding; (2) recognition of relations among entities rather than entities in isolation; (3) temporal extraction to understand past, current, and future clinical events; (4) exploitation of alternative sources of clinical knowledge; and (5) availability of large-scale, de-identified clinical corpora. Finally, our review has reinforced the fact that availability of public datasets remains scarce. This outcome was largely expected given the sensitivity of clinical data in addition to all the legal and regulatory issues, including the Health Insurance Portability and Accountability Act and the Data Protection Directive (Directive 95/46/EC) of the European Law (superseded by the General Data Protection Regulation 2016/679). As a result, the studies reviewed in this paper typically came from research-based health care institutions with in-house NLP teams having access to clinical data. Therefore, the need remains for shared tasks such as i2b2 and access to data that would increase participation in clinical NLP and contribute to improvements of NLP methods and algorithms targeting clinical applications.","Natural Language Processing of Clinical Notes on Chronic Diseases: Systematic Reviewelectronic health records; clinical notes; chronic diseases; natural language processing; machine learning; deep learning; heart disease; stroke; cancer; diabetes; lung diseaseBackground: Novel approaches that complement and go beyond evidence-based medicine are required in the domain of chronic diseases, given the growing incidence of such conditions on the worldwide population. A promising avenue is the secondary use of electronic health records (EHRs), where patient data are analyzed to conduct clinical and translational research. Methods based on machine learning to process EHRs are resulting in improved understanding of patient clinical trajectories and chronic disease risk prediction, creating a unique opportunity to derive previously unknown clinical insights. However, a wealth of clinical histories remains locked behind clinical narratives in free-form text. Consequently, unlocking the full potential of EHR data is contingent on the development of natural language processing (NLP) methods to automatically transform clinical text into structured clinical data that can guide clinical decisions and potentially delay or prevent disease onset. Objective: The goal of the research was to provide a comprehensive overview of the development and uptake of NLP methods applied to free-text clinical notes related to chronic diseases, including the investigation of challenges faced by NLP methodologies in understanding clinical narratives. Methods: Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed and searches were conducted in 5 databases using “clinical notes,” “natural language processing,” and “chronic disease” and their variations as keywords to maximize coverage of the articles. Results: Of the 2652 articles considered, 106 met the inclusion criteria. Review of the included papers resulted in identification of 43 chronic diseases, which were then further classified into 10 disease categories using the International Classification of Diseases, 10th Revision. The majority of studies focused on diseases of the circulatory system (n=38) while endocrine and metabolic diseases were fewest (n=14). This was due to the structure of clinical records related to metabolic diseases, which typically contain much more structured data, compared with medical records for diseases of the circulatory system, which focus more on unstructured data and consequently have seen a stronger focus of NLP. The review has shown that there is a significant increase in the use of machine learning methods compared to rule-based approaches; however, deep learning methods remain emergent (n=3). Consequently, the majority of works focus on classification of disease phenotype with only a handful of papers addressing extraction of comorbidities from the free text or integration of clinical notes with structured data. There is a notable use of relatively simple methods, such as shallow classifiers (or combination with rule-based methods), due to the interpretability of predictions, which still represents a significant issue for more complex methods. Finally, scarcity of publicly available data may also have contributed to insufficient development of more advanced methods, such as extraction of word embeddings from clinical notes.Efforts are still required to improve (1) progression of clinical NLP methods from extraction toward understanding; (2) recognition of relations among entities rather than entities in isolation; (3) temporal extraction to understand past, current, and future clinical events; (4) exploitation of alternative sources of clinical knowledge; and (5) availability of large-scale, de-identified clinical corpora. Finally, our review has reinforced the fact that availability of public datasets remains scarce. This outcome was largely expected given the sensitivity of clinical data in addition to all the legal and regulatory issues, including the Health Insurance Portability and Accountability Act and the Data Protection Directive (Directive 95/46/EC) of the European Law (superseded by the General Data Protection Regulation 2016/679). As a result, the studies reviewed in this paper typically came from research-based health care institutions with in-house NLP teams having access to clinical data. Therefore, the need remains for shared tasks such as i2b2 and access to data that would increase participation in clinical NLP and contribute to improvements of NLP methods and algorithms targeting clinical applications.Number of articles 12 10 Gl machine learning linear (machine learning) 2011 2012 2013 Year of publication 2014 92015 mmm rule-based linear (rule-based) I | | | | 2016 2017x Tog . - we . om Ss . ney a ak oak oe 8) sii (uns Osborne Kasthurirathne zor Ni 2015 — Ashish 2or4 = = mee son jou 2014 Leopet 2019 Ss izal ano = Sena lo iBarceton 2 009 ee ey Might 5017 807 0 ocr i eX A ; 2 po SC oreo GaN ice eh ok wor see “ehho 2016 00% « s , oy ang Nov an eet ""249507, 0g Ss eh So 2 ‘0 $7, ‘0, 3 °% 9102 WEN Roberts 2015 lmNumber of publications 2s 20 15 10 uw 2010 = 2011 2012 2013, 2014 =. 2015 2016 Year of publication ™ Medical informatics Clinical medicine | ™ Computer science 2017 2018Publication venues If categorized as clinical medicine Clinical medicine Search in Master Journal List in Clarivate Analytics If categorized as engineering and computing Computer science If includes biomedical informatics, health informatics Manual search of aims and scopes Medical informaticsIdentification initial database searching, refined database searching. Scopus (n=401) Web of Science (n=58) ACM (n=13) Additional records identified through other sources (n=6) 2652 articles identified through 478 articles identified through bo £ < o 2 5 a Included Titles and abstracts screened (n=432) Articles assessed by full-text (n=159) 106 Articles selected for review Duplication removal (n=46) 273 articles excluded after first review: not NLP, not clinical (n=115), not chronic disease (n=123), not English, not journal, review papers (n=35) 53 articles excluded after full text review: not NLP, not clinical (n=9), not chronic disease (n=32), not English, not full text, not journal (n=12)",Natural Language Processing,"The article discusses the potential of using natural language processing (NLP) methods to extract useful information from electronic health records (EHRs) related to chronic diseases. The review of 106 studies highlighted the challenges faced by NLP in understanding clinical narratives and the need for improvement in the progression of clinical NLP methods. The studies focused on diseases of the circulatory system and used machine learning methods for disease phenotype classification. However, deep learning methods and the extraction of word embeddings from clinical notes remain relatively new. The scarcity of publicly available data also limits the development of NLP methods. The article suggests that shared tasks and access to data could increase participation in clinical NLP and contribute to improvements in NLP methods for clinical applications.",Natural Language Processing,"Number of articles 12 10 Gl machine learning linear (machine learning) 2011 2012 2013 Year of publication 2014 92015 mmm rule-based linear (rule-based) I | | | | 2016 2017x Tog . - we . om Ss . ney a ak oak oe 8) sii (uns Osborne Kasthurirathne zor Ni 2015 — Ashish 2or4 = = mee son jou 2014 Leopet 2019 Ss izal ano = Sena lo iBarceton 2 009 ee ey Might 5017 807 0 ocr i eX A ; 2 po SC oreo GaN ice eh ok wor see “ehho 2016 00% « s , oy ang Nov an eet ""249507, 0g Ss eh So 2 ‘0 $7, ‘0, 3 °% 9102 WEN Roberts 2015 lmNumber of publications 2s 20 15 10 uw 2010 = 2011 2012 2013, 2014 =. 2015 2016 Year of publication ™ Medical informatics Clinical medicine | ™ Computer science 2017 2018Publication venues If categorized as clinical medicine Clinical medicine Search in Master Journal List in Clarivate Analytics If categorized as engineering and computing Computer science If includes biomedical informatics, health informatics Manual search of aims and scopes Medical informaticsIdentification initial database searching, refined database searching. Scopus (n=401) Web of Science (n=58) ACM (n=13) Additional records identified through other sources (n=6) 2652 articles identified through 478 articles identified through bo £ < o 2 5 a Included Titles and abstracts screened (n=432) Articles assessed by full-text (n=159) 106 Articles selected for review Duplication removal (n=46) 273 articles excluded after first review: not NLP, not clinical (n=115), not chronic disease (n=123), not English, not journal, review papers (n=35) 53 articles excluded after full text review: not NLP, not clinical (n=9), not chronic disease (n=32), not English, not full text, not journal (n=12)",Deep Learning and Machine Learning
53,On Deep Neural Networks for Detecting Heart Disease,"Machine learning, DNN, cardiology, translational medicine, artificial intelligence, diagnosis, cardiovascular disease, diagnostic medicine, hyperparameter optimization.","Heart disease is the leading cause of death, and experts estimate that approximately half of all heart attacks and strokes occur in people who have not been flagged as ’at risk.’ Thus, there is an urgent need to improve the accuracy of heart disease diagnosis. To this end, we investigate the potential of using data analysis, and in particular the design and use of deep neural networks (DNNs) for detecting heart disease based on routine clinical data. Our main contribution is the design, evaluation, and optimization of DNN architectures of increasing depth for heart disease diagnosis. This work led to the discovery of a novel five layer DNN architecture – named Heart Evaluation for Algorithmic Risk-reduction and Optimization Five (HEARO-5) – that yields best prediction accuracy. HEARO-5’s design employs regularization optimization and automatically deals with missing data and/or data outliers. To evaluate and tune the architectures we use k-way cross-validation as well as Matthews correlation coefficient (MCC) to measure the quality of our classifications. The study is performed on the publicly available Cleveland dataset of medical information, and we are making our developments open source, to further facilitate openness and research on the use of DNNs in medicine. The HEARO-5 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms currently published research in the area.","This work investigated and showed the potential of using DNN-based data analysis for detecting heart disease based on routine clinical data. The results show that, enhanced with flexible designs and tuning, DNN data analysis techniques can yield very high accuracy (99% accuracy and 0.98 MCC), which significantly outperforms currently published research in the area, to further establish the appeal of using ML DNN data analysis in diagnostic medicine. Pending reviews and publication, we are preparing to release the HEARO software framework as an open source, and HEARO-5 as a benchmark, making the software available for comparison and further facilitating openness and research on the use of DNN techniques in medicine. While the current developments are mostly research with excellent proof-of-concept results, further research and development is necessary in order to turn it into a robust diagnostic tool, e.g., that doctors consult and use routinely to make a more informed diagnosis. Research is needed in the data analytics area and its intersection with data-based medical diagnosis – including automatic search for best features, as well as possible features expansion or features reduction, e.g., due to lack of certain clinical data. Future directions include extending this analysis to construct a more thorough model that includes heart visualizations and CT image data. More features can provide more data for the algorithm to learn from, creating a more complex model and ensuring a more accurate and detailed prediction. Another area of future research would involve using speed optimization tools and accelerated linear algebra backends such as MagmaDNN for GPUs to improve the algorithm’s ability to process large amounts of data and find best configurations in parallel. In the future, HEARO will also be developed into a production quality software package with a friendly user interface, e.g., to facilitate use by doctors or even patients directly.","On Deep Neural Networks for Detecting Heart DiseaseMachine learning, DNN, cardiology, translational medicine, artificial intelligence, diagnosis, cardiovascular disease, diagnostic medicine, hyperparameter optimization.Heart disease is the leading cause of death, and experts estimate that approximately half of all heart attacks and strokes occur in people who have not been flagged as ’at risk.’ Thus, there is an urgent need to improve the accuracy of heart disease diagnosis. To this end, we investigate the potential of using data analysis, and in particular the design and use of deep neural networks (DNNs) for detecting heart disease based on routine clinical data. Our main contribution is the design, evaluation, and optimization of DNN architectures of increasing depth for heart disease diagnosis. This work led to the discovery of a novel five layer DNN architecture – named Heart Evaluation for Algorithmic Risk-reduction and Optimization Five (HEARO-5) – that yields best prediction accuracy. HEARO-5’s design employs regularization optimization and automatically deals with missing data and/or data outliers. To evaluate and tune the architectures we use k-way cross-validation as well as Matthews correlation coefficient (MCC) to measure the quality of our classifications. The study is performed on the publicly available Cleveland dataset of medical information, and we are making our developments open source, to further facilitate openness and research on the use of DNNs in medicine. The HEARO-5 architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms currently published research in the area.This work investigated and showed the potential of using DNN-based data analysis for detecting heart disease based on routine clinical data. The results show that, enhanced with flexible designs and tuning, DNN data analysis techniques can yield very high accuracy (99% accuracy and 0.98 MCC), which significantly outperforms currently published research in the area, to further establish the appeal of using ML DNN data analysis in diagnostic medicine. Pending reviews and publication, we are preparing to release the HEARO software framework as an open source, and HEARO-5 as a benchmark, making the software available for comparison and further facilitating openness and research on the use of DNN techniques in medicine. While the current developments are mostly research with excellent proof-of-concept results, further research and development is necessary in order to turn it into a robust diagnostic tool, e.g., that doctors consult and use routinely to make a more informed diagnosis. Research is needed in the data analytics area and its intersection with data-based medical diagnosis – including automatic search for best features, as well as possible features expansion or features reduction, e.g., due to lack of certain clinical data. Future directions include extending this analysis to construct a more thorough model that includes heart visualizations and CT image data. More features can provide more data for the algorithm to learn from, creating a more complex model and ensuring a more accurate and detailed prediction. Another area of future research would involve using speed optimization tools and accelerated linear algebra backends such as MagmaDNN for GPUs to improve the algorithm’s ability to process large amounts of data and find best configurations in parallel. In the future, HEARO will also be developed into a production quality software package with a friendly user interface, e.g., to facilitate use by doctors or even patients directly.Accuracy of a 7-layer neural network with varied learning rates 100 90 80 70 + 60 5 01 0.01 40 7 0.001 30 5 50 7 Percent accuracy 20 + 10 5 0 500 1500 2500 3500 4500 5500 Number of iterations Percent accuracy ‘Accuracy comparison between applications of regularization to a S-layer neural network 100 95 90 85 80 egularized training 75 + “S regularized 70 test 65 + ~unregularized training 60 55 50 +— . . 500 1500 2500 3500 4500 5500, Number of iterations Figure 3: Left: Effect of \ on 7-layer HEARO architecture. Right: Effect on regularization on HEARO-5 variants.95 85 1s Percent accuracy 70 65 55 Percent Accuracy for 2, 5, and 7-layer neural networks 500 1500 2500 + 3500S 45005500 Number of iterations >2-layer “S-layer ~©7-layer Figure 2: Left: Accuracy comparison of HEARO-5 vs. Percent accuracy Accuracy of a S-layer neural network with varied learning rates 100 + 95 + 90 85 > 0 + acy 01 0.01 70 + 0.001 65 > 60 55 50 + 500 1500 2500 3500 4500 5500 Number of iterations 2 and 5 layer networks. Right: Effect of \ in HEARO-5.0) 1) L-1) L) Ay=X cc a =W, ty ee Zu4=WosAet by 2. = WAL by, = 1 =O ( 4, propagation” As=O11(41) A =o(2) 4 ' ' ' wo Training data N 43 matrix a a . 1 Outputs output (size 1xN) size 13xN : : . layer L hidden Tayer L-1 2U ; Lat Wz, = W1, dZ,."".0°,(Z,) Back ""dz, =A,-Y dW, = dZ; AT) / N (oes) OW, =0Z, ATL / N db, =np.sum(dZ,, axis=1, keepdims =TrueIN Propagation db, = np.sum(dZ,, axis=1, keepdims =True)/N Figure 1: Parametrized DNN architecture and the main computational steps for its training. Training data X consists of 13 features (routine clinical data per patient; can also be parametrized) and N training examples. Weights W,b are trained using batch stochastic gradient descent method to make predictions A; ”match” the given outcomes Y.",Deep Learning and Machine Learning,"The article discusses the potential of using deep neural networks (DNNs) for detecting heart disease based on routine clinical data, and presents a novel five layer DNN architecture named HEARO-5 that yields best prediction accuracy. The study is performed on the publicly available Cleveland dataset of medical information and the results show that DNN data analysis techniques can yield very high accuracy (99% accuracy and 0.98 MCC), which significantly outperforms currently published research in the area. The article emphasizes the need for further research and development to turn this into a robust diagnostic tool, including automatic search for best features, feature expansion or reduction, and extending the analysis to construct a more thorough model that includes heart visualizations and CT image data. The HEARO software framework will be released as an open source, and HEARO-5 as a benchmark, making the software available for comparison and further research on the use of DNN techniques in medicine.",Deep Learning and Machine Learning,"Accuracy of a 7-layer neural network with varied learning rates 100 90 80 70 + 60 5 01 0.01 40 7 0.001 30 5 50 7 Percent accuracy 20 + 10 5 0 500 1500 2500 3500 4500 5500 Number of iterations Percent accuracy ‘Accuracy comparison between applications of regularization to a S-layer neural network 100 95 90 85 80 egularized training 75 + “S regularized 70 test 65 + ~unregularized training 60 55 50 +— . . 500 1500 2500 3500 4500 5500, Number of iterations Figure 3: Left: Effect of \ on 7-layer HEARO architecture. Right: Effect on regularization on HEARO-5 variants.95 85 1s Percent accuracy 70 65 55 Percent Accuracy for 2, 5, and 7-layer neural networks 500 1500 2500 + 3500S 45005500 Number of iterations >2-layer “S-layer ~©7-layer Figure 2: Left: Accuracy comparison of HEARO-5 vs. Percent accuracy Accuracy of a S-layer neural network with varied learning rates 100 + 95 + 90 85 > 0 + acy 01 0.01 70 + 0.001 65 > 60 55 50 + 500 1500 2500 3500 4500 5500 Number of iterations 2 and 5 layer networks. Right: Effect of \ in HEARO-5.0) 1) L-1) L) Ay=X cc a =W, ty ee Zu4=WosAet by 2. = WAL by, = 1 =O ( 4, propagation” As=O11(41) A =o(2) 4 ' ' ' wo Training data N 43 matrix a a . 1 Outputs output (size 1xN) size 13xN : : . layer L hidden Tayer L-1 2U ; Lat Wz, = W1, dZ,."".0°,(Z,) Back ""dz, =A,-Y dW, = dZ; AT) / N (oes) OW, =0Z, ATL / N db, =np.sum(dZ,, axis=1, keepdims =TrueIN Propagation db, = np.sum(dZ,, axis=1, keepdims =True)/N Figure 1: Parametrized DNN architecture and the main computational steps for its training. Training data X consists of 13 features (routine clinical data per patient; can also be parametrized) and N training examples. Weights W,b are trained using batch stochastic gradient descent method to make predictions A; ”match” the given outcomes Y.",Deep Learning and Machine Learning
54,Patient empowerment for cancer patients through a novel ICT infrastructure,Patient empowerment Psychoemotional monitoring Personal health system Cancer,"As a result of recent advances in cancer research and “precision medicine” approaches, i.e. the idea of treating each patient with the right drug at the right time, more and more cancer patients are being cured, or might have to cope with a life with cancer. For many people, cancer survival today means living with a complex and chronic condition. Surviving and living with or beyond cancer requires the long-term management of the disease, leading to a significant need for active rehabilitation of the patients. In this paper, we present a novel methodology employed in the iManageCancer project for cancer patient empowerment in which personal health systems, serious games, psycho-emotional monitoring and other novel decision-support tools are combined into an integrated patient empowerment platform. We present in detail the ICT infrastructure developed and our evaluation with the involvement of cancer patients on two sites, a large-scale pilot for adults and a small-scale test for children. The evaluation showed mixed evidences on the improvement of patient empowerment, while ability to cope with cancer, including improvement in mood and resilience to cancer, increased for the participants of the adults′ pilot.","Studies have shown that up to two thirds of cancer patients experience substantial physical, psychological and social problems due to the disease and treatment [68]. Support from healthcare and social services is often inappropriate, and patients report high levels of unmet needs following treatment and even a long time after [69]. One key such, currently unmet, need is the effective support of continuous and integrated care [70]. In this paper, we presented the elements of a novel a novel ICT infrastructure developed and employed for supporting integration of care and empowering cancer patients to take better part in the management of their disease. We presented in detail the various technological components developed and their integration in a unique, modular platform. The platform was tested on two sites with the involvement of cancer patients, a large-scale pilot for adults and a small scale test for children. The results of these pilot activities are presented and discussed in detail. A key conclusion from our work is that the platform has been shown to be effective in integrating all involved stakeholders through a multicomponent ICT platform, ensuring continuity and consistency of clinical management, including providing and sharing information and care planning, coordinating the care required by the patient. The work reported in this manuscript provides additional evidence indicating that continuity is valuable and important to both doctors and patients. It not only enables health doctors to have an improved relationship with their patients but also enables them to work more effectively and, most importantly, results in improved clinical outcomes, a finding that is in line with prior published evidence [71].","Patient empowerment for cancer patients through a novel ICT infrastructurePatient empowerment Psychoemotional monitoring Personal health system CancerAs a result of recent advances in cancer research and “precision medicine” approaches, i.e. the idea of treating each patient with the right drug at the right time, more and more cancer patients are being cured, or might have to cope with a life with cancer. For many people, cancer survival today means living with a complex and chronic condition. Surviving and living with or beyond cancer requires the long-term management of the disease, leading to a significant need for active rehabilitation of the patients. In this paper, we present a novel methodology employed in the iManageCancer project for cancer patient empowerment in which personal health systems, serious games, psycho-emotional monitoring and other novel decision-support tools are combined into an integrated patient empowerment platform. We present in detail the ICT infrastructure developed and our evaluation with the involvement of cancer patients on two sites, a large-scale pilot for adults and a small-scale test for children. The evaluation showed mixed evidences on the improvement of patient empowerment, while ability to cope with cancer, including improvement in mood and resilience to cancer, increased for the participants of the adults′ pilot.Studies have shown that up to two thirds of cancer patients experience substantial physical, psychological and social problems due to the disease and treatment [68]. Support from healthcare and social services is often inappropriate, and patients report high levels of unmet needs following treatment and even a long time after [69]. One key such, currently unmet, need is the effective support of continuous and integrated care [70]. In this paper, we presented the elements of a novel a novel ICT infrastructure developed and employed for supporting integration of care and empowering cancer patients to take better part in the management of their disease. We presented in detail the various technological components developed and their integration in a unique, modular platform. The platform was tested on two sites with the involvement of cancer patients, a large-scale pilot for adults and a small scale test for children. The results of these pilot activities are presented and discussed in detail. A key conclusion from our work is that the platform has been shown to be effective in integrating all involved stakeholders through a multicomponent ICT platform, ensuring continuity and consistency of clinical management, including providing and sharing information and care planning, coordinating the care required by the patient. The work reported in this manuscript provides additional evidence indicating that continuity is valuable and important to both doctors and patients. It not only enables health doctors to have an improved relationship with their patients but also enables them to work more effectively and, most importantly, results in improved clinical outcomes, a finding that is in line with prior published evidence [71].Assessment Analysis scenarios Data Analysis and Visualization Poyeno-aulodos) perability Game for Kids Monitoring i : ' SGS. ’ Game for Adults Life Style BED Monitoring Familial Resilience FORTH ? Evaluation FORTH Health Inquiries Doctors View Health Information Recommender Drug Self- Management Side Effect Decision Aid for Management Workflows Predictions Consultations Management and Decision Support InformationEvaluation Phase Pilot Design Phase Clinic ians Development Phase ve So Me a La End-users workshops \¢ Exploitation Phase Business Modelsov @ 14:11 Fe iManageMyHealth oe) 7 ard) WY NA ete fy Management Rate functionality Rate functionality lol Really z VOR Ie Red Rate functionality Rate functionalityDeveloping decision models _. | ping Diverse datasets I Mate) (1024-9 Models Execute Models/ Get Valuesof model Clinical Research ~ = Ctra Model Repository Clinical Repositories Applying decision models Clinical CareModel Repository Framework Disease Execution Models Engine recommend Patient App iManageMyHealth Physician App Care Flow Engine request Care Flow|(—_) Execution Designer || Care Flow Engine r Diagrams J dataNotifications iPHR Notifications ys «o PHIR a Careflow Engine Drug Repositories (EU, UK, German, Italy)a iIPHR Diana Allen, Born 9/4/1975 App Settings for Diana Allen In most cases, an application should be shared with all of your carenets, You can limm your carenets’ access by choosing which data to share with them. Sometimes, you may want to restrict an application to only some of your carenets when the application Itself, |e. ""My Pregnancy Manager,” reveals information you want to Keep private. “- & © Ss @Q Appointments Medications Upload documents Recommencer ~ tt Drag the apps to the carenets you want them to be accessible from A © o “= HB Patients Select a Patient Diana Allea pps ~ Gm Alergies © 4q Appointments @ ™ Calendar@ & Demographics @ G& Forum@ & Labs @ 7} Measurements @ @ Medications @ () Other apps @ A Problems @ \@) Procedures @ Q Recommender @ % Settings i Inbox \™ Language~iMC Portal iPHR iManageMyHealth Game for Kids MyHealthAvatar iSupportMy 4iMC (Breast & Patients Prostate Cancer) \uAccessControl..| | R&R Management| |. Audit Trail... re API Querying Integrated CeSsin, Oring t i Semantic Layer/ Data. a inki ee Data “ a iMC Data StoreI think having such a platform is an added value for the hospital How do you rate the platform in general? I think other patients would like to use the platform I think | would like to use this system frequently | had no difficulties using the different apps I thought the system was easy to use | found the various features of the apps well integrated | found the various features of the platform useful The words used in the apps are easy to understand | felt confident using the apps oO 10 20 30 40 50 BH i(strongly disagree) M2 M3 M4 m5 (strongly agree) 70 ey fe) NI (es) a (e) ay i) (ag an ES I) IN ary rs -) haat 100CO a) > =",Medical Data Analysis,"This paper discusses the need for long-term management of cancer and the importance of patient empowerment in achieving this. The iManageCancer project is presented as a novel methodology for cancer patient empowerment, which combines personal health systems, serious games, psycho-emotional monitoring, and other decision-support tools into an integrated platform. The paper details the ICT infrastructure developed and evaluated through a large-scale pilot for adults and a small-scale test for children. The evaluation showed mixed evidence on the improvement of patient empowerment but demonstrated an increase in coping with cancer and improvement in mood and resilience to cancer for the adult pilot. The platform was shown to be effective in integrating all involved stakeholders, ensuring continuity and consistency of clinical management, providing and sharing information, and coordinating patient care. The work reported in this manuscript provides evidence that continuity is valuable and important for both doctors and patients and can result in improved clinical outcomes.",Medical Data Analysis,"Assessment Analysis scenarios Data Analysis and Visualization Poyeno-aulodos) perability Game for Kids Monitoring i : ' SGS. ’ Game for Adults Life Style BED Monitoring Familial Resilience FORTH ? Evaluation FORTH Health Inquiries Doctors View Health Information Recommender Drug Self- Management Side Effect Decision Aid for Management Workflows Predictions Consultations Management and Decision Support InformationEvaluation Phase Pilot Design Phase Clinic ians Development Phase ve So Me a La End-users workshops \¢ Exploitation Phase Business Modelsov @ 14:11 Fe iManageMyHealth oe) 7 ard) WY NA ete fy Management Rate functionality Rate functionality lol Really z VOR Ie Red Rate functionality Rate functionalityDeveloping decision models _. | ping Diverse datasets I Mate) (1024-9 Models Execute Models/ Get Valuesof model Clinical Research ~ = Ctra Model Repository Clinical Repositories Applying decision models Clinical CareModel Repository Framework Disease Execution Models Engine recommend Patient App iManageMyHealth Physician App Care Flow Engine request Care Flow|(—_) Execution Designer || Care Flow Engine r Diagrams J dataNotifications iPHR Notifications ys «o PHIR a Careflow Engine Drug Repositories (EU, UK, German, Italy)a iIPHR Diana Allen, Born 9/4/1975 App Settings for Diana Allen In most cases, an application should be shared with all of your carenets, You can limm your carenets’ access by choosing which data to share with them. Sometimes, you may want to restrict an application to only some of your carenets when the application Itself, |e. ""My Pregnancy Manager,” reveals information you want to Keep private. “- & © Ss @Q Appointments Medications Upload documents Recommencer ~ tt Drag the apps to the carenets you want them to be accessible from A © o “= HB Patients Select a Patient Diana Allea pps ~ Gm Alergies © 4q Appointments @ ™ Calendar@ & Demographics @ G& Forum@ & Labs @ 7} Measurements @ @ Medications @ () Other apps @ A Problems @ \@) Procedures @ Q Recommender @ % Settings i Inbox \™ Language~iMC Portal iPHR iManageMyHealth Game for Kids MyHealthAvatar iSupportMy 4iMC (Breast & Patients Prostate Cancer) \uAccessControl..| | R&R Management| |. Audit Trail... re API Querying Integrated CeSsin, Oring t i Semantic Layer/ Data. a inki ee Data “ a iMC Data StoreI think having such a platform is an added value for the hospital How do you rate the platform in general? I think other patients would like to use the platform I think | would like to use this system frequently | had no difficulties using the different apps I thought the system was easy to use | found the various features of the apps well integrated | found the various features of the platform useful The words used in the apps are easy to understand | felt confident using the apps oO 10 20 30 40 50 BH i(strongly disagree) M2 M3 M4 m5 (strongly agree) 70 ey fe) NI (es) a (e) ay i) (ag an ES I) IN ary rs -) haat 100CO a) > =",Medical Data Analysis
55,Periodontal Disease and Rheumatoid Arthritis: A Systematic Review,"inflammation, chronic disease, periodontics, rheumatology, biological markers, risk factors.","This systematic review considers the evidence available for a relationship between periodontal disease and rheumatoid arthritis. MEDLINE/PubMed, CINAHL, DOSS, Embase, Scopus, Web of Knowledge, MedNar, and ProQuest Theses and Dissertations were searched from the inception of the database until June 2012 for any quantitative studies that examined the association between periodontal disease and rheumatoid arthritis. Nineteen studies met our inclusion criteria. Good evidence was found to support an association between these conditions with regard to tooth loss, clinical attachment levels, and erythrocyte sedimentation rates. Moderate evidence was noted for C-reactive protein and interleukin-1β. Some evidence for a positive outcome of periodontal treatment on the clinical features of rheumatoid arthritis was noted. These results provide moderate evidence based on biochemical markers and stronger evidence with regard to clinical parameters that common risk factors or common pathologic processes may be responsible for an association between rheumatoid arthritis and periodontal disease. Further studies are required to fully explore both the biochemical processes and clinical relationships between these 2 chronic inflammatory conditions. There is a need to move from case-control studies to more rigorous studies using well-defined populations and well-defined biochemical and clinical outcomes as the primary outcome measures with consideration of potential confounding factors.","This systematic review has considered the best available evidence for an association between the clinical features of periodontal disease and biochemical markers for rheumatoid arthritis. While most of the studies were case-control and had relatively low sample sizes, potential associations between periodontal disease and rheumatoid arthritis have been identified. From this review, it is clear that there is a need for larger population-based studies to confirm this link. There is a need to move from case control studies to more rigorous studies with well-defined populations and well-defined biochemical and clinical outcomes as the primary outcome measures, along with consideration of potential confounding factors.","Periodontal Disease and Rheumatoid Arthritis: A Systematic Reviewinflammation, chronic disease, periodontics, rheumatology, biological markers, risk factors.This systematic review considers the evidence available for a relationship between periodontal disease and rheumatoid arthritis. MEDLINE/PubMed, CINAHL, DOSS, Embase, Scopus, Web of Knowledge, MedNar, and ProQuest Theses and Dissertations were searched from the inception of the database until June 2012 for any quantitative studies that examined the association between periodontal disease and rheumatoid arthritis. Nineteen studies met our inclusion criteria. Good evidence was found to support an association between these conditions with regard to tooth loss, clinical attachment levels, and erythrocyte sedimentation rates. Moderate evidence was noted for C-reactive protein and interleukin-1β. Some evidence for a positive outcome of periodontal treatment on the clinical features of rheumatoid arthritis was noted. These results provide moderate evidence based on biochemical markers and stronger evidence with regard to clinical parameters that common risk factors or common pathologic processes may be responsible for an association between rheumatoid arthritis and periodontal disease. Further studies are required to fully explore both the biochemical processes and clinical relationships between these 2 chronic inflammatory conditions. There is a need to move from case-control studies to more rigorous studies using well-defined populations and well-defined biochemical and clinical outcomes as the primary outcome measures with consideration of potential confounding factors.This systematic review has considered the best available evidence for an association between the clinical features of periodontal disease and biochemical markers for rheumatoid arthritis. While most of the studies were case-control and had relatively low sample sizes, potential associations between periodontal disease and rheumatoid arthritis have been identified. From this review, it is clear that there is a need for larger population-based studies to confirm this link. There is a need to move from case control studies to more rigorous studies with well-defined populations and well-defined biochemical and clinical outcomes as the primary outcome measures, along with consideration of potential confounding factors.A Study Pischon (2008) Okada (2011) Kasser (1997) Joseph (2012) Gleissner (1998) Garib (2011) Farah Vakar (2010) DePaula Ishi (2008) Bozkurt (2006) Biyikoglu (2006) Non-RA patient -4.0 Overall Overall -4.0 Overall Z=3.11, P=0.0020 Cc Study Pischon (2008) Joseph (2012) ikoglu (2006) ‘Mercado (2001) Non-perio patient -29.0 Overall Overall -29.0 Overall Z=1.47, P=0.1442 CAL DerSimonian & Laird WMD Weight (C1 95% Random) 9.94% 0.97 (0.55,1.39) | 10.26% -0.10 (-0.14,-0.06) Se aminauz + 10.10% 1.90 (1.61,2.19) _ 10.00% 1.50 (1.12,1.88) H porsazt¢aocean | Fi ws ras ——9.83% 3.49 (3.01,3.97) 9.67% -0.70 (-1.27,-0.13) 0.0 4.0 RA patient = 0.0 40 Heterogeneity Chi squared=5324.17, 10.00% 1.17 (0.43,1.90) ESR DerSimonian & Laird WMD Weight (Cl 95% Random) I 26.45% 1.00 (-1.38,-0.62) ae 24.54% 17.34 (9.59,25.09) -—|-- 23,58% 3.40 (-6.27,13.07) —frssasx 23.46 (17.58,28.74) 09 29.0 Perio patient +—+—- 100.00% 10.68 (-3.56,24.93) 00 29.0 Heterogeneity Chi squared=93.73, P=0.0 Study Garib (2011) Gleissner (1998) Joseph (2012) Kasser (1997) Pischon (2008) Kobayashi (2007) Mercado (2001) Non-RA patient -7.0 Overall Overall -7.0 Overall Z=5.16, p<0.0001 D Study Pischon (2008) Joseph (2012) Biyikoglu (2006) Mercado (2001), Non-perio patient -35.0 Overall Overall -35.0 Overall Z=3.20, P=0.0014 Missing teeth DerSimonian & Laird WMD Weight (Cl 95% Random) ae 17.75% 2.61 (1.73,3.49) --|--- 13.53% 2.20 (0.71,3.69) fe 15.21% 1.04 (-0.24,2.26) me 13.53% 1.60 (0.11,3.09) _—— 7.58% 0.08 (-2.56,2.72) f raon san assay 11.11% 4.90 (3.02,6.78) 0.0 7.0 RA patient = 10.00% 2.38 (1.48,3.29) 00 70 Heterogeneity Chi squared=30.14,P=0.0 cRP DerSimonian & Laird WMD Weight (Ci 95% Random) 29.99% -0.06 (0.08,0.04) 27.57% 1.78 (0.39,3.17) 29.36% -0.16 (-0.85,0.53) —fH1300% 28.89 (23.55,34.23) 0.0 35.0 Perio patient it 35.0 Heterogeneity Chi squared=119.49, P=0.0 100.0% 4.20 (1.63,6.78) Figure 2. Forest plots showing the weighted mean difference (WMD) in (A) clinical attachment level (CAL) between rheumatoid arthritis patients and non-RA controls; (B) tooth loss between RA patients and nonRA controls; (C) erythrocyte sedimentation rate (ESR) between periodontitis-positive patients and periodontitisnegative controls; and (D) C-reactive protein (CRP) between periodontitis-positive patients and periodontitisnegative controls.Concept Proposals for Interplay Between Periodontitis and Rheumatoid Arthritis A. Periodontitis Precedes Rheumatoid B. Common Inflammatory Pathways: Osteoclast Activation and Vascular Damage Arthri Inflammation Periodontal infection & inflammation * Osteoprotegerin (OPG) expression decreased * Receptor Activator of NF-kappa Ligand B (RANKL) expression increased Citrullination of proteins by * TNF-related Apoptosis Inducing Ligand (TRAIL) expression increased endogenous inflammation or exogenous: P gingivalis Results in both vascular damage and increased osteoclast activation Production of autoantibodies to citrullinated proteins C. Two Hit Model of Mutual Exacerbation | Systemic Chronic Cross reactivity with Inflammation Cartilage components . . | . Periodontitis Rheumatoid Arthritis Auto-immunity to self-antigens (1st hit) (2nd hit) Rheumatoid arthritis Inflammatory mediators Figure 3. Concept of how periodontitis and rheumatoid arthritis might interact. (A) A model for how periodontal inflammation could prime an auto-immune response to citrullinated proteins prior to the development of rheumatoid arthritis; (B) a model representing how dysregulation of key inflammatory pathways could underlie both periodontitis and rheumatoid arthritis; and (C) a model demonstrating how both periodontitis and rheumatoid arthritis miaht exacerbate each other.Potentially relevant papers identified by literature search n=127 y Abstracts retrieved for examination n=112 Papers excluded after evaluation of abstract n= 51 y Papers retrieved for detailed examination n=61 Papers excluded after review of full paper v Papers assessed for methodological quality n=29 v Papers included in the systematic review n=19 y Papers included in the quantitative synthesis (meta-analysis) n=12 CAL n= 10 Missing Teeth n=7 ESR n=4 CRP n=4 Figure 1. Joanna Briggs Institute flow diagram for studies retrieved through the searching and selection process in accordance with PRISMA guidelines.",Medical Data Analysis,"This systematic review examines the available evidence for an association between periodontal disease and rheumatoid arthritis. Nineteen studies were analyzed, and moderate evidence was found for biochemical markers, while stronger evidence was found for clinical parameters such as tooth loss and clinical attachment levels. The study suggests that common risk factors or pathologic processes may be responsible for the association between these two inflammatory conditions. However, larger population-based studies with well-defined populations and outcomes are needed to confirm this link and consider potential confounding factors. The review highlights the need for more rigorous studies in the future to fully explore the relationship between periodontal disease and rheumatoid arthritis.",Medical Data Analysis,"A Study Pischon (2008) Okada (2011) Kasser (1997) Joseph (2012) Gleissner (1998) Garib (2011) Farah Vakar (2010) DePaula Ishi (2008) Bozkurt (2006) Biyikoglu (2006) Non-RA patient -4.0 Overall Overall -4.0 Overall Z=3.11, P=0.0020 Cc Study Pischon (2008) Joseph (2012) ikoglu (2006) ‘Mercado (2001) Non-perio patient -29.0 Overall Overall -29.0 Overall Z=1.47, P=0.1442 CAL DerSimonian & Laird WMD Weight (C1 95% Random) 9.94% 0.97 (0.55,1.39) | 10.26% -0.10 (-0.14,-0.06) Se aminauz + 10.10% 1.90 (1.61,2.19) _ 10.00% 1.50 (1.12,1.88) H porsazt¢aocean | Fi ws ras ——9.83% 3.49 (3.01,3.97) 9.67% -0.70 (-1.27,-0.13) 0.0 4.0 RA patient = 0.0 40 Heterogeneity Chi squared=5324.17, 10.00% 1.17 (0.43,1.90) ESR DerSimonian & Laird WMD Weight (Cl 95% Random) I 26.45% 1.00 (-1.38,-0.62) ae 24.54% 17.34 (9.59,25.09) -—|-- 23,58% 3.40 (-6.27,13.07) —frssasx 23.46 (17.58,28.74) 09 29.0 Perio patient +—+—- 100.00% 10.68 (-3.56,24.93) 00 29.0 Heterogeneity Chi squared=93.73, P=0.0 Study Garib (2011) Gleissner (1998) Joseph (2012) Kasser (1997) Pischon (2008) Kobayashi (2007) Mercado (2001) Non-RA patient -7.0 Overall Overall -7.0 Overall Z=5.16, p<0.0001 D Study Pischon (2008) Joseph (2012) Biyikoglu (2006) Mercado (2001), Non-perio patient -35.0 Overall Overall -35.0 Overall Z=3.20, P=0.0014 Missing teeth DerSimonian & Laird WMD Weight (Cl 95% Random) ae 17.75% 2.61 (1.73,3.49) --|--- 13.53% 2.20 (0.71,3.69) fe 15.21% 1.04 (-0.24,2.26) me 13.53% 1.60 (0.11,3.09) _—— 7.58% 0.08 (-2.56,2.72) f raon san assay 11.11% 4.90 (3.02,6.78) 0.0 7.0 RA patient = 10.00% 2.38 (1.48,3.29) 00 70 Heterogeneity Chi squared=30.14,P=0.0 cRP DerSimonian & Laird WMD Weight (Ci 95% Random) 29.99% -0.06 (0.08,0.04) 27.57% 1.78 (0.39,3.17) 29.36% -0.16 (-0.85,0.53) —fH1300% 28.89 (23.55,34.23) 0.0 35.0 Perio patient it 35.0 Heterogeneity Chi squared=119.49, P=0.0 100.0% 4.20 (1.63,6.78) Figure 2. Forest plots showing the weighted mean difference (WMD) in (A) clinical attachment level (CAL) between rheumatoid arthritis patients and non-RA controls; (B) tooth loss between RA patients and nonRA controls; (C) erythrocyte sedimentation rate (ESR) between periodontitis-positive patients and periodontitisnegative controls; and (D) C-reactive protein (CRP) between periodontitis-positive patients and periodontitisnegative controls.Concept Proposals for Interplay Between Periodontitis and Rheumatoid Arthritis A. Periodontitis Precedes Rheumatoid B. Common Inflammatory Pathways: Osteoclast Activation and Vascular Damage Arthri Inflammation Periodontal infection & inflammation * Osteoprotegerin (OPG) expression decreased * Receptor Activator of NF-kappa Ligand B (RANKL) expression increased Citrullination of proteins by * TNF-related Apoptosis Inducing Ligand (TRAIL) expression increased endogenous inflammation or exogenous: P gingivalis Results in both vascular damage and increased osteoclast activation Production of autoantibodies to citrullinated proteins C. Two Hit Model of Mutual Exacerbation | Systemic Chronic Cross reactivity with Inflammation Cartilage components . . | . Periodontitis Rheumatoid Arthritis Auto-immunity to self-antigens (1st hit) (2nd hit) Rheumatoid arthritis Inflammatory mediators Figure 3. Concept of how periodontitis and rheumatoid arthritis might interact. (A) A model for how periodontal inflammation could prime an auto-immune response to citrullinated proteins prior to the development of rheumatoid arthritis; (B) a model representing how dysregulation of key inflammatory pathways could underlie both periodontitis and rheumatoid arthritis; and (C) a model demonstrating how both periodontitis and rheumatoid arthritis miaht exacerbate each other.Potentially relevant papers identified by literature search n=127 y Abstracts retrieved for examination n=112 Papers excluded after evaluation of abstract n= 51 y Papers retrieved for detailed examination n=61 Papers excluded after review of full paper v Papers assessed for methodological quality n=29 v Papers included in the systematic review n=19 y Papers included in the quantitative synthesis (meta-analysis) n=12 CAL n= 10 Missing Teeth n=7 ESR n=4 CRP n=4 Figure 1. Joanna Briggs Institute flow diagram for studies retrieved through the searching and selection process in accordance with PRISMA guidelines.",Medical Data Analysis
56,Prediction of Cancer Disease using Machine learning Approach,"Cancer , Deep learning , ML , ANN , SVM , Decision tress.","Cancer has identified a diverse condition of several various subtypes. The timely screening and course of treatment of a cancer form is now a requirement in early cancer research because it supports the medical treatment of patients. Many research teams studied the application of ML and Deep Learning methods in the field of biomedicine and bioinformatics in the classification of people with cancer across high- or low- risk categories. These techniques have therefore been used as a model for the development and treatment of cancer. As, it is important that ML instruments are capable of detecting key features from complex datasets. Many of these methods are widely used for the development of predictive models for predicating a cure for cancer, some of the methods are artificial neural networks (ANNs), support vector machine (SVMs) and decision trees (DTs). While we can understand cancer progression with the use of ML methods, an adequate validity level is needed to take these methods into consideration in clinical practice every day. In this study, the ML & DL approaches used in cancer progression modeling are reviewed. The predictions addressed are mostly linked to specific ML, input, and data samples supervision.","The whole study explains and compares the findings of various machine learning and in-depth learning implemented to cancer prognosis. Specifically, several trends related to those same kinds of machines techniques to be used, the kinds of training data to be incorporated, the kind of endpoint forecasts to be made, sorts of cancers being investigated, and the overall performance of cancer prediction or outcome methods have been identified. While the ANNs are common, it is clear that a broader variety of alternative learning approaches is also used to predict at least three different cancer types. ANNs continue to be prevalent. Furthermore, it is clear that machine training methods typically increase the efficiency or predictable accuracy of most pronostics, in particular when matched with conventional statistical or expert systems. Although most researches are usually excellently-designed and fairly validated, more focus is quite desirable for the planning and implementation of experiments, in particular with regard to quantity and quality of biological data. Improving the experimental design and the biological validation of several device classification systems would undoubtedly increase the general Quality, replicability and reproductivity of many systems. In total, we believe that the usage of the devices education & deep learning classificatory will probably be quite common in many clinical and hospital settings if the quality of study continues to improve. The assimilation of multifaceted heterogeneous data, which can offer a promising tool for cancer infection and foresee the disease, also demonstrates the incorporation in the application of different analytical and classification methods. In future, by using the proposed framework, we would like to use other state of the art machine learning algorithms and extraction methods to allow more intensive comparative analysis.","Prediction of Cancer Disease using Machine learning ApproachCancer , Deep learning , ML , ANN , SVM , Decision tress.Cancer has identified a diverse condition of several various subtypes. The timely screening and course of treatment of a cancer form is now a requirement in early cancer research because it supports the medical treatment of patients. Many research teams studied the application of ML and Deep Learning methods in the field of biomedicine and bioinformatics in the classification of people with cancer across high- or low- risk categories. These techniques have therefore been used as a model for the development and treatment of cancer. As, it is important that ML instruments are capable of detecting key features from complex datasets. Many of these methods are widely used for the development of predictive models for predicating a cure for cancer, some of the methods are artificial neural networks (ANNs), support vector machine (SVMs) and decision trees (DTs). While we can understand cancer progression with the use of ML methods, an adequate validity level is needed to take these methods into consideration in clinical practice every day. In this study, the ML & DL approaches used in cancer progression modeling are reviewed. The predictions addressed are mostly linked to specific ML, input, and data samples supervision.The whole study explains and compares the findings of various machine learning and in-depth learning implemented to cancer prognosis. Specifically, several trends related to those same kinds of machines techniques to be used, the kinds of training data to be incorporated, the kind of endpoint forecasts to be made, sorts of cancers being investigated, and the overall performance of cancer prediction or outcome methods have been identified. While the ANNs are common, it is clear that a broader variety of alternative learning approaches is also used to predict at least three different cancer types. ANNs continue to be prevalent. Furthermore, it is clear that machine training methods typically increase the efficiency or predictable accuracy of most pronostics, in particular when matched with conventional statistical or expert systems. Although most researches are usually excellently-designed and fairly validated, more focus is quite desirable for the planning and implementation of experiments, in particular with regard to quantity and quality of biological data. Improving the experimental design and the biological validation of several device classification systems would undoubtedly increase the general Quality, replicability and reproductivity of many systems. In total, we believe that the usage of the devices education & deep learning classificatory will probably be quite common in many clinical and hospital settings if the quality of study continues to improve. The assimilation of multifaceted heterogeneous data, which can offer a promising tool for cancer infection and foresee the disease, also demonstrates the incorporation in the application of different analytical and classification methods. In future, by using the proposed framework, we would like to use other state of the art machine learning algorithms and extraction methods to allow more intensive comparative analysis.",Deep Learning and Machine Learning,"This study reviews the use of machine learning (ML) and deep learning (DL) methods in the field of cancer prognosis modeling. The study focuses on the development of predictive models for cancer treatment, and the use of ML and DL methods in detecting key features from complex datasets. The review compares the findings of various machine learning and deep learning techniques that have been implemented in cancer prognosis. The study identifies trends in the types of machine learning approaches used, the types of training data incorporated, the types of endpoint forecasts made, and the overall performance of cancer prediction or outcome methods. The study concludes that the use of ML and DL classification tools will likely become more common in clinical and hospital settings if the quality of research continues to improve. The authors suggest that improving the experimental design and biological validation of machine learning classification systems will increase the general quality, replicability, and reproductivity of these systems. The study proposes using other state-of-the-art machine learning algorithms and extraction methods to allow for more intensive comparative analysis in future research.",Deep Learning and Machine Learning,,Deep Learning and Machine Learning
57,Prediction of Heart Disease at early stage using Data Mining and Big Data Analytics: A Survey,Data Mining; big data; CVD; risk factors; accuracy; prediction rate; heart disease; DMT and data sets,"In this paper, the various technologies of data mining (DM) models for forecast of heart disease are discussed. Data mining plays an important role in building an intelligent model for medical systems to detect heart disease (HD) using data sets of the patients, which involves risk factor associated with heart disease. Medical practitioners can help the patients by predicting the heart disease before occurring. The large data available from medical diagnosis is analyzed by using data mining tools and useful information known as knowledge is extracted. Mining is a method of exploring massive sets of data to take out patterns which are hidden and previously unknown relationships and knowledge detection to help the better understanding of medical data to prevent heart disease. There are many DM techniques available namely Classification techniques involving Naïve bayes (NB), Decision tree (DT), Neural network (NN), Genetic algorithm (GA), Artificial intelligence (AI) and Clustering algorithms like K-NN, and Support vector machine (SVM). Several studies have been carried out for developing prediction model using individual technique and also by combining two or more techniques. This paper provides a quick and easy review and understanding of available prediction models using data mining from 2004 to 2016. The comparison shows the accuracy level of each model given by different researchers.","In this paper, a survey conducted from 2004 to 2015 gives the idea of different models available and the different data mining techniques used. The accuracy obtained with these models is also mentioned. It is observed that all the techniques available have not used big data analytics. Use of big data analytics along with data mining will give promising results to get the best accuracy in designing the prediction model. The Main objective is to identify the key patterns and features from the medical data of the patient by combining data mining techniques along with big data analytics to predict the heart disease before it causes to help the medical practitioners. The other objective will be to reduce the data sets and increase the accuracy of prediction model.","Prediction of Heart Disease at early stage using Data Mining and Big Data Analytics: A SurveyData Mining; big data; CVD; risk factors; accuracy; prediction rate; heart disease; DMT and data setsIn this paper, the various technologies of data mining (DM) models for forecast of heart disease are discussed. Data mining plays an important role in building an intelligent model for medical systems to detect heart disease (HD) using data sets of the patients, which involves risk factor associated with heart disease. Medical practitioners can help the patients by predicting the heart disease before occurring. The large data available from medical diagnosis is analyzed by using data mining tools and useful information known as knowledge is extracted. Mining is a method of exploring massive sets of data to take out patterns which are hidden and previously unknown relationships and knowledge detection to help the better understanding of medical data to prevent heart disease. There are many DM techniques available namely Classification techniques involving Naïve bayes (NB), Decision tree (DT), Neural network (NN), Genetic algorithm (GA), Artificial intelligence (AI) and Clustering algorithms like K-NN, and Support vector machine (SVM). Several studies have been carried out for developing prediction model using individual technique and also by combining two or more techniques. This paper provides a quick and easy review and understanding of available prediction models using data mining from 2004 to 2016. The comparison shows the accuracy level of each model given by different researchers.In this paper, a survey conducted from 2004 to 2015 gives the idea of different models available and the different data mining techniques used. The accuracy obtained with these models is also mentioned. It is observed that all the techniques available have not used big data analytics. Use of big data analytics along with data mining will give promising results to get the best accuracy in designing the prediction model. The Main objective is to identify the key patterns and features from the medical data of the patient by combining data mining techniques along with big data analytics to predict the heart disease before it causes to help the medical practitioners. The other objective will be to reduce the data sets and increase the accuracy of prediction model.",Deep Learning and Machine Learning,"This paper discusses the importance of data mining in building intelligent models for detecting heart disease. The paper explores various data mining techniques, including classification techniques like Naïve Bayes, decision tree, neural network, genetic algorithm, artificial intelligence, and clustering algorithms like K-NN and support vector machine. The paper also provides a review of available prediction models using data mining from 2004 to 2016, and a comparison of their accuracy levels. The study concludes that combining data mining techniques with big data analytics can help identify key patterns and features in patient medical data to predict heart disease and reduce data sets while increasing prediction model accuracy.",Medical Data Analysis,,Medical Data Analysis
58,Prognostic role of blood KL-6 in rheumatoid arthritis–associated interstitial lung disease,"Rheumatoid arthritis, interstitial lung disease, prognosis, biomarkers, Krebs von den Lungen-6 (KL-6), UIP pattern.","Rheumatoid arthritis–associated interstitial lung disease (RA-ILD) has a variable clinical course for which predicting prognosis is difficult. However, the role of blood biomarkers in RA-ILD is ill-defined. The aim of this study was to investigate the prognostic value of Krebs von den Lungen-6 (KL-6) levels in RA-ILD patients. The medical records of 84 patients with RA-ILD were retrospectively reviewed. Plasma KL-6 levels were measured by Nanopia KL-6 assay (SEKISUI MEDICAL, Tokyo), using latex-enhanced immunoturbidimetric assay. The median follow-up period was 61 months. Mean age was 61.4 years, 45.2% were men, 44.0% were ever-smokers, and 35.7% showed a usual interstitial pneumonia (UIP) pattern on high-resolution computed tomography. The median KL-6 level at baseline was 741.2 U/ mL (interquartile range, 439.7–1308.9 U/mL). On multivariate logistic regression analysis, a high KL-6 level (640 U/mL) was an independently associated with a UIP pattern (odds ratio [OR], 5.173; P = 0.005) with old age (OR, 1.104, P = 0.005). On multivariate Cox analysis, a high KL-6 level ( 685 U/mL) was an independent prognostic factor for the mortality (hazard ratio [HR], 2.984; P = 0.016) with a older age (HR, 1.061; P = 0.030), male sex (HR, 3.610; P = 0.001), lower forced vital capacity (HR, 0.957; P = 0.002), and a UIP pattern (HR, 4.034; P = 0.002). Our results suggest that high KL-6 levels might be useful as a biomarker for the presence of a UIP pattern and prognosis in patients with RA-ILD.","In conclusion, our results suggest that high KL-6 levels might be useful as a biomarker for the presence of a UIP pattern and prognosis in patients with RA-ILD. These findings warrant validation in further larger-scale studies.","Prognostic role of blood KL-6 in rheumatoid arthritis–associated interstitial lung diseaseRheumatoid arthritis, interstitial lung disease, prognosis, biomarkers, Krebs von den Lungen-6 (KL-6), UIP pattern.Rheumatoid arthritis–associated interstitial lung disease (RA-ILD) has a variable clinical course for which predicting prognosis is difficult. However, the role of blood biomarkers in RA-ILD is ill-defined. The aim of this study was to investigate the prognostic value of Krebs von den Lungen-6 (KL-6) levels in RA-ILD patients. The medical records of 84 patients with RA-ILD were retrospectively reviewed. Plasma KL-6 levels were measured by Nanopia KL-6 assay (SEKISUI MEDICAL, Tokyo), using latex-enhanced immunoturbidimetric assay. The median follow-up period was 61 months. Mean age was 61.4 years, 45.2% were men, 44.0% were ever-smokers, and 35.7% showed a usual interstitial pneumonia (UIP) pattern on high-resolution computed tomography. The median KL-6 level at baseline was 741.2 U/ mL (interquartile range, 439.7–1308.9 U/mL). On multivariate logistic regression analysis, a high KL-6 level (640 U/mL) was an independently associated with a UIP pattern (odds ratio [OR], 5.173; P = 0.005) with old age (OR, 1.104, P = 0.005). On multivariate Cox analysis, a high KL-6 level ( 685 U/mL) was an independent prognostic factor for the mortality (hazard ratio [HR], 2.984; P = 0.016) with a older age (HR, 1.061; P = 0.030), male sex (HR, 3.610; P = 0.001), lower forced vital capacity (HR, 0.957; P = 0.002), and a UIP pattern (HR, 4.034; P = 0.002). Our results suggest that high KL-6 levels might be useful as a biomarker for the presence of a UIP pattern and prognosis in patients with RA-ILD.In conclusion, our results suggest that high KL-6 levels might be useful as a biomarker for the presence of a UIP pattern and prognosis in patients with RA-ILD. These findings warrant validation in further larger-scale studies.(A) Total patients (B) UIP group 1001. 100: = 80 Moots KL-6 < 685 U/ml = soy po 45 KL-6 < 780 U/ml 2 totems. dete ned 2 beneene penser 5 60 5 60 Meveensctinsebesee a KL-6 = 685 U/ml a = = @ 40 g 40 KL-6 > 780 U/ml G o “ 2 2 P=0.001 0 0. 0 50 100 150 0 50 100 150 Follow up duration (months) Follow up duration (months) Fig 1. Comparison of survival curves according to KL-6 levels. (A) Total patients, (B) the UIP group. UIP, usual interstitial pneumonia.150 r=-0.460 P<0.001 FVC, % predicted DLco, % predicted 0 4000 2000 3000 4000 0 1000 2000 3000 © 4000 KL-6 (U/ml) KL-6 (U/ml) 150: r=-0.381 800 r=-0.210 3 P=0.013 P=0.060 3 5 2 100 3 3 2 io a 8 2 ; 50 g 3 z 0 0 1000 2000 3000 4000 0 1000 2000 3000 4000 KL-6 (U/ml) KL-6 (U/ml) Fig 2. Correlation between KL-6 levels and lung function or exercise capacity. FVC, forced vital capacity; DLco, diffusing capacity for carbon monoxide; TLC, total lung capacity; 6MWD, 6-minute walk test distance.",Medical Data Analysis,"This study aimed to investigate the prognostic value of Krebs von den Lungen-6 (KL-6) levels in patients with rheumatoid arthritis-associated interstitial lung disease (RA-ILD). The study retrospectively reviewed the medical records of 84 RA-ILD patients and measured plasma KL-6 levels. The study found that high KL-6 levels were independently associated with a usual interstitial pneumonia (UIP) pattern and were an independent prognostic factor for mortality in RA-ILD patients, along with age, sex, and lung function. The study suggests that KL-6 levels might be useful as a biomarker for the presence of a UIP pattern and prognosis in patients with RA-ILD and calls for further validation in larger-scale studies.",Medical Data Analysis,"(A) Total patients (B) UIP group 1001. 100: = 80 Moots KL-6 < 685 U/ml = soy po 45 KL-6 < 780 U/ml 2 totems. dete ned 2 beneene penser 5 60 5 60 Meveensctinsebesee a KL-6 = 685 U/ml a = = @ 40 g 40 KL-6 > 780 U/ml G o “ 2 2 P=0.001 0 0. 0 50 100 150 0 50 100 150 Follow up duration (months) Follow up duration (months) Fig 1. Comparison of survival curves according to KL-6 levels. (A) Total patients, (B) the UIP group. UIP, usual interstitial pneumonia.150 r=-0.460 P<0.001 FVC, % predicted DLco, % predicted 0 4000 2000 3000 4000 0 1000 2000 3000 © 4000 KL-6 (U/ml) KL-6 (U/ml) 150: r=-0.381 800 r=-0.210 3 P=0.013 P=0.060 3 5 2 100 3 3 2 io a 8 2 ; 50 g 3 z 0 0 1000 2000 3000 4000 0 1000 2000 3000 4000 KL-6 (U/ml) KL-6 (U/ml) Fig 2. Correlation between KL-6 levels and lung function or exercise capacity. FVC, forced vital capacity; DLco, diffusing capacity for carbon monoxide; TLC, total lung capacity; 6MWD, 6-minute walk test distance.",Medical Data Analysis
59,CONTEXT BASED TEXT-GENERATION USING LSTM NETWORKS," Natural language generation · LSTM networks · Sequence models · language models
","Long short-term memory(LSTM) units on sequence-based models are being used in translation,
question-answering systems, classification tasks due to their capability of learning long-term dependencies. In Natural language generation, LSTM networks are providing impressive results on text
generation models by learning language models with grammatically stable syntaxes. But the downside
is that the network does not learn about the context. The network only learns the input-output function
and generates text given a set of input words irrespective of pragmatics. As the model is trained
without any such context, there is no semantic consistency among the generated sentences.
The proposed model is trained to generate text for a given set of input words along with a context
vector. A context vector is similar to a paragraph vector that grasps the semantic meaning(context)
of the sentence. Several methods of extracting the context vectors are proposed in this work. While
training a language model, in addition to the input-output sequences, context vectors are also trained
along with the inputs. Due to this structure, the model learns the relation among the input words,
context vector and the target word. Given a set of context terms, a well trained model will generate
text around the provided context.
Based on the nature of computing context vectors, the model has been tried out with two variations
(word importance and word clustering). In the word clustering method, the suitable embeddings
among various domains are also explored. The results are evaluated based on the semantic closeness
of the generated text to the given context.","The paper discussed about the need for applying contextual information to train language models for text generation
tasks. LSTM networks have been chosen as the language model for the considered use-case. The main focus of the
work is on finding out the best way of extracting context from the sentences, so as to train a text generation model along
with the contexts aiming for a better language model. Multiple methods have been attempted for context extraction and
among those methods, contexts extracted from word clusters in word vector spaces worked better. Also, it has been
clear from the results that the word embeddings generated from the same dataset provided better context vectors. The
evaluation method used cosine similarity measures to calculate the semantic closeness between the generated sentences
and the provided context. The evaluation results also emphasized that context based models perform better compared to
base models. The evaluation method also provided a feedback to the training process to know when the model overfits
so as to stop training the model.","CONTEXT BASED TEXT-GENERATION USING LSTM NETWORKS Natural language generation · LSTM networks · Sequence models · language models
Long short-term memory(LSTM) units on sequence-based models are being used in translation,
question-answering systems, classification tasks due to their capability of learning long-term dependencies. In Natural language generation, LSTM networks are providing impressive results on text
generation models by learning language models with grammatically stable syntaxes. But the downside
is that the network does not learn about the context. The network only learns the input-output function
and generates text given a set of input words irrespective of pragmatics. As the model is trained
without any such context, there is no semantic consistency among the generated sentences.
The proposed model is trained to generate text for a given set of input words along with a context
vector. A context vector is similar to a paragraph vector that grasps the semantic meaning(context)
of the sentence. Several methods of extracting the context vectors are proposed in this work. While
training a language model, in addition to the input-output sequences, context vectors are also trained
along with the inputs. Due to this structure, the model learns the relation among the input words,
context vector and the target word. Given a set of context terms, a well trained model will generate
text around the provided context.
Based on the nature of computing context vectors, the model has been tried out with two variations
(word importance and word clustering). In the word clustering method, the suitable embeddings
among various domains are also explored. The results are evaluated based on the semantic closeness
of the generated text to the given context.The paper discussed about the need for applying contextual information to train language models for text generation
tasks. LSTM networks have been chosen as the language model for the considered use-case. The main focus of the
work is on finding out the best way of extracting context from the sentences, so as to train a text generation model along
with the contexts aiming for a better language model. Multiple methods have been attempted for context extraction and
among those methods, contexts extracted from word clusters in word vector spaces worked better. Also, it has been
clear from the results that the word embeddings generated from the same dataset provided better context vectors. The
evaluation method used cosine similarity measures to calculate the semantic closeness between the generated sentences
and the provided context. The evaluation results also emphasized that context based models perform better compared to
base models. The evaluation method also provided a feedback to the training process to know when the model overfits
so as to stop training the model.",Text generation,"This paper proposes a method for improving the performance of text generation models by incorporating contextual information during training. The authors explore different methods for extracting context vectors and find that those extracted from word clusters in word vector spaces work best. They also evaluate the performance of their model using cosine similarity measures and find that context-based models perform better than base models. Overall, the authors show that incorporating context during training can improve the semantic consistency of generated text.",Natural Language Processing,,Deep Learning and Machine Learning
60,"A Text Generation and Prediction System: 
Pre-training on New Corpora Using BERT and 
GPT-2","text generation; OpenAI GPT-2;
BERT","Using a given starting word to make a sentence or filling 
in sentences is an important direction of natural language 
processing. From one aspect, it reflects whether the machine can 
have human thinking and creativity. We train the machine for 
specific tasks and then use it in natural language processing, which 
will help solve some sentence generation problems, especially for 
application scenarios such as summary generation, machine 
translation, and automatic question answering. The OpenAI GPT2 and BERT models are currently widely used language models 
for text generation and prediction. There have been many 
experiments to verify the outstanding performance of these two 
models in the field of text generation. This paper will use two new 
corpora to train OpenAI GPT-2 model, used to generate long 
sentences and articles, and finally perform a comparative analysis. 
At the same time, we will use the BERT model to complete the task 
of predicting intermediate words based on the context.","We have pre-trained the model by using different corpora, 
and used the trained model to complete the long sentence 
generation and masked word generation prediction task. The 
former mainly generates sentences by looping down from the 
start word, and the latter is based on the surroundings word to 
generate intermediate words. Through the experimental results, 
we can know that the GPT-2 and BERT models perform very 
well in text generation tasks. However, there are still some 
shortcomings, such as readability, corpora data, and training 
methods, which may cause generated sentences to be repeated, 
etc. In the future, we will try to find some ways to solve this 
defect.","A Text Generation and Prediction System: 
Pre-training on New Corpora Using BERT and 
GPT-2text generation; OpenAI GPT-2;
BERTUsing a given starting word to make a sentence or filling 
in sentences is an important direction of natural language 
processing. From one aspect, it reflects whether the machine can 
have human thinking and creativity. We train the machine for 
specific tasks and then use it in natural language processing, which 
will help solve some sentence generation problems, especially for 
application scenarios such as summary generation, machine 
translation, and automatic question answering. The OpenAI GPT2 and BERT models are currently widely used language models 
for text generation and prediction. There have been many 
experiments to verify the outstanding performance of these two 
models in the field of text generation. This paper will use two new 
corpora to train OpenAI GPT-2 model, used to generate long 
sentences and articles, and finally perform a comparative analysis. 
At the same time, we will use the BERT model to complete the task 
of predicting intermediate words based on the context.We have pre-trained the model by using different corpora, 
and used the trained model to complete the long sentence 
generation and masked word generation prediction task. The 
former mainly generates sentences by looping down from the 
start word, and the latter is based on the surroundings word to 
generate intermediate words. Through the experimental results, 
we can know that the GPT-2 and BERT models perform very 
well in text generation tasks. However, there are still some 
shortcomings, such as readability, corpora data, and training 
methods, which may cause generated sentences to be repeated, 
etc. In the future, we will try to find some ways to solve this 
defect.",Text generation,"This paper discusses the importance of using given starting words to generate sentences and filling in sentences in natural language processing tasks. The paper focuses on using OpenAI GPT-2 and BERT models for text generation and prediction, and presents experiments using two new corpora to train the GPT-2 model for generating long sentences and articles, and BERT model for predicting intermediate words based on context. The paper concludes with a comparative analysis of the performance of the two models in text generation.",Natural Language Processing,,Deep Learning and Machine Learning
61,"Survey on Automatic Text Summarization and Transformer
Models Applicability","Natural language generation;
Neural networks; Supervised learning.","This survey talks about Automatic Text Summarization. Information explosion, the problem caused by the rapid growth of the internet, increased more and more necessity of powerful summarizers.
This article briefly reviews different methods and evaluation metrics. The main attention is on the applications of the latest trends,
neural network-based, and pre-trained transformer language models. Pre-trained language models now are ruling the NLP field, as
one of the main down-stream tasks, Automatic Text Summarization
is quite an interdisciplinary task and requires more advanced techniques. But there is a limitation of input and context length results
in that the whole article cannot be encoded completely. Motivated
by the application of recurrent mechanism in Transformer-XL, we
build an abstractive summarizer for long text and evaluate how
well it performs on dataset CNN/Daily Mail. The model is under
general sequence to sequence structure with a recurrent encoder
and stacked Transformer decoder. The obtained ROUGE scores tell
that the performance is good as expected","In this survey, we reviewed the task of Automatic Text Summarization. We introduced the background including its history from its
beginning to the current state, application, and its taxonomy. The
current famous classification is Extractive and Abstractive Summarization, since the neural network improved the generative ability.
After Transformer published, it became the new trend in NLP field.
More and more researchers turn their attention to transformer.
This resulted in the powerful pre-trained language models like GPT,
BERT, XLNet, etc. Pre-trained models nowadays are becoming new
trend in NLP field. Many works using pre-trained language models
on Automatic Text Summarization and proved the huge compact.
During our own work, there are two issues we marked as the
future works, 1) decoder alternatives, 2) batch size implementation.
Another future work is to implement the use of batch size. As
mentioned above in Implementation part, since the length varies
across the whole dataset, this results in various numbers of segments. Using batch size requires the same length of batch items,
that means each batch item should contain the same number of
segments.
In this paper, we also proposed a model based on XLNet for the
first trial to explore the application of XLNet on summarization.
Base on the experience from previous works using transformerbased language models, the models using pre-trained language
model can effectively improve the performance on the task of Automatic Text Summarization. Also, we can see that there is still huge
space for the application of XLNet on summarization. Also, the
decoder can be advanced by replacing other more powerful ones
like GPT, GPT2, etc. than general Transformer Decoder.
","Survey on Automatic Text Summarization and Transformer
Models ApplicabilityNatural language generation;
Neural networks; Supervised learning.This survey talks about Automatic Text Summarization. Information explosion, the problem caused by the rapid growth of the internet, increased more and more necessity of powerful summarizers.
This article briefly reviews different methods and evaluation metrics. The main attention is on the applications of the latest trends,
neural network-based, and pre-trained transformer language models. Pre-trained language models now are ruling the NLP field, as
one of the main down-stream tasks, Automatic Text Summarization
is quite an interdisciplinary task and requires more advanced techniques. But there is a limitation of input and context length results
in that the whole article cannot be encoded completely. Motivated
by the application of recurrent mechanism in Transformer-XL, we
build an abstractive summarizer for long text and evaluate how
well it performs on dataset CNN/Daily Mail. The model is under
general sequence to sequence structure with a recurrent encoder
and stacked Transformer decoder. The obtained ROUGE scores tell
that the performance is good as expectedIn this survey, we reviewed the task of Automatic Text Summarization. We introduced the background including its history from its
beginning to the current state, application, and its taxonomy. The
current famous classification is Extractive and Abstractive Summarization, since the neural network improved the generative ability.
After Transformer published, it became the new trend in NLP field.
More and more researchers turn their attention to transformer.
This resulted in the powerful pre-trained language models like GPT,
BERT, XLNet, etc. Pre-trained models nowadays are becoming new
trend in NLP field. Many works using pre-trained language models
on Automatic Text Summarization and proved the huge compact.
During our own work, there are two issues we marked as the
future works, 1) decoder alternatives, 2) batch size implementation.
Another future work is to implement the use of batch size. As
mentioned above in Implementation part, since the length varies
across the whole dataset, this results in various numbers of segments. Using batch size requires the same length of batch items,
that means each batch item should contain the same number of
segments.
In this paper, we also proposed a model based on XLNet for the
first trial to explore the application of XLNet on summarization.
Base on the experience from previous works using transformerbased language models, the models using pre-trained language
model can effectively improve the performance on the task of Automatic Text Summarization. Also, we can see that there is still huge
space for the application of XLNet on summarization. Also, the
decoder can be advanced by replacing other more powerful ones
like GPT, GPT2, etc. than general Transformer Decoder.
",Natural Language Processing,"This survey discusses the importance of Automatic Text Summarization in dealing with the problem of information overload on the internet. It reviews different methods and evaluation metrics with a focus on the latest trends of neural network-based and pre-trained transformer language models. The article introduces the background of Automatic Text Summarization, its classification into extractive and abstractive summarization, and the current trend of pre-trained language models like GPT, BERT, and XLNet. The paper proposes a model based on XLNet for summarization and identifies future works such as decoder alternatives and batch size implementation. The survey concludes that pre-trained language models effectively improve the performance of Automatic Text Summarization, leaving room for more applications of XLNet in the field.",Natural Language Processing,,Deep Learning and Machine Learning
62,"Bilingual Automatic Text Summarization Using 
Unsupervised Deep Learning"," Automatic Summarization, Deep Learning 
RBM, Bilingual, dataset, unsupervised.","In the world of digitization, the growth of big 
data is raising at large scale with usage of high 
performance computing. The huge data in English and 
Hindi is available on internet and social media which need 
to be extracted or summarized in user required form. 
In this paper we are presenting Bilingual (Hindi and 
English) unsupervised automatic text summarization 
using deep learning. which is an important research area 
with in Natural Language Processing, Machine Learning 
and data mining, to improve result accuracy, we are using 
restricted Boltzmann machine to generate a shorter 
version of original document without losing its important 
information. In this algorithm we are exploring the 
features to improve the relevance of sentences in the 
dataset. ","Much Research has been done in field of text 
summarization and most of them are using supervised 
approach. We have developed an automatic 
summarizer, which works on two languages Hindi and 
English using unsupervised deep learning approach. 
Here we are extracting eleven features from each 
sentence of document and generating the feature matrix. 
The generated feature matrix is then passed through 
Restricted Boltzmann Machine to enhance importance 
of relevant sentences. Open source technologies are 
used to implement the proposed algorithm. The output 
result of proposed algorithm is almost 85% accurate 
and also preserves the meaning of summarized 
document. In future, enhancement can be done by adding more features to get more relevant sentences and 
meaningful summary and further we will be applying 
the concept to generate multiple documents 
summarization. ","Bilingual Automatic Text Summarization Using 
Unsupervised Deep Learning Automatic Summarization, Deep Learning 
RBM, Bilingual, dataset, unsupervised.In the world of digitization, the growth of big 
data is raising at large scale with usage of high 
performance computing. The huge data in English and 
Hindi is available on internet and social media which need 
to be extracted or summarized in user required form. 
In this paper we are presenting Bilingual (Hindi and 
English) unsupervised automatic text summarization 
using deep learning. which is an important research area 
with in Natural Language Processing, Machine Learning 
and data mining, to improve result accuracy, we are using 
restricted Boltzmann machine to generate a shorter 
version of original document without losing its important 
information. In this algorithm we are exploring the 
features to improve the relevance of sentences in the 
dataset. Much Research has been done in field of text 
summarization and most of them are using supervised 
approach. We have developed an automatic 
summarizer, which works on two languages Hindi and 
English using unsupervised deep learning approach. 
Here we are extracting eleven features from each 
sentence of document and generating the feature matrix. 
The generated feature matrix is then passed through 
Restricted Boltzmann Machine to enhance importance 
of relevant sentences. Open source technologies are 
used to implement the proposed algorithm. The output 
result of proposed algorithm is almost 85% accurate 
and also preserves the meaning of summarized 
document. In future, enhancement can be done by adding more features to get more relevant sentences and 
meaningful summary and further we will be applying 
the concept to generate multiple documents 
summarization. Erodes ar",Natural Language Processing,"This paper discusses the need for bilingual (Hindi and English) unsupervised automatic text summarization using deep learning, as there is an increase in the amount of big data available on the internet and social media. The proposed algorithm uses Restricted Boltzmann Machine to generate a shorter version of the original document without losing important information. Eleven features are extracted from each sentence of the document to enhance the relevance of the summary. The proposed algorithm has an 85% accuracy rate and preserves the meaning of the original document. Future enhancements can be made by adding more features for a more relevant and meaningful summary.",Natural Language Processing,Erodes ar,Deep Learning and Machine Learning
63,"The Impact of Local Attention in LSTM for 
Abstractive Text Summarization","abstractive, local attention, LSTM, text 
summarization ","An attentional mechanism is very important to 
enhance a neural machine translation (NMT). There are two 
classes of attentions: global and local attentions. This paper 
focuses on comparing the impact of the local attention in Long 
Short-Term Memory (LSTM) model to generate an abstractive 
text summarization (ATS). Developing a model using a dataset 
of Amazon Fine Food Reviews and evaluating it using dataset 
of GloVe shows that the global attention-based model produces 
better ROUGE-1, where it generates more words contained in 
the actual summary. But, the local attention-based gives higher 
ROUGE-2, where it generates more pairs of words contained 
in the actual summary, since the mechanism of local attention 
considers the subset of input words instead of the whole input 
words.","The global attention-based model produces better 
ROUGE-1, where it generates more words contained in the 
actual summary. But, the local attention-based gives higher 
ROUGE-2, where it generates more pairs of words 
contained in the actual summary, since the mechanism of 
local attention considers the subset of input words instead of 
the whole input words. Since the dataset is written using 
informal words, it contains a lot of symbols and unknown 
phrases those are not listed in the word embedding dataset. 
Therefore, the ROUGE score is not higher than the score 
from usual English text model. Resetting all parameters may 
give higher scores for both models. Some methods can be 
developed to improve the performance of both models, such 
us changing the dataset into any other containing article text 
instead of review text, rebuilding the model using more 
optimal parameters, or handling the OOV in data preprocessing. ","The Impact of Local Attention in LSTM for 
Abstractive Text Summarizationabstractive, local attention, LSTM, text 
summarization An attentional mechanism is very important to 
enhance a neural machine translation (NMT). There are two 
classes of attentions: global and local attentions. This paper 
focuses on comparing the impact of the local attention in Long 
Short-Term Memory (LSTM) model to generate an abstractive 
text summarization (ATS). Developing a model using a dataset 
of Amazon Fine Food Reviews and evaluating it using dataset 
of GloVe shows that the global attention-based model produces 
better ROUGE-1, where it generates more words contained in 
the actual summary. But, the local attention-based gives higher 
ROUGE-2, where it generates more pairs of words contained 
in the actual summary, since the mechanism of local attention 
considers the subset of input words instead of the whole input 
words.The global attention-based model produces better 
ROUGE-1, where it generates more words contained in the 
actual summary. But, the local attention-based gives higher 
ROUGE-2, where it generates more pairs of words 
contained in the actual summary, since the mechanism of 
local attention considers the subset of input words instead of 
the whole input words. Since the dataset is written using 
informal words, it contains a lot of symbols and unknown 
phrases those are not listed in the word embedding dataset. 
Therefore, the ROUGE score is not higher than the score 
from usual English text model. Resetting all parameters may 
give higher scores for both models. Some methods can be 
developed to improve the performance of both models, such 
us changing the dataset into any other containing article text 
instead of review text, rebuilding the model using more 
optimal parameters, or handling the OOV in data preprocessing. Input Data 1 Sentence Segmentation y Tokenization 1 ‘Stop Word Removal i POS Tagging | PreProcessed Data",Natural Language Processing,"This paper compares global and local attention mechanisms in LSTM models for abstractive text summarization using a dataset of Amazon reviews. The global attention-based model produces more words in the summary, while the local attention-based model generates more word pairs. However, since the dataset contains informal words and unknown phrases, ROUGE scores are not high. Resetting parameters or using other datasets could improve performance.",Natural Language Processing,Input Data 1 Sentence Segmentation y Tokenization 1 ‘Stop Word Removal i POS Tagging | PreProcessed Data,Deep Learning and Machine Learning
64,"Enhancements of Attention-Based Bidirectional
LSTM for Hybrid Automatic Text Summarization
","Natural language processing (NLP), automatic text summarization (ATS), sequenceto-sequence (Seq2Seq) model, attention mechanism, bidirectional LSTM (Bi-LSTM), pointer network,
coverage mechanism, mixed learning objective (MLO) function."," The automatic generation of a text summary is a task of generating a short summary for a
relatively long text document by capturing its key information. In the past, supervised statistical machine
learning was widely used for the Automatic Text Summarization (ATS) task, but due to its high dependence
on the quality of text features, the generated summaries lack accuracy and coherence, while the computational
power involved, and performance achieved, could not easily meet the current needs. This paper proposes
four novel ATS models with a Sequence-to-Sequence (Seq2Seq) structure, utilizing an attention-based
bidirectional Long Short-Term Memory (LSTM), with added enhancements for increasing the correlation
between the generated text summary and the source text, and solving the problem of out-of-vocabulary
(OOV) words, suppressing the repeated words, and preventing the spread of cumulative errors in generated
text summaries. Experiments conducted on two public datasets confirmed that the proposed ATS models
achieve indeed better performance than the baselines and some of the state-of-the-art models considered.
","This paper has put forward enhancements to the structure of the attention-based bi-directional LSTM model
(‘Bi-LSTM + Attention’) and the attention-based sequence
model (‘Seq2Seq + Attention’) in order to improve the Automatic Text Summarization (ATS). Firstly, a novel enhanced
semantic network (ESN) model has been proposed, which
works out the semantic similarity between the encoder and
decoder, and maximizes the semantic relevance during training, which increases the probability of generating more accurate text summaries, thus also improving the correlation with
the source text. Secondly, aiming at solving the problem
of unregistered words, a novel ‘DA-PN’ model has been
proposed, which utilizes decoder attention (DA) based on
a pointer network (PN). In addition, simultaneous attention
is paid to both the encoder and decoder, resulting in more accurate text summaries. Thirdly, it has been proposed
to combine the elaborated ‘DA-PN’ model with a coverage mechanism integrating multi-attention. In the resultant
‘DA-PN + Cover’ model, by using the attention distribution
of the encoder and decoder in all the previous time steps,
the attention of the current time step is affected in a positive
way, leading to finding more accurate words for inclusion
in the text summaries by avoiding repeated words. Lastly,
in order to prevent the spread of cumulative errors in generated text summaries, it has been proposed to add a mixed
learning objective (MLO) function [12] to the ‘DA-PN +
Cover’ model. The resultant ‘DA-PN + Cover + MLO’
model is the best performing one among the ATS models
proposed in this paper. It considers the evaluation as a part of
the model iteration along with a global reward, thus further
increasing the readability of generated text summaries.
The performance of the four proposed ATS models was
compared to that of two baselines, ‘Bi-LSTM + Attention’
and ‘Seq2Seq + Attention’, and seven state-of-the-art models, using short-text and long-text corpora. The obtained
experimental results demonstrated the superiority of the elaborated ATS models compared to the baselines and some of
the state-of-the-art models. By using the blended learning
of MLO, the best performing proposed model, ‘DA-PN +
Cover + MLO’, opens up a prospective to improve further the
accuracy of automatically generated text summaries by optimization of the evaluation indexes, which could effectively
solve the problem of insufficient attention at the decoder. This
research direction will be explored in the future.","Enhancements of Attention-Based Bidirectional
LSTM for Hybrid Automatic Text Summarization
Natural language processing (NLP), automatic text summarization (ATS), sequenceto-sequence (Seq2Seq) model, attention mechanism, bidirectional LSTM (Bi-LSTM), pointer network,
coverage mechanism, mixed learning objective (MLO) function. The automatic generation of a text summary is a task of generating a short summary for a
relatively long text document by capturing its key information. In the past, supervised statistical machine
learning was widely used for the Automatic Text Summarization (ATS) task, but due to its high dependence
on the quality of text features, the generated summaries lack accuracy and coherence, while the computational
power involved, and performance achieved, could not easily meet the current needs. This paper proposes
four novel ATS models with a Sequence-to-Sequence (Seq2Seq) structure, utilizing an attention-based
bidirectional Long Short-Term Memory (LSTM), with added enhancements for increasing the correlation
between the generated text summary and the source text, and solving the problem of out-of-vocabulary
(OOV) words, suppressing the repeated words, and preventing the spread of cumulative errors in generated
text summaries. Experiments conducted on two public datasets confirmed that the proposed ATS models
achieve indeed better performance than the baselines and some of the state-of-the-art models considered.
This paper has put forward enhancements to the structure of the attention-based bi-directional LSTM model
(‘Bi-LSTM + Attention’) and the attention-based sequence
model (‘Seq2Seq + Attention’) in order to improve the Automatic Text Summarization (ATS). Firstly, a novel enhanced
semantic network (ESN) model has been proposed, which
works out the semantic similarity between the encoder and
decoder, and maximizes the semantic relevance during training, which increases the probability of generating more accurate text summaries, thus also improving the correlation with
the source text. Secondly, aiming at solving the problem
of unregistered words, a novel ‘DA-PN’ model has been
proposed, which utilizes decoder attention (DA) based on
a pointer network (PN). In addition, simultaneous attention
is paid to both the encoder and decoder, resulting in more accurate text summaries. Thirdly, it has been proposed
to combine the elaborated ‘DA-PN’ model with a coverage mechanism integrating multi-attention. In the resultant
‘DA-PN + Cover’ model, by using the attention distribution
of the encoder and decoder in all the previous time steps,
the attention of the current time step is affected in a positive
way, leading to finding more accurate words for inclusion
in the text summaries by avoiding repeated words. Lastly,
in order to prevent the spread of cumulative errors in generated text summaries, it has been proposed to add a mixed
learning objective (MLO) function [12] to the ‘DA-PN +
Cover’ model. The resultant ‘DA-PN + Cover + MLO’
model is the best performing one among the ATS models
proposed in this paper. It considers the evaluation as a part of
the model iteration along with a global reward, thus further
increasing the readability of generated text summaries.
The performance of the four proposed ATS models was
compared to that of two baselines, ‘Bi-LSTM + Attention’
and ‘Seq2Seq + Attention’, and seven state-of-the-art models, using short-text and long-text corpora. The obtained
experimental results demonstrated the superiority of the elaborated ATS models compared to the baselines and some of
the state-of-the-art models. By using the blended learning
of MLO, the best performing proposed model, ‘DA-PN +
Cover + MLO’, opens up a prospective to improve further the
accuracy of automatically generated text summaries by optimization of the evaluation indexes, which could effectively
solve the problem of insufficient attention at the decoder. This
research direction will be explored in the future.",Natural Language Processing,"This paper proposes four novel ATS models utilizing a Seq2Seq structure with an attention-based bidirectional LSTM to improve the Automatic Text Summarization task. The proposed models include an enhanced semantic network, a DA-PN model, a coverage mechanism, and a mixed learning objective function. The models were compared to baselines and state-of-the-art models on short and long-text corpora, and the results showed their superiority. The best-performing model, 'DA-PN + Cover + MLO,' could further improve the accuracy of generated summaries by optimizing evaluation indexes. The study suggests future research directions to explore this possibility.",Natural Language Processing,,Deep Learning and Machine Learning
65,"An Optimized Abstractive Text Summarization Model
Using Peephole Convolutional LSTM","abstractive text summarization; deep learning; convolutional neural network; lstm;
design of experiment (DoE)
"," Abstractive text summarization that generates a summary by paraphrasing a long text
remains an open significant problem for natural language processing. In this paper, we present
an abstractive text summarization model, multi-layered attentional peephole convolutional LSTM
(long short-term memory) (MAPCoL) that automatically generates a summary from a long text.
We optimize parameters of MAPCoL using central composite design (CCD) in combination with
the response surface methodology (RSM), which gives the highest accuracy in terms of summary
generation. We record the accuracy of our model (MAPCoL) on a CNN/DailyMail dataset.
We perform a comparative analysis of the accuracy of MAPCoL with that of the state-of-the-art
models in different experimental settings. The MAPCoL also outperforms the traditional LSTM-based
models in respect of semantic coherence in the output summary.","The developed abstractive summarization model using multi-layered peephole convolutional
LSTM achieves better performance than any state-of-the-art model with respect to semantic and syntactic
coherence. The developed model has been optimized using the central composite design in combination
with response surface design. The predicted ROUGE-1, ROUGE-2, and ROUGE-L scores found by the
response surface method are 41.98%, 21.67%, and 39.84%, which are very close to the experimental of
41.21%, 21.30%, and 39.42%, respectively. The developed MAPCoL model overcomes some problems that
are associated with the existing abstractive text summarization techniques. The semantic and syntactic
coherence is also guaranteed in our developed model. Though our model overcomes some problems of
other models, it also has some limitations. Our model works less efficiently when we generate a large
text as a summary. In the future, we will work on that issue to make our model more efficient in order
to generate a long summary. We have applied the developed model on the CNN-Daily Mail data set,
and the MAPCoL works better than the traditional LSTM-based models.","An Optimized Abstractive Text Summarization Model
Using Peephole Convolutional LSTMabstractive text summarization; deep learning; convolutional neural network; lstm;
design of experiment (DoE)
 Abstractive text summarization that generates a summary by paraphrasing a long text
remains an open significant problem for natural language processing. In this paper, we present
an abstractive text summarization model, multi-layered attentional peephole convolutional LSTM
(long short-term memory) (MAPCoL) that automatically generates a summary from a long text.
We optimize parameters of MAPCoL using central composite design (CCD) in combination with
the response surface methodology (RSM), which gives the highest accuracy in terms of summary
generation. We record the accuracy of our model (MAPCoL) on a CNN/DailyMail dataset.
We perform a comparative analysis of the accuracy of MAPCoL with that of the state-of-the-art
models in different experimental settings. The MAPCoL also outperforms the traditional LSTM-based
models in respect of semantic coherence in the output summary.The developed abstractive summarization model using multi-layered peephole convolutional
LSTM achieves better performance than any state-of-the-art model with respect to semantic and syntactic
coherence. The developed model has been optimized using the central composite design in combination
with response surface design. The predicted ROUGE-1, ROUGE-2, and ROUGE-L scores found by the
response surface method are 41.98%, 21.67%, and 39.84%, which are very close to the experimental of
41.21%, 21.30%, and 39.42%, respectively. The developed MAPCoL model overcomes some problems that
are associated with the existing abstractive text summarization techniques. The semantic and syntactic
coherence is also guaranteed in our developed model. Though our model overcomes some problems of
other models, it also has some limitations. Our model works less efficiently when we generate a large
text as a summary. In the future, we will work on that issue to make our model more efficient in order
to generate a long summary. We have applied the developed model on the CNN-Daily Mail data set,
and the MAPCoL works better than the traditional LSTM-based models.Probability siseibution Dojoder Aatgtion Hidden layerstate of decoderOutput Layer Attention Layer Bi-LSTM Layer Embedding Layer Input LayerWont Embedding Input Layer Convolution Layer Pooling Layer Fully Connected USTM Layer Layer",Natural Language Processing,"The paper presents a new abstractive text summarization model, called MAPCoL, that uses multi-layered attentional peephole convolutional LSTM to generate summaries from long texts. The model is optimized using central composite design and response surface methodology, resulting in higher accuracy and semantic coherence compared to state-of-the-art models. MAPCoL also outperforms traditional LSTM-based models. However, the model is less efficient in generating long summaries. Overall, MAPCoL is a promising approach to abstractive text summarization.",Natural Language Processing,Probability siseibution Dojoder Aatgtion Hidden layerstate of decoderOutput Layer Attention Layer Bi-LSTM Layer Embedding Layer Input LayerWont Embedding Input Layer Convolution Layer Pooling Layer Fully Connected USTM Layer Layer,Deep Learning and Machine Learning
66,"Indonesian Abstractive Text Summarization Using Bidirectional
Gated Recurrent Unit",abstractive text summarization; Bahasa Indonesia; bidirectional gated recurrent unit; recurrent neural network,"Abstractive text summarization is more challenging than the extractive one since it is performed by paraphrasing the entire contents
of the text, which has a higher difficulty. But, it produces a more natural summary and higher inter-sentence cohesion. Recurrent
Neural Network (RNN) has experienced success in summarizing abstractive texts for English and Chinese texts. The Bidirectional
Gated Recurrent Unit (BiGRU) RNN architecture is used so that the resulted summaries are influenced by the surrounding words.
In this research, such a method is applied for Bahasa Indonesia to improve the text summarizations those are commonly developed
using some extractive methods with low inter-sentence cohesion. An evaluation on a dataset of Indonesian journal documents
shows that the proposed model is capable of summarizing the overall contents of testing documents into some summaries with
high similarities to the provided abstracts. The proposed model resulting success in understanding source text for generating
summarization.","In our proposed model, we applied Bidirectional GRU with attention for Indonesian text data that has a ratio of
1.0 between source and abstract text. The first scenario has outperformed the second one with score 0.11975 and
ROUGE-1 score from all scenarios is better than ROUGE-2 score. But the evaluation score of two scenarios is not
higher than the score from English text model. This caused by the size of the source text and linguistic factors. From
the qualitative side, the models are able to learn and understand words that contained by source text and can produce a summary with the core words of the text. But it has poor grammar structure and cohesion among sentences since
the model generates words spreadly to the result which does not have correct grammar and not able to generate
punctuation. From both analyses, Scenario 1 has better performance than scenario 2, because it has a bigger hidden
unit size. It proves that a longer sequence needs a bigger hidden size to improve performance. Here we can conclude
that our proposed model has successfully learned individual words from source text for the Indonesian language. The
challenges left for the next experiment are overcoming the linguistic problem and handling grammar structure for
producing a better summary result
","Indonesian Abstractive Text Summarization Using Bidirectional
Gated Recurrent Unitabstractive text summarization; Bahasa Indonesia; bidirectional gated recurrent unit; recurrent neural networkAbstractive text summarization is more challenging than the extractive one since it is performed by paraphrasing the entire contents
of the text, which has a higher difficulty. But, it produces a more natural summary and higher inter-sentence cohesion. Recurrent
Neural Network (RNN) has experienced success in summarizing abstractive texts for English and Chinese texts. The Bidirectional
Gated Recurrent Unit (BiGRU) RNN architecture is used so that the resulted summaries are influenced by the surrounding words.
In this research, such a method is applied for Bahasa Indonesia to improve the text summarizations those are commonly developed
using some extractive methods with low inter-sentence cohesion. An evaluation on a dataset of Indonesian journal documents
shows that the proposed model is capable of summarizing the overall contents of testing documents into some summaries with
high similarities to the provided abstracts. The proposed model resulting success in understanding source text for generating
summarization.In our proposed model, we applied Bidirectional GRU with attention for Indonesian text data that has a ratio of
1.0 between source and abstract text. The first scenario has outperformed the second one with score 0.11975 and
ROUGE-1 score from all scenarios is better than ROUGE-2 score. But the evaluation score of two scenarios is not
higher than the score from English text model. This caused by the size of the source text and linguistic factors. From
the qualitative side, the models are able to learn and understand words that contained by source text and can produce a summary with the core words of the text. But it has poor grammar structure and cohesion among sentences since
the model generates words spreadly to the result which does not have correct grammar and not able to generate
punctuation. From both analyses, Scenario 1 has better performance than scenario 2, because it has a bigger hidden
unit size. It proves that a longer sequence needs a bigger hidden size to improve performance. Here we can conclude
that our proposed model has successfully learned individual words from source text for the Indonesian language. The
challenges left for the next experiment are overcoming the linguistic problem and handling grammar structure for
producing a better summary result
",Natural Language Processing,"The study explores the use of Bidirectional GRU with attention to summarize Bahasa Indonesian text data. The proposed model outperforms extractive methods and can generate summaries with high similarity to the provided abstracts. However, the evaluation scores are lower than those for English text models due to linguistic factors and the size of the source text. The model can learn individual words but has poor grammar structure and cohesion among sentences. The authors conclude that the proposed model has successfully learned words from the source text, but challenges remain in improving grammar structure and linguistic problems for better summary results.",Natural Language Processing,,Deep Learning and Machine Learning
67,"Faceless Person Recognition;
Privacy Implications in Social Media","Privacy, Person recognition, Social media","As we shift more of our lives into the virtual domain, the
volume of data shared on the web keeps increasing and presents a threat
to our privacy. This works contributes to the understanding of privacy
implications of such data sharing by analysing how well people are recognisable in social media data. To facilitate a systematic study we define
a number of scenarios considering factors such as how many heads of a
person are tagged and if those heads are obfuscated or not. We propose
a robust person recognition system that can handle large variations in
pose and clothing, and can be trained with few training samples. Our
results indicate that a handful of images is enough to threaten users’ privacy, even in the presence of obfuscation. We show detailed experimental
results, and discuss their implications.","Within the limitation of any study based on public data, we believe the results
presented here are a fresh view on the capabilities of machine learning to enable
person recognition in social media under adversarial condition. From a privacy
perspective, the results presented here should raise concern. We show that, when
using state of the art techniques, blurring a head has limited effect. We also
show that only a handful of tagged heads are enough to enable recognition, even
across different events (different day, clothes, poses, point of view). In the most
aggressive scenario considered (all user heads blacked-out, tagged images from a
different event), the recognition accuracy of our system is 12× higher than chance
level. It is very probable that undisclosed systems similar to the ones described
here already operate online. We believe it is the responsibility of the computer
vision community to quantify, and disseminate the privacy implications of the
images users share online. This work is a first step in this direction. We conclude
by discussing some future challenges and directions on privacy implications of
social visual media","Faceless Person Recognition;
Privacy Implications in Social MediaPrivacy, Person recognition, Social mediaAs we shift more of our lives into the virtual domain, the
volume of data shared on the web keeps increasing and presents a threat
to our privacy. This works contributes to the understanding of privacy
implications of such data sharing by analysing how well people are recognisable in social media data. To facilitate a systematic study we define
a number of scenarios considering factors such as how many heads of a
person are tagged and if those heads are obfuscated or not. We propose
a robust person recognition system that can handle large variations in
pose and clothing, and can be trained with few training samples. Our
results indicate that a handful of images is enough to threaten users’ privacy, even in the presence of obfuscation. We show detailed experimental
results, and discuss their implications.Within the limitation of any study based on public data, we believe the results
presented here are a fresh view on the capabilities of machine learning to enable
person recognition in social media under adversarial condition. From a privacy
perspective, the results presented here should raise concern. We show that, when
using state of the art techniques, blurring a head has limited effect. We also
show that only a handful of tagged heads are enough to enable recognition, even
across different events (different day, clothes, poses, point of view). In the most
aggressive scenario considered (all user heads blacked-out, tagged images from a
different event), the recognition accuracy of our system is 12× higher than chance
level. It is very probable that undisclosed systems similar to the ones described
here already operate online. We believe it is the responsibility of the computer
vision community to quantify, and disseminate the privacy implications of the
images users share online. This work is a first step in this direction. We conclude
by discussing some future challenges and directions on privacy implications of
social visual mediaTraining set Testing set Preprocessing Preprocessing T T Preprocessedtext Preprocessedtext ¥ ¥ ‘Vectorization Vectorization T T Input vectors Input vectors ¥ ¥ BIGRU training = | —Modet—> BIGRU testing | ‘Summary and Evaluation",Person recognition,The increase in data shared online presents a threat to privacy. This study analyzed how well people can be recognized in social media data and proposed a robust person recognition system that can handle variations in pose and clothing. The results showed that even obfuscation had limited effect and only a handful of tagged heads were enough for recognition. The study highlights the need for the computer vision community to quantify and disseminate the privacy implications of online image sharing. Future challenges and directions on privacy implications of social visual media are discussed.,Object and Sentiment Recognition,Training set Testing set Preprocessing Preprocessing T T Preprocessedtext Preprocessedtext ¥ ¥ ‘Vectorization Vectorization T T Input vectors Input vectors ¥ ¥ BIGRU training = | —Modet—> BIGRU testing | ‘Summary and Evaluation,Deep Learning and Machine Learning
68,"Knowledge of words: An interpretable approach for personality
recognition from social media","Personality recognition
Big five
Lexicon
Social media","Personality is one of the fundamental and stable individual characteristics that can be detected from
human behavioral data. With the rise of social media, increasing attention has been paid to the ability
to recognize personality traits by analyzing the contents of user-generated text. Existing studies have
used general psychological lexicons or machine learning, and even deep learning models, to predict
personality, but their performance has been relatively poor or they have lacked the ability to interpret
personality. In this paper, we present a novel interpretable personality recognition model based on
a personality lexicon. First, we use word embedding techniques and prior-knowledge lexicons to
automatically construct a Chinese semantic lexicon suitable for personality analysis. Based on this
personality lexicon, we analyze the correlations between personality traits and semantic categories
of words, and extract the semantic features of users’ microblogs to construct personality recognition
models using classification algorithms. Extensive experiments are conducted to demonstrate that the
proposed model can achieve significantly better performances compared to previous approaches.","In this paper, we present an automatic approach to constructing a personality lexicon suitable for personality recognition.
From users’ microblogs, we first extract representative words
using text-mining methods. We then use a clustering algorithm
to separate the representative words into various groups, each
of which is defined as a semantic category. The identified semantic categories form a semantic lexicon as the first Chinese
personality lexicon. During the lexicon’s construction, a main
challenge pertains to how to accurately represent the words. We
use word embedding in a large corpus to obtain original word
vectors, and then use popular Chinese lexicons to refine word
vectors. Based on the personality lexicon, we analyze the correlations between personality traits and word categories, and thereby
enable exploration and interpretation of personality traits using
knowledge of words. The personality lexicon is used to generate
semantic features of words, which are used to train personality
recognition models with the aid of machine learning classifiers.
A large number of experiments are carried out to verify that the
proposed personality recognition model can perform significantly
better compared to previous approaches.
In future, we will utilize distributional contextual representations of the keywords to obtain better word vectors. Furthermore,
the hierarchical clustering and information extraction approaches ill be presented to obtain a personality lexicon with more complicated semantic relations, including hierarchical semantics and
fine-grained semantics of words. This will provide psychologists
with a vital tool to deeply study and interpret personality traits.
","Knowledge of words: An interpretable approach for personality
recognition from social mediaPersonality recognition
Big five
Lexicon
Social mediaPersonality is one of the fundamental and stable individual characteristics that can be detected from
human behavioral data. With the rise of social media, increasing attention has been paid to the ability
to recognize personality traits by analyzing the contents of user-generated text. Existing studies have
used general psychological lexicons or machine learning, and even deep learning models, to predict
personality, but their performance has been relatively poor or they have lacked the ability to interpret
personality. In this paper, we present a novel interpretable personality recognition model based on
a personality lexicon. First, we use word embedding techniques and prior-knowledge lexicons to
automatically construct a Chinese semantic lexicon suitable for personality analysis. Based on this
personality lexicon, we analyze the correlations between personality traits and semantic categories
of words, and extract the semantic features of users’ microblogs to construct personality recognition
models using classification algorithms. Extensive experiments are conducted to demonstrate that the
proposed model can achieve significantly better performances compared to previous approaches.In this paper, we present an automatic approach to constructing a personality lexicon suitable for personality recognition.
From users’ microblogs, we first extract representative words
using text-mining methods. We then use a clustering algorithm
to separate the representative words into various groups, each
of which is defined as a semantic category. The identified semantic categories form a semantic lexicon as the first Chinese
personality lexicon. During the lexicon’s construction, a main
challenge pertains to how to accurately represent the words. We
use word embedding in a large corpus to obtain original word
vectors, and then use popular Chinese lexicons to refine word
vectors. Based on the personality lexicon, we analyze the correlations between personality traits and word categories, and thereby
enable exploration and interpretation of personality traits using
knowledge of words. The personality lexicon is used to generate
semantic features of words, which are used to train personality
recognition models with the aid of machine learning classifiers.
A large number of experiments are carried out to verify that the
proposed personality recognition model can perform significantly
better compared to previous approaches.
In future, we will utilize distributional contextual representations of the keywords to obtain better word vectors. Furthermore,
the hierarchical clustering and information extraction approaches ill be presented to obtain a personality lexicon with more complicated semantic relations, including hierarchical semantics and
fine-grained semantics of words. This will provide psychologists
with a vital tool to deeply study and interpret personality traits.
accuracy 8s 8 8 (a) Within events39| Eunary-+pairwise [Mlunary-+pairwise (no pruning) go) Eines pairwise (oracle) so| e 2 oo g g 0 20] St Sy (a) Within events100, 100, s0| s0| 60| 60| 40| 40| 4 20| 20| ol ol T2525 5 10 #tagged examples/person 525 ‘agged examples/person (a) Across Events (b) Across Events Incorrect pair Fig. 4: Single person recogniser at different tag rates. Fig.5:_ Matching in social media.",Person recognition,"The paper proposes a novel interpretable model for personality recognition based on a Chinese personality lexicon constructed using word embedding techniques and prior-knowledge lexicons. The model analyzes the correlations between personality traits and semantic categories of words to extract semantic features of users' microblogs, which are then used to construct personality recognition models using classification algorithms. The proposed model outperforms previous approaches in terms of accuracy. Future work includes using distributional contextual representations of keywords to obtain better word vectors and developing a personality lexicon with more complicated semantic relations for deeper study and interpretation of personality traits.",Object and Sentiment Recognition,"accuracy 8s 8 8 (a) Within events39| Eunary-+pairwise [Mlunary-+pairwise (no pruning) go) Eines pairwise (oracle) so| e 2 oo g g 0 20] St Sy (a) Within events100, 100, s0| s0| 60| 60| 40| 40| 4 20| 20| ol ol T2525 5 10 #tagged examples/person 525 ‘agged examples/person (a) Across Events (b) Across Events Incorrect pair Fig. 4: Single person recogniser at different tag rates. Fig.5:_ Matching in social media.",Object Recognition
69,Face Recognition Using Laplacianfaces,"Face recognition, principal component analysis, linear discriminant analysis, locality preserving projections, face
manifold, subspace learning.","We propose an appearance-based face recognition method called the Laplacianface approach. By using Locality
Preserving Projections (LPP), the face images are mapped into a face subspace for analysis. Different from Principal Component
Analysis (PCA) and Linear Discriminant Analysis (LDA) which effectively see only the Euclidean structure of face space, LPP finds an
embedding that preserves local information, and obtains a face subspace that best detects the essential face manifold structure. The
Laplacianfaces are the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the face manifold. In
this way, the unwanted variations resulting from changes in lighting, facial expression, and pose may be eliminated or reduced.
Theoretical analysis shows that PCA, LDA, and LPP can be obtained from different graph models. We compare the proposed
Laplacianface approach with Eigenface and Fisherface methods on three different face data sets. Experimental results suggest that
the proposed Laplacianface approach provides a better representation and achieves lower error rates in face recognition.","The manifold ways of face analysis (representation and
recognition) are introduced in this paper in order to detect
the underlying nonlinear manifold structure in the manner
of linear subspace learning. To the best of our knowledge,
this is the first devoted work on face representation and
recognition which explicitly considers the manifold structure. The manifold structure is approximated by the
adjacency graph computed from the data points. Using the
notion of the Laplacian of the graph, we then compute a
transformation matrix which maps the face images into a face
subspace. We call this the Laplacianfaces approach. The
Laplacianfaces are obtained by finding the optimal linear
approximations to the eigenfunctions of the Laplace Beltrami
operator on the face manifold. This linear transformation
optimally preserves local manifold structure. Theoretical
analysis of the LPP algorithm and its connections to PCA and
LDA are provided. Experimental results on PIE, Yale, and
MSRA databases show the effectiveness of our method.","Face Recognition Using LaplacianfacesFace recognition, principal component analysis, linear discriminant analysis, locality preserving projections, face
manifold, subspace learning.We propose an appearance-based face recognition method called the Laplacianface approach. By using Locality
Preserving Projections (LPP), the face images are mapped into a face subspace for analysis. Different from Principal Component
Analysis (PCA) and Linear Discriminant Analysis (LDA) which effectively see only the Euclidean structure of face space, LPP finds an
embedding that preserves local information, and obtains a face subspace that best detects the essential face manifold structure. The
Laplacianfaces are the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the face manifold. In
this way, the unwanted variations resulting from changes in lighting, facial expression, and pose may be eliminated or reduced.
Theoretical analysis shows that PCA, LDA, and LPP can be obtained from different graph models. We compare the proposed
Laplacianface approach with Eigenface and Fisherface methods on three different face data sets. Experimental results suggest that
the proposed Laplacianface approach provides a better representation and achieves lower error rates in face recognition.The manifold ways of face analysis (representation and
recognition) are introduced in this paper in order to detect
the underlying nonlinear manifold structure in the manner
of linear subspace learning. To the best of our knowledge,
this is the first devoted work on face representation and
recognition which explicitly considers the manifold structure. The manifold structure is approximated by the
adjacency graph computed from the data points. Using the
notion of the Laplacian of the graph, we then compute a
transformation matrix which maps the face images into a face
subspace. We call this the Laplacianfaces approach. The
Laplacianfaces are obtained by finding the optimal linear
approximations to the eigenfunctions of the Laplace Beltrami
operator on the face manifold. This linear transformation
optimally preserves local manifold structure. Theoretical
analysis of the LPP algorithm and its connections to PCA and
LDA are provided. Experimental results on PIE, Yale, and
MSRA databases show the effectiveness of our method.Test dataset layer Model: Training dataset extreme -nonextrememodel edict High] tow] Middle Predicting ‘ ‘ Extreme Nonextreme t Y samples samples Extreme Nonextreme samples samples 2 Layer Model: high — low model Training | Training Predicting fo i Layer Mode 7 Layer Model hidhlowmodei | [extreme -nonextrememosel | High Low Middle (a) Model training (b) Model test=a . 1 x ! | Big-Five personality —j Text j | questionnaires ! corpus j Sores | ' Keyword Personality relevant Personalit ' (3)Word Key words - | | extraction based |}—»| words discovery based | >) rans words L+>| embedding Clusterin Personalit : on TF-IDF on chi-square test refinement 8 y lexicon (4) Personality lexicon construction Hypernym-hyponym relations Hownet lexicon Integrated lexicon Tongyici Cilin lexicon Synonym relations (2) External knowledge combination4. Data preparation 2. Personality lexicon construction cringe rom] Cwoaentewine |_[ veywors microblogs selevant words refinement clustering blogs ‘extraction "" ering Big Five Personality revonalty | “sna penonaty sees peony econ atures onatenana Clsitenon relaton arcs sa 3. Personality explanation model 4. Personality recognition model",Person recognition,"This paper proposes a method for face recognition called the Laplacianface approach, which uses Locality Preserving Projections (LPP) to map face images into a face subspace for analysis. Unlike other methods, LPP preserves local information and detects the essential face manifold structure, which can reduce or eliminate unwanted variations resulting from changes in lighting, facial expression, and pose. The approach is compared with Eigenface and Fisherface methods on three different face data sets, and results show that Laplacianface provides a better representation and achieves lower error rates in face recognition. The paper also discusses the theoretical analysis of LPP algorithm and its connections to PCA and LDA, and experimental results on PIE, Yale, and MSRA databases demonstrate the effectiveness of the method.",Object and Sentiment Recognition,"Test dataset layer Model: Training dataset extreme -nonextrememodel edict High] tow] Middle Predicting ‘ ‘ Extreme Nonextreme t Y samples samples Extreme Nonextreme samples samples 2 Layer Model: high — low model Training | Training Predicting fo i Layer Mode 7 Layer Model hidhlowmodei | [extreme -nonextrememosel | High Low Middle (a) Model training (b) Model test=a . 1 x ! | Big-Five personality —j Text j | questionnaires ! corpus j Sores | ' Keyword Personality relevant Personalit ' (3)Word Key words - | | extraction based |}—»| words discovery based | >) rans words L+>| embedding Clusterin Personalit : on TF-IDF on chi-square test refinement 8 y lexicon (4) Personality lexicon construction Hypernym-hyponym relations Hownet lexicon Integrated lexicon Tongyici Cilin lexicon Synonym relations (2) External knowledge combination4. Data preparation 2. Personality lexicon construction cringe rom] Cwoaentewine |_[ veywors microblogs selevant words refinement clustering blogs ‘extraction "" ering Big Five Personality revonalty | “sna penonaty sees peony econ atures onatenana Clsitenon relaton arcs sa 3. Personality explanation model 4. Personality recognition model",Object Recognition
70,"Attention-Aware and Regularization for
Face Recognition With Reinforcement Learning","Attention-aware, reinforcement learning,
regularization, face recognition.","Different face regions have different contributions to
recognition. Especially in the wild environment, the difference of
contributions will be further amplified due to a lot of interference.
Based on this, this paper proposes an attention-aware face recognition method based on a deep convolutional neural network and
reinforcement learning. The proposed method composes of an
Attention-Net and a Feature-net. The Attention-Net is used to
select patches in the input face image according to the facial
landmarks and trained with reinforcement learning to maximize
the recognition accuracy. The Feature-net is used for extracting
discriminative embedding features. In addition, a regularization
method has also been introduced. The mask of the input layer is
also applied to the intermediate feature maps, which is an approximation to train a series of models for different face patches
and provide a combined model. Our method achieves satisfactory recognition performance on its application to the public
prevailing face verification database.","In this work, ARFace, an attention-aware face recognition
method has been introduced. It consists of two components, namely Attention-Net and Feature-Net. By simulating
the attention mechanism of the human visual system, the
Attention-Net outputs the mask of a face image and is optimized by reinforcement learning. Feature-Net aims to obtain
the embedding features from the image and provide supervision signals for the Attention-Net. In addition to the above
system, we have also proposed an effective regularization
method, in which the mask is scaled down by the size
of the intermediate feature maps and employed in the feature maps. Extensive experiment was conducted to compare
performance of the proposed method with baseline. The results
thus obtained reveal that ARFace achieves competitive face
recognition performance.
This work has demonstrated the superiority of our method
as well as the feasibility of using the attention mechanism in
face recognition. However, we only used five face landmarks
as the center of dropping in our experiments. In future, we
could even subdivide the face area with ten or more points,
which would lead to higher accuracy results. Our work only
demonstrates the feasibility of this approach, and AttentionNet based on reinforcement learning can help improve the
performance of face recognition to some extent.","Attention-Aware and Regularization for
Face Recognition With Reinforcement LearningAttention-aware, reinforcement learning,
regularization, face recognition.Different face regions have different contributions to
recognition. Especially in the wild environment, the difference of
contributions will be further amplified due to a lot of interference.
Based on this, this paper proposes an attention-aware face recognition method based on a deep convolutional neural network and
reinforcement learning. The proposed method composes of an
Attention-Net and a Feature-net. The Attention-Net is used to
select patches in the input face image according to the facial
landmarks and trained with reinforcement learning to maximize
the recognition accuracy. The Feature-net is used for extracting
discriminative embedding features. In addition, a regularization
method has also been introduced. The mask of the input layer is
also applied to the intermediate feature maps, which is an approximation to train a series of models for different face patches
and provide a combined model. Our method achieves satisfactory recognition performance on its application to the public
prevailing face verification database.In this work, ARFace, an attention-aware face recognition
method has been introduced. It consists of two components, namely Attention-Net and Feature-Net. By simulating
the attention mechanism of the human visual system, the
Attention-Net outputs the mask of a face image and is optimized by reinforcement learning. Feature-Net aims to obtain
the embedding features from the image and provide supervision signals for the Attention-Net. In addition to the above
system, we have also proposed an effective regularization
method, in which the mask is scaled down by the size
of the intermediate feature maps and employed in the feature maps. Extensive experiment was conducted to compare
performance of the proposed method with baseline. The results
thus obtained reveal that ARFace achieves competitive face
recognition performance.
This work has demonstrated the superiority of our method
as well as the feasibility of using the attention mechanism in
face recognition. However, we only used five face landmarks
as the center of dropping in our experiments. In future, we
could even subdivide the face area with ten or more points,
which would lead to higher accuracy results. Our work only
demonstrates the feasibility of this approach, and AttentionNet based on reinforcement learning can help improve the
performance of face recognition to some extent.",Person recognition,"The paper proposes an attention-aware face recognition method based on a deep convolutional neural network and reinforcement learning. The method includes an Attention-Net that selects patches in the input face image and a Feature-Net that extracts discriminative embedding features. The Attention-Net is trained with reinforcement learning to maximize recognition accuracy, and a regularization method is introduced. The method achieves satisfactory recognition performance on a public face verification database. The paper demonstrates the feasibility of using the attention mechanism in face recognition and suggests that subdividing the face area with more points could lead to higher accuracy results.",Object and Sentiment Recognition,,Object Recognition
71,"LOCAL BINARY PATTERN NETWORK : A DEEP LEARNING APPROACH FOR FACE
RECOGNITION"," Deep learning, Local Binary Pattern,
PCA, Convolutional Neural Network","Deep learning is well known as a method to extract hierarchical representations of data. In this paper a novel unsupervised
deep learning based methodology, named Local Binary Pattern Network (LBPNet), is proposed to efficiently extract and
compare high-level over-complete features in multilayer hierarchy. The LBPNet retains the same topology of Convolutional Neural Network (CNN) - one of the most well studied
deep learning architectures - whereas the trainable kernels are
replaced by the off-the-shelf computer vision descriptor (i.e.,
LBP). This enables the LBPNet to achieve a high recognition accuracy without requiring any costly model learning approach on massive data. Through extensive numerical experiments using the public benchmarks (i.e., FERET and LFW),
LBPNet has shown that it is comparable to other unsupervised
methods","In this paper, a novel tool for face recognition named Local
Binary Pattern Network (LBPNet) was proposed. The key
ideas in LBPNet were inspired by the successful LBP descriptor and the deep learning architecture. LBPNet retains
a similar topology of CNN while avoiding the costly model
learning on massive data by replacing its convolutional kernels with off-the-shelf computer vision descriptors.
Extensive experiments were conducted using two public
benchmarks (i.e., FERET and LFW) to evaluate the proposed
LBPNet. The results showed that LBPNet achieved promising performance in these benchmarks compared with other
state-of-the-art methods","LOCAL BINARY PATTERN NETWORK : A DEEP LEARNING APPROACH FOR FACE
RECOGNITION Deep learning, Local Binary Pattern,
PCA, Convolutional Neural NetworkDeep learning is well known as a method to extract hierarchical representations of data. In this paper a novel unsupervised
deep learning based methodology, named Local Binary Pattern Network (LBPNet), is proposed to efficiently extract and
compare high-level over-complete features in multilayer hierarchy. The LBPNet retains the same topology of Convolutional Neural Network (CNN) - one of the most well studied
deep learning architectures - whereas the trainable kernels are
replaced by the off-the-shelf computer vision descriptor (i.e.,
LBP). This enables the LBPNet to achieve a high recognition accuracy without requiring any costly model learning approach on massive data. Through extensive numerical experiments using the public benchmarks (i.e., FERET and LFW),
LBPNet has shown that it is comparable to other unsupervised
methodsIn this paper, a novel tool for face recognition named Local
Binary Pattern Network (LBPNet) was proposed. The key
ideas in LBPNet were inspired by the successful LBP descriptor and the deep learning architecture. LBPNet retains
a similar topology of CNN while avoiding the costly model
learning on massive data by replacing its convolutional kernels with off-the-shelf computer vision descriptors.
Extensive experiments were conducted using two public
benchmarks (i.e., FERET and LFW) to evaluate the proposed
LBPNet. The results showed that LBPNet achieved promising performance in these benchmarks compared with other
state-of-the-art methodsyeN-uonuany (2) 9N-aumead (9) Bi ~ BEB ms - Yorn YorN YorN Yorn YorN v a __ Seale down Bi - Conv input feature maps feature mapswm Bi ~~ om vile (i 4 mh bbe:",Person recognition,"This paper introduces a new method for face recognition called Local Binary Pattern Network (LBPNet), which combines the deep learning architecture of Convolutional Neural Network (CNN) with the computer vision descriptor LBP. The LBPNet is able to extract and compare high-level over-complete features in a multilayer hierarchy, achieving high recognition accuracy without requiring costly model learning on massive data. The method was evaluated using public benchmarks FERET and LFW and was shown to be comparable to other unsupervised methods, demonstrating promising performance in face recognition.",Object and Sentiment Recognition,yeN-uonuany (2) 9N-aumead (9) Bi ~ BEB ms - Yorn YorN YorN Yorn YorN v a __ Seale down Bi - Conv input feature maps feature mapswm Bi ~~ om vile (i 4 mh bbe:,Object Recognition
72,"A Deep Learning Approach for Face Detection 
using YOLO","Face Detection, YOLO, Neural Network, object 
detection, Convolutional Neural Network ","Deep learning is nowadays a buzzword and is 
considered a new era of machine learning which trains the 
computers in finding the pattern from a massive amount of 
data. It mainly describes the learning at multiple levels of 
representation which helps to make sense on the data 
consisting of text, sound and images. Many organizations are 
using a type of deep learning known as a convolutional neural 
network to deal with the objects in a video sequence. Deep 
Convolution Neural Networks (CNNs) have proved to be 
impressive in terms of performance for detecting the objects, 
classification of images and semantic segmentation. Object 
detection is defined as a combination of classification and 
localization. Face detection is one of the most challenging 
problems of pattern recognition. Various face related 
applications like face verification, facial recognition, clustering 
of face etc. are a part of face detection. Effective training needs 
to be carried out for detection and recognition. The accuracy in 
face detection using the traditional approach did not yield a 
good result. This paper focuses on improving the accuracy of 
detecting the face using the model of deep learning. YOLO 
(You only look once), a popular deep learning library is used to 
implement the proposed work. The paper compares the 
accuracy of detecting the face in an efficient manner with 
respect to the traditional approach. The proposed model uses 
the convolutional neural network as an approach of deep 
learning for detecting faces from videos. The FDDB dataset is 
used for training and testing of our model. A model is finetuned on various performance parameters and the best 
suitable values are taken into consideration. It is also 
compared the execution of training time and the performance 
of the model on two different GPUs.","It can be concluded that processing a huge amount of 
data using deep learning requires a high configuration 
NVIDIA graphics card (GPU). If the configuration of the 
GPU is high, then computation of the task can be achieved at 
a faster rate. There are various parameters which are 
responsible for detecting the face from either an image or a 
video. Based on the analysis carried out by the proposed 
model, the following points can be concluded. Firstly, the 
learning rate depends on the network size and size of object 
too. If the network is medium or large and size of object is 
compared to less, then the learning rate should be kept small. 
In our work, the network consists of 18 layers, so the 
calculated learning rate = 0.0001. Secondly, if number of 
times the dataset are trained on the network, better results are 
obtained. It also provokes data overfitting issue so, epoch 
size should be kept at an optimal number which can produce 
neither network overfitting nor underfitting. In our work, 
after 20 epochs it was observed that the IoU accuracy 
obtained is the best i.e 92.2%. Also, resolution of the image 
plays a very important role. Resolution of the image as 
concluded is inversely proportional to the frames per second. 
In future work, proposed model can be further optimized for 
very small face detections, on different viewpoint variations, 
and partial face detection.","A Deep Learning Approach for Face Detection 
using YOLOFace Detection, YOLO, Neural Network, object 
detection, Convolutional Neural Network Deep learning is nowadays a buzzword and is 
considered a new era of machine learning which trains the 
computers in finding the pattern from a massive amount of 
data. It mainly describes the learning at multiple levels of 
representation which helps to make sense on the data 
consisting of text, sound and images. Many organizations are 
using a type of deep learning known as a convolutional neural 
network to deal with the objects in a video sequence. Deep 
Convolution Neural Networks (CNNs) have proved to be 
impressive in terms of performance for detecting the objects, 
classification of images and semantic segmentation. Object 
detection is defined as a combination of classification and 
localization. Face detection is one of the most challenging 
problems of pattern recognition. Various face related 
applications like face verification, facial recognition, clustering 
of face etc. are a part of face detection. Effective training needs 
to be carried out for detection and recognition. The accuracy in 
face detection using the traditional approach did not yield a 
good result. This paper focuses on improving the accuracy of 
detecting the face using the model of deep learning. YOLO 
(You only look once), a popular deep learning library is used to 
implement the proposed work. The paper compares the 
accuracy of detecting the face in an efficient manner with 
respect to the traditional approach. The proposed model uses 
the convolutional neural network as an approach of deep 
learning for detecting faces from videos. The FDDB dataset is 
used for training and testing of our model. A model is finetuned on various performance parameters and the best 
suitable values are taken into consideration. It is also 
compared the execution of training time and the performance 
of the model on two different GPUs.It can be concluded that processing a huge amount of 
data using deep learning requires a high configuration 
NVIDIA graphics card (GPU). If the configuration of the 
GPU is high, then computation of the task can be achieved at 
a faster rate. There are various parameters which are 
responsible for detecting the face from either an image or a 
video. Based on the analysis carried out by the proposed 
model, the following points can be concluded. Firstly, the 
learning rate depends on the network size and size of object 
too. If the network is medium or large and size of object is 
compared to less, then the learning rate should be kept small. 
In our work, the network consists of 18 layers, so the 
calculated learning rate = 0.0001. Secondly, if number of 
times the dataset are trained on the network, better results are 
obtained. It also provokes data overfitting issue so, epoch 
size should be kept at an optimal number which can produce 
neither network overfitting nor underfitting. In our work, 
after 20 epochs it was observed that the IoU accuracy 
obtained is the best i.e 92.2%. Also, resolution of the image 
plays a very important role. Resolution of the image as 
concluded is inversely proportional to the frames per second. 
In future work, proposed model can be further optimized for 
very small face detections, on different viewpoint variations, 
and partial face detection.",Person recognition,"This paper discusses the use of deep learning for face detection, using the YOLO library and a convolutional neural network. The paper compares the accuracy of face detection using the traditional approach with that achieved through deep learning. The authors fine-tune the model on various parameters and test it on the FDDB dataset. The paper concludes that deep learning requires a high configuration NVIDIA graphics card and that several factors affect the accuracy of face detection, including learning rate, number of times the dataset is trained, and image resolution. The proposed model achieved an IoU accuracy of 92.2% after 20 epochs. Future work includes optimizing the model for very small face detections, different viewpoint variations, and partial face detection.",Object and Sentiment Recognition,,Object Recognition
73,Analyzing the epidemiological outbreak of COVID‐19: A visual exploratory data analysis approach,"China, coronavirus, COVID‐19, data analysis, SARS‐CoV‐2, visualization.","There is an obvious concern globally regarding the fact about the emerging coronavirus 2019 novel coronavirus (2019‐nCoV) as a worldwide public health threat. As the outbreak of COVID‐19 causes by the severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2) progresses within China and beyond, rapidly available epidemiological data are needed to guide strategies for situational awareness and intervention. The recent outbreak of pneumonia in Wuhan, China, caused by the SARS‐CoV‐2 emphasizes the importance of analyzing the epidemiological data of this novel virus and predicting their risks of infecting people all around the globe. In this study, we present an effort to compile and analyze epidemiological outbreak information on COVID‐19 based on the several open datasets on 2019‐nCoV provided by the Johns Hopkins University, World Health Organization, Chinese Center for Disease Control and Prevention, National Health Commission, and DXY. An exploratory data analysis with visualizations has been made to understand the number of different cases reported (confirmed, death, and recovered) in different provinces of China and outside of China. Overall, at the outset of an outbreak like this, it is highly important to readily provide information to begin the evaluation necessary to understand the risks and begin containment activities.","In conclusion, the dataset we have used for our experiment 2019 coronavirus dataset (January‐February 2020), COVID‐19 (nCOV‐19) coronavirus spread dataset, and 2019‐nCoV dataset can be useful to monitor the emerging outbreaks, such as 2019‐nCoV. Such activities can help us to generate and disseminate detailed information to the scientific community, especially in the early stages of an outbreak, when there is a little else available, allowing for independent assessments of key parameters that influence interventions. We observe an interesting different case reported based on the different datasets of 2019‐nCoV, which helps us to understand that it needs more epidemiological and serological studies. We also investigated early indications that the response is being strengthened in China and worldwide on the basis of a decrease in the case of detection time and rapid management of internationally identified travel‐related cases. As a caveat, this is an early data analysis and visualization approach of a situation that is rapidly evolving. To the best of our knowledge, this is the very first attempt on COVID‐19, which focuses on the V‐EDA based on different data sources. However, knowledge about this novel SARS‐CoV‐2 virus remains limited among general people around the globe. Raw data released by various sources are not adequately capable to provide an informative understanding of COVID‐19, caused of SARS‐CoV‐2. Therefore, A user‐friendly data visualization model will be more effective to understand the epidemic outbreak of this severe disease. Visualization model like map view and tree map view provides an interactive interface and visualize each and every raw fact in a comprehensive manner. Hopefully, in the coming weeks, we will continue to monitor this outbreak's epidemiology data that we have used in this study and from other official sources.","Analyzing the epidemiological outbreak of COVID‐19: A visual exploratory data analysis approachChina, coronavirus, COVID‐19, data analysis, SARS‐CoV‐2, visualization.There is an obvious concern globally regarding the fact about the emerging coronavirus 2019 novel coronavirus (2019‐nCoV) as a worldwide public health threat. As the outbreak of COVID‐19 causes by the severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2) progresses within China and beyond, rapidly available epidemiological data are needed to guide strategies for situational awareness and intervention. The recent outbreak of pneumonia in Wuhan, China, caused by the SARS‐CoV‐2 emphasizes the importance of analyzing the epidemiological data of this novel virus and predicting their risks of infecting people all around the globe. In this study, we present an effort to compile and analyze epidemiological outbreak information on COVID‐19 based on the several open datasets on 2019‐nCoV provided by the Johns Hopkins University, World Health Organization, Chinese Center for Disease Control and Prevention, National Health Commission, and DXY. An exploratory data analysis with visualizations has been made to understand the number of different cases reported (confirmed, death, and recovered) in different provinces of China and outside of China. Overall, at the outset of an outbreak like this, it is highly important to readily provide information to begin the evaluation necessary to understand the risks and begin containment activities.In conclusion, the dataset we have used for our experiment 2019 coronavirus dataset (January‐February 2020), COVID‐19 (nCOV‐19) coronavirus spread dataset, and 2019‐nCoV dataset can be useful to monitor the emerging outbreaks, such as 2019‐nCoV. Such activities can help us to generate and disseminate detailed information to the scientific community, especially in the early stages of an outbreak, when there is a little else available, allowing for independent assessments of key parameters that influence interventions. We observe an interesting different case reported based on the different datasets of 2019‐nCoV, which helps us to understand that it needs more epidemiological and serological studies. We also investigated early indications that the response is being strengthened in China and worldwide on the basis of a decrease in the case of detection time and rapid management of internationally identified travel‐related cases. As a caveat, this is an early data analysis and visualization approach of a situation that is rapidly evolving. To the best of our knowledge, this is the very first attempt on COVID‐19, which focuses on the V‐EDA based on different data sources. However, knowledge about this novel SARS‐CoV‐2 virus remains limited among general people around the globe. Raw data released by various sources are not adequately capable to provide an informative understanding of COVID‐19, caused of SARS‐CoV‐2. Therefore, A user‐friendly data visualization model will be more effective to understand the epidemic outbreak of this severe disease. Visualization model like map view and tree map view provides an interactive interface and visualize each and every raw fact in a comprehensive manner. Hopefully, in the coming weeks, we will continue to monitor this outbreak's epidemiology data that we have used in this study and from other official sources.",Medical Data Analysis,"This article discusses the importance of epidemiological data in understanding and predicting the risks associated with the outbreak of COVID-19 caused by the SARS-CoV-2 virus. The authors analyze various open datasets on the outbreak and present an exploratory data analysis with visualizations to understand the number of cases reported in different provinces of China and outside of China. They conclude that user-friendly data visualization models such as map view and tree map view can provide a comprehensive understanding of the outbreak and help monitor its epidemiological data. However, they caution that this is an early data analysis of a situation that is rapidly evolving and that more epidemiological and serological studies are needed to fully understand the virus.",Medical Data Analysis,,Object Recognition
74,Associations between signs and symptoms of dry eye disease: a systematic review,"Associations, correlations, dry eye disease, signs, symptoms, systematic literature review.","Purpose: The accurate diagnosis and classification of dry eye disease (DED) is challenging owing to wide variations in symptoms and lack of a single reliable clinical assessment. In addition, changes and severity of clinical signs often do not correspond to patient-reported symptoms. To better understand the inconsistencies observed between signs and symptoms, we conducted a systematic literature review to evaluate published studies reporting associations between patient-reported symptoms and clinical signs of DED. Methods: PubMed and Embase were searched for English-language articles on the association between clinical signs and symptoms of DED up to February 2014 (no lower limit was set). Results: Thirty-four articles were identified that assessed associations between signs and symptoms, among which 33 unique studies were reported. These included 175 individual sign– symptom association analyses. Statistical significance was reported for associations between sign and symptom measures in 21 of 33 (64%) studies, but for only 42 of 175 (24%) individual analyses. Of 175 individual analyses, 148 reported correlation coefficients, of which the majority (129/148; 87%) were between -0.4 and 0.4, indicating low-to-moderate correlation. Of all individual analyses that demonstrated a statistically significant association, one-half (56%) of reported correlation coefficients were in this range. No clear trends were observed in relation to the strength of associations relative to study size, statistical methods, or study region, although results from three studies did suggest that disease severity may be a factor. Conclusion: Associations between DED signs and symptoms are low and inconsistent, which may have implications for monitoring the response to treatment, both in the clinic and in clinical trials. Further studies to increase understanding of the etiopathogenesis of DED and to identify the most reliable and relevant measures of disease are needed to enhance clinical assessment of DED and the measurement of response to therapeutic interventions.",he available evidence suggests that associations between commonly used assessments of signs and symptoms of DED are low and inconsistent. Further studies to increase understanding of the etiopathogenesis of DED and to identify the most reliable and relevant measures of disease are needed to enhance clinical assessment of DED and the measurement of response to therapeutic interventions.,"Associations between signs and symptoms of dry eye disease: a systematic reviewAssociations, correlations, dry eye disease, signs, symptoms, systematic literature review.Purpose: The accurate diagnosis and classification of dry eye disease (DED) is challenging owing to wide variations in symptoms and lack of a single reliable clinical assessment. In addition, changes and severity of clinical signs often do not correspond to patient-reported symptoms. To better understand the inconsistencies observed between signs and symptoms, we conducted a systematic literature review to evaluate published studies reporting associations between patient-reported symptoms and clinical signs of DED. Methods: PubMed and Embase were searched for English-language articles on the association between clinical signs and symptoms of DED up to February 2014 (no lower limit was set). Results: Thirty-four articles were identified that assessed associations between signs and symptoms, among which 33 unique studies were reported. These included 175 individual sign– symptom association analyses. Statistical significance was reported for associations between sign and symptom measures in 21 of 33 (64%) studies, but for only 42 of 175 (24%) individual analyses. Of 175 individual analyses, 148 reported correlation coefficients, of which the majority (129/148; 87%) were between -0.4 and 0.4, indicating low-to-moderate correlation. Of all individual analyses that demonstrated a statistically significant association, one-half (56%) of reported correlation coefficients were in this range. No clear trends were observed in relation to the strength of associations relative to study size, statistical methods, or study region, although results from three studies did suggest that disease severity may be a factor. Conclusion: Associations between DED signs and symptoms are low and inconsistent, which may have implications for monitoring the response to treatment, both in the clinic and in clinical trials. Further studies to increase understanding of the etiopathogenesis of DED and to identify the most reliable and relevant measures of disease are needed to enhance clinical assessment of DED and the measurement of response to therapeutic interventions.he available evidence suggests that associations between commonly used assessments of signs and symptoms of DED are low and inconsistent. Further studies to increase understanding of the etiopathogenesis of DED and to identify the most reliable and relevant measures of disease are needed to enhance clinical assessment of DED and the measurement of response to therapeutic interventions.FIGURE 3 Comparative analysis of different cases reported by Hubei, other provinces of China, and the rest of the world till 16 February 2020. Hubei has confirmed 58 182 infected patients, whereas other provinces in China and the rest of the world confirmed 12 264 and 425 cases, respectively Comparative case analysis of Hubei, Other provinces of China, and worldwide Region 58182 Other Chinese Provinces| 4109 69 | 425 Rest of the World 117 5 0 20k Count 40k 60k Case Deaths Mi Recovered ConfirmedNumber of Recovered Cases outside China 112 Recovered 100 oe 100 80 = es 80 5 70 ov Vv 60 2 60 a a7 a 40 40 25 28 20 2 20 a 838 99999 6 6 52 2 8 8 aaa 0 Jan 26 Feb 2 Feb 9 Feb 16 2020 Date FIGURE 2 Exploratory data analysis of the number of recovered cases outside China with data visualization. It depicts that the rate of recovery outside China also increases regularly and till 16 February 2020, the number of recovered patients was 112 worldwideRecovered Number of Recovered Cases in China 6000 5000 4000 3000 2000 1000 0 522 295 386 28 28 31 32 42 45 80 88 90 141 168 Jan 26 2020 Feb 2 Date 633 26392686 4774 3459 6 | Fel 639 b 16 Recovered 6000 5000 4000 3000 2000 1000 FIGURE 1. Exploratory data analysis of the number of recovered cases in China with data visualization. Initially, till 28 January 2020, the number of recovered patients in China was 23, but surprisingly it increases gradually and lastly till 16 February 2020, the number of recovered patients was 6639",Medical Data Analysis,"This systematic literature review evaluates published studies reporting associations between patient-reported symptoms and clinical signs of dry eye disease (DED). The review identified 34 articles assessing associations between signs and symptoms, including 175 individual sign-symptom association analyses. Statistical significance was reported for associations between sign and symptom measures in 64% of studies, but for only 24% of individual analyses. The majority of reported correlation coefficients between signs and symptoms were low-to-moderate, indicating low correlation. The review concludes that associations between commonly used assessments of signs and symptoms of DED are low and inconsistent, highlighting the need for further studies to enhance clinical assessment of DED and the measurement of response to therapeutic interventions.",Medical Data Analysis,"FIGURE 3 Comparative analysis of different cases reported by Hubei, other provinces of China, and the rest of the world till 16 February 2020. Hubei has confirmed 58 182 infected patients, whereas other provinces in China and the rest of the world confirmed 12 264 and 425 cases, respectively Comparative case analysis of Hubei, Other provinces of China, and worldwide Region 58182 Other Chinese Provinces| 4109 69 | 425 Rest of the World 117 5 0 20k Count 40k 60k Case Deaths Mi Recovered ConfirmedNumber of Recovered Cases outside China 112 Recovered 100 oe 100 80 = es 80 5 70 ov Vv 60 2 60 a a7 a 40 40 25 28 20 2 20 a 838 99999 6 6 52 2 8 8 aaa 0 Jan 26 Feb 2 Feb 9 Feb 16 2020 Date FIGURE 2 Exploratory data analysis of the number of recovered cases outside China with data visualization. It depicts that the rate of recovery outside China also increases regularly and till 16 February 2020, the number of recovered patients was 112 worldwideRecovered Number of Recovered Cases in China 6000 5000 4000 3000 2000 1000 0 522 295 386 28 28 31 32 42 45 80 88 90 141 168 Jan 26 2020 Feb 2 Date 633 26392686 4774 3459 6 | Fel 639 b 16 Recovered 6000 5000 4000 3000 2000 1000 FIGURE 1. Exploratory data analysis of the number of recovered cases in China with data visualization. Initially, till 28 January 2020, the number of recovered patients in China was 23, but surprisingly it increases gradually and lastly till 16 February 2020, the number of recovered patients was 6639",Medical Data Analysis
75,Automatic Detection of Diabetic Eye Disease Through Deep Learning Using Fundus Images: A Survey,"Diabetic eye disease, diabetic retinopathy, deep leaning, glaucoma, image processing, macular edema, transfer learning.","Diabetes Mellitus, or Diabetes, is a disease in which a person’s body fails to respond to insulin released by their pancreas, or it does not produce sufficient insulin. People suffering from diabetes are at high risk of developing various eye diseases over time. As a result of advances in machine learning techniques, early detection of diabetic eye disease using an automated system brings substantial benefits over manual detection. A variety of advanced studies relating to the detection of diabetic eye disease have recently been published. This article presents a systematic survey of automated approaches to diabetic eye disease detection from several aspects, namely: i) available datasets, ii) image preprocessing techniques, iii) deep learning models and iv) performance evaluation metrics. The survey provides a comprehensive synopsis of diabetic eye disease detection approaches, including state of the art field approaches, which aim to provide valuable insight into research communities, healthcare professionals and patients with diabetes.","This review paper provides a comprehensive overview of the state of the art on Diabetic Eye Disease (DED) detection methods. To achieve this goal, a rigorous systematic review of relevant publications was conducted. After the final selection of relevant records, following the inclusion criteria and quality assessment, the studies have been analyzed from the perspectives of 1) Datasets used, 2) Image preprocessing techniques adopted and 3) Classification method employed. The works were categorized into the specific DED types, i.e. DR, Gl, DME and Ca for clarity and comparison. In terms of classification techniques, our review included studies that 1) Adopted TL, 2) Build DL network architecture and 3) Used combined DL and ML approach. Details of the findings obtained are included in Section VI. We have also identified several limitations associated with our study. First, we narrowed down the review conducted from April 2014 - January 2020 due to rapid advances in the field. Second, we limited our review to DL based approaches due to their state of the art performance, in particular on the image classification task. Finally, our review focused on a collection of predefined keywords that provides a thorough coverage of the DED area of detection but may not be exhaustive. Furthermore, we hope that our research can be further expanded in the future to include an all encompassing and up-to-date overview of the rapidly developing and challenging field of DED detection.","Automatic Detection of Diabetic Eye Disease Through Deep Learning Using Fundus Images: A SurveyDiabetic eye disease, diabetic retinopathy, deep leaning, glaucoma, image processing, macular edema, transfer learning.Diabetes Mellitus, or Diabetes, is a disease in which a person’s body fails to respond to insulin released by their pancreas, or it does not produce sufficient insulin. People suffering from diabetes are at high risk of developing various eye diseases over time. As a result of advances in machine learning techniques, early detection of diabetic eye disease using an automated system brings substantial benefits over manual detection. A variety of advanced studies relating to the detection of diabetic eye disease have recently been published. This article presents a systematic survey of automated approaches to diabetic eye disease detection from several aspects, namely: i) available datasets, ii) image preprocessing techniques, iii) deep learning models and iv) performance evaluation metrics. The survey provides a comprehensive synopsis of diabetic eye disease detection approaches, including state of the art field approaches, which aim to provide valuable insight into research communities, healthcare professionals and patients with diabetes.This review paper provides a comprehensive overview of the state of the art on Diabetic Eye Disease (DED) detection methods. To achieve this goal, a rigorous systematic review of relevant publications was conducted. After the final selection of relevant records, following the inclusion criteria and quality assessment, the studies have been analyzed from the perspectives of 1) Datasets used, 2) Image preprocessing techniques adopted and 3) Classification method employed. The works were categorized into the specific DED types, i.e. DR, Gl, DME and Ca for clarity and comparison. In terms of classification techniques, our review included studies that 1) Adopted TL, 2) Build DL network architecture and 3) Used combined DL and ML approach. Details of the findings obtained are included in Section VI. We have also identified several limitations associated with our study. First, we narrowed down the review conducted from April 2014 - January 2020 due to rapid advances in the field. Second, we limited our review to DL based approaches due to their state of the art performance, in particular on the image classification task. Finally, our review focused on a collection of predefined keywords that provides a thorough coverage of the DED area of detection but may not be exhaustive. Furthermore, we hope that our research can be further expanded in the future to include an all encompassing and up-to-date overview of the rapidly developing and challenging field of DED detection.Articles identified through database searches: PubMed: 225 Embase: 295 Literature on file: 39 | Duplicates removed: 271 Vv Title/abstracts reviewed: 288 Excluded: 225 Further articles from reference lists of identified literature: 14 Articles for final inclusion: 34% Figure | Preferred reporting items for systematic reviews and meta-analyses flow diagram. 2Of which 33 articles reported unique studies. ;+———__ Did not assess association of signs vs symptoms v Articles for full-text review: 63 | Excluded: 43 >>| Did not assess association of signs vs symptoms v",Deep Learning and Machine Learning,"This article discusses the use of machine learning techniques in the early detection of diabetic eye disease, which is a common complication of diabetes. The article provides a comprehensive review of automated approaches to detecting diabetic eye disease, including datasets, image preprocessing techniques, deep learning models, and performance evaluation metrics. The article also categorizes the studies based on the specific types of diabetic eye diseases, such as diabetic retinopathy, glaucoma, diabetic macular edema, and cataract. The review focuses on deep learning-based approaches and identifies limitations associated with the study, such as the narrow timeframe and predefined keywords used in the review. The article concludes by calling for further research in the rapidly developing field of diabetic eye disease detection.",Deep Learning and Machine Learning,Articles identified through database searches: PubMed: 225 Embase: 295 Literature on file: 39 | Duplicates removed: 271 Vv Title/abstracts reviewed: 288 Excluded: 225 Further articles from reference lists of identified literature: 14 Articles for final inclusion: 34% Figure | Preferred reporting items for systematic reviews and meta-analyses flow diagram. 2Of which 33 articles reported unique studies. ;+———__ Did not assess association of signs vs symptoms v Articles for full-text review: 63 | Excluded: 43 >>| Did not assess association of signs vs symptoms v,Medical Data Analysis
76,Querying large graphs in biomedicine with colored graphs and decomposition,"Large graphs , OLAP , Coloredgraphs , Betweenness.","In graph networks, graph structural analytics such as betweenness centrality has played an important role in finding the most central vertices in graph data. Hence, betweenness centrality has been heavily applied to discover the most important genes with respect to multiple diseases in biomedicine research. Considering color as a property of graph data to represent different categories for the nodes and edges in the graph, we may investigate the betweenness centrality of each colored subgraph composed of a specific color. However, as investigators may be interested in querying betweenness centrality on multiple combinations of the colored subgraphs, the total execution time on all the subgraphs may be excessively long, considering all the possible combinations. In addition, the performance could be worse when the size of the graph grows larger. In this research, we propose an approach to computing betweenness centrality by incorporating node colors and edge colors. We propose that the node with the highest betweenness centrality can be computed for a very large and colored graph by decomposing the graph into colored subgraphs and merging the result from the base cases. Furthermore, we compare our approach with the conventional approaches in the experiments, and we demonstrate that our scalable approach is more efficient when finding the global backbone node with the highest betweenness centrality.","Betweenness centrality has been one of the most important structure analytics in graph networks. Especially, biomedical scientists have applied betweenness centrality to discover the importance of gene nodes and find the most important genes in genetic networks. In this paper, we propose a scalable approach which is more efficient to compute betweenness centrality on graphs that is composed of multiple color subgraphs. Conventionally, when querying betweenness centrality on different subgraphs with different colors, we need to repetitively run the same algorithm on different subgraphs. However, this study propose a method to find the common BBN of multiple subgraphs by merging the results of each subgraph’s BBN. In addition, the method is further extended to a hierarchical approach to compute BBN for multiple colored subgraphs efficiently. With such, we can answer various combinations of subgraphs by computing the subgraphs’ BBNs and merging them to find the common BBN. This approach is more efficient than running the conventional approach on each of the combination over and over. We conduct experiments on the human disease graph data based on the human PPI network using the GWAS catalog. We compare the proposed approach with the conventional approach, and we discover that the performance is influenced by the number of subgraphs and the degree of how the subgraphs are overlapped. As a result, the greater the number of colored subgraphs, the better performance the SCB can have.","Querying large graphs in biomedicine with colored graphs and decompositionLarge graphs , OLAP , Coloredgraphs , Betweenness.In graph networks, graph structural analytics such as betweenness centrality has played an important role in finding the most central vertices in graph data. Hence, betweenness centrality has been heavily applied to discover the most important genes with respect to multiple diseases in biomedicine research. Considering color as a property of graph data to represent different categories for the nodes and edges in the graph, we may investigate the betweenness centrality of each colored subgraph composed of a specific color. However, as investigators may be interested in querying betweenness centrality on multiple combinations of the colored subgraphs, the total execution time on all the subgraphs may be excessively long, considering all the possible combinations. In addition, the performance could be worse when the size of the graph grows larger. In this research, we propose an approach to computing betweenness centrality by incorporating node colors and edge colors. We propose that the node with the highest betweenness centrality can be computed for a very large and colored graph by decomposing the graph into colored subgraphs and merging the result from the base cases. Furthermore, we compare our approach with the conventional approaches in the experiments, and we demonstrate that our scalable approach is more efficient when finding the global backbone node with the highest betweenness centrality.Betweenness centrality has been one of the most important structure analytics in graph networks. Especially, biomedical scientists have applied betweenness centrality to discover the importance of gene nodes and find the most important genes in genetic networks. In this paper, we propose a scalable approach which is more efficient to compute betweenness centrality on graphs that is composed of multiple color subgraphs. Conventionally, when querying betweenness centrality on different subgraphs with different colors, we need to repetitively run the same algorithm on different subgraphs. However, this study propose a method to find the common BBN of multiple subgraphs by merging the results of each subgraph’s BBN. In addition, the method is further extended to a hierarchical approach to compute BBN for multiple colored subgraphs efficiently. With such, we can answer various combinations of subgraphs by computing the subgraphs’ BBNs and merging them to find the common BBN. This approach is more efficient than running the conventional approach on each of the combination over and over. We conduct experiments on the human disease graph data based on the human PPI network using the GWAS catalog. We compare the proposed approach with the conventional approach, and we discover that the performance is influenced by the number of subgraphs and the degree of how the subgraphs are overlapped. As a result, the greater the number of colored subgraphs, the better performance the SCB can have.Anatomical structures of the retiAcademic Database FIGURE 4. Search and filter results: A. Query! (Q1) = diabetic eye disease, fundus images, image processing, image classification, deep learning, transfer learning; B. Query2 (Q2) = diabetic retinopathy, fundus images, image processing, image classification, deep learning, transfer learning; C. Query3 (Q3) = glaucoma, fundus images, image Processing, image classification, deep learning, transfer learning; D. Query4 (Q4) = diabetic macular edema, fundus it images, image processing, image classification, deep learning, transfer learning; E. Query5 (Q5) = cataract, fundus images, image processing, image classification, deep learning, transfer learning.FIGURE 2. Complications of DED in retina; A. Microaneurysms, narrow bulges (Diabetic Retinopathy), B. Optic nerve damage (Glaucoma), C. Exudates with retinal thickening (Diabetic Macular Edema), D. Degeneration of lens (Cataract). Survey Target Sao Filters s Critical Review , & Conclusion Reyes “Lh 3 Discussion FIGURE 3. Research method flowchart.",Medical Data Analysis,"This paper proposes a scalable approach to computing betweenness centrality in graph networks, incorporating node and edge colors to efficiently query on multiple combinations of colored subgraphs. The approach decomposes the graph into subgraphs, computes the betweenness centrality for each subgraph, and merges the results to find the common backbone node (BBN) of multiple subgraphs. Experiments on human disease graph data demonstrate the efficiency of the proposed approach compared to conventional methods. The study concludes that the proposed approach is more efficient for graphs composed of multiple colored subgraphs, and the performance is influenced by the number of subgraphs and their overlap.",Medical Data Analysis,"Anatomical structures of the retiAcademic Database FIGURE 4. Search and filter results: A. Query! (Q1) = diabetic eye disease, fundus images, image processing, image classification, deep learning, transfer learning; B. Query2 (Q2) = diabetic retinopathy, fundus images, image processing, image classification, deep learning, transfer learning; C. Query3 (Q3) = glaucoma, fundus images, image Processing, image classification, deep learning, transfer learning; D. Query4 (Q4) = diabetic macular edema, fundus it images, image processing, image classification, deep learning, transfer learning; E. Query5 (Q5) = cataract, fundus images, image processing, image classification, deep learning, transfer learning.FIGURE 2. Complications of DED in retina; A. Microaneurysms, narrow bulges (Diabetic Retinopathy), B. Optic nerve damage (Glaucoma), C. Exudates with retinal thickening (Diabetic Macular Edema), D. Degeneration of lens (Cataract). Survey Target Sao Filters s Critical Review , & Conclusion Reyes “Lh 3 Discussion FIGURE 3. Research method flowchart.",Deep Learning and Machine Learning
77,Radiology Imaging Scans for Early Diagnosis of Kidney Tumors: A Review of Data Analytics-Based Machine Learning and Deep Learning Approaches,Kidney Tumors; deep learning; artificial intelligence; machine learning; radiology imaging scans; early diagnosis,"Plenty of disease types exist in world communities that can be explained by humans’ lifestyles or the economic, social, genetic, and other factors of the country of residence. Recently, most research has focused on studying common diseases in the population to reduce death risks, take the best procedure for treatment, and enhance the healthcare level of the communities. Kidney Disease is one of the common diseases that have affected our societies. Sectionicularly Kidney Tumors (KT) are the 10th most prevalent tumor for men and women worldwide. Overall, the lifetime likelihood of developing a kidney tumor for males is about 1 in 466 (2.02 percent) and it is around 1 in 80 (1.03 percent) for females. Still, more research is needed on new diagnostic, early, and innovative methods regarding finding an appropriate treatment method for KT. Compared to the tedious and time-consuming traditional diagnosis, automatic detection algorithms of machine learning can save diagnosis time, improve test accuracy, and reduce costs. Previous studies have shown that deep learning can play a role in dealing with complex tasks, diagnosis and segmentation, and classification of Kidney Tumors, one of the most malignant tumors. The goals of this review article on deep learning in radiology imaging are to summarize what has already been accomplished, determine the techniques used by the researchers in previous years in diagnosing Kidney Tumors through medical imaging, and identify some promising future avenues, whether in terms of applications or technological developments, as well as identifying common problems, describing ways to expand the data set, summarizing the knowledge and best practices, and determining remaining challenges and future directions.","This review gives a deep investigation of recent studies of kidney tumor detection, in which it focuses on the classical machine learning techniques and deep learning approaches built for Kidney Tumors’ early detection based on radiology imaging scans. Furthermore, it covers machine learning techniques, deep learning approaches, and a comparison between Kidney Tumors’ early detection methods. Many studies investigated using databases of just a few images or data sets, and only a few used hundreds and thousands of images. Most of the studies were not comprehensive in covering the entire diagnosis of the tumor. Although many studies performed a predictor model and achieved high results, none implemented multi-models that performed more than one task that covered all tumor diagnosis, detection, segmentation, and classification. In general, deep learning’s application to kidney radiology imaging is still in its infancy. On the other hand, deep learning will continue to progress because it has many advantages over other imaging approaches, and attempts are being made to address the current challenges. The directions for future research revolve around using more images from previous studies and diagnosing all aspects of the tumor in one process by employing deep learning algorithms to create multi-models that performs detection, classification, segmentation, and other tasks; this helps the medical staff to diagnose the patient’s condition with high accuracy from all sides and to ensure that there is no shortage of diagnostic information, which leads to the selection of the appropriate treatment method. Moreover, it is possible to incorporate advanced data analytics-based AI techniques (i.e., machine and deep learning techniques) into radiology practice in the future. As given in the literature, various data analytics-based learning techniques can be improved and employed to help solve radiology imaging scans for the early diagnosis of Kidney Tumors. The early detection of KT has developed dramatically in reducing death rates, treating early, and producing preventive measures that reduce effects and overcome KT.","Radiology Imaging Scans for Early Diagnosis of Kidney Tumors: A Review of Data Analytics-Based Machine Learning and Deep Learning ApproachesKidney Tumors; deep learning; artificial intelligence; machine learning; radiology imaging scans; early diagnosisPlenty of disease types exist in world communities that can be explained by humans’ lifestyles or the economic, social, genetic, and other factors of the country of residence. Recently, most research has focused on studying common diseases in the population to reduce death risks, take the best procedure for treatment, and enhance the healthcare level of the communities. Kidney Disease is one of the common diseases that have affected our societies. Sectionicularly Kidney Tumors (KT) are the 10th most prevalent tumor for men and women worldwide. Overall, the lifetime likelihood of developing a kidney tumor for males is about 1 in 466 (2.02 percent) and it is around 1 in 80 (1.03 percent) for females. Still, more research is needed on new diagnostic, early, and innovative methods regarding finding an appropriate treatment method for KT. Compared to the tedious and time-consuming traditional diagnosis, automatic detection algorithms of machine learning can save diagnosis time, improve test accuracy, and reduce costs. Previous studies have shown that deep learning can play a role in dealing with complex tasks, diagnosis and segmentation, and classification of Kidney Tumors, one of the most malignant tumors. The goals of this review article on deep learning in radiology imaging are to summarize what has already been accomplished, determine the techniques used by the researchers in previous years in diagnosing Kidney Tumors through medical imaging, and identify some promising future avenues, whether in terms of applications or technological developments, as well as identifying common problems, describing ways to expand the data set, summarizing the knowledge and best practices, and determining remaining challenges and future directions.This review gives a deep investigation of recent studies of kidney tumor detection, in which it focuses on the classical machine learning techniques and deep learning approaches built for Kidney Tumors’ early detection based on radiology imaging scans. Furthermore, it covers machine learning techniques, deep learning approaches, and a comparison between Kidney Tumors’ early detection methods. Many studies investigated using databases of just a few images or data sets, and only a few used hundreds and thousands of images. Most of the studies were not comprehensive in covering the entire diagnosis of the tumor. Although many studies performed a predictor model and achieved high results, none implemented multi-models that performed more than one task that covered all tumor diagnosis, detection, segmentation, and classification. In general, deep learning’s application to kidney radiology imaging is still in its infancy. On the other hand, deep learning will continue to progress because it has many advantages over other imaging approaches, and attempts are being made to address the current challenges. The directions for future research revolve around using more images from previous studies and diagnosing all aspects of the tumor in one process by employing deep learning algorithms to create multi-models that performs detection, classification, segmentation, and other tasks; this helps the medical staff to diagnose the patient’s condition with high accuracy from all sides and to ensure that there is no shortage of diagnostic information, which leads to the selection of the appropriate treatment method. Moreover, it is possible to incorporate advanced data analytics-based AI techniques (i.e., machine and deep learning techniques) into radiology practice in the future. As given in the literature, various data analytics-based learning techniques can be improved and employed to help solve radiology imaging scans for the early diagnosis of Kidney Tumors. The early detection of KT has developed dramatically in reducing death rates, treating early, and producing preventive measures that reduce effects and overcome KT.Btw = 4+6+3+24+18412 =67 PEW =9t99 = 90 Btw = 11+48=59 (i)00 © 900 (a) . coarse node coarse node aN O-©—_@—_e-0 (e) coarse nodeShortest_path(BBN(G, y2), BBN(G; yaq)) Shortest_path| O/ C) BBN(G5 ua) BBN(Giu2) ©) Shortest_path(BBN(G;), BBN(G,)) somo G=G, UG, UG, UG,$ Behind=1 Behind=1 Behind=1 1 Behind=50 ! Behind=100 I I ! I Behind=100 Behind=100Execution time (ms) Execution time & Speedup 30000 28362 25000 20000 Speedup = 2.15 15000 13167 10000 5000 Human disease graph @ Brandes mSCBExecution time (ms) 30000 25000 20000 15000 10000 5000 479 479 Cancer 639 586 be Cardiovascular Execution time 305 338 Immmune system disease B Brandes mSCB 26243 9988 Metabolic disease 726 726 Le Nervous system disease",Deep Learning and Machine Learning,"This review article discusses the use of machine learning, particularly deep learning, in the detection of kidney tumors through radiology imaging scans. Kidney tumors are common and pose a significant threat to public health, and traditional diagnostic methods are time-consuming and costly. Deep learning algorithms can save diagnosis time, improve test accuracy, and reduce costs. The article highlights previous research in this area, including the techniques used, the data sets analyzed, and the limitations of the studies. The review identifies promising avenues for future research, such as creating multi-models that perform detection, classification, segmentation, and other tasks to diagnose all aspects of the tumor in one process, and incorporating advanced data analytics-based AI techniques into radiology practice in the future. The early detection of kidney tumors through deep learning algorithms can greatly reduce death rates, provide early treatment, and produce preventive measures that reduce the effects of the disease.",Deep Learning and Machine Learning,"Btw = 4+6+3+24+18412 =67 PEW =9t99 = 90 Btw = 11+48=59 (i)00 © 900 (a) . coarse node coarse node aN O-©—_@—_e-0 (e) coarse nodeShortest_path(BBN(G, y2), BBN(G; yaq)) Shortest_path| O/ C) BBN(G5 ua) BBN(Giu2) ©) Shortest_path(BBN(G;), BBN(G,)) somo G=G, UG, UG, UG,$ Behind=1 Behind=1 Behind=1 1 Behind=50 ! Behind=100 I I ! I Behind=100 Behind=100Execution time (ms) Execution time & Speedup 30000 28362 25000 20000 Speedup = 2.15 15000 13167 10000 5000 Human disease graph @ Brandes mSCBExecution time (ms) 30000 25000 20000 15000 10000 5000 479 479 Cancer 639 586 be Cardiovascular Execution time 305 338 Immmune system disease B Brandes mSCB 26243 9988 Metabolic disease 726 726 Le Nervous system disease",Medical Data Analysis
78,RAHM: Relation augmented hierarchical multi-task learning framework for reasonable medication stocking,"Preventive healthcare management , Reasonable medication stocking , Hierarchical multi-task learning , Long short-term memory networks.","As an important task in digital preventive healthcare management, especially in the secondary prevention stage, active medication stocking refers to the process of preparing necessary medications in advance according to the predicted disease progression of patients. However, predicting preventive or even life-saving medicine for each patient is a non-trivial task. Existing models usually overlook the implicit hierarchical relation between patient’s predicted diseases and medications, and mainly focus on single tasks (medication recommendation or disease prediction). To tackle this limitation, we propose a relation augmented hierarchical multi-task learning framework, named RAHM. which is capable of learning multi-level relation-aware patient representation for reasonable medication stocking. Specifically, the framework first leverages the underlying structural relations of Electronic Health Record (EHR) data to learn the low-level patient visit representation. Then, it uses a regular LSTM to encode the historical temporal disease information for disease-level patient representation learning. Further, a relation-aware LSTM (R-LSTM) is proposed to handle the relations between diseases and medication in longitudinal patient records, which can better integrate the historical information into the medication-level patient representation. In the learning process, two pseudo residual structures are introduced to mitigate the error propagation and preserve the valuable relation information of EHRs. To validate our method, extensive experiments have been conducted based on the real-world clinical dataset. The results demonstrate a consistent superiority of our framework over several baselines in suggesting reasonable stock medication.","Reasonable medication stocking is an essential task in preventive healthcare management, especially in the secondary prevention stage, which might prevent disease progression and even save patients’ lives. In our work, we propose a relation augmented hierarchical multi-task learning framework (RAHM), which can leverage the implicit relations between diseases and medications, to jointly predict the diseases for patients and recommend corresponding medications. Extensive experiments on a dataset demonstrate that RAHM performs better on disease prediction and medication recommendation for patients compared with previous methods. And we were able to find one case that makes sense to us via a case study, which shows that the recommended medications could find corresponding reasons from predicted diseases. As for the next stage of the research, we aim to tackle the model limitations and implement the model in medication management systems for hospitals or clinics.","RAHM: Relation augmented hierarchical multi-task learning framework for reasonable medication stockingPreventive healthcare management , Reasonable medication stocking , Hierarchical multi-task learning , Long short-term memory networks.As an important task in digital preventive healthcare management, especially in the secondary prevention stage, active medication stocking refers to the process of preparing necessary medications in advance according to the predicted disease progression of patients. However, predicting preventive or even life-saving medicine for each patient is a non-trivial task. Existing models usually overlook the implicit hierarchical relation between patient’s predicted diseases and medications, and mainly focus on single tasks (medication recommendation or disease prediction). To tackle this limitation, we propose a relation augmented hierarchical multi-task learning framework, named RAHM. which is capable of learning multi-level relation-aware patient representation for reasonable medication stocking. Specifically, the framework first leverages the underlying structural relations of Electronic Health Record (EHR) data to learn the low-level patient visit representation. Then, it uses a regular LSTM to encode the historical temporal disease information for disease-level patient representation learning. Further, a relation-aware LSTM (R-LSTM) is proposed to handle the relations between diseases and medication in longitudinal patient records, which can better integrate the historical information into the medication-level patient representation. In the learning process, two pseudo residual structures are introduced to mitigate the error propagation and preserve the valuable relation information of EHRs. To validate our method, extensive experiments have been conducted based on the real-world clinical dataset. The results demonstrate a consistent superiority of our framework over several baselines in suggesting reasonable stock medication.Reasonable medication stocking is an essential task in preventive healthcare management, especially in the secondary prevention stage, which might prevent disease progression and even save patients’ lives. In our work, we propose a relation augmented hierarchical multi-task learning framework (RAHM), which can leverage the implicit relations between diseases and medications, to jointly predict the diseases for patients and recommend corresponding medications. Extensive experiments on a dataset demonstrate that RAHM performs better on disease prediction and medication recommendation for patients compared with previous methods. And we were able to find one case that makes sense to us via a case study, which shows that the recommended medications could find corresponding reasons from predicted diseases. As for the next stage of the research, we aim to tackle the model limitations and implement the model in medication management systems for hospitals or clinics.Artificial Intelligence De) Deep Learningx2 — input (Gaza4s) 11.1000) 200) (1280) 7718) 27320) raago) (4.14112) 5050.24) (222840) 12,112.22) 2nigse) 6088.28) (24204)Li i-e-e im a A bla —— Clee@ea v _ A ba A © Concat > Conv > Convi+ BN) + ReLu {Y Max poo! AUpConvInput 209°290""2, Output 882048 Input: 299*299°3, Output: 8°8°2048 an by en Final Part: 8°8°2048 > 1001 i Gricomne= / hat problem are we trying to solve? -— 2-8 @Ya \ 1. Problem definition om I Whatdata_Whatdefines Whatfeatures Whatkind of what have we tried! dowehave? success? should we model? model should hat else can we try? 3.Evaluation 4.Features 5, Modeling 6. Experiments Iterative processBenign Tumor (BT) Malignant Tumor (MT)Conv.Layer Fee} +f fay correc ax pasng yer X 5) (rice) “| rte) cre (ore)input ——>|] _ Conv | >] max poot | —» 777428 3°3+28 Inception 79 « con) < oy vera ee | c= — Sota |__ sot ouputPooling i QP} [0 li, O © cows ees. we OutputTnput Image Cam Pooling(prediction) Input (features) Hidden Layers lots of layers - ""deep learning”",Artificial Neural Network,"The article discusses the importance of active medication stocking in preventive healthcare management, especially in the secondary prevention stage, and the difficulty of predicting preventive or life-saving medication for each patient. The proposed solution is a relation augmented hierarchical multi-task learning framework (RAHM) that can learn multi-level relation-aware patient representation for reasonable medication stocking. The framework leverages the underlying structural relations of Electronic Health Record (EHR) data to learn the low-level patient visit representation, encodes the historical temporal disease information for disease-level patient representation learning, and handles the relations between diseases and medication in longitudinal patient records. The results of extensive experiments on a real-world clinical dataset demonstrate that RAHM performs better on disease prediction and medication recommendation for patients compared to previous methods. The authors aim to implement the model in medication management systems for hospitals or clinics.",Deep Learning and Machine Learning,"Artificial Intelligence De) Deep Learningx2 — input (Gaza4s) 11.1000) 200) (1280) 7718) 27320) raago) (4.14112) 5050.24) (222840) 12,112.22) 2nigse) 6088.28) (24204)Li i-e-e im a A bla —— Clee@ea v _ A ba A © Concat > Conv > Convi+ BN) + ReLu {Y Max poo! AUpConvInput 209°290""2, Output 882048 Input: 299*299°3, Output: 8°8°2048 an by en Final Part: 8°8°2048 > 1001 i Gricomne= / hat problem are we trying to solve? -— 2-8 @Ya \ 1. Problem definition om I Whatdata_Whatdefines Whatfeatures Whatkind of what have we tried! dowehave? success? should we model? model should hat else can we try? 3.Evaluation 4.Features 5, Modeling 6. Experiments Iterative processBenign Tumor (BT) Malignant Tumor (MT)Conv.Layer Fee} +f fay correc ax pasng yer X 5) (rice) “| rte) cre (ore)input ——>|] _ Conv | >] max poot | —» 777428 3°3+28 Inception 79 « con) < oy vera ee | c= — Sota |__ sot ouputPooling i QP} [0 li, O © cows ees. we OutputTnput Image Cam Pooling(prediction) Input (features) Hidden Layers lots of layers - ""deep learning”",Deep Learning and Machine Learning
79,Research and Analysis of Sport Medical Data Processing Algorithms Based on Deep Learning and Internet of Things,"Sport medicine, sport medicine big data, tensor convolution self-coding deep learning algorithm, cloud-end fusion hardware-in-the-loop simulation model.","With the development of computer and information technology, more and more data and image information are generated in medical field. Sports medicine, as an important branch of medical cause, is responsible for ensuring national sports safety and rehabilitation after injury. How to use a large number of sports medical data and cases to accurately analyze and mine useful data and information has become an important research direction of sports medical data processing and mining. This paper will focus on the information mining and analysis of large sports medical data, focusing on the loss of training mode and the accuracy of convolution algorithm. In order to achieve effective prediction and risk assessment of sports medicine-related diseases, this paper starts with the improved convolutional neural network deep learning algorithm, and adopts the resampling algorithm with self-adjusting function, supplemented by tensor convolution self-coding algorithm. Ural network model assists multi-dimensional data analysis of sports medicine. Finally, in order to build an intelligent medical data platform for sports medicine, this paper innovatively proposes a cloud-based hardware-in-the-loop simulation model. Experiments show that this method provides reference and technical support for the realization of a real cloud-based fusion system.","With the increase of sports medical data year by year, the requirement of data processing and analysis technology is becoming higher and higher. With the huge increase of data, traditional deep learning algorithm appears weak and inefficient in sports medical data mining. Therefore, efficient and precise sports medical data mining methods are very important and meaningful. In order to solve the above problems, this paper systematically analyses and studies the disadvantages of the current convolution neural network algorithm combined with sports medical data, and improves the convolution neural network algorithm based on the resampling algorithm with self-adjusting function. In order to process huge amounts of data more accurately, this paper also innovatively introduces the auxiliary model tensor convolution self-coding neural network model to realize the analysis and processing of multi-dimensional data of sports medicine. Finally, in order to further realize the construction of intelligent medical data platform for sports medicine, this paper designs and builds a cloud-based fusion hardware-in-the-loop simulation model, and carries out systematic analysis and Research on the model, thus providing technical support and experience for the real cloud-based fusion hardware-in-theloop system. Overall, the work of this paper is relatively complete, but due to the limited space, the follow-up work will focus on the application and analysis of improved convolutional neural network in time series data feature learning, in order to achieve efficient processing and analysis of sport medical image data.","Research and Analysis of Sport Medical Data Processing Algorithms Based on Deep Learning and Internet of ThingsSport medicine, sport medicine big data, tensor convolution self-coding deep learning algorithm, cloud-end fusion hardware-in-the-loop simulation model.With the development of computer and information technology, more and more data and image information are generated in medical field. Sports medicine, as an important branch of medical cause, is responsible for ensuring national sports safety and rehabilitation after injury. How to use a large number of sports medical data and cases to accurately analyze and mine useful data and information has become an important research direction of sports medical data processing and mining. This paper will focus on the information mining and analysis of large sports medical data, focusing on the loss of training mode and the accuracy of convolution algorithm. In order to achieve effective prediction and risk assessment of sports medicine-related diseases, this paper starts with the improved convolutional neural network deep learning algorithm, and adopts the resampling algorithm with self-adjusting function, supplemented by tensor convolution self-coding algorithm. Ural network model assists multi-dimensional data analysis of sports medicine. Finally, in order to build an intelligent medical data platform for sports medicine, this paper innovatively proposes a cloud-based hardware-in-the-loop simulation model. Experiments show that this method provides reference and technical support for the realization of a real cloud-based fusion system.With the increase of sports medical data year by year, the requirement of data processing and analysis technology is becoming higher and higher. With the huge increase of data, traditional deep learning algorithm appears weak and inefficient in sports medical data mining. Therefore, efficient and precise sports medical data mining methods are very important and meaningful. In order to solve the above problems, this paper systematically analyses and studies the disadvantages of the current convolution neural network algorithm combined with sports medical data, and improves the convolution neural network algorithm based on the resampling algorithm with self-adjusting function. In order to process huge amounts of data more accurately, this paper also innovatively introduces the auxiliary model tensor convolution self-coding neural network model to realize the analysis and processing of multi-dimensional data of sports medicine. Finally, in order to further realize the construction of intelligent medical data platform for sports medicine, this paper designs and builds a cloud-based fusion hardware-in-the-loop simulation model, and carries out systematic analysis and Research on the model, thus providing technical support and experience for the real cloud-based fusion hardware-in-theloop system. Overall, the work of this paper is relatively complete, but due to the limited space, the follow-up work will focus on the application and analysis of improved convolutional neural network in time series data feature learning, in order to achieve efficient processing and analysis of sport medical image data.Medication Risl recommendation (b) module Disease prediction module Medical codes relational embedding Module Fig. 2. RAHM Framework Overview. It mainly contains three parts: medical codes relational embedding module, disease prediction module and medication re- commendation module. First, we embed the high dimensional medical codes vector to a dense low-level visit representation using the Transformer encoder. Second, we use LSTM encoder and a pseudo residual structure (PRS) to obtain the disease-level patient representation p{, which are further used to predict the disease we. Then, we design a relation-aware LSTM (R-LSTM), in which the cell state of LSTM is regulated by the integrated relational decay structure with the help of attention mechanism (Eq. (8), Eq. (6) and Eq. (7)). Finally, the output of R-LSTM h,, concatenates with v' transmitted by PRS to form the medication-level patient re- presentation Di, to recommend the medication. for prevention.Reasonable medication stocking Emergency C \ Diseases Life-saving ‘Asthma Ipratropium Bromide e@ How? &. <—~ Standing = [B55] Hypertension Nifedipine Ibuprofen First-aid Influenza ip! : Medications Patient Preventive Fig. 1. Illustration of our problem background. In some clinical scenarios, hospitals, clinics and patients need reasonable medication stocking, which should consider the inherent relationships between diseases and medications..© Medications | +» Vitamin B1 and compound with Be. Diseases Veins and Lymphatics<-+- soph as ye ++Drugs for Peptic Ulcer Disease sophagus and Stomach*~T + Hypothalamic Hormones Neurotic Disorders#+-~ : Irrigating SoLutions Blood-Forming Organsv.. » Drugs for CoNstipation **Vitamin B12 and Folic AcId Metabolic Disorders’ I I I I I I I I oe a I Digestive System «. I I +Other Mineral Supplements Fig. 4. Case Study: the implicit relations between predicted diseases and cor- responding medications from a real patient.Prediction 2 poa | pa | ior | imi | wp2 | im2 uos_| ims | toa | ima 410-414 NO2B 570-579 AQGA 570-579 AQGA 510-519 NO1A 570-579 NO2B 40-49 A02B 510-519 C10A 420-429 AO7A 480-488 AQIA 510-519 AQ6A 1.0 401-405 AOLA 280-289 NO3A 790-796 A12C 580-589 ADGA 420-429 BOSC 510-519 AGA 530-539 ROIA 270-279 A10A 790-796 CO1C 790-796 A10A 480-488 BOSC 560-569 RO3A 280-289 BO1A 110-118 AO7A 580-589 CO3C 208 0.670 580-589 AQTA 249-259 NO2B 249-259 AQIA 740-759 BOSC 290-294 BOA asf "" 249-259 A12C 420-429 A02B. V40-V49_ NOSC 270-279 A12C 270-279 COTA 3 270-279 BOA V40-V49 BOSC 530-539 C10A 780-789 BOIA 280-289 ADIA = 0.6 420-429 CO7A 300-316 NOGA RO3A 401-405 C10A 249-259 C10A | 780-789 C10A ROIA 249-259 AQ2B 930-939 ROA g 380-389 NO6A ‘A02B 420-429 NOC 300-316 NO3A 5.0.4] 0.317 RO3A NO3A 415-417 ROA V40-V49_RO3A = ROA A128 200-316 NOSA A128 3g AoA NNoza vao-vao A022 02 co1c No2B 393-398 NOGA NOGA 0.013 predicted disease PM: predicted medication Disease codes: ICD-10 0.0 TI TD 3 label for medication LD: label for disease Medication codes: ATC (a) Attention weights (b) Attentive process analysis in R-LSTM. Fig. 3. Attention visualization in R-LSTM. (a) Attention weights. The attention score measures the influence degree of historical diseases. (b) Attentive process analysis in R-LSTM. Three critical influence process is used to interpret the attentive influence process through the concrete case.",Deep Learning and Machine Learning,"This paper focuses on the analysis and mining of large sports medical data, aiming to achieve effective prediction and risk assessment of sports medicine-related diseases. The paper proposes an improved convolutional neural network algorithm based on the resampling algorithm with self-adjusting function, supplemented by the tensor convolution self-coding algorithm. The paper also introduces a cloud-based hardware-in-the-loop simulation model to build an intelligent medical data platform for sports medicine. The experiments show that this method provides reference and technical support for the realization of a real cloud-based fusion system. The paper concludes that efficient and precise sports medical data mining methods are important and meaningful due to the increase of sports medical data year by year, and suggests further work focusing on the application and analysis of the improved convolutional neural network in time series data feature learning.",Medical Data Analysis,"Medication Risl recommendation (b) module Disease prediction module Medical codes relational embedding Module Fig. 2. RAHM Framework Overview. It mainly contains three parts: medical codes relational embedding module, disease prediction module and medication re- commendation module. First, we embed the high dimensional medical codes vector to a dense low-level visit representation using the Transformer encoder. Second, we use LSTM encoder and a pseudo residual structure (PRS) to obtain the disease-level patient representation p{, which are further used to predict the disease we. Then, we design a relation-aware LSTM (R-LSTM), in which the cell state of LSTM is regulated by the integrated relational decay structure with the help of attention mechanism (Eq. (8), Eq. (6) and Eq. (7)). Finally, the output of R-LSTM h,, concatenates with v' transmitted by PRS to form the medication-level patient re- presentation Di, to recommend the medication. for prevention.Reasonable medication stocking Emergency C \ Diseases Life-saving ‘Asthma Ipratropium Bromide e@ How? &. <—~ Standing = [B55] Hypertension Nifedipine Ibuprofen First-aid Influenza ip! : Medications Patient Preventive Fig. 1. Illustration of our problem background. In some clinical scenarios, hospitals, clinics and patients need reasonable medication stocking, which should consider the inherent relationships between diseases and medications..© Medications | +» Vitamin B1 and compound with Be. Diseases Veins and Lymphatics<-+- soph as ye ++Drugs for Peptic Ulcer Disease sophagus and Stomach*~T + Hypothalamic Hormones Neurotic Disorders#+-~ : Irrigating SoLutions Blood-Forming Organsv.. » Drugs for CoNstipation **Vitamin B12 and Folic AcId Metabolic Disorders’ I I I I I I I I oe a I Digestive System «. I I +Other Mineral Supplements Fig. 4. Case Study: the implicit relations between predicted diseases and cor- responding medications from a real patient.Prediction 2 poa | pa | ior | imi | wp2 | im2 uos_| ims | toa | ima 410-414 NO2B 570-579 AQGA 570-579 AQGA 510-519 NO1A 570-579 NO2B 40-49 A02B 510-519 C10A 420-429 AO7A 480-488 AQIA 510-519 AQ6A 1.0 401-405 AOLA 280-289 NO3A 790-796 A12C 580-589 ADGA 420-429 BOSC 510-519 AGA 530-539 ROIA 270-279 A10A 790-796 CO1C 790-796 A10A 480-488 BOSC 560-569 RO3A 280-289 BO1A 110-118 AO7A 580-589 CO3C 208 0.670 580-589 AQTA 249-259 NO2B 249-259 AQIA 740-759 BOSC 290-294 BOA asf "" 249-259 A12C 420-429 A02B. V40-V49_ NOSC 270-279 A12C 270-279 COTA 3 270-279 BOA V40-V49 BOSC 530-539 C10A 780-789 BOIA 280-289 ADIA = 0.6 420-429 CO7A 300-316 NOGA RO3A 401-405 C10A 249-259 C10A | 780-789 C10A ROIA 249-259 AQ2B 930-939 ROA g 380-389 NO6A ‘A02B 420-429 NOC 300-316 NO3A 5.0.4] 0.317 RO3A NO3A 415-417 ROA V40-V49_RO3A = ROA A128 200-316 NOSA A128 3g AoA NNoza vao-vao A022 02 co1c No2B 393-398 NOGA NOGA 0.013 predicted disease PM: predicted medication Disease codes: ICD-10 0.0 TI TD 3 label for medication LD: label for disease Medication codes: ATC (a) Attention weights (b) Attentive process analysis in R-LSTM. Fig. 3. Attention visualization in R-LSTM. (a) Attention weights. The attention score measures the influence degree of historical diseases. (b) Attentive process analysis in R-LSTM. Three critical influence process is used to interpret the attentive influence process through the concrete case.",Deep Learning and Machine Learning
80,"Analysis and best parameters selection 
for person recognition based on gait model 
using CNN algorithm and image augmentation"," Person recognition, Convolution neural network, Gait model, Deep 
learning, Image augmentation","Person Recognition based on Gait Model (PRGM) and motion features is are indeed 
a challenging and novel task due to their usages and to the critical issues of human 
pose variation, human body occlusion, camera view variation, etc. In this project, a 
deep convolution neural network (CNN) was modifed and adapted for person recognition with Image Augmentation (IA) technique depending on gait features. Adaptation 
aims to get best values for CNN parameters to get best CNN model. In Addition to 
the CNN parameters Adaptation, the design of CNN model itself was adapted to get 
best model structure; Adaptation in the design was afected the type, the number of 
layers in CNN and normalization between them. After choosing best parameters and 
best design, Image augmentation was used to increase the size of train dataset with 
many copies of the image to boost the number of diferent images that will be used to 
train Deep learning algorithms. The tests were achieved using known dataset (Market 
dataset). The dataset contains sequential pictures of people in diferent gait status. The 
image in CNN model as matrix is extracted to many images or matrices by the convolution, so dataset size may be bigger by hundred times to make the problem a big data 
issue. In this project, results show that adaptation has improved the accuracy of person 
recognition using gait model comparing to model without adaptation. In addition, 
dataset contains images of person carrying things. IA technique improved the model 
to be robust to some variations such as image dimensions (quality and resolution), 
rotations and carried things by persons. Results for 200 persons recognition, validation 
accuracy was about 82% without IA and 96.23 with IA. For 800 persons recognition, 
validation accuracy was 93.62% without IA.","In summary, Tis work proposed simple and robust model for person recognition using 
gait model features based on CNN algorithm, this model resulted with edits in the 
design of CNN model and choosing hyper parameters for some parts of CNN model. 
Tis study also introduce Image augmentation in recognition; that helps to make the 
simple models robust to some changes in images of persons, it generate images of the 
same frame or view with diferent conditions. Te fnal model was validated and it performed well and better than background studies.We can think about some points:
• Improve person recognition by rebuilding implemented methods can be rebuilt with 
other batch normalization modes and with some pre-processing steps for data-set.
• Using genetic algorithm for some parameters; it needs more processing time and 
high performance processors.
• Search deeply in Fully connected layer to fgure out the validity of changing activation function, or manually select activation function.
• For IA, new conditions can be added to generate more images that handling other 
variation in images.
Te main efective scope is to implement it in real time system for real useful goals","Analysis and best parameters selection 
for person recognition based on gait model 
using CNN algorithm and image augmentation Person recognition, Convolution neural network, Gait model, Deep 
learning, Image augmentationPerson Recognition based on Gait Model (PRGM) and motion features is are indeed 
a challenging and novel task due to their usages and to the critical issues of human 
pose variation, human body occlusion, camera view variation, etc. In this project, a 
deep convolution neural network (CNN) was modifed and adapted for person recognition with Image Augmentation (IA) technique depending on gait features. Adaptation 
aims to get best values for CNN parameters to get best CNN model. In Addition to 
the CNN parameters Adaptation, the design of CNN model itself was adapted to get 
best model structure; Adaptation in the design was afected the type, the number of 
layers in CNN and normalization between them. After choosing best parameters and 
best design, Image augmentation was used to increase the size of train dataset with 
many copies of the image to boost the number of diferent images that will be used to 
train Deep learning algorithms. The tests were achieved using known dataset (Market 
dataset). The dataset contains sequential pictures of people in diferent gait status. The 
image in CNN model as matrix is extracted to many images or matrices by the convolution, so dataset size may be bigger by hundred times to make the problem a big data 
issue. In this project, results show that adaptation has improved the accuracy of person 
recognition using gait model comparing to model without adaptation. In addition, 
dataset contains images of person carrying things. IA technique improved the model 
to be robust to some variations such as image dimensions (quality and resolution), 
rotations and carried things by persons. Results for 200 persons recognition, validation 
accuracy was about 82% without IA and 96.23 with IA. For 800 persons recognition, 
validation accuracy was 93.62% without IA.In summary, Tis work proposed simple and robust model for person recognition using 
gait model features based on CNN algorithm, this model resulted with edits in the 
design of CNN model and choosing hyper parameters for some parts of CNN model. 
Tis study also introduce Image augmentation in recognition; that helps to make the 
simple models robust to some changes in images of persons, it generate images of the 
same frame or view with diferent conditions. Te fnal model was validated and it performed well and better than background studies.We can think about some points:
• Improve person recognition by rebuilding implemented methods can be rebuilt with 
other batch normalization modes and with some pre-processing steps for data-set.
• Using genetic algorithm for some parameters; it needs more processing time and 
high performance processors.
• Search deeply in Fully connected layer to fgure out the validity of changing activation function, or manually select activation function.
• For IA, new conditions can be added to generate more images that handling other 
variation in images.
Te main efective scope is to implement it in real time system for real useful goalsImproving Data Processing Algorithms of Convolutional Neural Networks O98 VSN I ws | e@0c0e : $83883 Sports | eee @@ Medical | Data Point | Electronie| health records Blood pressure sensor oe Brain Detectioy a ae ey a 3 + blood a skin sensor aD wearable sensor FIGURE 11. Hardware-in-the-loop simulation architecture proposed in this paper.ty uw request excessive Sports Medical Cloud Data Center send data ' Electronic medical record: FIGURE 12. Network layer design diagram.‘Output Characteristic Map Input Characteristic Map ‘Tensor reconstruction ‘Output Characteristic Map Characteristic Map FIGURE 9. Characteristic sketch of convolutional self-codii learning. in-depthSOmaps 30°30 5omaps 28°28 SOmaps nei S0maps 66 500 features sOmaps 60°60 FIGURE 10. Structural diagram of improved convolutional neural network.O’ re-input Encode f Decode f” FIGURE 8. Schematic diagram of self-encoder learning image features.0,064 [Average delay (s) 0.060 0.058 0.056 0.054 =O 0.03s 0.050 —a— 0.04 —3— 0.05s —— 0.06s 0.048 800 900 1000 1100 1200 1300 = 1400 1500 Packet interval (B) FIGURE 14. Effect of packet size and transmission rate on maximum delay.0.064 | Average delay (s) 0.060 0.058 0.056 0.054 —@- 13008 0.050 ~-Me-- 1200B ~-K3-- 1100B —%— 10008 0.048 800 900 1000 1100 :1200--1300 1400-1500 Packet interval (B) FIGURE 15. Overall delay of moving medical data from data source node to data center storage in cloud-end hardware-in-the-loop simulation model.Accurac io Improved | Traditional! neural convolution convolution, network | learning FIGURE 17. Comparing and analyzing diagrams of diagnostic accuracy of four models.0,064 [Average delay (8) 0.060 0.058 “ 0.056 0.054 0.050 0.048 800 900 1000 1100 1200 1300 = 1400-1500 Packet interval (B) @) 0.085 | Average delay (s) 0.080 0.075 0.070 0.065 Packet interval (B) (b) FIGURE 13. (a). Gateway delay breakdown diagram with 600 KB packet. (b). Gateway delay breakdown diagram with 600 KB packet.Prestraining Input 128*128 y Shallow model Shallow model Shallow model for 128 for 256 for 512 a oe FIGURE 7. Training data process of improved neural network self-adjusting resampling algorithm.5*5.conv,32/2, 3*3.conv,32 Seon 62 a 3*3.conv,64 3*3.conv,256 3*3.conv,512 (eabisxcccla] 33.00 256 eon S12 3*3.conv,128 3*3.conv,256 3*3.conv,512 Max pool Block B- 3°3.conv,128 Block C 3*3.conv.128 Block A FIGURE 5. Hierarchical modular diagram of convolutional neural network.Probability of different classifications Softmax FIGURE 3. Data processing flow diagram of convolution part of convolution neural network.convolution pooling Addpadding output put CONVOLUTION NEURAL NETWORK ARCHITECTURE FIGURE 2. Structural chart of convolutional neural network algorithms in deep learning algorithms.w*sntel Convolution of the n-th data FIGURE 4. SJ convolution of the nth data in convolution layer.Cl:deal with 7 Intput First layer 5&4 Third layer layer After dealin; After pooling After . Full linki pooling ull linking ha FIGURE 1. Network layer architecture of universal convolutional neural network.",Person recognition,"This project proposes a deep convolutional neural network (CNN) model with image augmentation (IA) technique for person recognition using gait features. The model was adapted to improve its performance, including the CNN parameters and design. The IA technique was used to increase the dataset size and make the model robust to variations in the images. The results show that the adapted model with IA outperformed the model without adaptation in person recognition accuracy. The study suggests further improvements in the model, such as using genetic algorithms, exploring activation functions, and adding more IA conditions. The proposed model has potential applications in real-time systems for person recognition.",Object and Sentiment Recognition,"Improving Data Processing Algorithms of Convolutional Neural Networks O98 VSN I ws | e@0c0e : $83883 Sports | eee @@ Medical | Data Point | Electronie| health records Blood pressure sensor oe Brain Detectioy a ae ey a 3 + blood a skin sensor aD wearable sensor FIGURE 11. Hardware-in-the-loop simulation architecture proposed in this paper.ty uw request excessive Sports Medical Cloud Data Center send data ' Electronic medical record: FIGURE 12. Network layer design diagram.‘Output Characteristic Map Input Characteristic Map ‘Tensor reconstruction ‘Output Characteristic Map Characteristic Map FIGURE 9. Characteristic sketch of convolutional self-codii learning. in-depthSOmaps 30°30 5omaps 28°28 SOmaps nei S0maps 66 500 features sOmaps 60°60 FIGURE 10. Structural diagram of improved convolutional neural network.O’ re-input Encode f Decode f” FIGURE 8. Schematic diagram of self-encoder learning image features.0,064 [Average delay (s) 0.060 0.058 0.056 0.054 =O 0.03s 0.050 —a— 0.04 —3— 0.05s —— 0.06s 0.048 800 900 1000 1100 1200 1300 = 1400 1500 Packet interval (B) FIGURE 14. Effect of packet size and transmission rate on maximum delay.0.064 | Average delay (s) 0.060 0.058 0.056 0.054 —@- 13008 0.050 ~-Me-- 1200B ~-K3-- 1100B —%— 10008 0.048 800 900 1000 1100 :1200--1300 1400-1500 Packet interval (B) FIGURE 15. Overall delay of moving medical data from data source node to data center storage in cloud-end hardware-in-the-loop simulation model.Accurac io Improved | Traditional! neural convolution convolution, network | learning FIGURE 17. Comparing and analyzing diagrams of diagnostic accuracy of four models.0,064 [Average delay (8) 0.060 0.058 “ 0.056 0.054 0.050 0.048 800 900 1000 1100 1200 1300 = 1400-1500 Packet interval (B) @) 0.085 | Average delay (s) 0.080 0.075 0.070 0.065 Packet interval (B) (b) FIGURE 13. (a). Gateway delay breakdown diagram with 600 KB packet. (b). Gateway delay breakdown diagram with 600 KB packet.Prestraining Input 128*128 y Shallow model Shallow model Shallow model for 128 for 256 for 512 a oe FIGURE 7. Training data process of improved neural network self-adjusting resampling algorithm.5*5.conv,32/2, 3*3.conv,32 Seon 62 a 3*3.conv,64 3*3.conv,256 3*3.conv,512 (eabisxcccla] 33.00 256 eon S12 3*3.conv,128 3*3.conv,256 3*3.conv,512 Max pool Block B- 3°3.conv,128 Block C 3*3.conv.128 Block A FIGURE 5. Hierarchical modular diagram of convolutional neural network.Probability of different classifications Softmax FIGURE 3. Data processing flow diagram of convolution part of convolution neural network.convolution pooling Addpadding output put CONVOLUTION NEURAL NETWORK ARCHITECTURE FIGURE 2. Structural chart of convolutional neural network algorithms in deep learning algorithms.w*sntel Convolution of the n-th data FIGURE 4. SJ convolution of the nth data in convolution layer.Cl:deal with 7 Intput First layer 5&4 Third layer layer After dealin; After pooling After . Full linki pooling ull linking ha FIGURE 1. Network layer architecture of universal convolutional neural network.",Medical Data Analysis
81,"Influence of Thermal Imagery Resolution on
Accuracy of Deep Learning based Face Recognition
","ace recognition, thermal imagery, deep neural
networks, image enhancement","Human-system interactions frequently require a
retrieval of the key context information about the user and
the environment. Image processing techniques have been widely
applied in this area, providing details about recognized objects,
people and actions. Considering remote diagnostics solutions, e.g.
non-contact vital signs estimation and smart home monitoring
systems that utilize person’s identity, security is a very important
factor. Thus, thermal imaging has become more and more
popular, as it does not reveal features that are often used for
person recognition, i.e. sharp edges, clear changes of pixel values
between areas, etc. On the other hand, there are much more
visible light data available for deep model training. Taking it
into account, person recognition from thermography is much
more challenging due to specific characteristics (blurring and
smooth representation of features) and small amount of training
data. Moreover, when low resolution data is used, features
become even less visible, so this problem may become more
difficult. This study focuses on verifying whether model trained
to extract important facial embedding from RGB images can
perform equally well if applied to thermal domain, without
additional re-training. We also perform a set of experiments
aim at evaluating the influence of resolution degradation by
down-scaling images on the recognition accuracy. In addition,
we present deep super-resolution (SR) model that by enhancing
donw-scaled images can improve results for data acquired in
scenarios that simulate real-life environment, i.e. mimicking facial
expressions and performing head motions. Preliminary results
proved that in such cases SR helps to increase accuracy by
6.5% for data 8 times smaller than original images. It has
also been shown that it is possible to accurately recognize even
40 volunteers using only 4 images per person as a reference
embedding. Thus, the initial profiles can be easily created in a
real time, what is an additional advantage considering a solution
setup in a new environment.","This study aimed at verifying whether image resolution
has influence on person recognition task using facial features
embedding gathered from thermal imagery. In addition, it was
evaluated if model trained to extract high frequency components (on RGB images) will be able to generate a meaningful
person embedding from thermal data that is characterized by
blurring and smoothness due to the heat flow in objects. The
preliminary results proved the need to enhance image resolution in order to achieve a high accuracy of person recognition.
The presented Super-Resolution solution allowed to improve
results of person recognition from images downscaled with
bicubic interpolation by 8 % for the resizing scale of 4 on
the IRIS dataset. Yet, for our database that assumed strictly
defined measurement conditions (no movements, volunteer
loking toward the camera) we did not observe any gain of
performance.
Thus, we would like to perform similar research on data
collected by us but during various measuring scenarios, e.g.
volunteers turning their head horizontally and vertically. In
future study, we would also like to evaluate the proposed
approach of person recognition on images with original bit
resolution (14-bit represented as 16-bit image to avoid lossy
conversion to 8-bit data).","Influence of Thermal Imagery Resolution on
Accuracy of Deep Learning based Face Recognition
ace recognition, thermal imagery, deep neural
networks, image enhancementHuman-system interactions frequently require a
retrieval of the key context information about the user and
the environment. Image processing techniques have been widely
applied in this area, providing details about recognized objects,
people and actions. Considering remote diagnostics solutions, e.g.
non-contact vital signs estimation and smart home monitoring
systems that utilize person’s identity, security is a very important
factor. Thus, thermal imaging has become more and more
popular, as it does not reveal features that are often used for
person recognition, i.e. sharp edges, clear changes of pixel values
between areas, etc. On the other hand, there are much more
visible light data available for deep model training. Taking it
into account, person recognition from thermography is much
more challenging due to specific characteristics (blurring and
smooth representation of features) and small amount of training
data. Moreover, when low resolution data is used, features
become even less visible, so this problem may become more
difficult. This study focuses on verifying whether model trained
to extract important facial embedding from RGB images can
perform equally well if applied to thermal domain, without
additional re-training. We also perform a set of experiments
aim at evaluating the influence of resolution degradation by
down-scaling images on the recognition accuracy. In addition,
we present deep super-resolution (SR) model that by enhancing
donw-scaled images can improve results for data acquired in
scenarios that simulate real-life environment, i.e. mimicking facial
expressions and performing head motions. Preliminary results
proved that in such cases SR helps to increase accuracy by
6.5% for data 8 times smaller than original images. It has
also been shown that it is possible to accurately recognize even
40 volunteers using only 4 images per person as a reference
embedding. Thus, the initial profiles can be easily created in a
real time, what is an additional advantage considering a solution
setup in a new environment.This study aimed at verifying whether image resolution
has influence on person recognition task using facial features
embedding gathered from thermal imagery. In addition, it was
evaluated if model trained to extract high frequency components (on RGB images) will be able to generate a meaningful
person embedding from thermal data that is characterized by
blurring and smoothness due to the heat flow in objects. The
preliminary results proved the need to enhance image resolution in order to achieve a high accuracy of person recognition.
The presented Super-Resolution solution allowed to improve
results of person recognition from images downscaled with
bicubic interpolation by 8 % for the resizing scale of 4 on
the IRIS dataset. Yet, for our database that assumed strictly
defined measurement conditions (no movements, volunteer
loking toward the camera) we did not observe any gain of
performance.
Thus, we would like to perform similar research on data
collected by us but during various measuring scenarios, e.g.
volunteers turning their head horizontally and vertically. In
future study, we would also like to evaluate the proposed
approach of person recognition on images with original bit
resolution (14-bit represented as 16-bit image to avoid lossy
conversion to 8-bit data).Convolution input + Basie Layers Basic Basie || pe + output Fig. 3 Basic CNN Designil",Person recognition,"The study focuses on person recognition from thermal imagery and evaluates the influence of image resolution on recognition accuracy. The researchers tested if a model trained on RGB images can perform well on thermal images without additional training. They also developed a deep super-resolution model to enhance low-resolution thermal images for better recognition accuracy. The preliminary results showed that enhancing image resolution improves person recognition accuracy, and the super-resolution model improved results by 8% on the IRIS dataset. However, no gain in performance was observed on their database under strictly defined measurement conditions. The researchers plan to perform similar studies under various scenarios and evaluate the proposed approach on images with the original bit resolution.",Object and Sentiment Recognition,Convolution input + Basie Layers Basic Basie || pe + output Fig. 3 Basic CNN Designil,Object Recognition
82,Unifying Identification and Context Learning for Person Recognition,"Person recognition
Contextual cues
Region Attention Network
Social contexts
Unconstrained environments","Despite the great success of face recognition techniques,
recognizing persons under unconstrained settings remains
challenging. Issues like profile views, unfavorable lighting,
and occlusions can cause substantial difficulties. Previous
works have attempted to tackle this problem by exploiting
the context, e.g. clothes and social relations. While showing
promising improvement, they are usually limited in two important aspects, relying on simple heuristics to combine different cues and separating the construction of context from
people identities. In this work, we aim to move beyond such
limitations and propose a new framework to leverage context for person recognition. In particular, we propose a Region Attention Network, which is learned to adaptively combine visual cues with instance-dependent weights. We also
develop a unified formulation, where the social contexts are
learned along with the reasoning of people identities. These
models substantially improve the robustness when working
with the complex contextual relations in unconstrained environments. On two large datasets, PIPA [27] and Cast In
Movies (CIM), a new dataset proposed in this work, our
method consistently achieves state-of-the-art performance
under multiple evaluation policies.
","We presented a new framework for person recognition,
which integrates a Region Attention Network to adaptively
combine visual cues and a model that unifies person identification and context learning in joint inference. We conducted experiments on both PIPA and a new dataset CIM
constructed from movies. On PIPA, our method consistently outperformed previous state-of-the-art methods by a
notable margin, under all splits. On CIM, the new components developed in this work also demonstrated strong effectiveness in raising the recognition accuracy. Both quantitative and qualitative studies showed that adaptive combination of visual cues is important in a generic context and
that the social context often conveys useful information especially when the visual appearance causes ambiguities.","Unifying Identification and Context Learning for Person RecognitionPerson recognition
Contextual cues
Region Attention Network
Social contexts
Unconstrained environmentsDespite the great success of face recognition techniques,
recognizing persons under unconstrained settings remains
challenging. Issues like profile views, unfavorable lighting,
and occlusions can cause substantial difficulties. Previous
works have attempted to tackle this problem by exploiting
the context, e.g. clothes and social relations. While showing
promising improvement, they are usually limited in two important aspects, relying on simple heuristics to combine different cues and separating the construction of context from
people identities. In this work, we aim to move beyond such
limitations and propose a new framework to leverage context for person recognition. In particular, we propose a Region Attention Network, which is learned to adaptively combine visual cues with instance-dependent weights. We also
develop a unified formulation, where the social contexts are
learned along with the reasoning of people identities. These
models substantially improve the robustness when working
with the complex contextual relations in unconstrained environments. On two large datasets, PIPA [27] and Cast In
Movies (CIM), a new dataset proposed in this work, our
method consistently achieves state-of-the-art performance
under multiple evaluation policies.
We presented a new framework for person recognition,
which integrates a Region Attention Network to adaptively
combine visual cues and a model that unifies person identification and context learning in joint inference. We conducted experiments on both PIPA and a new dataset CIM
constructed from movies. On PIPA, our method consistently outperformed previous state-of-the-art methods by a
notable margin, under all splits. On CIM, the new components developed in this work also demonstrated strong effectiveness in raising the recognition accuracy. Both quantitative and qualitative studies showed that adaptive combination of visual cues is important in a generic context and
that the social context often conveys useful information especially when the visual appearance causes ambiguities.",Person recognition,"The article presents a new framework for person recognition under unconstrained settings, which integrates a Region Attention Network to combine visual cues with instance-dependent weights and a model that unifies person identification and context learning in joint inference. The proposed method consistently outperforms previous state-of-the-art methods on both PIPA and a new dataset CIM constructed from movies, demonstrating the importance of adaptive combination of visual cues and the usefulness of social context information in person recognition.",Object and Sentiment Recognition,,Object Recognition
83,Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues," People In Photo Albums (PIPA), unconstrained person recognition, Pose Invariant PErson Recognition (PIPER), poselet-level person recognizers, instance co-identification.","We explore the task of recognizing peoples’ identities
in photo albums in an unconstrained setting. To facilitate
this, we introduce the new People In Photo Albums (PIPA)
dataset, consisting of over 60000 instances of ∼2000 individuals collected from public Flickr photo albums. With
only about half of the person images containing a frontal
face, the recognition task is very challenging due to the
large variations in pose, clothing, camera viewpoint, image
resolution and illumination. We propose the Pose Invariant
PErson Recognition (PIPER) method, which accumulates
the cues of poselet-level person recognizers trained by deep
convolutional networks to discount for the pose variations,
combined with a face recognizer and a global recognizer.
Experiments on three different settings confirm that in our
unconstrained setup PIPER significantly improves on the
performance of DeepFace, which is one of the best face recognizers as measured on the LFW dataset.
","We described PIPER, our method for viewpoint and pose
independent person recognition. We showed that PIPER
significantly outperforms our very strong baseline – combining a state-of-the-art CNN on the full body fine-tuned on
our dataset with a state-of-the-art frontal face recognizer.
PIPER can learn effectively even with a single training example and performs surprisingly well at the task of image
retrieval. While we have used PIPER for person recognition, the algorithm readily applies to generic instance coidentification, such as finding instances of the same car or
the same dog. We introduced the People In Photo Albums
dataset, the first of its kind large scale data set for person
coidentification in photo albums. We hope our dataset will
steer the vision community towards the very important and
largely unsolved problem of person recognition in the wild.","Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues People In Photo Albums (PIPA), unconstrained person recognition, Pose Invariant PErson Recognition (PIPER), poselet-level person recognizers, instance co-identification.We explore the task of recognizing peoples’ identities
in photo albums in an unconstrained setting. To facilitate
this, we introduce the new People In Photo Albums (PIPA)
dataset, consisting of over 60000 instances of ∼2000 individuals collected from public Flickr photo albums. With
only about half of the person images containing a frontal
face, the recognition task is very challenging due to the
large variations in pose, clothing, camera viewpoint, image
resolution and illumination. We propose the Pose Invariant
PErson Recognition (PIPER) method, which accumulates
the cues of poselet-level person recognizers trained by deep
convolutional networks to discount for the pose variations,
combined with a face recognizer and a global recognizer.
Experiments on three different settings confirm that in our
unconstrained setup PIPER significantly improves on the
performance of DeepFace, which is one of the best face recognizers as measured on the LFW dataset.
We described PIPER, our method for viewpoint and pose
independent person recognition. We showed that PIPER
significantly outperforms our very strong baseline – combining a state-of-the-art CNN on the full body fine-tuned on
our dataset with a state-of-the-art frontal face recognizer.
PIPER can learn effectively even with a single training example and performs surprisingly well at the task of image
retrieval. While we have used PIPER for person recognition, the algorithm readily applies to generic instance coidentification, such as finding instances of the same car or
the same dog. We introduced the People In Photo Albums
dataset, the first of its kind large scale data set for person
coidentification in photo albums. We hope our dataset will
steer the vision community towards the very important and
largely unsolved problem of person recognition in the wild.",Person recognition,"The paper introduces the People In Photo Albums (PIPA) dataset for unconstrained person recognition, which contains over 60,000 instances of 2,000 individuals from Flickr photo albums. The Pose Invariant PErson Recognition (PIPER) method is proposed, which combines poselet-level person recognizers, a face recognizer, and a global recognizer to overcome challenges such as pose variations, clothing, camera viewpoint, image resolution, and illumination. PIPER outperforms state-of-the-art methods and can also be applied to generic instance co-identification.",Object and Sentiment Recognition,,Object Recognition
84,"Incorporation of Extra Pseudo Labels for
CNN-based Gait Recognition","Gait Recognition, Attribute, Pseudo label, CNN","CNN is a major model used for image-based recognition tasks, including gait recognition, and many CNN-based network structures and/or learning frameworks have been proposed.
Among them, we focus on approaches that use multiple labels
for learning, typified by multi-task learning. These approaches
are sometimes used to improve the accuracy of the main task
by incorporating extra labels associated with sub-tasks. The
incorporated labels for learning are usually selected from real
tasks heuristically; for example, gender and/or age labels are
incorporated together with subject identity labels. We take a
different approach and consider a virtual task as a sub-task, and
incorporate pseudo output labels together with labels associated
with the main task and/or real task. In this paper, we focus on
a gait-based person recognition task as the main task, and we
discuss the effectiveness of virtual tasks with different pseudo
labels for construction of a CNN-based gait feature extractor","In this paper, we focus on a CNN-based gait feature
extractor for person identification, and we propose an extended
CNN-based feature extractor called exGEINet by incorporating extra labels. For the extra labels, we consider pseudo
labels together with real labels such as gender and age, and
we evaluate the efficiency of exGEINet. Experimental results
using the OULP gait dataset showed that not only the labels associated with real tasks but also pseudo labels can contribute
to improved accuracy of gait recognition. Moreover, incorporating multiple extra labels can achieve a further improvement.
In the case where we use labels associated with real tasks,
additional manual annotation or additional information is
necessary, but in the case where we use pseudo labels, we
can produce multiple types of pseudo labels without manual
annotation. This is an advantage of incorporating pseudo
labels, because we can generate different types of virtual tasks
by combining multiple pseudo labels.
This is the first step of our research; we only consider
randomly selected pseudo labels, and simply concatenate
multiple subnetworks for the feature extractor. The number of
fc3 associated with each subnetwork is fixed, and the number
of classes in each task is set heuristically. We plan to tackle
these issues in future work.","Incorporation of Extra Pseudo Labels for
CNN-based Gait RecognitionGait Recognition, Attribute, Pseudo label, CNNCNN is a major model used for image-based recognition tasks, including gait recognition, and many CNN-based network structures and/or learning frameworks have been proposed.
Among them, we focus on approaches that use multiple labels
for learning, typified by multi-task learning. These approaches
are sometimes used to improve the accuracy of the main task
by incorporating extra labels associated with sub-tasks. The
incorporated labels for learning are usually selected from real
tasks heuristically; for example, gender and/or age labels are
incorporated together with subject identity labels. We take a
different approach and consider a virtual task as a sub-task, and
incorporate pseudo output labels together with labels associated
with the main task and/or real task. In this paper, we focus on
a gait-based person recognition task as the main task, and we
discuss the effectiveness of virtual tasks with different pseudo
labels for construction of a CNN-based gait feature extractorIn this paper, we focus on a CNN-based gait feature
extractor for person identification, and we propose an extended
CNN-based feature extractor called exGEINet by incorporating extra labels. For the extra labels, we consider pseudo
labels together with real labels such as gender and age, and
we evaluate the efficiency of exGEINet. Experimental results
using the OULP gait dataset showed that not only the labels associated with real tasks but also pseudo labels can contribute
to improved accuracy of gait recognition. Moreover, incorporating multiple extra labels can achieve a further improvement.
In the case where we use labels associated with real tasks,
additional manual annotation or additional information is
necessary, but in the case where we use pseudo labels, we
can produce multiple types of pseudo labels without manual
annotation. This is an advantage of incorporating pseudo
labels, because we can generate different types of virtual tasks
by combining multiple pseudo labels.
This is the first step of our research; we only consider
randomly selected pseudo labels, and simply concatenate
multiple subnetworks for the feature extractor. The number of
fc3 associated with each subnetwork is fixed, and the number
of classes in each task is set heuristically. We plan to tackle
these issues in future work.",Person recognition,"This paper focuses on using virtual tasks as sub-tasks in multi-label learning for gait-based person recognition. By incorporating pseudo labels with the real task labels, they achieve improved accuracy in gait recognition. The advantage of using pseudo labels is that multiple types of virtual tasks can be generated without manual annotation. The authors plan to address the issue of fixed subnetworks and class numbers in future work.",Object and Sentiment Recognition,,Object Recognition
85,A survey of opinion mining and sentimrnt analysis,"opinion mining ,sentiment analysis, NLP","Sentiment analysis or opinion mining is the computational study of people’s opinions, appraisals, attitudes, and emotions toward entities, individuals, issues, events, topics and their attributes. The task is technically challenging and practically very useful. For example, businesses always want to find public or consumer opinions about their products and services. Potential customers also want to know the opinions of existing users before they use a service or purchase a product. With the explosive growth of social media (i.e., reviews, forum discussions, blogs and social networks) on the Web, individuals and organizations are increasingly using public opinions in these media for their decision making. However, finding and monitoring opinion sites on the Web and distilling the information contained in them remains a formidable task because of the proliferation of diverse sites. Each site typically contains a huge volume of opinionated text that is not always easily deciphered in long forum postings and blogs. The average human reader will have difficulty identifying relevant sites and accurately summarizing the information and opinions contained in them. Moreover, it is also known that human analysis of text information is subject to considerable biases, e.g., people often pay greater attention to opinions that are consistent with their own preferences. People also have difficulty, owing to their mental and physical limitations, producing consistent © Springer Science+Business Media, LLC 2012 C.C. Aggarwal and C.X. Zhai (eds.), Mining Text Data, DOI 10.1007/978-1-4614-3223-4_13, 415 416 MINING TEXT DATA results when the amount of information to be processed is large. Automated opinion mining and summarization systems are thus needed, as subjective biases and mental limitations can be overcome with an objective sentiment analysis system. In the past decade, a considerable amount of research has been done in academia [58,76]. There are also numerous commercial companies that provide opinion mining services. In this chapter, we first define the opinion mining problem. From the definition, we will see the key technical issues that need to be addressed. We then describe various key mining tasks that have been studied in the research literature and their representative techniques. After that, we discuss the issue of detecting opinion spam or fake reviews. Finally, we also introduce the research topic of assessing the utility or quality of online reviews.","This chapter introduced and surveyed the field of sentiment analysis and opinion mining. Due to many challenging research problems and a wide variety of practical applications, it has been a very active research area in recent years. In fact, it has spread from computer science to management science. This chapter first presented an abstract model of sentiment analysis, which formulated the problem and provided a common framework to unify different research directions. It then discussed the most widely studied topic of sentiment and subjectivity classification, which determines whether a document or sentence is opinionated, and if so whether it carries a positive or negative opinion. We then described aspect-based sentiment analysis which exploits the full power of the abstract model. After that we briefly introduced the problem of analyzing comparative sentences. Last but not least, we discussed opinion spam, which is increasingly becoming an important issue as more and more people are relying on opinions on the Web for decision making. Several initial algorithms were described. Finally, we conclude the chapter by saying that all the sentiment analysis tasks are very challenging. Our understanding and knowledge of the problem and its solution are still limited. The main reason is that it is a natural language processing task, A Survey of Opinion Mining and Sentiment Analysis 453 and natural language processing has no easy problems. However, many significant progresses have been made. This is evident from the large number of start-up companies that offer sentiment analysis or opinion mining services. There is a real and huge need in the industry for such services because every company wants to know how consumers perceive their products and services and those of their competitors. These practical needs and the technical challenges will keep the field vibrant and lively for years to come.","A survey of opinion mining and sentimrnt analysisopinion mining ,sentiment analysis, NLPSentiment analysis or opinion mining is the computational study of people’s opinions, appraisals, attitudes, and emotions toward entities, individuals, issues, events, topics and their attributes. The task is technically challenging and practically very useful. For example, businesses always want to find public or consumer opinions about their products and services. Potential customers also want to know the opinions of existing users before they use a service or purchase a product. With the explosive growth of social media (i.e., reviews, forum discussions, blogs and social networks) on the Web, individuals and organizations are increasingly using public opinions in these media for their decision making. However, finding and monitoring opinion sites on the Web and distilling the information contained in them remains a formidable task because of the proliferation of diverse sites. Each site typically contains a huge volume of opinionated text that is not always easily deciphered in long forum postings and blogs. The average human reader will have difficulty identifying relevant sites and accurately summarizing the information and opinions contained in them. Moreover, it is also known that human analysis of text information is subject to considerable biases, e.g., people often pay greater attention to opinions that are consistent with their own preferences. People also have difficulty, owing to their mental and physical limitations, producing consistent © Springer Science+Business Media, LLC 2012 C.C. Aggarwal and C.X. Zhai (eds.), Mining Text Data, DOI 10.1007/978-1-4614-3223-4_13, 415 416 MINING TEXT DATA results when the amount of information to be processed is large. Automated opinion mining and summarization systems are thus needed, as subjective biases and mental limitations can be overcome with an objective sentiment analysis system. In the past decade, a considerable amount of research has been done in academia [58,76]. There are also numerous commercial companies that provide opinion mining services. In this chapter, we first define the opinion mining problem. From the definition, we will see the key technical issues that need to be addressed. We then describe various key mining tasks that have been studied in the research literature and their representative techniques. After that, we discuss the issue of detecting opinion spam or fake reviews. Finally, we also introduce the research topic of assessing the utility or quality of online reviews.This chapter introduced and surveyed the field of sentiment analysis and opinion mining. Due to many challenging research problems and a wide variety of practical applications, it has been a very active research area in recent years. In fact, it has spread from computer science to management science. This chapter first presented an abstract model of sentiment analysis, which formulated the problem and provided a common framework to unify different research directions. It then discussed the most widely studied topic of sentiment and subjectivity classification, which determines whether a document or sentence is opinionated, and if so whether it carries a positive or negative opinion. We then described aspect-based sentiment analysis which exploits the full power of the abstract model. After that we briefly introduced the problem of analyzing comparative sentences. Last but not least, we discussed opinion spam, which is increasingly becoming an important issue as more and more people are relying on opinions on the Web for decision making. Several initial algorithms were described. Finally, we conclude the chapter by saying that all the sentiment analysis tasks are very challenging. Our understanding and knowledge of the problem and its solution are still limited. The main reason is that it is a natural language processing task, A Survey of Opinion Mining and Sentiment Analysis 453 and natural language processing has no easy problems. However, many significant progresses have been made. This is evident from the large number of start-up companies that offer sentiment analysis or opinion mining services. There is a real and huge need in the industry for such services because every company wants to know how consumers perceive their products and services and those of their competitors. These practical needs and the technical challenges will keep the field vibrant and lively for years to come.ae Extractor. {_ Feature Extractor Person ID. 3) (main task) B) Person 3p Other labels (sub-tasks) Fig. 1. (left) Network structure of GEINet. (right) Network structure of extended GEINet.",Data mining,"Sentiment analysis or opinion mining is the process of computationally analyzing people's opinions and emotions towards entities, individuals, issues, events, topics and their attributes. It is a challenging but useful task for businesses and consumers who want to know the public's opinions about products and services. With the growth of social media, analyzing public opinions on the web has become increasingly important but difficult due to the vast amount of opinionated text on diverse sites. Automated opinion mining and summarization systems are needed to overcome human biases and limitations. The chapter surveys the field of sentiment analysis and opinion mining, discussing various key mining tasks, including sentiment and subjectivity classification, aspect-based sentiment analysis, and opinion spam detection. The chapter concludes that sentiment analysis tasks are challenging, but significant progress has been made, and the field will remain vibrant and essential in the future.",Natural Language Processing,ae Extractor. {_ Feature Extractor Person ID. 3) (main task) B) Person 3p Other labels (sub-tasks) Fig. 1. (left) Network structure of GEINet. (right) Network structure of extended GEINet.,Object Recognition
86,Thumbs up? Sentiment Classification using Machine Learning Techniques,sentiment classification,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.","On the other hand, we were not able to achieve accuracies on the sentiment classification problem comparable to those reported for standard topic-based categorization, despite the several different types of features we tried. Unigram presence information turned out to be the most effective; in fact, none of the alternative features we employed provided consistently better performance once unigram presence was incorporated. Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous observations made in topic-classification work (McCallum and Nigam, 1998). What accounts for these two differences — difficulty and types of information proving useful — between topic and sentiment classification, and how might we improve the latter? To answer these questions, we examined the data further. (All examples below are drawn from the full 2053-document corpus.) As it turns out, a common phenomenon in the documents was a kind of “thwarted expectations” narrative, where the author sets up a deliberate contrast to earlier discussion: for example, “This film should be brilliant. It sounds like a great plot, the actors are first grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance. However, it can’t hold up” or “I hate the Spice Girls. ...[3 things the author hates about them]... Why I saw this movie is a really, really, really long story, but I did, and one would think I’d despise every minute of it. But... Okay, I’m really ashamed of it, but I enjoyed it. I mean, I admit it’s a really awful movie ...the ninth floor of hell...The plot is such a mess that it’s terrible. But I loved it.” 15","Thumbs up? Sentiment Classification using Machine Learning Techniquessentiment classificationWe consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.On the other hand, we were not able to achieve accuracies on the sentiment classification problem comparable to those reported for standard topic-based categorization, despite the several different types of features we tried. Unigram presence information turned out to be the most effective; in fact, none of the alternative features we employed provided consistently better performance once unigram presence was incorporated. Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous observations made in topic-classification work (McCallum and Nigam, 1998). What accounts for these two differences — difficulty and types of information proving useful — between topic and sentiment classification, and how might we improve the latter? To answer these questions, we examined the data further. (All examples below are drawn from the full 2053-document corpus.) As it turns out, a common phenomenon in the documents was a kind of “thwarted expectations” narrative, where the author sets up a deliberate contrast to earlier discussion: for example, “This film should be brilliant. It sounds like a great plot, the actors are first grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance. However, it can’t hold up” or “I hate the Spice Girls. ...[3 things the author hates about them]... Why I saw this movie is a really, really, really long story, but I did, and one would think I’d despise every minute of it. But... Okay, I’m really ashamed of it, but I enjoyed it. I mean, I admit it’s a really awful movie ...the ninth floor of hell...The plot is such a mess that it’s terrible. But I loved it.” 15",Deep Learning and Machine Learning,"The article discusses the problem of classifying documents based on sentiment, rather than topic. Machine learning techniques outperform human-produced baselines for this task, but the performance is not as good as traditional topic-based categorization. Unigram presence information is the most effective feature for sentiment classification, despite previous observations in topic-classification work. The article examines a common phenomenon in the documents, where authors set up deliberate contrasts to earlier discussions, which makes sentiment classification more challenging. The article concludes by discussing factors that make sentiment classification more difficult and how to improve it.",Deep Learning and Machine Learning,,Deep Learning and Machine Learning
87,Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions,"autoencoders, sentiment distribution","We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.","We presented a novel algorithm that can accurately predict sentence-level sentiment distributions. Without using any hand-engineered resources such as sentiment lexica, parsers or sentiment shifting rules, our model achieves state-of-the-art performance on commonly used sentiment datasets. Furthermore, we introduce a new dataset that contains distributions over a broad range of human emotions. Our evaluation shows that our model can more accurately predict these distributions than other models.","Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributionsautoencoders, sentiment distributionWe introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.We presented a novel algorithm that can accurately predict sentence-level sentiment distributions. Without using any hand-engineered resources such as sentiment lexica, parsers or sentiment shifting rules, our model achieves state-of-the-art performance on commonly used sentiment datasets. Furthermore, we introduce a new dataset that contains distributions over a broad range of human emotions. Our evaluation shows that our model can more accurately predict these distributions than other models.",Deep Learning and Machine Learning,"The text describes a machine learning framework that uses recursive autoencoders to predict sentiment label distributions for multi-word phrases without using any pre-defined sentiment lexica or polarity shifting rules. The framework outperforms other state-of-the-art approaches on commonly used datasets, such as movie reviews. The model is also evaluated on a new dataset of personal user stories annotated with multiple labels, and it is shown to accurately predict distributions over these labels compared to several competitive baselines. The framework is able to accurately predict sentence-level sentiment distributions without using any hand-engineered resources.",Deep Learning and Machine Learning,,Deep Learning and Machine Learning
88,Combining Lexicon-based and Learning-based Methods for Twitter Sentiment Analysis,"sentiment analysis, lexicon based approach,, sentiment classifier","With the booming of microblogs on the Web, people have begun to express their opinions on a wide variety of topics on Twitter and other similar services. Sentiment analysis on entities (e.g., products, organizations, people, etc.) in tweets (posts on Twitter) thus becomes a rapid and effective way of gauging public opinion for business marketing or social studies. However, Twitter's unique characteristics give rise to new problems for current sentiment analysis methods, which originally focused on large opinionated corpora such as product reviews. In this paper, we propose a new entity-level sentiment analysis method for Twitter. The method first adopts a lexiconbased approach to perform entity-level sentiment analysis. This method can give high precision, but low recall. To improve recall, additional tweets that are likely to be opinionated are identified automatically by exploiting the information in the result of the lexicon-based method. A classifier is then trained to assign polarities to the entities in the newly identified tweets. Instead of being labeled manually, the training examples are given by the lexicon-based approach. Experimental results show that the proposed method dramatically improves the recall and the F-score, and outperforms the state-of-the-art baselines.","The unique characteristics of Twitter data pose new problems for current lexicon-based and learning-based sentiment analysis approaches. In this paper, we proposed a novel method to deal with the problems. An augmented lexicon-based method specific to the Twitter data was first applied to perform sentiment analysis. Through Chi-square test on its output, additional opinionated tweets could be identified. A binary sentiment classifier is then trained to assign sentiment polarities to the newly-identified opinionated tweets, whose training data is provided by the lexicon-based method. Empirical experiments show the proposed method is highly effective and promising.","Combining Lexicon-based and Learning-based Methods for Twitter Sentiment Analysissentiment analysis, lexicon based approach,, sentiment classifierWith the booming of microblogs on the Web, people have begun to express their opinions on a wide variety of topics on Twitter and other similar services. Sentiment analysis on entities (e.g., products, organizations, people, etc.) in tweets (posts on Twitter) thus becomes a rapid and effective way of gauging public opinion for business marketing or social studies. However, Twitter's unique characteristics give rise to new problems for current sentiment analysis methods, which originally focused on large opinionated corpora such as product reviews. In this paper, we propose a new entity-level sentiment analysis method for Twitter. The method first adopts a lexiconbased approach to perform entity-level sentiment analysis. This method can give high precision, but low recall. To improve recall, additional tweets that are likely to be opinionated are identified automatically by exploiting the information in the result of the lexicon-based method. A classifier is then trained to assign polarities to the entities in the newly identified tweets. Instead of being labeled manually, the training examples are given by the lexicon-based approach. Experimental results show that the proposed method dramatically improves the recall and the F-score, and outperforms the state-of-the-art baselines.The unique characteristics of Twitter data pose new problems for current lexicon-based and learning-based sentiment analysis approaches. In this paper, we proposed a novel method to deal with the problems. An augmented lexicon-based method specific to the Twitter data was first applied to perform sentiment analysis. Through Chi-square test on its output, additional opinionated tweets could be identified. A binary sentiment classifier is then trained to assign sentiment polarities to the newly-identified opinionated tweets, whose training data is provided by the lexicon-based method. Empirical experiments show the proposed method is highly effective and promising.",lexicon-based approach,"The article discusses the rise of microblogs, such as Twitter, and how sentiment analysis on entities (products, organizations, people) can be used to gauge public opinion for business marketing or social studies. However, Twitter's unique characteristics present new problems for current sentiment analysis methods. The article proposes a new entity-level sentiment analysis method for Twitter, which uses a lexicon-based approach followed by a classifier to improve recall. The proposed method is effective and outperforms state-of-the-art baselines. Experimental results are provided to support the effectiveness of the method.",Object and Sentiment Recognition,,Deep Learning and Machine Learning
89,Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches,"semantic orientation, machine learning,, opinion mining , movie review mining","Web content mining is intended to help people discover valuable information from large amount of unstructured data on the web. Movie review mining classifies movie reviews into two polarities: positive and negative. As a type of sentiment-based classification, movie review mining is different from other topic-based classifications. Few empirical studies have been conducted in this domain. This paper investigates movie review mining using two approaches: machine learning and semantic orientation. The approaches are adapted to movie review domain for comparison. The results show that our results are comparable to or even better than previous findings. We also find that movie review mining is a more challenging application than many other types of review mining. The challenges of movie review mining lie in that factual information is always mixed with real-life review data and ironic words are used in writing movie reviews. Future work for improving existing approaches is also suggested.","Movie review mining is a challenging sentimental classification problem. Not only does it deal with classification of personal opinions, but diverse opinions from product reviews as well. Due to the sparsity of words in movie reviews, it is difficult for supervised learning approach to use bag-of-words features. Pang et al.’s experiment also confirmed such difficulty in using machine learning approach to classify movie reviews [9]. Moreover, some parts of a review may not express opinions. Some reviewers prefer to describe factual background information about a movie before expressing their opinions, which can be considered as noise to the classification. Lastly, movie review mining is a very challenging issue for semantic orientation techniques. The findings of this study not only advance the research on movie review mining, but also contribute to other text classification problems such as separate “flames” messages in bulletin boards as mentioned in [3, 9].","Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approachessemantic orientation, machine learning,, opinion mining , movie review miningWeb content mining is intended to help people discover valuable information from large amount of unstructured data on the web. Movie review mining classifies movie reviews into two polarities: positive and negative. As a type of sentiment-based classification, movie review mining is different from other topic-based classifications. Few empirical studies have been conducted in this domain. This paper investigates movie review mining using two approaches: machine learning and semantic orientation. The approaches are adapted to movie review domain for comparison. The results show that our results are comparable to or even better than previous findings. We also find that movie review mining is a more challenging application than many other types of review mining. The challenges of movie review mining lie in that factual information is always mixed with real-life review data and ironic words are used in writing movie reviews. Future work for improving existing approaches is also suggested.Movie review mining is a challenging sentimental classification problem. Not only does it deal with classification of personal opinions, but diverse opinions from product reviews as well. Due to the sparsity of words in movie reviews, it is difficult for supervised learning approach to use bag-of-words features. Pang et al.’s experiment also confirmed such difficulty in using machine learning approach to classify movie reviews [9]. Moreover, some parts of a review may not express opinions. Some reviewers prefer to describe factual background information about a movie before expressing their opinions, which can be considered as noise to the classification. Lastly, movie review mining is a very challenging issue for semantic orientation techniques. The findings of this study not only advance the research on movie review mining, but also contribute to other text classification problems such as separate “flames” messages in bulletin boards as mentioned in [3, 9].",Deep Learning and Machine Learning,"The article discusses movie review mining as a sentiment-based classification problem that is different from topic-based classifications. Two approaches, machine learning and semantic orientation, are used and adapted to movie review mining for comparison. The results show that movie review mining is a challenging application due to the mixture of factual information and real-life review data, ironic words used in writing movie reviews, and the sparsity of words in movie reviews. The study suggests future work for improving existing approaches to movie review mining and states that the findings can contribute to other text classification problems.",Deep Learning and Machine Learning,,Sentiment Analysis
90,SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining,"sentiwordnet, opinion mining,, synset, NLP","Opinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. OM has a rich set of applications, ranging from tracking users’ opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the “PN-polarity” of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been, instead, much more scarce. In this work we describe SENTIWORDNET, a lexical resource in which each WORDNET synset s is associated to three numerical scores Obj(s), P os(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop SENTIWORDNET is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. The three scores are derived by combining the results produced by a committee of eight ternary classifiers, all characterized by similar accuracy levels but different classification behaviour. SENTIWORDNET is freely available for research purposes, and is endowed with a Web-based graphical user interface.","We believe that SentiWordNet can prove a useful tool for opinion mining applications, because of its wide coverage (all WordNet synsets are tagged according to each of the three labels Objective, Positive, Negative) and because of its fine grain, obtained by qualifying the labels by means of numerical scores.","SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Miningsentiwordnet, opinion mining,, synset, NLPOpinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. OM has a rich set of applications, ranging from tracking users’ opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the “PN-polarity” of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been, instead, much more scarce. In this work we describe SENTIWORDNET, a lexical resource in which each WORDNET synset s is associated to three numerical scores Obj(s), P os(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop SENTIWORDNET is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. The three scores are derived by combining the results produced by a committee of eight ternary classifiers, all characterized by similar accuracy levels but different classification behaviour. SENTIWORDNET is freely available for research purposes, and is endowed with a Web-based graphical user interface.We believe that SentiWordNet can prove a useful tool for opinion mining applications, because of its wide coverage (all WordNet synsets are tagged according to each of the three labels Objective, Positive, Negative) and because of its fine grain, obtained by qualifying the labels by means of numerical scores.",opinion mining,"Opinion mining (OM) is a field that focuses on extracting opinions expressed in text, rather than the topic of the text. It has many applications, including tracking user opinions about products and political candidates. Recent research has focused on automatically determining the polarity (positive or negative connotation) of subjective terms. SENTIWORDNET is a lexical resource that provides numerical scores for how objective, positive, or negative terms in a synset are. It was developed by analyzing the glosses associated with synsets and using vectorial term representations for semi-supervised synset classification. SENTIWORDNET has a web-based graphical user interface and can be useful for opinion mining due to its wide coverage and fine-grained numerical scores.",Natural Language Processing,,Deep Learning and Machine Learning
91,Review of medical image recognition technologies to detect melanomas using neural networks,"Melanoma classification, Skin cancer, Deep learning neural network, Convolutional neural network, Fuzzy clustering algorithm","Background: Melanoma is one of the most aggressive types of cancer that has become a world-class problem. According to the World Health Organization estimates, 132,000 cases of the disease and 66,000 deaths from malignant melanoma and other forms of skin cancer are reported annually worldwide (https://apps.who. int/gho/data/?theme=main) and those numbers continue to grow. In our opinion, due to the increasing incidence of the disease, it is necessary to find new, easy to use and sensitive methods for the early diagnosis of melanoma in a large number of people around the world. Over the last decade, neural networks show highly sensitive, specific, and accurate results. Objective: This study presents a review of PubMed papers including requests «melanoma neural network» and «melanoma neural network dermatoscopy». We review recent researches and discuss their opportunities acceptable in clinical practice. Methods: We searched the PubMed database for systematic reviews and original research papers on the requests «melanoma neural network» and «melanoma neural network dermatoscopy» published in English. Only papers that reported results, progress and outcomes are included in this review. Results: We found 11 papers that match our requests that observed convolutional and deep-learning neural networks combined with fuzzy clustering or World Cup Optimization algorithms in analyzing dermatoscopic images. All of them require an ABCD (asymmetry, border, color, and differential structures) algorithm and its derivates (in combination with ABCD algorithm or separately). Also, they require a large dataset of dermatoscopic images and optimized estimation parameters to provide high specificity, accuracy and sensitivity. Conclusions: According to the analyzed papers, neural networks show higher specificity, accuracy and sensitivity than dermatologists. Neural networks are able to evaluate features that might be unavailable to the naked human eye. Despite that, we need more datasets to confirm those statements. Nowadays machine learning becomes a helpful tool in early diagnosing skin diseases, especially melanoma.","The analysis of this study leads us to the following statements:  Neural networks in 2019 have greater sensitivity and specificity than dermatologists  A neural network can evaluate features that might be unavailable to the naked human eye  CNN provides the possibility of early detection of signs of melanoma and early treatment  The need for staff training and the purchase of expensive equipment for creating dermatoscopic images can be replaced by software using FC-neural networks By 2019, neural networks show significantly higher sensitivity, specificity and accuracy in comparison with the previous versions and work of dermatologists. The prospect of using this method as an auxiliary and later, perhaps, the main diagnostic is expressed in its increasing availability, accuracy and ease of use. In the age of rapid technology development, information systems and machine learning, the neural network is a learning algorithm that can greatly facilitate and improve the level of accuracy and timeliness of diagnostics.","Review of medical image recognition technologies to detect melanomas using neural networksMelanoma classification, Skin cancer, Deep learning neural network, Convolutional neural network, Fuzzy clustering algorithmBackground: Melanoma is one of the most aggressive types of cancer that has become a world-class problem. According to the World Health Organization estimates, 132,000 cases of the disease and 66,000 deaths from malignant melanoma and other forms of skin cancer are reported annually worldwide (https://apps.who. int/gho/data/?theme=main) and those numbers continue to grow. In our opinion, due to the increasing incidence of the disease, it is necessary to find new, easy to use and sensitive methods for the early diagnosis of melanoma in a large number of people around the world. Over the last decade, neural networks show highly sensitive, specific, and accurate results. Objective: This study presents a review of PubMed papers including requests «melanoma neural network» and «melanoma neural network dermatoscopy». We review recent researches and discuss their opportunities acceptable in clinical practice. Methods: We searched the PubMed database for systematic reviews and original research papers on the requests «melanoma neural network» and «melanoma neural network dermatoscopy» published in English. Only papers that reported results, progress and outcomes are included in this review. Results: We found 11 papers that match our requests that observed convolutional and deep-learning neural networks combined with fuzzy clustering or World Cup Optimization algorithms in analyzing dermatoscopic images. All of them require an ABCD (asymmetry, border, color, and differential structures) algorithm and its derivates (in combination with ABCD algorithm or separately). Also, they require a large dataset of dermatoscopic images and optimized estimation parameters to provide high specificity, accuracy and sensitivity. Conclusions: According to the analyzed papers, neural networks show higher specificity, accuracy and sensitivity than dermatologists. Neural networks are able to evaluate features that might be unavailable to the naked human eye. Despite that, we need more datasets to confirm those statements. Nowadays machine learning becomes a helpful tool in early diagnosing skin diseases, especially melanoma.The analysis of this study leads us to the following statements:  Neural networks in 2019 have greater sensitivity and specificity than dermatologists  A neural network can evaluate features that might be unavailable to the naked human eye  CNN provides the possibility of early detection of signs of melanoma and early treatment  The need for staff training and the purchase of expensive equipment for creating dermatoscopic images can be replaced by software using FC-neural networks By 2019, neural networks show significantly higher sensitivity, specificity and accuracy in comparison with the previous versions and work of dermatologists. The prospect of using this method as an auxiliary and later, perhaps, the main diagnostic is expressed in its increasing availability, accuracy and ease of use. In the age of rapid technology development, information systems and machine learning, the neural network is a learning algorithm that can greatly facilitate and improve the level of accuracy and timeliness of diagnostics.",Medical Data Analysis,"This study is a review of recent research on the use of neural networks in the early diagnosis of melanoma, a highly aggressive type of cancer. The study found that neural networks, specifically convolutional and deep-learning neural networks, combined with fuzzy clustering or World Cup Optimization algorithms, show highly sensitive, specific, and accurate results. The study concludes that neural networks have greater sensitivity and specificity than dermatologists, can evaluate features that might be unavailable to the naked human eye, and provide the possibility of early detection of signs of melanoma and early treatment. Additionally, the study suggests that the need for staff training and expensive equipment for creating dermatoscopic images can be replaced by software using FC-neural networks. Overall, the study highlights the potential of using neural networks as an auxiliary or even the main diagnostic tool for the early diagnosis of melanoma.",Medical Data Analysis,,Deep Learning and Machine Learning
92,Review paper on research direction towards cancer prediction and prognosis using machine learning and deep learning models,Cancer disease diagnosis and prognosis · Deep learning · Machine learning · Support vector machines · Bayesian networks · Decision trees · Artifcial neural networks · Recurrent neural network · Deep neural network · Performance analysis,"Cancer is characterized as a heterogeneous disease of various types. The early detection and prognosis of a cancer type have turned into a major requirement, as it facilitates successive medical treatment of patients. The research team has classified the cancer patients into high or low-risk groups. This makes it a significant task for the medical teams to study the application of deep learning and machine learning models. As a result, such techniques have been employed for modeling the development and treatment of cancer conditions. Additionally, the machine learning tools can have the ability the significant detection features from complex datasets. Numerous techniques like Support Vector Machines (SVM), Bayesian Networks (BN), Decision Trees (DT), Artificial Neural Networks (ANN), Recurrent Neural Network (RNN), and Deep Neural Network (DNN) has been broadly utilized in cancer research. As per the current survey, the detection rate is about 99.89%, which shows the prediction models’ efficiency and precise decision making. However, it is proven that deep learning and machine learning approaches can enhance cancer progression. An adequate level of estimation is required for such approaches for considering the daily medical practice. This survey analyzes and learns the diverse contributions of cancer prediction models using intelligent approaches. Further, the paper tries to categorize the different algorithms, the utilized datasets, and utilized environments. Along with this, various performance measures evaluated in each contribution is sorted out. An extensive search is conducted relevant to machine learning and deep learning methods in cancer susceptibility, recurrence, and survivability prediction, and the existing challenges in this area are clearly described. However, ML models are still in the testing as well as the experimentation phase for cancer prognoses. As the datasets are getting larger with higher quality, researchers are building increasingly accurate models. Moreover, ML models have a long way to go, and most of the models still lack sufficient data and suffer from bias.","This paper has presented a detailed survey of various cancer diagnosis types and prognosis using data mining approaches based on the past ten years’ literature study. This research has also presented the details concerned with diverse machine learning approaches. Furthermore, it has given information about the datasets and simulation platforms used for validating the implemented models. This study has also provided detailed information on the research gaps and challenges concerning the cancer diagnosis and prognosis models through machine learning approaches. Generally, this research gives necessary knowledge related to the traditional cancer prediction system and its approaches that give motivation and many ideas to develop a robust model in the future compared to traditional models. This research gives new researchers a central idea to develop a machine-assisted intelligent model for cancer detection. Still, most of the research discussed in the literature is far behind the maturity in detection accuracy, which needs to be addressed in the future. Thus survey has identified research gaps like the development of pre-clinical trials, handling complex cancer images, precise and early treatment for the clinical opinion, innovative and intelligent strategy in fast detection of disease. Many ideas can be gathered through this survey, and the performance of the deep learners and machine learners in the study can be examined. This survey can give hope for further research in cancer detection.","Review paper on research direction towards cancer prediction and prognosis using machine learning and deep learning modelsCancer disease diagnosis and prognosis · Deep learning · Machine learning · Support vector machines · Bayesian networks · Decision trees · Artifcial neural networks · Recurrent neural network · Deep neural network · Performance analysisCancer is characterized as a heterogeneous disease of various types. The early detection and prognosis of a cancer type have turned into a major requirement, as it facilitates successive medical treatment of patients. The research team has classified the cancer patients into high or low-risk groups. This makes it a significant task for the medical teams to study the application of deep learning and machine learning models. As a result, such techniques have been employed for modeling the development and treatment of cancer conditions. Additionally, the machine learning tools can have the ability the significant detection features from complex datasets. Numerous techniques like Support Vector Machines (SVM), Bayesian Networks (BN), Decision Trees (DT), Artificial Neural Networks (ANN), Recurrent Neural Network (RNN), and Deep Neural Network (DNN) has been broadly utilized in cancer research. As per the current survey, the detection rate is about 99.89%, which shows the prediction models’ efficiency and precise decision making. However, it is proven that deep learning and machine learning approaches can enhance cancer progression. An adequate level of estimation is required for such approaches for considering the daily medical practice. This survey analyzes and learns the diverse contributions of cancer prediction models using intelligent approaches. Further, the paper tries to categorize the different algorithms, the utilized datasets, and utilized environments. Along with this, various performance measures evaluated in each contribution is sorted out. An extensive search is conducted relevant to machine learning and deep learning methods in cancer susceptibility, recurrence, and survivability prediction, and the existing challenges in this area are clearly described. However, ML models are still in the testing as well as the experimentation phase for cancer prognoses. As the datasets are getting larger with higher quality, researchers are building increasingly accurate models. Moreover, ML models have a long way to go, and most of the models still lack sufficient data and suffer from bias.This paper has presented a detailed survey of various cancer diagnosis types and prognosis using data mining approaches based on the past ten years’ literature study. This research has also presented the details concerned with diverse machine learning approaches. Furthermore, it has given information about the datasets and simulation platforms used for validating the implemented models. This study has also provided detailed information on the research gaps and challenges concerning the cancer diagnosis and prognosis models through machine learning approaches. Generally, this research gives necessary knowledge related to the traditional cancer prediction system and its approaches that give motivation and many ideas to develop a robust model in the future compared to traditional models. This research gives new researchers a central idea to develop a machine-assisted intelligent model for cancer detection. Still, most of the research discussed in the literature is far behind the maturity in detection accuracy, which needs to be addressed in the future. Thus survey has identified research gaps like the development of pre-clinical trials, handling complex cancer images, precise and early treatment for the clinical opinion, innovative and intelligent strategy in fast detection of disease. Many ideas can be gathered through this survey, and the performance of the deep learners and machine learners in the study can be examined. This survey can give hope for further research in cancer detection.",Deep Learning and Machine Learning,"This paper presents a survey of various cancer diagnosis and prognosis models using data mining and machine learning approaches. The study discusses the application of deep learning and machine learning models for modeling the development and treatment of cancer conditions. The paper analyzes the different algorithms, datasets, and environments utilized in cancer prediction models using intelligent approaches. It provides detailed information on the research gaps and challenges concerning cancer diagnosis and prognosis models through machine learning approaches. The study identifies research gaps, such as the development of pre-clinical trials, handling complex cancer images, precise and early treatment, and innovative and intelligent strategies for fast detection of disease. The paper concludes that while machine learning models have shown promise in cancer detection, there is still a long way to go, and most models lack sufficient data and suffer from bias. Overall, the survey provides useful insights for developing a robust model in the future compared to traditional models and gives hope for further research in cancer detection.",Deep Learning and Machine Learning,,Medical Data Analysis
93,Risk classification of cancer survival using ANN with gene expression data from multiple laboratories,"Microarray , Gene expression , Neural network , Machine learning , Survival analysis , Outcome prediction , Lung cancer.","Numerous cancer studies have combined gene expression experiments and clinical survival data to predict the prognosis of patients of specific gene types. However, most results of these studies were data dependent and were not suitable for other data sets. This study performed cross-laboratory validations for the cancer patient data from 4 hospitals. We investigated the feasibility of survival risk predictions using high-throughput gene expression data and clinical data. We analyzed multiple data sets for prognostic applications in lung cancer diagnosis. After building tens of thousands of various ANN architectures using the training data, five survival-time correlated genes were identified from 4 microarray gene expression data sets by examining the correlation between gene signatures and patient survival time. The experimental results showed that gene expression data can be used for valid predictions of cancer patient survival classification with an overall accuracy of 83.0% based on survival time trusted data. The results show the prediction model yielded excellent predictions given that patients in the high-risk group obtained a lower median overall survival compared with low-risk patients (log-rank test P-valueo0.00001). This study provides a foundation for further clinical studies and research into other types of cancer. We hope these findings will improve the prognostic methods of cancer patients.","The results of this study showed that predicting cancer patients' survival was feasible using cross-laboratory gene expression data. The key to this analysis was identifying the pertinent genes. Previous studies have used data from a single limited source to build a prediction model, and the results might thus have been affected by sample bias. To avoid sample bias, we combined data from several sources to perform the cross-prediction. The majority of previous studies have used methods other than ANN, whereas we used ANN to construct a prediction model and used the sample sets to perform cross-laboratory training and prediction. The results showed that the survival of cancer patients across four hospitals predicted a good performance. However, the predictions showed some inconsistencies across the data sets, which might have been attributable to different medical treatments. Further research and clinical studies are recommended. Other types of cancer should be investigated to improve the prognostic methods of cancer patients.","Risk classification of cancer survival using ANN with gene expression data from multiple laboratoriesMicroarray , Gene expression , Neural network , Machine learning , Survival analysis , Outcome prediction , Lung cancer.Numerous cancer studies have combined gene expression experiments and clinical survival data to predict the prognosis of patients of specific gene types. However, most results of these studies were data dependent and were not suitable for other data sets. This study performed cross-laboratory validations for the cancer patient data from 4 hospitals. We investigated the feasibility of survival risk predictions using high-throughput gene expression data and clinical data. We analyzed multiple data sets for prognostic applications in lung cancer diagnosis. After building tens of thousands of various ANN architectures using the training data, five survival-time correlated genes were identified from 4 microarray gene expression data sets by examining the correlation between gene signatures and patient survival time. The experimental results showed that gene expression data can be used for valid predictions of cancer patient survival classification with an overall accuracy of 83.0% based on survival time trusted data. The results show the prediction model yielded excellent predictions given that patients in the high-risk group obtained a lower median overall survival compared with low-risk patients (log-rank test P-valueo0.00001). This study provides a foundation for further clinical studies and research into other types of cancer. We hope these findings will improve the prognostic methods of cancer patients.The results of this study showed that predicting cancer patients' survival was feasible using cross-laboratory gene expression data. The key to this analysis was identifying the pertinent genes. Previous studies have used data from a single limited source to build a prediction model, and the results might thus have been affected by sample bias. To avoid sample bias, we combined data from several sources to perform the cross-prediction. The majority of previous studies have used methods other than ANN, whereas we used ANN to construct a prediction model and used the sample sets to perform cross-laboratory training and prediction. The results showed that the survival of cancer patients across four hospitals predicted a good performance. However, the predictions showed some inconsistencies across the data sets, which might have been attributable to different medical treatments. Further research and clinical studies are recommended. Other types of cancer should be investigated to improve the prognostic methods of cancer patients.TensorFlow 1.0 LIBSVM 5% 5%. DMwR 5% Fig.3 Simulation environments used for cancer prediction and prog- nosis100 Fig.4 Best prediction rate analysis 9}eY VOID Various TechniquesMeta -heuristic Data mining Other techniques for techniques techniques information retrieval Clustering Classification ICP [32] and Breathing ' ' ' ' i SVM - ' ' ‘ ' ' Fig. 2 Categorization of different algorithms for cancer prediction and prognosisFig. 1 A chronological review of diverse cancer diagnosis and prognosis Contributions 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 Years",Medical Data Analysis,"This study aimed to investigate the feasibility of using high-throughput gene expression data and clinical data for predicting cancer patients' survival risk. The researchers performed cross-laboratory validations for the cancer patient data from four hospitals and identified five survival-time correlated genes from four microarray gene expression data sets. They used artificial neural networks (ANN) to construct a prediction model and achieved an overall accuracy of 83.0% based on survival time trusted data. The study showed that predicting cancer patients' survival was feasible using cross-laboratory gene expression data, and the key to this analysis was identifying the pertinent genes. The results of the study provide a foundation for further clinical studies and research into other types of cancer to improve the prognostic methods of cancer patients.",Medical Data Analysis,TensorFlow 1.0 LIBSVM 5% 5%. DMwR 5% Fig.3 Simulation environments used for cancer prediction and prog- nosis100 Fig.4 Best prediction rate analysis 9}eY VOID Various TechniquesMeta -heuristic Data mining Other techniques for techniques techniques information retrieval Clustering Classification ICP [32] and Breathing ' ' ' ' i SVM - ' ' ‘ ' ' Fig. 2 Categorization of different algorithms for cancer prediction and prognosisFig. 1 A chronological review of diverse cancer diagnosis and prognosis Contributions 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 Years,Deep Learning and Machine Learning
94,Risk of dementia in diabetes mellitus: a systematic review,"Diabetes, dementia, incidence, risk factors, vascular disease, glucose, insulin, amyloid metabolism, comorbidity, epidemiological studies.","The relation between diabetes and major types of dementia is controversial. This systematic review examines the incidence of dementia in people with diabetes mellitus. We identified 14 eligible longitudinal population-based studies of variable methodological quality. The incidence of “any dementia” was higher in individuals with diabetes than in those without diabetes in seven of ten studies reporting this aggregate outcome. This high risk included both Alzheimer’s disease and vascular dementia (eight of 13 studies and six of nine studies respectively). Detailed data on modulating and mediating effects of glycaemic control, microvascular complications, and comorbidity (eg, hypertension and stroke) were generally absent. The findings of mechanistic studies suggest that vascular disease and alterations in glucose, insulin, and amyloid metabolism underlie the pathophysiology, but which of these mechanisms are clinically relevant is unclear. Further high quality studies need to be initiated, with objective diabetes assessment, together with reliable methods to establish the contribution of vascular disease and other comorbidity to dementia.","There is convincing evidence that shows an increased risk of dementia in people with diabetes, but there are few detailed epidemiological data for risk factors. There are mechanistic studies that provide pathophysiological leads, but do not indicate which of these leads are clinically relevant. This gap in evidence between epidemiological and mechanistic studies needs to be closed. The risk factors and mechanisms that drive the association between diabetes and accelerated cognitive decline and dementia need to be identified before adequate treatment measures can be developed. This process will require longitudinal studies that include detailed assessment of cognition, preferably in combination with neuroimaging, as well as detailed assessment of diabetes-related factors and comorbid conditions. Studies on large population-based cohorts of elderly people with diabetes and longitudinal studies of at-risk populations that examine the progress of vascular disease, metabolic syndrome, diabetes, and cognition, will be best suited for this approach.","Risk of dementia in diabetes mellitus: a systematic reviewDiabetes, dementia, incidence, risk factors, vascular disease, glucose, insulin, amyloid metabolism, comorbidity, epidemiological studies.The relation between diabetes and major types of dementia is controversial. This systematic review examines the incidence of dementia in people with diabetes mellitus. We identified 14 eligible longitudinal population-based studies of variable methodological quality. The incidence of “any dementia” was higher in individuals with diabetes than in those without diabetes in seven of ten studies reporting this aggregate outcome. This high risk included both Alzheimer’s disease and vascular dementia (eight of 13 studies and six of nine studies respectively). Detailed data on modulating and mediating effects of glycaemic control, microvascular complications, and comorbidity (eg, hypertension and stroke) were generally absent. The findings of mechanistic studies suggest that vascular disease and alterations in glucose, insulin, and amyloid metabolism underlie the pathophysiology, but which of these mechanisms are clinically relevant is unclear. Further high quality studies need to be initiated, with objective diabetes assessment, together with reliable methods to establish the contribution of vascular disease and other comorbidity to dementia.There is convincing evidence that shows an increased risk of dementia in people with diabetes, but there are few detailed epidemiological data for risk factors. There are mechanistic studies that provide pathophysiological leads, but do not indicate which of these leads are clinically relevant. This gap in evidence between epidemiological and mechanistic studies needs to be closed. The risk factors and mechanisms that drive the association between diabetes and accelerated cognitive decline and dementia need to be identified before adequate treatment measures can be developed. This process will require longitudinal studies that include detailed assessment of cognition, preferably in combination with neuroimaging, as well as detailed assessment of diabetes-related factors and comorbid conditions. Studies on large population-based cohorts of elderly people with diabetes and longitudinal studies of at-risk populations that examine the progress of vascular disease, metabolic syndrome, diabetes, and cognition, will be best suited for this approach.Estimated Survival Function Kaplan-Meier Estimates — High Risk ~~~ Low Risk —— High Risk Censored ~+- Low Risk Censored 176 453. 0 50 100 150 Survival time(months) Train serEstimated Survival Function Kaplan-Meier Estimates — High Risk ~~~ Low Risk —- High Risk Censored ~t- Low Risk Censored —* N=56 N=76 16 55.6 ! 0 50 100 160 200 Survival time(months) Test & validation sez",Medical Data Analysis,"This systematic review examines the relationship between diabetes and the incidence of dementia. The study identified 14 eligible longitudinal population-based studies, which revealed that individuals with diabetes have a higher risk of developing dementia than those without diabetes. This risk includes both Alzheimer's disease and vascular dementia. However, there are few detailed epidemiological data for risk factors, and the contribution of vascular disease and other comorbid conditions to dementia needs to be established. Mechanistic studies suggest that vascular disease and alterations in glucose, insulin, and amyloid metabolism underlie the pathophysiology, but it is unclear which of these mechanisms are clinically relevant. Further high-quality studies are needed to identify the risk factors and mechanisms that drive the association between diabetes and accelerated cognitive decline and dementia. Longitudinal studies that include detailed assessment of cognition, preferably in combination with neuroimaging, as well as detailed assessment of diabetes-related factors and comorbid conditions, will be best suited for this approach.",Medical Data Analysis,Estimated Survival Function Kaplan-Meier Estimates — High Risk ~~~ Low Risk —— High Risk Censored ~+- Low Risk Censored 176 453. 0 50 100 150 Survival time(months) Train serEstimated Survival Function Kaplan-Meier Estimates — High Risk ~~~ Low Risk —- High Risk Censored ~t- Low Risk Censored —* N=56 N=76 16 55.6 ! 0 50 100 160 200 Survival time(months) Test & validation sez,Medical Data Analysis
95,Risk of interstitial lung disease associated with EGFR-TKIs in advanced non-small-cell lung cancer: a meta-analysis of 24 phase III clinical trials,"Non-small-cell lung cancer, Interstitial lung disease, Erlotinib, Gefitinib, Afatinib, Meta-analysis","Purpose: To assess the risk of interstitial lung disease (ILD) with epidermal growth factor receptor tyrosine kinase inhibitors (EGFR-TKIs) gefitinib, erlotinib, and afatinib. Method: PubMed databases were searched for relevant articles. Statistical analyses were conducted to calculate the summary incidence, odds ratio (OR), and 95% confidence intervals (CIs) by using either random-effects or fixed-effect models. Results: The incidence of all-grade and high-grade (§grade 3) ILD associated with EGFR-TKIs was 1.6% (95% CI, 1.0–2.4%) and 0.9% (95% CI, 0.6%–1.4%), with a mortality of 13.0% (95% CI, 7.6–21.6%). Patients treated with EGFR-TKIs had a significantly increased risk of developing all-grade (OR, 1.74; 95% CI, 1.25– 2.43; P50.001) and high-grade (OR, 4.38; 95% CI, 2.18–8.79; P,0.001) ILD. No significant difference in the risk of ILD was found in sub-group analysis according to EGFR-TKIs, percentage of EGFR mutation, study location, EGFR-TKIs-based regimens, and controlled therapy. Conclusions: Treatment with EGFR-TKIs is associated with a significantly increased risk of developing ILD.","In conclusion, our study has demonstrated that the use of EGFR-TKIs is associated with a substantial risk of developing ILD in patients with advanced NSCLC. Clinicians should have a high suspicion for the diagnosis of pulmonary toxicity when respiratory symptoms appear in patients receiving EGFR-TKIs treatment. Further investigation is necessary to establish if concomitant use of corticoids and EGFR-TKIs can lower the incidence of ILD associated with the EGFR-TKIs.","Risk of interstitial lung disease associated with EGFR-TKIs in advanced non-small-cell lung cancer: a meta-analysis of 24 phase III clinical trialsNon-small-cell lung cancer, Interstitial lung disease, Erlotinib, Gefitinib, Afatinib, Meta-analysisPurpose: To assess the risk of interstitial lung disease (ILD) with epidermal growth factor receptor tyrosine kinase inhibitors (EGFR-TKIs) gefitinib, erlotinib, and afatinib. Method: PubMed databases were searched for relevant articles. Statistical analyses were conducted to calculate the summary incidence, odds ratio (OR), and 95% confidence intervals (CIs) by using either random-effects or fixed-effect models. Results: The incidence of all-grade and high-grade (§grade 3) ILD associated with EGFR-TKIs was 1.6% (95% CI, 1.0–2.4%) and 0.9% (95% CI, 0.6%–1.4%), with a mortality of 13.0% (95% CI, 7.6–21.6%). Patients treated with EGFR-TKIs had a significantly increased risk of developing all-grade (OR, 1.74; 95% CI, 1.25– 2.43; P50.001) and high-grade (OR, 4.38; 95% CI, 2.18–8.79; P,0.001) ILD. No significant difference in the risk of ILD was found in sub-group analysis according to EGFR-TKIs, percentage of EGFR mutation, study location, EGFR-TKIs-based regimens, and controlled therapy. Conclusions: Treatment with EGFR-TKIs is associated with a significantly increased risk of developing ILD.In conclusion, our study has demonstrated that the use of EGFR-TKIs is associated with a substantial risk of developing ILD in patients with advanced NSCLC. Clinicians should have a high suspicion for the diagnosis of pulmonary toxicity when respiratory symptoms appear in patients receiving EGFR-TKIs treatment. Further investigation is necessary to establish if concomitant use of corticoids and EGFR-TKIs can lower the incidence of ILD associated with the EGFR-TKIs.Ageing Jinsulin, insulin receptor, Linsulin signalling, J insulin degrading enzyme Alzheimer's disease vs elderly controls =insulin, Tinsulin receptor, | insulin receptor signalling, Linsulin degrading enzyme ip P (B) The source of cerebral insulin (©) Cerebral insulin in ageing and dementia © Insulin §8 Insulin receptor @ Anyboids @ Insulin-degrading enzyme (D) Insulin and amyloid metabolism (A) Hyperinsulimaemia: atherosclerosis Insulin resistance syndrome Figure 2: The potential role of insulin in the pathogenesis of dementia Hyperinsulinaemia has been identified as a risk factor for accelerated cognitive decline and dementia, which may be mediated through vascular effects and direct effects of insulin on the brain. Hyperinsulinaemia, in the context of the insulin resistance syndrome, is a risk factor for artherosclerosis (A). Insulin is transported actively across the blood-brain barrier (1), may be produced locally in the brain (2), and—acting through cerebral insulin receptors (3)—modulates food-intake and energy homoeostasis (B).""? Ageing is associated with changes in insulin and its receptor in the brain, and these changes may be even more pronounced in individuals with Alzheimer's disease(C). Insulin also affects amyloid metabolism (D). Insulin stimulates the secretion of amyloid B (4) into the extracellular space where it can aggregate with other proteins to form senile plaques (5). Alternatively, excessive amyloid B can be cleared through endocytosis (6), or through direct extracellular proteolytic degradation by insulin-degrading enzyme (7).2**Diabetes + Comorbidity Genetic + Medication >| |e predisposition Underlying mechanisms Atherosclerosis Microvascular Glucose toxicity Insulin disease + Brain infarcts + Insidious + Advanced protein + T secretion and <y ischaemia @ glycation @ 1 breakdown of + Oxidative stress amyloid ws ws Brain pathology Dementia Figure 1: Proposed pathophysiological mechanisms linking diabetes to changes in the brain and dementia Diabetes and its comorbid conditions are associated with an increased risk of atherosclerosis and stroke, leading to vascular pathology in the brain, Glucose-mediated toxicity can lead to microvascular abnormalities and more widespread changes in cognition and brain structure, referred to as accelerated brain ageing. Additionally diabetes and its treatment might interfere with amyloid metabolism, giving rise to Alzheimer’s type pathology.",Medical Data Analysis,"This review paper aimed to assess the risk of developing interstitial lung disease (ILD) with the use of epidermal growth factor receptor tyrosine kinase inhibitors (EGFR-TKIs) gefitinib, erlotinib, and afatinib in patients with advanced non-small cell lung cancer. The study found that treatment with EGFR-TKIs was associated with a significantly increased risk of developing ILD, with an incidence of 1.6% for all-grade ILD and 0.9% for high-grade ILD, and a mortality rate of 13%. The study recommends that clinicians should monitor patients for respiratory symptoms and consider pulmonary toxicity as a diagnosis in patients receiving EGFR-TKI treatment. Further investigation is needed to determine whether concomitant use of corticoids and EGFR-TKIs can reduce the incidence of ILD associated with EGFR-TKIs.",Medical Data Analysis,"Ageing Jinsulin, insulin receptor, Linsulin signalling, J insulin degrading enzyme Alzheimer's disease vs elderly controls =insulin, Tinsulin receptor, | insulin receptor signalling, Linsulin degrading enzyme ip P (B) The source of cerebral insulin (©) Cerebral insulin in ageing and dementia © Insulin §8 Insulin receptor @ Anyboids @ Insulin-degrading enzyme (D) Insulin and amyloid metabolism (A) Hyperinsulimaemia: atherosclerosis Insulin resistance syndrome Figure 2: The potential role of insulin in the pathogenesis of dementia Hyperinsulinaemia has been identified as a risk factor for accelerated cognitive decline and dementia, which may be mediated through vascular effects and direct effects of insulin on the brain. Hyperinsulinaemia, in the context of the insulin resistance syndrome, is a risk factor for artherosclerosis (A). Insulin is transported actively across the blood-brain barrier (1), may be produced locally in the brain (2), and—acting through cerebral insulin receptors (3)—modulates food-intake and energy homoeostasis (B).""? Ageing is associated with changes in insulin and its receptor in the brain, and these changes may be even more pronounced in individuals with Alzheimer's disease(C). Insulin also affects amyloid metabolism (D). Insulin stimulates the secretion of amyloid B (4) into the extracellular space where it can aggregate with other proteins to form senile plaques (5). Alternatively, excessive amyloid B can be cleared through endocytosis (6), or through direct extracellular proteolytic degradation by insulin-degrading enzyme (7).2**Diabetes + Comorbidity Genetic + Medication >| |e predisposition Underlying mechanisms Atherosclerosis Microvascular Glucose toxicity Insulin disease + Brain infarcts + Insidious + Advanced protein + T secretion and <y ischaemia @ glycation @ 1 breakdown of + Oxidative stress amyloid ws ws Brain pathology Dementia Figure 1: Proposed pathophysiological mechanisms linking diabetes to changes in the brain and dementia Diabetes and its comorbid conditions are associated with an increased risk of atherosclerosis and stroke, leading to vascular pathology in the brain, Glucose-mediated toxicity can lead to microvascular abnormalities and more widespread changes in cognition and brain structure, referred to as accelerated brain ageing. Additionally diabetes and its treatment might interfere with amyloid metabolism, giving rise to Alzheimer’s type pathology.",Medical Data Analysis
96,Role of machine learning in medical research: A survey,"Medical research , Machine learning , Deep learning , Medical data.","Machine learning is one of the essential and effective tools in analyzing highly complex medical data. With vast amounts of medical data being generated, there is an urgent need to effectively use this data to benefit the medical and health care sectors all across the world. This survey paper presents a systematic literature review for the investigation of various machine learning techniques used for numerous medical applications which are published in highly reputable venues in recent years. Considering only the recent work, we are able to survey the current machine learning and deep learning models that are being used for medical data. This literature review identifies a clear shift of artificial intelligence techniques used in the medical domain, with deep learning methods taking precedence over machine learning methods.","With the emergence of big data, machine learning techniques are used to learn, analyze, and extrapolate details in the medical research. This survey provides a comprehensive overview of the ML techniques including support vector machines, K-means clustering, decision trees, random forests, Naïve Bayes, K nearest neighbors, neural networks, and convolution neural networks, that are being used for various types of medical data and applications in the recent years. A systemic literature review was conducted to search and select research articles from highly reputed and relevant journals in the recent years. The papers selected to survey and document in this survey paper are from journals with a high SJR and google scholar h-5 index. Other criteria like year of publication and number of citations were also taken into consideration in the selection process. Further this survey paper discusses about the current influx of medical data and the challenges that medical data faces with respect to its analysis. Medical data, due to its complex nature, needs intricate analysis techniques. ML techniques, but more recently DL models, have proven to understand medical image, and multi-variate numerical data. More recently, neural networks (and convolution neural networks) have been used for medical image segmentation and classification like brain lesion segmentation, lymph node detection, classification of nodules in human chest from CT scans, brain tumor classification, among many other applications. For other medical applications like medical diagnosis, and dementia prognosis, DL models are also being used more. In fact, for tabular datasets from UCI and KEEL, we have seen applications being shifted from traditional ML techniques to DL. Using empirical evidence, we have concluded that, in the recent years, DL has been preferred more (19 out of 33 papers) by the researchers to work with medical data. We believe that this survey paper can be of high importance for the researchers to observe relevant details and trends about the current research in the computer science and the medical research.","Role of machine learning in medical research: A surveyMedical research , Machine learning , Deep learning , Medical data.Machine learning is one of the essential and effective tools in analyzing highly complex medical data. With vast amounts of medical data being generated, there is an urgent need to effectively use this data to benefit the medical and health care sectors all across the world. This survey paper presents a systematic literature review for the investigation of various machine learning techniques used for numerous medical applications which are published in highly reputable venues in recent years. Considering only the recent work, we are able to survey the current machine learning and deep learning models that are being used for medical data. This literature review identifies a clear shift of artificial intelligence techniques used in the medical domain, with deep learning methods taking precedence over machine learning methods.With the emergence of big data, machine learning techniques are used to learn, analyze, and extrapolate details in the medical research. This survey provides a comprehensive overview of the ML techniques including support vector machines, K-means clustering, decision trees, random forests, Naïve Bayes, K nearest neighbors, neural networks, and convolution neural networks, that are being used for various types of medical data and applications in the recent years. A systemic literature review was conducted to search and select research articles from highly reputed and relevant journals in the recent years. The papers selected to survey and document in this survey paper are from journals with a high SJR and google scholar h-5 index. Other criteria like year of publication and number of citations were also taken into consideration in the selection process. Further this survey paper discusses about the current influx of medical data and the challenges that medical data faces with respect to its analysis. Medical data, due to its complex nature, needs intricate analysis techniques. ML techniques, but more recently DL models, have proven to understand medical image, and multi-variate numerical data. More recently, neural networks (and convolution neural networks) have been used for medical image segmentation and classification like brain lesion segmentation, lymph node detection, classification of nodules in human chest from CT scans, brain tumor classification, among many other applications. For other medical applications like medical diagnosis, and dementia prognosis, DL models are also being used more. In fact, for tabular datasets from UCI and KEEL, we have seen applications being shifted from traditional ML techniques to DL. Using empirical evidence, we have concluded that, in the recent years, DL has been preferred more (19 out of 33 papers) by the researchers to work with medical data. We believe that this survey paper can be of high importance for the researchers to observe relevant details and trends about the current research in the computer science and the medical research.a1 2012 2012 Rosell R. ot al 2012 Zhou et ar 2014 Takeda I< 2010 Mitsuctomi T. ot al2010 Maemondo M. et al2010 Maruyama 2008. kim ©. st al 2008 Gatzemeier U. et a!2007 Milter VA 2012 CVier al 2013 ‘Uohneon 6.6. et al 2013 Studies = Zhang L. et al2012 ~ Sun J.M. et al 2012 = Roselli R. et al2012 = Zhou C. et al 2011 Takeda K. et al2010 = Mitsudomi T. et al.2010 = Maemondo M. et al.2010 = Mok T.S. et al2009 - Maruyama Ret al,2008 = Gatzemeier U. et al.2007 ~ Herbst R.S. et al.2005 = Lee D.H. et al 2010 = Miller V.A. et al 2012 = Wu ¥.L. et al 2014 = Johnson B.E. et al 2013 = Han J.¥. et al. 2012 2.549 (1-088, 2-204) iisas (iloee, 2.204) 31873 Gio! 21240) iisee (a:103; 2.230) iissa (i1079! 21209) Glove: 2lie” sia (21061; 21182) 31490 Gioia, 2.109) (21332) 21370) ileoo (1122) 21203) (lia) ais7e (21107, Eetimate 2.880 (1-388, 2.ss2 (1.389, 2.995 (1.483, 3.007 (1-458, 31120 (1-426, 2.878 (1.387, 2.733 (1.313, 2.884 (1.363, 2.737 (1-315, 2.882 (1-389, 2.696 (1.268, 2.689 (1.393, 3.065 (1.486, 2.998 (1.443, 2.796 (1.344, 2.805 (1.349, (ase cere) 5.975) 5.979) 6.277) 6.202) 6.829) 5.972) 5.689) 6.103) 51697) 5.980) 5.732) 5.995) 6.320) 6.214) 5.813) 5.832) (Cdds Ratio (log scale)studies Zhang L. et at 2012 Bnou Cet arzort Taneda Kat at 2010 Mitsudomi Tet aloo ‘Macmondo Met ai2010 Mok T'S. et ai 3009 Maruyama R.'2008, Kim ©. et al 2008 ‘Gatsomoier U. at ai2007 Kee D4. et al 2010 Miter VA. wt at 2042 Sequiet Vet al 2033 wary tet al 2014 Man JY etal 2012 Sohnaon Se et at 2013 ‘Overall (280% . P=0.030) Studies Zhang L. et ai2012 ‘Sun JM. at al 2012 Rosell R. et al2012 Zhou C. et al 2011 Takeda K. et a!2010 Mitsudomi T. et al.2010 ‘Maemondo M. et al.2010 ‘Mok T-S. et al2009 ‘Maruyama Ret al.2008 Gatzemeler U. et al.2007 Herbst R.S. et al. 2005 Lee D.H. et al 2010 Miller V.A. et al 2012 Wa ¥.L etal 2014 Johnson 8.€. et al 2019 Han J.¥. et at 2012 ‘Overall (14220% , P=0.004) aia (o.330, 371982) al298) 8ra08) aoee) 25483) 38lo32) Eetimate (958 C1.) 2i6s9 0.976 risse 21768 4.832 21645 4.432 31824 21870 0.474 11392 aiere 302 (0.273, (0167, «0.039. (0.027, (0.330, (0.272, (0.630, (0.372. (0.620, (0.270, (0.769, (0.159, (0.007, (01072, (0.396, (01373) «2.204, 44.120) 43.592) 49.228) 44,202) 21708) 4a. se2) 321896) 18.625) 31.656) 43.473) 19.018) 41.813) 30.239) 27.143) 36-016) 351032) 8.783) 00/7006 ev/tre 3/367 1768 oves ove3 47300 a7e7 3yare 3/607 3/244 1/580 5/539 1701 07390 17239 27367 2/189 20/400: acyeoza zysctea ovi68 over ovez ov72 2/298 ves 9/233 1/589 97239 07879 1/540 or76 97195 7323 9/368 97350 assSUbgroup ertounib (520% , Pmo.570) Miter VA. 9t at 2012 Sequiet CVior al 2013 Wary tet al 2014 Subgroup atatinib (2630% , P=0.244) Overall (22=65% . P=0.000) ‘Sun JM. ot al 2012 Mitsudom! Tet a 2010 Maemonde M. et al.2010 Mok T-5. et al.2009. ‘Maruyama Ret al, 2008 Lee DH et al 2010 Han JY. et al, 2012 Subgroup gefitinib (I6280% . P=0.707) Scagtiotti G.V. et a1. 2012 Rosell R. ot al.2032 Zhou C. et 91.2011 Herbat RS. et a 2011 ‘Subgroup erlotinib (142=0% , P=0.712) Miter VA. et 21.2012 Wor. etal2zor Subgroup afatinib (1*280% , P=o.403) ‘Overall (42=0% , P=0.626) (o1016, 0.049) toloaa: 9:98) (81008, 0.018) (0.014, ©1035) (21002, 0,080) {o000; oleae) (©1002, 0.016) (9.000, 9.020) (9008; 9.040) fole03; 91029) toe02, 0.022) (o.020. 0.024) Hotimate (958 6.2.) olox2 1006 (0.001, 9.047) (0002; 0.097) (01002, 6.077) (0.909, 0.079) (01002, 0.018) (01004; 0.037) {0:002, 0.082) (01003; 0.049) (0.007, 0.019) «0.000, 9.026) {91000; 0.007) (01000, 01088) (0.002; 0.028) (0.002, 0.024) (0.900, 0.020) (91001, 01029) (0.001, 0.024) (0.006, 0.024) Loot Srepertion - -577 published articles identified through database searching 118 clinical trials and 76 abstracts were reviewed for further inclusion 24 phase If clinical trials eligible for meta-analysis Gefitinib (n=13) Erlotinib (n=8) Afatinib (n=3) 459 articles were excluded: Phase 1 trials; Case reports; Observational studies; Review articles; Letters or commentaries; meta-analysis 261 additional records identified through conference proceedings, clinical trial registries 94 articles and 76 abstracts excluded: 50 duplicated abstracts; 26 abstracts: data not adequate for ILD; 86 phase II clinical trials; 7 phase III trials: data not adequate for ILD; 1 phase III trials: all treatment arms received EGFR-TKIs combination therapy°",Deep Learning and Machine Learning,"This survey paper provides a systematic literature review of various machine learning and deep learning techniques used for medical data analysis in recent years. The paper identifies a clear shift in the use of artificial intelligence techniques in the medical domain, with deep learning methods taking precedence over machine learning methods. The review discusses the challenges that medical data faces with respect to its analysis and presents various ML and DL techniques that have been used for medical image segmentation, classification, medical diagnosis, and dementia prognosis. The paper concludes that DL has been preferred more by researchers to work with medical data in recent years. This survey paper can be of high importance for researchers to observe relevant details and trends about the current research in computer science and medical research.",Deep Learning and Machine Learning,"a1 2012 2012 Rosell R. ot al 2012 Zhou et ar 2014 Takeda I< 2010 Mitsuctomi T. ot al2010 Maemondo M. et al2010 Maruyama 2008. kim ©. st al 2008 Gatzemeier U. et a!2007 Milter VA 2012 CVier al 2013 ‘Uohneon 6.6. et al 2013 Studies = Zhang L. et al2012 ~ Sun J.M. et al 2012 = Roselli R. et al2012 = Zhou C. et al 2011 Takeda K. et al2010 = Mitsudomi T. et al.2010 = Maemondo M. et al.2010 = Mok T.S. et al2009 - Maruyama Ret al,2008 = Gatzemeier U. et al.2007 ~ Herbst R.S. et al.2005 = Lee D.H. et al 2010 = Miller V.A. et al 2012 = Wu ¥.L. et al 2014 = Johnson B.E. et al 2013 = Han J.¥. et al. 2012 2.549 (1-088, 2-204) iisas (iloee, 2.204) 31873 Gio! 21240) iisee (a:103; 2.230) iissa (i1079! 21209) Glove: 2lie” sia (21061; 21182) 31490 Gioia, 2.109) (21332) 21370) ileoo (1122) 21203) (lia) ais7e (21107, Eetimate 2.880 (1-388, 2.ss2 (1.389, 2.995 (1.483, 3.007 (1-458, 31120 (1-426, 2.878 (1.387, 2.733 (1.313, 2.884 (1.363, 2.737 (1-315, 2.882 (1-389, 2.696 (1.268, 2.689 (1.393, 3.065 (1.486, 2.998 (1.443, 2.796 (1.344, 2.805 (1.349, (ase cere) 5.975) 5.979) 6.277) 6.202) 6.829) 5.972) 5.689) 6.103) 51697) 5.980) 5.732) 5.995) 6.320) 6.214) 5.813) 5.832) (Cdds Ratio (log scale)studies Zhang L. et at 2012 Bnou Cet arzort Taneda Kat at 2010 Mitsudomi Tet aloo ‘Macmondo Met ai2010 Mok T'S. et ai 3009 Maruyama R.'2008, Kim ©. et al 2008 ‘Gatsomoier U. at ai2007 Kee D4. et al 2010 Miter VA. wt at 2042 Sequiet Vet al 2033 wary tet al 2014 Man JY etal 2012 Sohnaon Se et at 2013 ‘Overall (280% . P=0.030) Studies Zhang L. et ai2012 ‘Sun JM. at al 2012 Rosell R. et al2012 Zhou C. et al 2011 Takeda K. et a!2010 Mitsudomi T. et al.2010 ‘Maemondo M. et al.2010 ‘Mok T-S. et al2009 ‘Maruyama Ret al.2008 Gatzemeler U. et al.2007 Herbst R.S. et al. 2005 Lee D.H. et al 2010 Miller V.A. et al 2012 Wa ¥.L etal 2014 Johnson 8.€. et al 2019 Han J.¥. et at 2012 ‘Overall (14220% , P=0.004) aia (o.330, 371982) al298) 8ra08) aoee) 25483) 38lo32) Eetimate (958 C1.) 2i6s9 0.976 risse 21768 4.832 21645 4.432 31824 21870 0.474 11392 aiere 302 (0.273, (0167, «0.039. (0.027, (0.330, (0.272, (0.630, (0.372. (0.620, (0.270, (0.769, (0.159, (0.007, (01072, (0.396, (01373) «2.204, 44.120) 43.592) 49.228) 44,202) 21708) 4a. se2) 321896) 18.625) 31.656) 43.473) 19.018) 41.813) 30.239) 27.143) 36-016) 351032) 8.783) 00/7006 ev/tre 3/367 1768 oves ove3 47300 a7e7 3yare 3/607 3/244 1/580 5/539 1701 07390 17239 27367 2/189 20/400: acyeoza zysctea ovi68 over ovez ov72 2/298 ves 9/233 1/589 97239 07879 1/540 or76 97195 7323 9/368 97350 assSUbgroup ertounib (520% , Pmo.570) Miter VA. 9t at 2012 Sequiet CVior al 2013 Wary tet al 2014 Subgroup atatinib (2630% , P=0.244) Overall (22=65% . P=0.000) ‘Sun JM. ot al 2012 Mitsudom! Tet a 2010 Maemonde M. et al.2010 Mok T-5. et al.2009. ‘Maruyama Ret al, 2008 Lee DH et al 2010 Han JY. et al, 2012 Subgroup gefitinib (I6280% . P=0.707) Scagtiotti G.V. et a1. 2012 Rosell R. ot al.2032 Zhou C. et 91.2011 Herbat RS. et a 2011 ‘Subgroup erlotinib (142=0% , P=0.712) Miter VA. et 21.2012 Wor. etal2zor Subgroup afatinib (1*280% , P=o.403) ‘Overall (42=0% , P=0.626) (o1016, 0.049) toloaa: 9:98) (81008, 0.018) (0.014, ©1035) (21002, 0,080) {o000; oleae) (©1002, 0.016) (9.000, 9.020) (9008; 9.040) fole03; 91029) toe02, 0.022) (o.020. 0.024) Hotimate (958 6.2.) olox2 1006 (0.001, 9.047) (0002; 0.097) (01002, 6.077) (0.909, 0.079) (01002, 0.018) (01004; 0.037) {0:002, 0.082) (01003; 0.049) (0.007, 0.019) «0.000, 9.026) {91000; 0.007) (01000, 01088) (0.002; 0.028) (0.002, 0.024) (0.900, 0.020) (91001, 01029) (0.001, 0.024) (0.006, 0.024) Loot Srepertion - -577 published articles identified through database searching 118 clinical trials and 76 abstracts were reviewed for further inclusion 24 phase If clinical trials eligible for meta-analysis Gefitinib (n=13) Erlotinib (n=8) Afatinib (n=3) 459 articles were excluded: Phase 1 trials; Case reports; Observational studies; Review articles; Letters or commentaries; meta-analysis 261 additional records identified through conference proceedings, clinical trial registries 94 articles and 76 abstracts excluded: 50 duplicated abstracts; 26 abstracts: data not adequate for ILD; 86 phase II clinical trials; 7 phase III trials: data not adequate for ILD; 1 phase III trials: all treatment arms received EGFR-TKIs combination therapy°",Medical Data Analysis
97,Smartphone use is a risk factor for pediatric dry eye disease according to region and age: a case control study,"Dry eye disease, Pediatrics, Smartphone, Outdoor activity, Video display terminal","Background: In 2014, the overall rate of smartphone use in Korea was 83 and 89.8 % in children and adolescents. The rate of smartphone use differs according to region (urban vs. rural) and age (younger grade vs. older grade). We investigated risk and protective factors associated with pediatric dry eye disease (DED) in relation to smartphone use rate according to region and age. Methods: We enrolled 916 children and performed an ocular exam that included slit lamp exam and tear break-up time. A questionnaire administered to children and their families consisted of video display terminal (VDT) use, outdoor activity, learning, and modified ocular surface disease index (OSDI) score. DED was defined based on the International Dry Eye Workshop guidelines (Objective signs: punctate epithelial erosion or short tear break-up time; subjective symptoms: modified OSDI score) We performed statistical analysis of risk factors and protective factors in children divided into groups as follows: DED vs. control, urban vs. rural, younger grade (1st to 3rd) vs. older grade (4th to 6th). Results: A total of 6.6 % of children were included in the DED group, and 8.3 % of children in the urban group were diagnosed with DED compared to 2.8 % in the rural group (P = 0.03). The rate of smartphone use was 61.3 % in the urban group and 51.0 % in the rural group (P = 0.04). In total, 9.1 % of children in the older-grade group were diagnosed with DED compared to 4 % in the younger-grade group (P = 0.03). The rate of smartphone use was 65.1 % in older-grade children and 50.9 % in younger-grade children (P < 0.001). The mean daily duration of smartphone use was longer in the DED group than controls (logistic regression analysis, P < 0.001, OR = 13.07), and the mean daily duration of outdoor activities was shorter in the DED group than controls (logistic regression analysis, P < 0. 01, OR = 0.33). After cessation of smartphone use for 4 weeks in the DED group, both subjective symptoms and objective signs had improved. Conclusions: Smartphone use in children was strongly associated with pediatric DED; however, outdoor activity appeared to be protective against pediatric DED. Older-grade students in urban environments had DED risk factors (long duration of smartphone use), and a short duration of outdoor activity time. Therefore, close observation and caution are needed when older children in urban areas use smartphones.","In conclusion, increased VDT use such as smartphones or computers in Korean children was found to be associated with the occurrence of ocular surface symptoms. Increased use of smartphones is a serious issue that may result in ocular problems: the prevalence of DED was 6.6 %. The mean duration of VDT use per day, especially smartphone use, was greater in the DED group than in the normal group. Longer daily smartphone use may be a risk factor for DED. The prevalence of DED was higher in the urban group and in older children. Older-grade children in the urban group used smartphones for longer time periods than younger-grade children in rural areas. DED prevalence was higher in older grade children from urban groups compared to younger grade children in rural groups. After cessation of smartphone use for 4 weeks in DED patients, both subjective symptoms and objective signs improved. Therefore, close observation and caution during VDT use, especially smartphones, is recommended for older children in urban areas. DED in children must be detected early and should be treated with appropriate medical and environmental interventions and education.","Smartphone use is a risk factor for pediatric dry eye disease according to region and age: a case control studyDry eye disease, Pediatrics, Smartphone, Outdoor activity, Video display terminalBackground: In 2014, the overall rate of smartphone use in Korea was 83 and 89.8 % in children and adolescents. The rate of smartphone use differs according to region (urban vs. rural) and age (younger grade vs. older grade). We investigated risk and protective factors associated with pediatric dry eye disease (DED) in relation to smartphone use rate according to region and age. Methods: We enrolled 916 children and performed an ocular exam that included slit lamp exam and tear break-up time. A questionnaire administered to children and their families consisted of video display terminal (VDT) use, outdoor activity, learning, and modified ocular surface disease index (OSDI) score. DED was defined based on the International Dry Eye Workshop guidelines (Objective signs: punctate epithelial erosion or short tear break-up time; subjective symptoms: modified OSDI score) We performed statistical analysis of risk factors and protective factors in children divided into groups as follows: DED vs. control, urban vs. rural, younger grade (1st to 3rd) vs. older grade (4th to 6th). Results: A total of 6.6 % of children were included in the DED group, and 8.3 % of children in the urban group were diagnosed with DED compared to 2.8 % in the rural group (P = 0.03). The rate of smartphone use was 61.3 % in the urban group and 51.0 % in the rural group (P = 0.04). In total, 9.1 % of children in the older-grade group were diagnosed with DED compared to 4 % in the younger-grade group (P = 0.03). The rate of smartphone use was 65.1 % in older-grade children and 50.9 % in younger-grade children (P < 0.001). The mean daily duration of smartphone use was longer in the DED group than controls (logistic regression analysis, P < 0.001, OR = 13.07), and the mean daily duration of outdoor activities was shorter in the DED group than controls (logistic regression analysis, P < 0. 01, OR = 0.33). After cessation of smartphone use for 4 weeks in the DED group, both subjective symptoms and objective signs had improved. Conclusions: Smartphone use in children was strongly associated with pediatric DED; however, outdoor activity appeared to be protective against pediatric DED. Older-grade students in urban environments had DED risk factors (long duration of smartphone use), and a short duration of outdoor activity time. Therefore, close observation and caution are needed when older children in urban areas use smartphones.In conclusion, increased VDT use such as smartphones or computers in Korean children was found to be associated with the occurrence of ocular surface symptoms. Increased use of smartphones is a serious issue that may result in ocular problems: the prevalence of DED was 6.6 %. The mean duration of VDT use per day, especially smartphone use, was greater in the DED group than in the normal group. Longer daily smartphone use may be a risk factor for DED. The prevalence of DED was higher in the urban group and in older children. Older-grade children in the urban group used smartphones for longer time periods than younger-grade children in rural areas. DED prevalence was higher in older grade children from urban groups compared to younger grade children in rural groups. After cessation of smartphone use for 4 weeks in DED patients, both subjective symptoms and objective signs improved. Therefore, close observation and caution during VDT use, especially smartphones, is recommended for older children in urban areas. DED in children must be detected early and should be treated with appropriate medical and environmental interventions and education.# of references Google 2 Scholar 0 h-5 index 1-100 101-200 201-300 301-400# OF TCLerenees 5 # of citations 0-100 101-1000 1000 +x2Vata Tree | Tree 2 Tree 3 Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No ClassA ClassB Class A Class B Class A ClassB = Class A Class B Class A ClassB Class A Class B# of references Years 1967-2000 9001-2015 2016-2020Distribution of references by quartile of SJR ranking Q2 - 12% QI - 88%a | nn) 1, Introduction 2. Methods Survey structure > Search Strategy 4. Machine Learning Techniques - Support vector machines - Clustering - K-means clustering - Decision trees 5. Results > Motivation Selection Strategy 3. Medical Data —> - Random forests - K nearest neighbors 6. Conclusion - Naive bayes - Deep learning - Neural networks - Convolutional neural networks - Other recent medical applications of ML and DLDataset (R): (x1, xa, -.. Define D: (Xp, y), where Xn is the set of input features, and y is the set of class variables Dataset of human brain images with traces of cancer cells Select Xm, where Xm is the set of important features to be used Selecting the features with most information to detect cancer Data Preparation Se Train ML algorithm with xm and y Models being trained to identify cancer cells w.r.t. pixels from brain images Model selection and training Model Evaluation and tuning Model DeploymentKNN -3 Random Forests -5 a Novel ML Techniques - 1 Clustering - 5 ty Naive Bayes -3 O Decision Trees - 6 - K-means -2 NN-6 CNN - 13 = SVM -732x32 6x28x28 6x14x14 16x10x10 16x5x5_—_120x1x1 ste, x3 x28x x14x x 10x x5x: x Lx — + all 10x1x1 ») 7 de AP Ne ward. 4 ; = : 2 wal AF? bald tice ull-connected layer F6 ? zs, Convolutional layer Subsampling layer Convolutional layer Subsampling layer Convolutional layer Convolutional layer C1 Convolutional layer C3 Cl : 6 kernels(5x5) $2: 2x2 C3 : 16 kernels (5x5) S4: (2x2) C5 : 1920 kernels (5x5) 6x28x28 16x10x10 (a) LeNet-5 network (b) Learned features",Medical Data Analysis,"The study investigated risk and protective factors associated with pediatric dry eye disease (DED) in relation to smartphone use rate according to region and age. The prevalence of DED was higher in the urban group and in older children, and smartphone use was found to be strongly associated with pediatric DED. The mean daily duration of smartphone use was longer in the DED group than controls, and outdoor activity appeared to be protective against pediatric DED. Older-grade students in urban environments had DED risk factors (long duration of smartphone use), and a short duration of outdoor activity time. The study recommends close observation and caution during VDT use, especially smartphones, for older children in urban areas. DED in children must be detected early and should be treated with appropriate medical and environmental interventions and education.",Medical Data Analysis,"# of references Google 2 Scholar 0 h-5 index 1-100 101-200 201-300 301-400# OF TCLerenees 5 # of citations 0-100 101-1000 1000 +x2Vata Tree | Tree 2 Tree 3 Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No Yes No ClassA ClassB Class A Class B Class A ClassB = Class A Class B Class A ClassB Class A Class B# of references Years 1967-2000 9001-2015 2016-2020Distribution of references by quartile of SJR ranking Q2 - 12% QI - 88%a | nn) 1, Introduction 2. Methods Survey structure > Search Strategy 4. Machine Learning Techniques - Support vector machines - Clustering - K-means clustering - Decision trees 5. Results > Motivation Selection Strategy 3. Medical Data —> - Random forests - K nearest neighbors 6. Conclusion - Naive bayes - Deep learning - Neural networks - Convolutional neural networks - Other recent medical applications of ML and DLDataset (R): (x1, xa, -.. Define D: (Xp, y), where Xn is the set of input features, and y is the set of class variables Dataset of human brain images with traces of cancer cells Select Xm, where Xm is the set of important features to be used Selecting the features with most information to detect cancer Data Preparation Se Train ML algorithm with xm and y Models being trained to identify cancer cells w.r.t. pixels from brain images Model selection and training Model Evaluation and tuning Model DeploymentKNN -3 Random Forests -5 a Novel ML Techniques - 1 Clustering - 5 ty Naive Bayes -3 O Decision Trees - 6 - K-means -2 NN-6 CNN - 13 = SVM -732x32 6x28x28 6x14x14 16x10x10 16x5x5_—_120x1x1 ste, x3 x28x x14x x 10x x5x: x Lx — + all 10x1x1 ») 7 de AP Ne ward. 4 ; = : 2 wal AF? bald tice ull-connected layer F6 ? zs, Convolutional layer Subsampling layer Convolutional layer Subsampling layer Convolutional layer Convolutional layer C1 Convolutional layer C3 Cl : 6 kernels(5x5) $2: 2x2 C3 : 16 kernels (5x5) S4: (2x2) C5 : 1920 kernels (5x5) 6x28x28 16x10x10 (a) LeNet-5 network (b) Learned features",Deep Learning and Machine Learning
98,State of the art and a mixed-method personalized approach to assess patient perceptions on medical record sharing and sensitivity,"Sensitive data , Patient preferences , Data sharing , Card sorting , Behavioral health , Electronic medical records.","Objective: Sensitive health information possesses risks, such as stigma and discrimination, when disclosed. Few studies have used a patient’s own electronic health records (EHRs) to explore what types of information are considered sensitive and how such perceptions affect data sharing preferences. After a systematic literature review, we designed and piloted a mixed-method approach that employs an individual’s own records to assess content sensitivity and preferences for granular data sharing for care and research. Methods: A systematic literature review of methodologies employed to assess data sharing willingness and perceptions on data sensitivity was conducted. A methodology was designed to organize and categorize sensitive health information from EHRs. Patients were asked permission to access their EHRs, including those available through the state’s health information exchange. A semi-structured interview script with closed card sorting was designed and personalized to each participant’s own EHRs using 30 items from each patient record. This mixed method combines the quantitative outcomes from the card sorting exercises with themes captured from interview audio recording analysis. Results: Eight publications on patients’ perspectives on data sharing and sensitivity were found. Based on our systematic review, the proposed method meets a need to use EHRs to systematize the study of data privacy issues. Twenty-five patients with behavioral health conditions, English and Spanish-speaking, were recruited. On average, participants recognized 82.7% of the 30 items from their own EHRs. Participants considered mental health (76.0%), sexual and reproductive health (75.0%) and alcohol use and alcoholism (50.0%) sensitive information. Participants were willing to share information related to other addictions (100.0%), genetic data (95.8%) and general physical health information (90.5%). Conclusion: The findings indicate diversity in patient views on EHR sensitivity and data sharing preferences and the need for more granular and patient-centered electronic consent mechanisms to accommodate patient needs. More research is needed to validate the generalizability of the proposed methodology.","Based on a comprehensive state of the art review on data sensitivity and sharing perceptions, we proposed a novel, personalized card sorting methodology using an individual’s own EHR to explore sensitive data definitions, perceptions, comprehension and willingness to share categories of health information for care and research. We identified diversity in patient perceptions of data sensitivity and desire for granular health records sharing. These outcomes provide new information about patient attitudes towards sensitive data and sharing preferences that will inform policy formation and guide the ongoing development of an electronic, patient-driven, informed consent platform for granular data sharing with personalized on-demand education.","State of the art and a mixed-method personalized approach to assess patient perceptions on medical record sharing and sensitivitySensitive data , Patient preferences , Data sharing , Card sorting , Behavioral health , Electronic medical records.Objective: Sensitive health information possesses risks, such as stigma and discrimination, when disclosed. Few studies have used a patient’s own electronic health records (EHRs) to explore what types of information are considered sensitive and how such perceptions affect data sharing preferences. After a systematic literature review, we designed and piloted a mixed-method approach that employs an individual’s own records to assess content sensitivity and preferences for granular data sharing for care and research. Methods: A systematic literature review of methodologies employed to assess data sharing willingness and perceptions on data sensitivity was conducted. A methodology was designed to organize and categorize sensitive health information from EHRs. Patients were asked permission to access their EHRs, including those available through the state’s health information exchange. A semi-structured interview script with closed card sorting was designed and personalized to each participant’s own EHRs using 30 items from each patient record. This mixed method combines the quantitative outcomes from the card sorting exercises with themes captured from interview audio recording analysis. Results: Eight publications on patients’ perspectives on data sharing and sensitivity were found. Based on our systematic review, the proposed method meets a need to use EHRs to systematize the study of data privacy issues. Twenty-five patients with behavioral health conditions, English and Spanish-speaking, were recruited. On average, participants recognized 82.7% of the 30 items from their own EHRs. Participants considered mental health (76.0%), sexual and reproductive health (75.0%) and alcohol use and alcoholism (50.0%) sensitive information. Participants were willing to share information related to other addictions (100.0%), genetic data (95.8%) and general physical health information (90.5%). Conclusion: The findings indicate diversity in patient views on EHR sensitivity and data sharing preferences and the need for more granular and patient-centered electronic consent mechanisms to accommodate patient needs. More research is needed to validate the generalizability of the proposed methodology.Based on a comprehensive state of the art review on data sensitivity and sharing perceptions, we proposed a novel, personalized card sorting methodology using an individual’s own EHR to explore sensitive data definitions, perceptions, comprehension and willingness to share categories of health information for care and research. We identified diversity in patient perceptions of data sensitivity and desire for granular health records sharing. These outcomes provide new information about patient attitudes towards sensitive data and sharing preferences that will inform policy formation and guide the ongoing development of an electronic, patient-driven, informed consent platform for granular data sharing with personalized on-demand education.",Medical Data Analysis,"The study aimed to explore patients' perceptions of sensitive health information and how such perceptions affect data sharing preferences. After conducting a systematic literature review, the researchers designed a mixed-method approach that used an individual's electronic health records (EHRs) to assess content sensitivity and preferences for granular data sharing for care and research. The study involved 25 patients with behavioral health conditions who were asked permission to access their EHRs, including those available through the state's health information exchange. The results showed that participants considered mental health, sexual and reproductive health, and alcohol use and alcoholism sensitive information, while they were willing to share information related to other addictions, genetic data, and general physical health information. The study concluded that patient views on EHR sensitivity and data sharing preferences are diverse, and there is a need for more patient-centered electronic consent mechanisms to accommodate patient needs. The proposed methodology provides new information about patient attitudes towards sensitive data and sharing preferences that can inform policy formation and guide the ongoing development of an electronic, patient-driven, informed consent platform for granular data sharing with personalized on-demand education.",Medical Data Analysis,,Medical Data Analysis
99,Sugar Industry and Coronary Heart Disease Research : A Historical Analysis of Internal Industry Documents,"coronary heart disease, sugar, research, industry sponsorship, risk factors, policy making, health consequences, biomarkers.","Early warning signals of the coronary heart disease (CHD) risk of sugar (sucrose) emerged in the 1950s. We examined Sugar Research Foundation (SRF) internal documents, historical reports, and statements relevant to early debates about the dietary causes of CHD and assembled findings chronologically into a narrative case study. The SRF sponsored its first CHD research project in 1965, a literature review published in the New England Journal of Medicine, which singled out fat and cholesterol as the dietary causes of CHD and downplayed evidence that sucrose consumption was also a risk factor. The SRF set the review’s objective, contributed articles for inclusion, and received drafts. The SRF’s funding and role was not disclosed. Together with other recent analyses of sugar industry documents, our findings suggest the industry sponsored a research program in the 1960s and 1970s that successfully cast doubt about the hazards of sucrose while promoting fat as the dietary culprit in CHD. Policymaking committees should consider giving less weight to food industry–funded studies and include mechanistic and animal studies as well as studies appraising the effect of added sugars on multiple CHD biomarkers and disease development.","This study suggests that the sugar industry sponsored its first CHD research project in 1965 to downplay early warning signals that sucrose consumption was a risk factor in CHD. As of 2016, sugar control policies are being promulgated in international,61 federal, state, and local venues. Yet CHD risk is inconsistently cited as a health consequence of added sugars consumption. Because CHD is the leading cause of death globally, the health community should ensure that CHD risk is evaluated in future risk assessments of added sugars. Policymaking committees should consider giving less weight to food industry–funded studies, and include mechanistic and animal studies as well as studies appraising the effect of added sugars on multiple CHD biomarkers and disease development.","Sugar Industry and Coronary Heart Disease Research : A Historical Analysis of Internal Industry Documentscoronary heart disease, sugar, research, industry sponsorship, risk factors, policy making, health consequences, biomarkers.Early warning signals of the coronary heart disease (CHD) risk of sugar (sucrose) emerged in the 1950s. We examined Sugar Research Foundation (SRF) internal documents, historical reports, and statements relevant to early debates about the dietary causes of CHD and assembled findings chronologically into a narrative case study. The SRF sponsored its first CHD research project in 1965, a literature review published in the New England Journal of Medicine, which singled out fat and cholesterol as the dietary causes of CHD and downplayed evidence that sucrose consumption was also a risk factor. The SRF set the review’s objective, contributed articles for inclusion, and received drafts. The SRF’s funding and role was not disclosed. Together with other recent analyses of sugar industry documents, our findings suggest the industry sponsored a research program in the 1960s and 1970s that successfully cast doubt about the hazards of sucrose while promoting fat as the dietary culprit in CHD. Policymaking committees should consider giving less weight to food industry–funded studies and include mechanistic and animal studies as well as studies appraising the effect of added sugars on multiple CHD biomarkers and disease development.This study suggests that the sugar industry sponsored its first CHD research project in 1965 to downplay early warning signals that sucrose consumption was a risk factor in CHD. As of 2016, sugar control policies are being promulgated in international,61 federal, state, and local venues. Yet CHD risk is inconsistently cited as a health consequence of added sugars consumption. Because CHD is the leading cause of death globally, the health community should ensure that CHD risk is evaluated in future risk assessments of added sugars. Policymaking committees should consider giving less weight to food industry–funded studies, and include mechanistic and animal studies as well as studies appraising the effect of added sugars on multiple CHD biomarkers and disease development.Extremely Willing to Share Quite Willing to Share 76% 1 Somewhat Willing to Share Not at All Willing to Share 2 64% 3 52% 2 48% z 40% z E oa 24% 24% 28% 28"" i S 16% 7 2 12% 8% we a: ° O% 0% a” BOlie= Study Site Universities Non-profit Government il a Organizations Agencies Type of Research Organization Fig. 4. Participant's Preferences for Sharing Medical Records with Different Types of Research Organizations.Biomed Central Elsevier IEEE Xplore PubMed Scopus I t I J Search Keywords (Share OR Sharing) AND (Sensitive OR Private) AND (Health Record OR EHR OR Medical Record OR EMR) Manual Search Y v v 129 | 62 327 \ | Y | | 129 467 25 25 315 Unique Articles after Removing Duplicates (1065-104=961) y Title and Abstract Review (961) Inclusion Criteria Focus: Design, assessment or evaluation of willingness to share data and data sensitivity perceptions Language: English Published Between: 2009-2019 Type: Journal and Conference Articles Review based on Inclusion and Exclusion Criteria Full Text Review (5) y Additional Articles from Author Citations Relevant Studies in Final Consideration (@) (8) Exclusion Criteria Do not meet inclusion criteria Incomplete studies, editorials, opinion papers, reviews and commentaries Articles in languages other than English Published outside of timeline No access to full text OT Studies Assessing BOTH Data Sharing Preferences and Perceptions of Data Sensitivity (3) Studies Assessing Data Sharing Preferences ONLY () Bie 1. literature cearch ctratecy and nracece.Medical Record Cards (as arranged by patients) Example: Your medical records can be sorted in various data categories. Could you sort the 30 medical record items in following categories? Sensitive Category Response Cards i Mental Health Information 1” Alcohol use and alcoholism refer | to excessive use of alcohol. It can q cause distress and harm. Person | may lose control and develop need ' to drink more alcohol. Other Information I have been diagnosed with Depression I have had a Urine Alcohol Screen test Ihave Grapefruit allergy Thi I have been prescribed Prozac for my Depression Card containing a ‘Fill in the blank’ Fig. 3. Card Sorting Components and Example. 1 1 determine the presence! and amount of alcohol in ! Education Material in the backSteps 1. Classify each item into sensitive data category 2. Assign sensitivity to each item ¥ 3. Define additional criteria for medical record selection Example Example Item - Urine Drug Screen Panel : Category - Drug Abuse Information Select 30 items with 2:1 sensitive to not sensitive ratio Fig. 2. Medical record sorting approach with example.",Medical Data Analysis,The study investigates the early debates about the dietary causes of coronary heart disease (CHD) and the role of the sugar industry in shaping these debates. The authors examine internal documents and historical reports of the Sugar Research Foundation (SRF) and assemble their findings chronologically into a narrative case study. The study suggests that the sugar industry sponsored a research program in the 1960s and 1970s that successfully downplayed evidence that sucrose consumption was a risk factor for CHD while promoting fat as the dietary culprit. The authors recommend that policymaking committees should consider giving less weight to food industry-funded studies and include mechanistic and animal studies as well as studies appraising the effect of added sugars on multiple CHD biomarkers and disease development.,Medical Data Analysis,"Extremely Willing to Share Quite Willing to Share 76% 1 Somewhat Willing to Share Not at All Willing to Share 2 64% 3 52% 2 48% z 40% z E oa 24% 24% 28% 28"" i S 16% 7 2 12% 8% we a: ° O% 0% a” BOlie= Study Site Universities Non-profit Government il a Organizations Agencies Type of Research Organization Fig. 4. Participant's Preferences for Sharing Medical Records with Different Types of Research Organizations.Biomed Central Elsevier IEEE Xplore PubMed Scopus I t I J Search Keywords (Share OR Sharing) AND (Sensitive OR Private) AND (Health Record OR EHR OR Medical Record OR EMR) Manual Search Y v v 129 | 62 327 \ | Y | | 129 467 25 25 315 Unique Articles after Removing Duplicates (1065-104=961) y Title and Abstract Review (961) Inclusion Criteria Focus: Design, assessment or evaluation of willingness to share data and data sensitivity perceptions Language: English Published Between: 2009-2019 Type: Journal and Conference Articles Review based on Inclusion and Exclusion Criteria Full Text Review (5) y Additional Articles from Author Citations Relevant Studies in Final Consideration (@) (8) Exclusion Criteria Do not meet inclusion criteria Incomplete studies, editorials, opinion papers, reviews and commentaries Articles in languages other than English Published outside of timeline No access to full text OT Studies Assessing BOTH Data Sharing Preferences and Perceptions of Data Sensitivity (3) Studies Assessing Data Sharing Preferences ONLY () Bie 1. literature cearch ctratecy and nracece.Medical Record Cards (as arranged by patients) Example: Your medical records can be sorted in various data categories. Could you sort the 30 medical record items in following categories? Sensitive Category Response Cards i Mental Health Information 1” Alcohol use and alcoholism refer | to excessive use of alcohol. It can q cause distress and harm. Person | may lose control and develop need ' to drink more alcohol. Other Information I have been diagnosed with Depression I have had a Urine Alcohol Screen test Ihave Grapefruit allergy Thi I have been prescribed Prozac for my Depression Card containing a ‘Fill in the blank’ Fig. 3. Card Sorting Components and Example. 1 1 determine the presence! and amount of alcohol in ! Education Material in the backSteps 1. Classify each item into sensitive data category 2. Assign sensitivity to each item ¥ 3. Define additional criteria for medical record selection Example Example Item - Urine Drug Screen Panel : Category - Drug Abuse Information Select 30 items with 2:1 sensitive to not sensitive ratio Fig. 2. Medical record sorting approach with example.",Medical Data Analysis
100,"Targeting the uncertainty of predictions at patient-level using an ensemble of classifiers coupled with calibration methods, Venn-ABERS, and Conformal Predictors: A case study in AD","Prognostic prediction , Mild cognitive impairment , Alzheimer’s disease , Uncertainty at patient-level , Venn-ABERS , Conformal prediction.","Despite being able to make accurate predictions, most existing prognostic models lack a proper indication about the uncertainty of each prediction, that is, the risk of prediction error for individual patients. This hampers their translation to primary care settings through decision support systems. To address this problem, we studied different methods for transforming classifiers into probabilistic/confidence-based predictors (here called uncertainty methods), where predictions are complemented with probability estimates/confidence regions reflecting their uncertainty (uncertainty estimates). We tested several uncertainty methods: two well-known calibration methods (Platt Scaling and Isotonic Regression), Conformal Predictors, and Venn-ABERS predictors. We evaluated whether these methods produce valid predictions, where uncertainty estimates reflect the ground truth probabilities. Furthermore, we assessed the proportion of valid predictions made at high-certainty thresholds (predictions with uncertainty measures above a given threshold) since this impacts their usefulness in clinical decisions. Finally, we proposed an ensemble-based approach where predictions from multiple pairs of (classifier, uncertainty method) are combined to predict whether a given MCI patient will convert to AD. This ensemble should putatively provide predictions for a larger number of patients while releasing users from deciding which pair of (classifier, uncertainty method) is more appropriate for data under study. The analysis was performed with a Portuguese cohort (CCC) of around 400 patients and validated in the publicly available ADNI cohort. Despite our focus on MCI to AD prognosis, the proposed approach can be applied to other diseases and prognostic problems.","The main contributions of this work are threefold: • An outright comparison between different methods to target uncertainty of predictions at patient-level, using different classifiers. The analysis was performed with a clinical dataset (CCC), including demographic and neuropsychological tests and around 400 MCI patients. • An ensemble-based approach combining different classifiers and methods to target uncertainty of predictions with the aim of optimizing the quality and quantity of predictions made. Two datasets were used to validate this approach, CCC and ADNI. • A new conformity measure for SVM.","Targeting the uncertainty of predictions at patient-level using an ensemble of classifiers coupled with calibration methods, Venn-ABERS, and Conformal Predictors: A case study in ADPrognostic prediction , Mild cognitive impairment , Alzheimer’s disease , Uncertainty at patient-level , Venn-ABERS , Conformal prediction.Despite being able to make accurate predictions, most existing prognostic models lack a proper indication about the uncertainty of each prediction, that is, the risk of prediction error for individual patients. This hampers their translation to primary care settings through decision support systems. To address this problem, we studied different methods for transforming classifiers into probabilistic/confidence-based predictors (here called uncertainty methods), where predictions are complemented with probability estimates/confidence regions reflecting their uncertainty (uncertainty estimates). We tested several uncertainty methods: two well-known calibration methods (Platt Scaling and Isotonic Regression), Conformal Predictors, and Venn-ABERS predictors. We evaluated whether these methods produce valid predictions, where uncertainty estimates reflect the ground truth probabilities. Furthermore, we assessed the proportion of valid predictions made at high-certainty thresholds (predictions with uncertainty measures above a given threshold) since this impacts their usefulness in clinical decisions. Finally, we proposed an ensemble-based approach where predictions from multiple pairs of (classifier, uncertainty method) are combined to predict whether a given MCI patient will convert to AD. This ensemble should putatively provide predictions for a larger number of patients while releasing users from deciding which pair of (classifier, uncertainty method) is more appropriate for data under study. The analysis was performed with a Portuguese cohort (CCC) of around 400 patients and validated in the publicly available ADNI cohort. Despite our focus on MCI to AD prognosis, the proposed approach can be applied to other diseases and prognostic problems.The main contributions of this work are threefold: • An outright comparison between different methods to target uncertainty of predictions at patient-level, using different classifiers. The analysis was performed with a clinical dataset (CCC), including demographic and neuropsychological tests and around 400 MCI patients. • An ensemble-based approach combining different classifiers and methods to target uncertainty of predictions with the aim of optimizing the quality and quantity of predictions made. Two datasets were used to validate this approach, CCC and ADNI. • A new conformity measure for SVM.",Medical Data Analysis,"The article discusses the problem of the lack of proper indication of uncertainty in most prognostic models, which hampers their translation to primary care settings. The authors studied different methods for transforming classifiers into probabilistic/confidence-based predictors, where predictions are complemented with probability estimates/confidence regions reflecting their uncertainty. The analysis was performed with a Portuguese cohort of around 400 patients and validated in the publicly available ADNI cohort. The proposed approach can be applied to other diseases and prognostic problems. The main contributions of this work are an outright comparison between different methods to target uncertainty of predictions at patient-level, an ensemble-based approach combining different classifiers and methods to target uncertainty of predictions, and a new conformity measure for SVM.",Medical Data Analysis,,Medical Data Analysis
101,The Role of Neural Network for the Detection of Parkinson’s Disease: A Scoping Review,Parkinson’s disease; classification,"Background: Parkinson’s Disease (PD) is a chronic neurodegenerative disorder that has been ranked second after Alzheimer’s disease worldwide. Early diagnosis of PD is crucial to combat against PD to allow patients to deal with it properly. However, there is no medical test(s) available to diagnose PD conclusively. Therefore, computer-aided diagnosis (CAD) systems offered a better solution to make the necessary data-driven decisions and assist the physician. Numerous studies were conducted to propose CAD to diagnose PD in the early stages. No comprehensive reviews have been conducted to summarize the role of AI tools to combat PD. Objective: The study aimed to explore and summarize the applications of neural networks to diagnose PD. Methods: PRISMA Extension for Scoping Reviews (PRISMA-ScR) was followed to conduct this scoping review. To identify the relevant studies, both medical databases (e.g., PubMed) and technical databases (IEEE) were searched. Three reviewers carried out the study selection and extracted the data from the included studies independently. Then, the narrative approach was adopted to synthesis the extracted data. Results: Out of 1061 studies, 91 studies satisfied the eligibility criteria in this review. About half of the included studies have implemented artificial neural networks to diagnose PD. Numerous studies included focused on the freezing of gait (FoG). Biomedical voice and signal datasets were the most commonly used data types to develop and validate these models. However, MRI- and CT-scan images were also utilized in the included studies. Conclusion: Neural networks play an integral and substantial role in combating PD. Many possible applications of neural networks were identified in this review, however, most of them are limited up to research purposes.","This scoping review summarized studies by investigating the use of neural networks, specifically deep learning algorithms, for early diagnosis of PD based on various data collected from different public and private sources (91 studies), including medical image, biomedical voice, and sensor signal, for both PD and healthy control samples. Included studies were categorized into different groups based on the neural network model, type of PD symptoms, and type of dataset. Additionally, the most used dataset and best performance model were highlighted based on the detection of particular symptoms of PD in this review. All technical experiment methods were reported, including submodel, dataset volume, training, testing, evaluation metrics, and validation type. We indicated any real-time implementation used in each hospital or university setting, and based on this review, we recommended particular suggestions for healthcare professionals. Future work could be a meta-analysis to examine each study and provide a comprehensive comparison between them in terms of quality.","The Role of Neural Network for the Detection of Parkinson’s Disease: A Scoping ReviewParkinson’s disease; classificationBackground: Parkinson’s Disease (PD) is a chronic neurodegenerative disorder that has been ranked second after Alzheimer’s disease worldwide. Early diagnosis of PD is crucial to combat against PD to allow patients to deal with it properly. However, there is no medical test(s) available to diagnose PD conclusively. Therefore, computer-aided diagnosis (CAD) systems offered a better solution to make the necessary data-driven decisions and assist the physician. Numerous studies were conducted to propose CAD to diagnose PD in the early stages. No comprehensive reviews have been conducted to summarize the role of AI tools to combat PD. Objective: The study aimed to explore and summarize the applications of neural networks to diagnose PD. Methods: PRISMA Extension for Scoping Reviews (PRISMA-ScR) was followed to conduct this scoping review. To identify the relevant studies, both medical databases (e.g., PubMed) and technical databases (IEEE) were searched. Three reviewers carried out the study selection and extracted the data from the included studies independently. Then, the narrative approach was adopted to synthesis the extracted data. Results: Out of 1061 studies, 91 studies satisfied the eligibility criteria in this review. About half of the included studies have implemented artificial neural networks to diagnose PD. Numerous studies included focused on the freezing of gait (FoG). Biomedical voice and signal datasets were the most commonly used data types to develop and validate these models. However, MRI- and CT-scan images were also utilized in the included studies. Conclusion: Neural networks play an integral and substantial role in combating PD. Many possible applications of neural networks were identified in this review, however, most of them are limited up to research purposes.This scoping review summarized studies by investigating the use of neural networks, specifically deep learning algorithms, for early diagnosis of PD based on various data collected from different public and private sources (91 studies), including medical image, biomedical voice, and sensor signal, for both PD and healthy control samples. Included studies were categorized into different groups based on the neural network model, type of PD symptoms, and type of dataset. Additionally, the most used dataset and best performance model were highlighted based on the detection of particular symptoms of PD in this review. All technical experiment methods were reported, including submodel, dataset volume, training, testing, evaluation metrics, and validation type. We indicated any real-time implementation used in each hospital or university setting, and based on this review, we recommended particular suggestions for healthcare professionals. Future work could be a meta-analysis to examine each study and provide a comprehensive comparison between them in terms of quality.Data -———>} Training set Learning the best pairs (classifier, uncertainty e-fold Cross Validation | =~ method) to assess uncertainty at patient-level ' (outer loop) ' k-fold Cross Validation 1 y t (inner loop) ' ' Testing set ' Preprocessing techniques 1 ' ' ' Handle missing values ' ' ' ' Handle class imbalance ' ' ' ' Feature selection ' ' ' Y Classifiers ! ' ' ' Sf P , ' Classifier, Classifier oe Classifierg for classifier ito C: ' run uncertainty method ' | ' ' Uncertainty methods ' ' ' ' DPE Calibration methods CPs VAs ' 1 ' ' ' 1 y ' ' Evaluation (threshold) ‘ ' ' Aggregator (select pairs of classifiers and uncertainty methods to be included in the ensemble) J Ensemble of pairs (classifier, uncertainty method) to Torts ss sss sss sss provide predictions within certainty thresholds + ' {____________», Classifier uncertainty method ' ' ' { Prediction, uncertainty measure } threshokd Fig. 1. Workflow of the ensemble-based approach using different classifiers and methods to target uncertainty of predictions at patient-level.0.12 4 0.10 4 0.08 4 0.06 4 0.04 4 0.02 4 0.00 0 0.2 0.4 0.6 0.8 1 Probability of progression to ADIR PS DPE No. of occurrences (mean) cP NB DT LR SVM Poly SVM RBF NN Fig. 3. Histogram plots showing the distribution osf we cao 0.06 °. of the uncertainty estimates for correctly (blue/ : 200 o poe ee] ° dark) and incorrectly (orange/light) predictions ; ey sos pee oof for each classifier and uncertainty method. on as a. ee o Values of Y-axis vary amongst the subplots to . ner ner oe ° maximize the view in each subplot. (For inter- ° a aa poe nocd oad pretation of the references to colour in this figure °. 0.104 078 pox ox loa legend, the reader is referred to the web version oo i aos] 50] poe on so of this article.) 2 oo ° ooh 004 J. a on ox joa 04 cod 054 oa fc. ox 004 oe oo oa lo. oo ee tse pe! ec fo 0 22 fo. sa 0] 0 oor . po oe oor oo aoe on “ oc ae al Pe a Pe a0] 08 eat ° oor ond 04 om “4 005 oa po oe ° esas aos} poe a. io onX24 Optimal hyperplane Fig. 2. Illustration of the proposed non-conformity measure for SVM.",Artificial Neural Network,"This scoping review provides a comprehensive summary of 91 studies that investigated the use of neural networks, specifically deep learning algorithms, for early diagnosis of Parkinson’s disease (PD) based on various data collected from different public and private sources, including medical image, biomedical voice, and sensor signal, for both PD and healthy control samples. The review identified the most commonly used data types and highlighted the best performance models based on the detection of specific symptoms of PD. Additionally, all technical experiment methods were reported, including submodel, dataset volume, training, testing, evaluation metrics, and validation type. The review concluded that neural networks play an integral and substantial role in combating PD, and suggested particular recommendations for healthcare professionals. The future work could be a meta-analysis to examine each study and provide a comprehensive comparison between them in terms of quality.",Medical Data Analysis,"Data -———>} Training set Learning the best pairs (classifier, uncertainty e-fold Cross Validation | =~ method) to assess uncertainty at patient-level ' (outer loop) ' k-fold Cross Validation 1 y t (inner loop) ' ' Testing set ' Preprocessing techniques 1 ' ' ' Handle missing values ' ' ' ' Handle class imbalance ' ' ' ' Feature selection ' ' ' Y Classifiers ! ' ' ' Sf P , ' Classifier, Classifier oe Classifierg for classifier ito C: ' run uncertainty method ' | ' ' Uncertainty methods ' ' ' ' DPE Calibration methods CPs VAs ' 1 ' ' ' 1 y ' ' Evaluation (threshold) ‘ ' ' Aggregator (select pairs of classifiers and uncertainty methods to be included in the ensemble) J Ensemble of pairs (classifier, uncertainty method) to Torts ss sss sss sss provide predictions within certainty thresholds + ' {____________», Classifier uncertainty method ' ' ' { Prediction, uncertainty measure } threshokd Fig. 1. Workflow of the ensemble-based approach using different classifiers and methods to target uncertainty of predictions at patient-level.0.12 4 0.10 4 0.08 4 0.06 4 0.04 4 0.02 4 0.00 0 0.2 0.4 0.6 0.8 1 Probability of progression to ADIR PS DPE No. of occurrences (mean) cP NB DT LR SVM Poly SVM RBF NN Fig. 3. Histogram plots showing the distribution osf we cao 0.06 °. of the uncertainty estimates for correctly (blue/ : 200 o poe ee] ° dark) and incorrectly (orange/light) predictions ; ey sos pee oof for each classifier and uncertainty method. on as a. ee o Values of Y-axis vary amongst the subplots to . ner ner oe ° maximize the view in each subplot. (For inter- ° a aa poe nocd oad pretation of the references to colour in this figure °. 0.104 078 pox ox loa legend, the reader is referred to the web version oo i aos] 50] poe on so of this article.) 2 oo ° ooh 004 J. a on ox joa 04 cod 054 oa fc. ox 004 oe oo oa lo. oo ee tse pe! ec fo 0 22 fo. sa 0] 0 oor . po oe oor oo aoe on “ oc ae al Pe a Pe a0] 08 eat ° oor ond 04 om “4 005 oa po oe ° esas aos} poe a. io onX24 Optimal hyperplane Fig. 2. Illustration of the proposed non-conformity measure for SVM.",Medical Data Analysis
102,"A Survey of Automatic Text Summarization: Progress, Process and Challenges","Automatic text summarization, feature extraction, summarization methods, performance measurement matrices, challenges.","With the evolution of the Internet and multimedia technology, the amount of text data has increased exponentially. This text volume is a precious source of information and knowledge that needs to be efficiently summarized. Text summarization is the method to reduce the source text into a compact variant, preserving its knowledge and the actual meaning. Here we thoroughly investigate the automatic text summarization (ATS) and summarize the widely recognized ATS architectures. This paper outlines extractive and abstractive text summarization technologies and provides a deep taxonomy of the ATS domain. The taxonomy presents the classical ATS algorithms to modern deep learning ATS architectures. Every modern text summarization approach’s workflow and significance are reviewed with the limitations with potential recovery methods, including the feature extraction approaches, datasets, performance measurement techniques, and challenges of the ATS domain, etc. In addition, this paper concisely presents the past, present, and future research directions in the ATS domain.","Text summarization is an old topic, but this field continues to gain the interest of researchers. Nonetheless, the performance of text summarization is average in general, and the summaries created are not always ideal. As a result, researchers are attempting to improve existing text summarizing methods. In addition, developing novel summarization approaches to produce higher-quality, human standards and robust summaries is a priority. Therefore, ATS should be made more intelligent by combining it with other integrated systems to perform better. Automatic text summarization is an eminent domain of research that is extensively implemented and integrated into diverse applications to summarize and reduce text volume. In this paper, we present a systematic survey of the vast ATS domain in various phases: the fundamental theories with previous research backgrounds, dataset inspections, feature extraction architectures, influential text summarization algorithms, performance measurement matrices, and challenges of current architectures. This paper also presents the current limitations and challenges of ATS methods and algorithms, which would encourage researchers to try to solve these limitations and overcome new challenges in the ATS domain.","A Survey of Automatic Text Summarization: Progress, Process and ChallengesAutomatic text summarization, feature extraction, summarization methods, performance measurement matrices, challenges.With the evolution of the Internet and multimedia technology, the amount of text data has increased exponentially. This text volume is a precious source of information and knowledge that needs to be efficiently summarized. Text summarization is the method to reduce the source text into a compact variant, preserving its knowledge and the actual meaning. Here we thoroughly investigate the automatic text summarization (ATS) and summarize the widely recognized ATS architectures. This paper outlines extractive and abstractive text summarization technologies and provides a deep taxonomy of the ATS domain. The taxonomy presents the classical ATS algorithms to modern deep learning ATS architectures. Every modern text summarization approach’s workflow and significance are reviewed with the limitations with potential recovery methods, including the feature extraction approaches, datasets, performance measurement techniques, and challenges of the ATS domain, etc. In addition, this paper concisely presents the past, present, and future research directions in the ATS domain.Text summarization is an old topic, but this field continues to gain the interest of researchers. Nonetheless, the performance of text summarization is average in general, and the summaries created are not always ideal. As a result, researchers are attempting to improve existing text summarizing methods. In addition, developing novel summarization approaches to produce higher-quality, human standards and robust summaries is a priority. Therefore, ATS should be made more intelligent by combining it with other integrated systems to perform better. Automatic text summarization is an eminent domain of research that is extensively implemented and integrated into diverse applications to summarize and reduce text volume. In this paper, we present a systematic survey of the vast ATS domain in various phases: the fundamental theories with previous research backgrounds, dataset inspections, feature extraction architectures, influential text summarization algorithms, performance measurement matrices, and challenges of current architectures. This paper also presents the current limitations and challenges of ATS methods and algorithms, which would encourage researchers to try to solve these limitations and overcome new challenges in the ATS domain.Publication Per Year CONFERENCE JOURNAL 8 10 12 Number of Publication: “ul = PubMed = 549 IEEE = 303 ACM = 19 Science Direct = 151 Google Scholar = 39 Total = 1061 190 duplicates removed 871 unique titles and abstracts 598 excluded after scanning titles and abstracts: - Irrelevant: 7 = 288 - Population: 7 = 72 - Intervention: a= 105 - Type of publication: a= 68 - Study design: 7 = 65 273 unique full-text studies 1&2 publications excluded after scanning full texts: - Irrelevant: n= 121 | - Population: 7 = 6 - Intervention: 7 = 22 - Study design: n= 31 - Language=2 9] studies included in the narrative synthesis= Freezing of gait (FOG) = PD = Tremor disorder = Vocal impairments = Vocal impairments Freezing of gait (FOG)Country ¥ @ Australia g Germany Malaysia Singapore ge Bangladesh gg Beeium mw Breil | Canada ge China g Colbmbs B Greece B ind @ italy B Japan @ Korea Lebanon g Morocco Netherlands g Pakistan gw Palestine g Portugal Bg RueSsa § Sbvoakia Spain @ Turkey USA BULK g@ Romania m@ France @ Lithuania eg Saudi Araba m Egypt",Natural Language Processing,"This paper provides a comprehensive survey of automatic text summarization (ATS), which is the method of reducing source text into a compact variant while preserving its knowledge and actual meaning. The paper outlines the different architectures of ATS, including extractive and abstractive text summarization technologies, and provides a deep taxonomy of the ATS domain. The taxonomy presents classical algorithms and modern deep learning ATS architectures, and reviews the significance and limitations of each approach. The paper also presents the past, present, and future research directions in the ATS domain, and highlights the challenges that need to be addressed in order to improve the quality of summaries. Overall, the paper emphasizes the importance of ATS as an eminent domain of research that is widely implemented and integrated into diverse applications to summarize and reduce text volume.",Natural Language Processing,Publication Per Year CONFERENCE JOURNAL 8 10 12 Number of Publication: “ul = PubMed = 549 IEEE = 303 ACM = 19 Science Direct = 151 Google Scholar = 39 Total = 1061 190 duplicates removed 871 unique titles and abstracts 598 excluded after scanning titles and abstracts: - Irrelevant: 7 = 288 - Population: 7 = 72 - Intervention: a= 105 - Type of publication: a= 68 - Study design: 7 = 65 273 unique full-text studies 1&2 publications excluded after scanning full texts: - Irrelevant: n= 121 | - Population: 7 = 6 - Intervention: 7 = 22 - Study design: n= 31 - Language=2 9] studies included in the narrative synthesis= Freezing of gait (FOG) = PD = Tremor disorder = Vocal impairments = Vocal impairments Freezing of gait (FOG)Country ¥ @ Australia g Germany Malaysia Singapore ge Bangladesh gg Beeium mw Breil | Canada ge China g Colbmbs B Greece B ind @ italy B Japan @ Korea Lebanon g Morocco Netherlands g Pakistan gw Palestine g Portugal Bg RueSsa § Sbvoakia Spain @ Turkey USA BULK g@ Romania m@ France @ Lithuania eg Saudi Araba m Egypt,Medical Data Analysis
103,Tobacco and tuberculosis: a qualitative systematic review and meta-analysis,Tuberculosis; smoking; second-hand smoke; risk factors.,"To assess the strength of evidence in published articles for an association between smoking and passive exposure to tobacco smoke and various manifestations and outcomes of tuberculosis (TB). Clinicians and public health workers working to fight TB may not see a role for themselves in tobacco control because the association between tobacco and TB has not been widely accepted. A qualitative review and meta-analysis was therefore undertaken. METHODS: Reference lists, PubMed, the database of the International Union Against Tuberculosis and Lung Disease and Google Scholar were searched for a final inclusion of 42 articles in English containing 53 outcomes for data extraction. A quality score was attributed to each study to classify the strength of evidence according to each TB outcome. A meta-analysis was then performed on results from included studies. RESULTS: Despite the limitations in the data available, the evidence was rated as strong for an association between smoking and TB disease, moderate for the association between second-hand smoke exposure and TB disease and between smoking and retreatment TB disease, and limited for the association between smoking and tuberculous infection and between smoking and TB mortality. There was insufficient evidence to support an association of smoking and delay, default, slower smear conversion, greater severity of disease or drug-resistant TB or of second-hand tobacco smoke exposure and infection. CONCLUSIONS: The association between smoking and TB disease appears to be causal. Smoking can have an important impact on many aspects of TB. Clinicians can confidently advise patients that quitting smoking and avoiding exposure to others’ tobacco smoke are important measures in TB control.","The relationship between smoking and TB is important knowledge for clinicians dealing with patients. This association, which has been shown to exist in individual studies and reviews, has not received the attention it merits in terms of TB care standards and research. This systematic review attempts to remedy this situation. Clinicians’ knowledge about a patient’s smoking status or exposure to passive smoking can help them to better manage treatment by providing counselling about stopping smoking. The results of this study show that smokers have greater risks for TB disease and retreatment and some evidence of higher risks for infection and TB disease severity. These results indicate that a clinician can confidently advise TB patients to stop smoking and remain non-smokers or to avoid exposure to others’ smoke in the context of good case management.","Tobacco and tuberculosis: a qualitative systematic review and meta-analysisTuberculosis; smoking; second-hand smoke; risk factors.To assess the strength of evidence in published articles for an association between smoking and passive exposure to tobacco smoke and various manifestations and outcomes of tuberculosis (TB). Clinicians and public health workers working to fight TB may not see a role for themselves in tobacco control because the association between tobacco and TB has not been widely accepted. A qualitative review and meta-analysis was therefore undertaken. METHODS: Reference lists, PubMed, the database of the International Union Against Tuberculosis and Lung Disease and Google Scholar were searched for a final inclusion of 42 articles in English containing 53 outcomes for data extraction. A quality score was attributed to each study to classify the strength of evidence according to each TB outcome. A meta-analysis was then performed on results from included studies. RESULTS: Despite the limitations in the data available, the evidence was rated as strong for an association between smoking and TB disease, moderate for the association between second-hand smoke exposure and TB disease and between smoking and retreatment TB disease, and limited for the association between smoking and tuberculous infection and between smoking and TB mortality. There was insufficient evidence to support an association of smoking and delay, default, slower smear conversion, greater severity of disease or drug-resistant TB or of second-hand tobacco smoke exposure and infection. CONCLUSIONS: The association between smoking and TB disease appears to be causal. Smoking can have an important impact on many aspects of TB. Clinicians can confidently advise patients that quitting smoking and avoiding exposure to others’ tobacco smoke are important measures in TB control.The relationship between smoking and TB is important knowledge for clinicians dealing with patients. This association, which has been shown to exist in individual studies and reviews, has not received the attention it merits in terms of TB care standards and research. This systematic review attempts to remedy this situation. Clinicians’ knowledge about a patient’s smoking status or exposure to passive smoking can help them to better manage treatment by providing counselling about stopping smoking. The results of this study show that smokers have greater risks for TB disease and retreatment and some evidence of higher risks for infection and TB disease severity. These results indicate that a clinician can confidently advise TB patients to stop smoking and remain non-smokers or to avoid exposure to others’ smoke in the context of good case management.",Medical Data Analysis,"This article presents a qualitative review and meta-analysis of 42 English articles to assess the strength of evidence for an association between smoking and passive exposure to tobacco smoke and various manifestations and outcomes of tuberculosis (TB). The evidence was rated as strong for an association between smoking and TB disease, moderate for the association between second-hand smoke exposure and TB disease and between smoking and retreatment TB disease, and limited for the association between smoking and tuberculous infection and between smoking and TB mortality. The study concludes that smoking can have an important impact on many aspects of TB, and clinicians can confidently advise patients that quitting smoking and avoiding exposure to others’ tobacco smoke are important measures in TB control. The article aims to raise awareness among clinicians and public health workers about the importance of considering smoking as a factor in TB care and research.",Medical Data Analysis,,Deep Learning and Machine Learning
104,Utilizing online stochastic optimization on scheduling of intensity-modulate radiotherapy therapy (IMRT),"Intensity-Modulated Radiation Therapy (IMRT) , Online stochastic scheduling , Genetic algorithm (GA) , Radiotherapy scheduling.","According to Ministry of Health and Welfare of Taiwan, cancer has been one of the major causes of death in Taiwan since 1982. The Intensive-Modulated Radiation Therapy (IMRT) is one of the most important radiotherapies of cancers, especially for Nasopharyngeal cancers, Digestive system cancers and Cervical cancers. For patients, if they can receive the treatment at the earliest possibility while diagnosed with cancers, their survival rate increases. However, the discussion of effective patient scheduling models of IMRT to reduce patients’ waiting time is still limited in literature. This study proposed a mathematical model to improve the efficiency of patient scheduling. The research was composed of two stages. In the first stage, the online stochastic algorithm was proposed to improve the performance of present scheduling system. In the second stage the impact of future treatment to reduce patients’ waiting time was considered. A genetic algorithm (GA) was then proposed to solve the online stochastic scheduling problem. This research collected data from a practical medical institute and the proposed model was validated with real data. It contributes to both theory and practice by proposing a practical model to assist the medical institute in implementing patient scheduling in a more efficient manner.","Several tests are conducted to validate the efficiency of the algorithms. The performance indicators we utilized are objective value, runtime, convergence process and relative improvement. After analyzing the results of different types GA in different problem scales, we observe that AGA performs better than traditional GA, and the performance of local search mechanism is better than FLC in this research. This research contributes to solve the OS scheduling for radiotherapy by considering the two types of patient, i.e. the general patient and the special patient. An adaptive GA is proposed to solve the OS scheduling problem for radiotherapy. In addition, the computational results present that the waiting time of patient could be reduce considering the future scenarios under the base demand. This research has some limitations. For instance, the biological growth of tumors is assumed to be equal among all patients. Also, there is no cancellation mechanism considered. For these limitations, this research can be extended by considering the cancellation mechanism in the OS scheduling problem for radiotherapy, the emergency of patient, and the effectiveness of machine. More types of patients, e.g. existing, new, and referrals, can be considered. Whether and how the AGA algorithm proposed in this research can be improved, this also worth a further study. Additionally, although we looked at the data to ensure that the algorithm did not produce outliers which would result in sacrificing wait times dangerously for one or two patients while optimizing the overall wait time, these algorithms may produce outlier when time limitation is considered, this would also need to examined. Lastly, more metrics could be employed to compare with other research work, and whether this result applies to a clinic which is not currently using an “as soon as possible” method could also be examined.","Utilizing online stochastic optimization on scheduling of intensity-modulate radiotherapy therapy (IMRT)Intensity-Modulated Radiation Therapy (IMRT) , Online stochastic scheduling , Genetic algorithm (GA) , Radiotherapy scheduling.According to Ministry of Health and Welfare of Taiwan, cancer has been one of the major causes of death in Taiwan since 1982. The Intensive-Modulated Radiation Therapy (IMRT) is one of the most important radiotherapies of cancers, especially for Nasopharyngeal cancers, Digestive system cancers and Cervical cancers. For patients, if they can receive the treatment at the earliest possibility while diagnosed with cancers, their survival rate increases. However, the discussion of effective patient scheduling models of IMRT to reduce patients’ waiting time is still limited in literature. This study proposed a mathematical model to improve the efficiency of patient scheduling. The research was composed of two stages. In the first stage, the online stochastic algorithm was proposed to improve the performance of present scheduling system. In the second stage the impact of future treatment to reduce patients’ waiting time was considered. A genetic algorithm (GA) was then proposed to solve the online stochastic scheduling problem. This research collected data from a practical medical institute and the proposed model was validated with real data. It contributes to both theory and practice by proposing a practical model to assist the medical institute in implementing patient scheduling in a more efficient manner.Several tests are conducted to validate the efficiency of the algorithms. The performance indicators we utilized are objective value, runtime, convergence process and relative improvement. After analyzing the results of different types GA in different problem scales, we observe that AGA performs better than traditional GA, and the performance of local search mechanism is better than FLC in this research. This research contributes to solve the OS scheduling for radiotherapy by considering the two types of patient, i.e. the general patient and the special patient. An adaptive GA is proposed to solve the OS scheduling problem for radiotherapy. In addition, the computational results present that the waiting time of patient could be reduce considering the future scenarios under the base demand. This research has some limitations. For instance, the biological growth of tumors is assumed to be equal among all patients. Also, there is no cancellation mechanism considered. For these limitations, this research can be extended by considering the cancellation mechanism in the OS scheduling problem for radiotherapy, the emergency of patient, and the effectiveness of machine. More types of patients, e.g. existing, new, and referrals, can be considered. Whether and how the AGA algorithm proposed in this research can be improved, this also worth a further study. Additionally, although we looked at the data to ensure that the algorithm did not produce outliers which would result in sacrificing wait times dangerously for one or two patients while optimizing the overall wait time, these algorithms may produce outlier when time limitation is considered, this would also need to examined. Lastly, more metrics could be employed to compare with other research work, and whether this result applies to a clinic which is not currently using an “as soon as possible” method could also be examined.McCurdy A Alcaide 4 Plant Combined Ariyothai 4 Anderson Den Boon a Altet + Hussain a 0. = Combined -] == —— T T 7 1 4.762 52 0.9 1 3.353 ""1 OR OR B Adelstein a D Alcaide Ft Gajalakshmi-urban | Ee Ariyothai ’ E- Crampin Gajalakshmi-rural | FA Kolappan 3 Leung 2004 —_-}—_ Lam 4 - Leung 2003 —_—_ Lienhardt a Liu-urban | Perez-Padilla Shah - Liu-rural | Tekkel a? Tocque a __ Sitas 4 Toledo, — Yu 7? Combined —— Combined 1 1 T t Le ; + 04 1 2.284 8.7 144 2.236 5 OR OR Figure Standard plots for main meta-analyses outcomes. For each study, the included effect estimates are graphed on the x-axis and the first author's last name is on the y-axis. Horizontal lines represent 95% confidence intervals and boxes represent the weight placed on each study (proportional to the inverse of the standard error of each estimate). The dotted vertical line represents odds ratio (OR) = 1, while the diamond shows the pooled estimate of each OR and its 95% confidence interval. A. OR of being infected for ever- vs. never-smokers. B. OR of developing TB disease for ever- vs. never-smokers (Note: the study by Leung 2004 estimates a hazard ratio; all others are odds ratios). C. OR of developing TB disease for passively-exposed individuals compared to those not. D. OR of TB mortality for ever- vs. never-smokers.",Medical Data Analysis,"This study proposes a mathematical model to improve the efficiency of patient scheduling for Intensive-Modulated Radiation Therapy (IMRT) in Taiwan. The research consists of two stages, with the first proposing an online stochastic algorithm to improve the performance of the present scheduling system, and the second considering the impact of future treatment to reduce patients' waiting time. The study validates the proposed model with real data and contributes to theory and practice by proposing a practical model to assist medical institutes in implementing patient scheduling more efficiently. The results show that the adaptive genetic algorithm performs better than traditional genetic algorithms and the waiting time of patients can be reduced by considering future scenarios under the base demand. However, the study has some limitations, such as assumptions regarding the biological growth of tumors and the absence of a cancellation mechanism, which could be addressed in future studies.",Medical Data Analysis,"McCurdy A Alcaide 4 Plant Combined Ariyothai 4 Anderson Den Boon a Altet + Hussain a 0. = Combined -] == —— T T 7 1 4.762 52 0.9 1 3.353 ""1 OR OR B Adelstein a D Alcaide Ft Gajalakshmi-urban | Ee Ariyothai ’ E- Crampin Gajalakshmi-rural | FA Kolappan 3 Leung 2004 —_-}—_ Lam 4 - Leung 2003 —_—_ Lienhardt a Liu-urban | Perez-Padilla Shah - Liu-rural | Tekkel a? Tocque a __ Sitas 4 Toledo, — Yu 7? Combined —— Combined 1 1 T t Le ; + 04 1 2.284 8.7 144 2.236 5 OR OR Figure Standard plots for main meta-analyses outcomes. For each study, the included effect estimates are graphed on the x-axis and the first author's last name is on the y-axis. Horizontal lines represent 95% confidence intervals and boxes represent the weight placed on each study (proportional to the inverse of the standard error of each estimate). The dotted vertical line represents odds ratio (OR) = 1, while the diamond shows the pooled estimate of each OR and its 95% confidence interval. A. OR of being infected for ever- vs. never-smokers. B. OR of developing TB disease for ever- vs. never-smokers (Note: the study by Leung 2004 estimates a hazard ratio; all others are odds ratios). C. OR of developing TB disease for passively-exposed individuals compared to those not. D. OR of TB mortality for ever- vs. never-smokers.",Medical Data Analysis
105,Weight-based multiple empirical kernel learning with neighbor discriminant constraint for heart failure mortality prediction,Heart Failure Mortality Prediction Electronic Health Records Feature Selection Multiple Kernel Learning,"Heart Failure (HF) is one of the most common causes of hospitalization and is burdened by short-term (in-hospital) and long-term (6–12 month) mortality. Accurate prediction of HF mortality plays a critical role in evaluating early treatment effects. However, due to the lack of a simple and effective prediction model, mortality prediction of HF is difficult, resulting in a low rate of control. To handle this issue, we propose a Weight-based Multiple Empirical Kernel Learning with Neighbor Discriminant Constraint (WMEKL-NDC) method for HF mortality prediction. In our method, feature selection by calculating the F-value of each feature is first performed to identify the crucial clinical features. Then, different weights are assigned to each empirical kernel space according to the centered kernel alignment criterion. To make use of the discriminant information of samples, neighbor discriminant constraint is finally integrated into multiple empirical kernel learning framework. Extensive experiments were performed on a real clinical dataset containing 10, 198 in-patients records collected from Shanghai Shuguang Hospital in March 2009 and April 2016. Experimental results demonstrate that our proposed WMEKL-NDC method achieves a highly competitive performance for HF mortality prediction of in hospital, 30-day and 1-year. Compared with the state-of-the-art multiple kernel learning and baseline algorithms, our proposed WMEKL-NDC is more accurate on mortality prediction Moreover, top 10 crucial clinical features are identified together with their meanings, which are very useful to assist clinicians in the treatment of HF disease.","In this paper, we propose a new MKL prediction model for HF mortality prediction of in-hospital, 30-day and 1-year, i.e., a Weight-based Multiple Empirical Kernel Learning with Neighbor Discriminant Constraint (WMEKL-NDC) method. By assigning different weights to different kernel spaces and integrating neighbor discriminant constraint into classifier, the proposed WMEKL-NDC method exhibits superior performance compared with the state-of-the-art multiple kernel learning methods and basic methods. In addition, top 10 clinical features which have the significant impact on HF mortality are identified according to their F-values. Meanwhile, medical meanings of these features are also identified to provide crucial decision information for clinicians in HF treatment.","Weight-based multiple empirical kernel learning with neighbor discriminant constraint for heart failure mortality predictionHeart Failure Mortality Prediction Electronic Health Records Feature Selection Multiple Kernel LearningHeart Failure (HF) is one of the most common causes of hospitalization and is burdened by short-term (in-hospital) and long-term (6–12 month) mortality. Accurate prediction of HF mortality plays a critical role in evaluating early treatment effects. However, due to the lack of a simple and effective prediction model, mortality prediction of HF is difficult, resulting in a low rate of control. To handle this issue, we propose a Weight-based Multiple Empirical Kernel Learning with Neighbor Discriminant Constraint (WMEKL-NDC) method for HF mortality prediction. In our method, feature selection by calculating the F-value of each feature is first performed to identify the crucial clinical features. Then, different weights are assigned to each empirical kernel space according to the centered kernel alignment criterion. To make use of the discriminant information of samples, neighbor discriminant constraint is finally integrated into multiple empirical kernel learning framework. Extensive experiments were performed on a real clinical dataset containing 10, 198 in-patients records collected from Shanghai Shuguang Hospital in March 2009 and April 2016. Experimental results demonstrate that our proposed WMEKL-NDC method achieves a highly competitive performance for HF mortality prediction of in hospital, 30-day and 1-year. Compared with the state-of-the-art multiple kernel learning and baseline algorithms, our proposed WMEKL-NDC is more accurate on mortality prediction Moreover, top 10 crucial clinical features are identified together with their meanings, which are very useful to assist clinicians in the treatment of HF disease.In this paper, we propose a new MKL prediction model for HF mortality prediction of in-hospital, 30-day and 1-year, i.e., a Weight-based Multiple Empirical Kernel Learning with Neighbor Discriminant Constraint (WMEKL-NDC) method. By assigning different weights to different kernel spaces and integrating neighbor discriminant constraint into classifier, the proposed WMEKL-NDC method exhibits superior performance compared with the state-of-the-art multiple kernel learning methods and basic methods. In addition, top 10 clinical features which have the significant impact on HF mortality are identified according to their F-values. Meanwhile, medical meanings of these features are also identified to provide crucial decision information for clinicians in HF treatment.1Q Cases 8 Cases 4 Cases Ise log Ive Ize log a] isz a] ist a] oz a]. izz { ‘ § S. °: ° e . 2 °. e. 10z Ist 191 Tel Ir TOL 1s 19 Ih 1Z . . . . . . : . . . . . . 1200 00 00 100 400 200 20 Cases Ig¢ 19¢ Ire Iz log gz 197 vz 1% 107 anyeA ears2[qo Isl 191 I¢l IZI I01 one sAns0fqO 15 Cases Ist anyeA aansalqo, 1450 5 > wo a ~ 1050 5 5 450 250 anyeA esaroalqo == =FIC Ise T9¢ Ire Ize Tog 18z 197 Itz It 10z Ist 191 Tvl Tr ToL Ts¢ 19¢ Ive Iz Tog Tsz 197 ItZ 1% 10z Ist T9L Tel T@I TOLBest Solution 280 290 300 310 320 4 Cases 8 Cases 10 Cases g g & s 3 5 eo 8 “ao & s o 3 oO oD a oO oO 8 g GA FLCGA LSGA AGA GA FLCGA LSGA AGA GA FLCGA LSGA AGA Algorithm Algorithm Algonthm 15 Cases 20 Cases 500 Best Solution 400 Best Solution 350 400 450 500 300 GA FLCGA LSGA- AGA GA FLCGA LSGA AGA Algorithm Algorithmwl wl J “apt ag! ag! , Tet Tet Tet onspring AT 14] 1013 | 8 | 5] 15] 9] of 3] 7 | 7] 11] 18] 4 | Pl Py? Pst Pst 1PL pet Pye pyr 1 2 3 4 5 6 7 8 9 10) ll 5] 16) 17 4 13) 14 onfpring a'| 14] 10] 2 | 815] 15] 9f 10] 3] 7 | 7 | PL Py? Pst pyr 1 Pl Pet Pet pet 1Utilization/% 1.00 0.90 0,80 0,70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 Low demand pewreper ot were 9 11:13 15 17 19 21 Utilization/% 23 25 27 29 31 33 35 37 39 41 1 3 5 1,00 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 Base demand 1.00 0.90 0.80 0.70 0.60 0.30 0.40 0,30 0.20 0.10 0.00 _- 9 Il 13 15 17 19 23 23 25 27 29 31 33 35 37 39 Utilization/% High demand — 25 Cases -- 20 Cases ==» 15 Cases teaeens Present 1 3 5 7 9 111315 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 46J 1 2 1 2 1 J 1 je jet je? J?? J 3 1] J jer w J J3 J 2 je? \3 Parent Offspringwl w2 wl w2 A \ JL 1 i j yer JQ Jel yor yo? yor dy je! jer yg? “yg? nabolsTs[sTis}s{sToTa] 3] 4 2 ‘ ' ‘ dj ae! ae? 1 Tj Tet ae Gene Encoding . Decoding Current",Medical Data Analysis,"This paper proposes a new method, called the Weight-based Multiple Empirical Kernel Learning with Neighbor Discriminant Constraint (WMEKL-NDC) method, for predicting mortality in patients with heart failure. The proposed method includes feature selection, assigning different weights to different kernel spaces, and integrating neighbor discriminant constraint into the classifier. The method is evaluated on a real clinical dataset and compared to state-of-the-art multiple kernel learning methods and basic methods. The results show that the proposed method achieves superior performance and identifies the top 10 clinical features that have a significant impact on heart failure mortality. These features are also described in terms of their medical meaning to provide crucial decision information for clinicians in heart failure treatment.",Medical Data Analysis,"1Q Cases 8 Cases 4 Cases Ise log Ive Ize log a] isz a] ist a] oz a]. izz { ‘ § S. °: ° e . 2 °. e. 10z Ist 191 Tel Ir TOL 1s 19 Ih 1Z . . . . . . : . . . . . . 1200 00 00 100 400 200 20 Cases Ig¢ 19¢ Ire Iz log gz 197 vz 1% 107 anyeA ears2[qo Isl 191 I¢l IZI I01 one sAns0fqO 15 Cases Ist anyeA aansalqo, 1450 5 > wo a ~ 1050 5 5 450 250 anyeA esaroalqo == =FIC Ise T9¢ Ire Ize Tog 18z 197 Itz It 10z Ist 191 Tvl Tr ToL Ts¢ 19¢ Ive Iz Tog Tsz 197 ItZ 1% 10z Ist T9L Tel T@I TOLBest Solution 280 290 300 310 320 4 Cases 8 Cases 10 Cases g g & s 3 5 eo 8 “ao & s o 3 oO oD a oO oO 8 g GA FLCGA LSGA AGA GA FLCGA LSGA AGA GA FLCGA LSGA AGA Algorithm Algorithm Algonthm 15 Cases 20 Cases 500 Best Solution 400 Best Solution 350 400 450 500 300 GA FLCGA LSGA- AGA GA FLCGA LSGA AGA Algorithm Algorithmwl wl J “apt ag! ag! , Tet Tet Tet onspring AT 14] 1013 | 8 | 5] 15] 9] of 3] 7 | 7] 11] 18] 4 | Pl Py? Pst Pst 1PL pet Pye pyr 1 2 3 4 5 6 7 8 9 10) ll 5] 16) 17 4 13) 14 onfpring a'| 14] 10] 2 | 815] 15] 9f 10] 3] 7 | 7 | PL Py? Pst pyr 1 Pl Pet Pet pet 1Utilization/% 1.00 0.90 0,80 0,70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 Low demand pewreper ot were 9 11:13 15 17 19 21 Utilization/% 23 25 27 29 31 33 35 37 39 41 1 3 5 1,00 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 Base demand 1.00 0.90 0.80 0.70 0.60 0.30 0.40 0,30 0.20 0.10 0.00 _- 9 Il 13 15 17 19 23 23 25 27 29 31 33 35 37 39 Utilization/% High demand — 25 Cases -- 20 Cases ==» 15 Cases teaeens Present 1 3 5 7 9 111315 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 46J 1 2 1 2 1 J 1 je jet je? J?? J 3 1] J jer w J J3 J 2 je? \3 Parent Offspringwl w2 wl w2 A \ JL 1 i j yer JQ Jel yor yo? yor dy je! jer yg? “yg? nabolsTs[sTis}s{sToTa] 3] 4 2 ‘ ' ‘ dj ae! ae? 1 Tj Tet ae Gene Encoding . Decoding Current",Medical Data Analysis
106,"Person Recognition Based On Deep Learning Using 
Convolutional Neural Network","Neural Networks, Image analysis, Metric 
learning, hierarchical Gaussian. ","Matching the same subject across various 
cameras is the aim of the person recognition task. Earlier, 
person recognition problems were primarily addressed by 
image-based techniques. However, as the use of cameras and 
monitoring increases, image-based solutions are being 
employed. Image-based systems yield better results for person 
recognition because they take into account the person's spatial 
and temporal information, which are not included in a single 
image. The CNN properties for person recognition are improved 
in this work. On huge annotated datasets like Image Net, recent 
studies have demonstrated the effectiveness of features 
produced from pre-trained Convolution Neural Network (CNN) 
top layers. In this study, we use a dataset of pedestrian attribute 
values to fine-tune the CNN features. We propose new labels 
that were produced by integrating a number of attribute labels 
included to the classification loss for the different pedestrian 
attribute labels. CNN is forced to develop more discriminative 
features that can identify more personally identifiable 
information as a result of combination attribute loss. Using 
traditional metric learning, we further enhance discriminative 
performance on a target identification dataset after extracting 
features from the trained CNN. ","In order to address the issue of appearance-based 
identification, we put forth a feature extraction and matching 
method. The usefulness of utilizing both lengthy image 
sequences and potent image-based person recognition
methods is theoretically demonstrated by us. We give two 
examples to demonstrate this idea, and We give two examples 
to demonstrate this idea, and compared to currently used stateof-the-art techniques, our suggested approach performs 
noticeably better. In order to do CNN fine-tuning for 
categorising combinations of pedestrian attribute data, we 
have proposed using an unique loss function. The suggested 
approach raises the discriminative power of attribute-based 
CNN features without ever raising the costs to the annotator. 
In the studies, four datasets for hard person identification were 
used, and the outcomes were promising. The pedestrian 
attribute dataset's fine-tuning produced a noticeably better 
result, demonstrating the value of the combination-attribute 
loss function. Features of CNN fared better than well written, 
hand-crafted descriptions. By combining pedestrian 
characteristics datasets with person recognition datasets, we 
hope to increase the amount of training samples and improve 
CNN features. We'll also examine how attribute and person 
recognition pairs are labeled when there are classification 
losses. ","Person Recognition Based On Deep Learning Using 
Convolutional Neural NetworkNeural Networks, Image analysis, Metric 
learning, hierarchical Gaussian. Matching the same subject across various 
cameras is the aim of the person recognition task. Earlier, 
person recognition problems were primarily addressed by 
image-based techniques. However, as the use of cameras and 
monitoring increases, image-based solutions are being 
employed. Image-based systems yield better results for person 
recognition because they take into account the person's spatial 
and temporal information, which are not included in a single 
image. The CNN properties for person recognition are improved 
in this work. On huge annotated datasets like Image Net, recent 
studies have demonstrated the effectiveness of features 
produced from pre-trained Convolution Neural Network (CNN) 
top layers. In this study, we use a dataset of pedestrian attribute 
values to fine-tune the CNN features. We propose new labels 
that were produced by integrating a number of attribute labels 
included to the classification loss for the different pedestrian 
attribute labels. CNN is forced to develop more discriminative 
features that can identify more personally identifiable 
information as a result of combination attribute loss. Using 
traditional metric learning, we further enhance discriminative 
performance on a target identification dataset after extracting 
features from the trained CNN. In order to address the issue of appearance-based 
identification, we put forth a feature extraction and matching 
method. The usefulness of utilizing both lengthy image 
sequences and potent image-based person recognition
methods is theoretically demonstrated by us. We give two 
examples to demonstrate this idea, and We give two examples 
to demonstrate this idea, and compared to currently used stateof-the-art techniques, our suggested approach performs 
noticeably better. In order to do CNN fine-tuning for 
categorising combinations of pedestrian attribute data, we 
have proposed using an unique loss function. The suggested 
approach raises the discriminative power of attribute-based 
CNN features without ever raising the costs to the annotator. 
In the studies, four datasets for hard person identification were 
used, and the outcomes were promising. The pedestrian 
attribute dataset's fine-tuning produced a noticeably better 
result, demonstrating the value of the combination-attribute 
loss function. Features of CNN fared better than well written, 
hand-crafted descriptions. By combining pedestrian 
characteristics datasets with person recognition datasets, we 
hope to increase the amount of training samples and improve 
CNN features. We'll also examine how attribute and person 
recognition pairs are labeled when there are classification 
losses. 90.00 wsalect 100.00 Select 80.00 Without select 90.00 mWithout select ithout sele 70.00 80.00 60.00 70.00 : = 60.00 = & mien & 50.00 2 40.00 = 40.00 30.00 30.00 20.00 20.00 10.00 10.00 0.00 0.00 ww * Ss & Ss we Ss s Ss & rs & ee s s ve é & Re of ! < & + s we & % Fig. 4. AUC of all algorithms with and without feature selection on SSHF da- Fig. 2. AA of all algorithms with and without feature selection on SSHF dataset. taset.Data collection Data processing Feature selection Prediction model WMEKL-NDC In-hospital 30-day 1-year mortality mortality mortality Mortality predictionMDiagnoses mMedications _m Laboratory tests 120.00% 100% 100% 100% 100.00% 80.00% « = £ 60.00% 8 8.48% 30.00% & 0.90% 40.00% 33.69 27.93 21.50 20.00% 0.00% SSHF SSHFmonth SSHFyear Datasets Fig. 7. The percentages of Diagnoses, Medications and Laboratory tests in the selected feature of F-value > 1.—e—SSHF —e—=SSHFmonth =—e— SSHFyear 82.5 82 81.5 81 80.5 80 79.5 79 78.5 78 AA(%) 775 7 0% 5% 10% 15% 20% 25% 30% 35% 40% Percentage of features after rangking Fig. 6. The relationship between different percentage of features after sorting and AA.NUMBER 5000 4500 4000 3500 3000 2500 2000 1500 1000 500 ™ Number of records im Number of survival 4291 /4066 (94.76%) @ Number of death 2694 2557 (94.91%) x = 9 & & of = = 8 z es 3 = ag a S S 22 a S 8 28 8 8 os Ss 83 243 (94,66%) 257 Z (5.34%) = 67 (5.13) 137 (5.09%) 225 (5.24%) 68 (6.92%) 8 (4.23%) [18,50) [50,60) [60,70) [70,80) [80,90) [90,104] THE RANGE OF AGE Fig. 8. The distribution of age in SSHF dataset.@F-value>100 @F-value>1 @F-value<1 400 350 © 300 250 200 150 100 50 0 F-value t Top 10 features Top 11-304 features 0 500 1000 1500 Feature ranked Fig. 5. The F-values of all 1302 features from high to low, points of different colors indicate the different range of F-value.40.00 35.00 30.00 25.00 20.00 15.00 10.00 5.00 0.00 ar mSelect mWithout select F,(%) tS & Fig. 3. F, of all algorithms with and without feature selection on SSHF dataset.",Person recognition,"The article discusses the use of Convolutional Neural Networks (CNN) for person recognition, specifically in addressing the issue of appearance-based identification. The authors propose a method that combines both image-based techniques and lengthy image sequences to improve the accuracy of person recognition. They fine-tune the CNN features using a unique loss function that combines pedestrian attribute data, resulting in better discriminative power. The proposed approach outperforms current state-of-the-art techniques, as demonstrated by promising outcomes on four hard person identification datasets. Overall, the authors aim to increase the amount of training samples and improve CNN features by combining pedestrian characteristics datasets with person recognition datasets.",Object and Sentiment Recognition,"90.00 wsalect 100.00 Select 80.00 Without select 90.00 mWithout select ithout sele 70.00 80.00 60.00 70.00 : = 60.00 = & mien & 50.00 2 40.00 = 40.00 30.00 30.00 20.00 20.00 10.00 10.00 0.00 0.00 ww * Ss & Ss we Ss s Ss & rs & ee s s ve é & Re of ! < & + s we & % Fig. 4. AUC of all algorithms with and without feature selection on SSHF da- Fig. 2. AA of all algorithms with and without feature selection on SSHF dataset. taset.Data collection Data processing Feature selection Prediction model WMEKL-NDC In-hospital 30-day 1-year mortality mortality mortality Mortality predictionMDiagnoses mMedications _m Laboratory tests 120.00% 100% 100% 100% 100.00% 80.00% « = £ 60.00% 8 8.48% 30.00% & 0.90% 40.00% 33.69 27.93 21.50 20.00% 0.00% SSHF SSHFmonth SSHFyear Datasets Fig. 7. The percentages of Diagnoses, Medications and Laboratory tests in the selected feature of F-value > 1.—e—SSHF —e—=SSHFmonth =—e— SSHFyear 82.5 82 81.5 81 80.5 80 79.5 79 78.5 78 AA(%) 775 7 0% 5% 10% 15% 20% 25% 30% 35% 40% Percentage of features after rangking Fig. 6. The relationship between different percentage of features after sorting and AA.NUMBER 5000 4500 4000 3500 3000 2500 2000 1500 1000 500 ™ Number of records im Number of survival 4291 /4066 (94.76%) @ Number of death 2694 2557 (94.91%) x = 9 & & of = = 8 z es 3 = ag a S S 22 a S 8 28 8 8 os Ss 83 243 (94,66%) 257 Z (5.34%) = 67 (5.13) 137 (5.09%) 225 (5.24%) 68 (6.92%) 8 (4.23%) [18,50) [50,60) [60,70) [70,80) [80,90) [90,104] THE RANGE OF AGE Fig. 8. The distribution of age in SSHF dataset.@F-value>100 @F-value>1 @F-value<1 400 350 © 300 250 200 150 100 50 0 F-value t Top 10 features Top 11-304 features 0 500 1000 1500 Feature ranked Fig. 5. The F-values of all 1302 features from high to low, points of different colors indicate the different range of F-value.40.00 35.00 30.00 25.00 20.00 15.00 10.00 5.00 0.00 ar mSelect mWithout select F,(%) tS & Fig. 3. F, of all algorithms with and without feature selection on SSHF dataset.",Medical Data Analysis
107,Gait Based Person Recognition,"gait recognition; silhouette; shadow biometrics; 
computer vision; segmentation","The gait is a special feature that needs to be 
identified as it affects many parts of the body. Each person's gait 
is individual. Unlike fingerprint or retinal identifiers, gait can be 
recognized at a great distance without direct contact. Also, gait 
recognition during epidemic periods is more relevant than face 
recognition. Using machine learning algorithms, you can train a 
neural network to recognize the identity of each person. The study 
considers splitting the stream into frames and various options for 
background segmentation such as GrabCut and Mask R-CNN. 
What follows is a detailed analysis of the optimal solution methods 
for human personality recognition based on the outline and the 
bounding box.","Based on the data in Table 4, it can be assumed that 
spatial features play an important role in human recognition. 
The lowest score was for temporary signs. Also, the
combination of these features gives small changes in the final 
result. Among the disadvantages of this algorithm, we can note 
that changing the shooting conditions, such as the distance and 
position of the camera from the object under consideration, can 
greatly affect the final result. In the future, we can improve the 
result by introducing additional parameters in the classification, 
and also try other types of classical algorithms. We can also use 
neural networks to determine the position of human limbs 
(skeleton) to combine the two algorithms in symbiosis to 
improve recognition accuracy.","Gait Based Person Recognitiongait recognition; silhouette; shadow biometrics; 
computer vision; segmentationThe gait is a special feature that needs to be 
identified as it affects many parts of the body. Each person's gait 
is individual. Unlike fingerprint or retinal identifiers, gait can be 
recognized at a great distance without direct contact. Also, gait 
recognition during epidemic periods is more relevant than face 
recognition. Using machine learning algorithms, you can train a 
neural network to recognize the identity of each person. The study 
considers splitting the stream into frames and various options for 
background segmentation such as GrabCut and Mask R-CNN. 
What follows is a detailed analysis of the optimal solution methods 
for human personality recognition based on the outline and the 
bounding box.Based on the data in Table 4, it can be assumed that 
spatial features play an important role in human recognition. 
The lowest score was for temporary signs. Also, the
combination of these features gives small changes in the final 
result. Among the disadvantages of this algorithm, we can note 
that changing the shooting conditions, such as the distance and 
position of the camera from the object under consideration, can 
greatly affect the final result. In the future, we can improve the 
result by introducing additional parameters in the classification, 
and also try other types of classical algorithms. We can also use 
neural networks to determine the position of human limbs 
(skeleton) to combine the two algorithms in symbiosis to 
improve recognition accuracy.Load Image Pre- Attention Cyele Consistency data set Processing >| Module |»! Classification Loss Person Re- Feature Map Color Result Identification |¢—] Tokening (Red |g—j — /Shape/Texture /Blue) Extraction",Person recognition,The article highlights the importance of gait recognition and proposes using machine learning algorithms for human personality recognition based on outline and bounding box. The authors analyze optimal solution methods and find that spatial features play an important role in recognition. They suggest improving accuracy by introducing additional parameters and using other algorithms or neural networks to determine the position of human limbs.,Object and Sentiment Recognition,Load Image Pre- Attention Cyele Consistency data set Processing >| Module |»! Classification Loss Person Re- Feature Map Color Result Identification |¢—] Tokening (Red |g—j — /Shape/Texture /Blue) Extraction,Object Recognition
108,Sequential Person Recognition in Photo Albums with a Recurrent Network,"person recognition, photo albums, relational information, recurrent network, contextual cues.","Recognizing the identities of people in everyday photos
is still a very challenging problem for machine vision, due
to issues such as non-frontal faces, changes in clothing, location and lighting. Recent studies have shown that rich
relational information between people in the same photo
can help in recognizing their identities. In this work, we
propose to model the relational information between people as a sequence prediction task. At the core of our work is
a novel recurrent network architecture, in which relational
information between instances’ labels and appearance are
modeled jointly. In addition to relational cues, scene context is incorporated in our sequence prediction model with
no additional cost. In this sense, our approach is a unified
framework for modeling both contextual cues and visual appearance of person instances. Our model is trained endto-end with a sequence of annotated instances in a photo
as inputs, and a sequence of corresponding labels as targets. We demonstrate that this simple but elegant formulation achieves state-of-the-art performance on the newly
released People In Photo Albums (PIPA) dataset","In this work, we have introduced a sequence prediction
formulation for the task of person recognition in photo albums. The advantage of our approach is that it can model
both the rich contextual information in the photo, and individual’s appearance in a unified framework. We trained
our model end-to-end and witnessed a significant boost in
the recognition performance, compared with baselines and
state-of-the-approaches which do not exploit the contextual
information for the task.
","Sequential Person Recognition in Photo Albums with a Recurrent Networkperson recognition, photo albums, relational information, recurrent network, contextual cues.Recognizing the identities of people in everyday photos
is still a very challenging problem for machine vision, due
to issues such as non-frontal faces, changes in clothing, location and lighting. Recent studies have shown that rich
relational information between people in the same photo
can help in recognizing their identities. In this work, we
propose to model the relational information between people as a sequence prediction task. At the core of our work is
a novel recurrent network architecture, in which relational
information between instances’ labels and appearance are
modeled jointly. In addition to relational cues, scene context is incorporated in our sequence prediction model with
no additional cost. In this sense, our approach is a unified
framework for modeling both contextual cues and visual appearance of person instances. Our model is trained endto-end with a sequence of annotated instances in a photo
as inputs, and a sequence of corresponding labels as targets. We demonstrate that this simple but elegant formulation achieves state-of-the-art performance on the newly
released People In Photo Albums (PIPA) datasetIn this work, we have introduced a sequence prediction
formulation for the task of person recognition in photo albums. The advantage of our approach is that it can model
both the rich contextual information in the photo, and individual’s appearance in a unified framework. We trained
our model end-to-end and witnessed a significant boost in
the recognition performance, compared with baselines and
state-of-the-approaches which do not exploit the contextual
information for the task.
",Person recognition,"This study proposes a novel recurrent network architecture to model the relational information between people in a photo for person recognition. The approach incorporates both contextual cues and visual appearance of person instances, and is trained end-to-end with annotated instances as inputs and corresponding labels as targets. The formulation achieves state-of-the-art performance on the PIPA dataset. ",Object and Sentiment Recognition,,Object Recognition
109,"Story Scrambler - Automatic Text Generation 
Using Word Level RNN-LSTM","Recurrent neural networks, Long shortterm memory, Text generation, Deep learning","With the advent of artificial intelligence, the 
way technology can assist humans is completely revived. 
Ranging from finance and medicine to music, gaming, 
and various other domains, it has slowly become an 
intricate part of our lives. A neural network, a computer 
system modeled on the human brain, is one of the 
methods of implementing artificial intelligence. In this 
paper, we have implemented a recurrent neural network 
methodology based text generation system called Story 
Scrambler. Our system aims to generate a new story
based on a series of inputted stories. For new story 
generation, we have considered two possibilities with 
respect to nature of inputted stories. Firstly, we have 
considered the stories with different storyline and 
characters. Secondly, we have worked with different 
volumes of the same stories where the storyline is in 
context with each other and characters are also similar. 
Results generated by the system are analyzed based on 
parameters like grammar correctness, linkage of events, 
interest level and uniqueness.
","n this paper, an implementation of story scrambler 
system using RNN and LSTM is discussed. By increasing 
the values of different parameters such as the number of 
neurons, number of layers, batch size and sequence 
length, we have tried to minimize the train loss. The 
stories formed are also evaluated by humans and an 
accuracy of 63% is obtained. The accuracy of the system 
can be improved further by considering the contextual 
meaning of the words. Also, synonyms can be used to 
further improve the accuracy of the system. This system 
can be further extended for the automatic generation of 
messages or news articles or jokes or posts.
","Story Scrambler - Automatic Text Generation 
Using Word Level RNN-LSTMRecurrent neural networks, Long shortterm memory, Text generation, Deep learningWith the advent of artificial intelligence, the 
way technology can assist humans is completely revived. 
Ranging from finance and medicine to music, gaming, 
and various other domains, it has slowly become an 
intricate part of our lives. A neural network, a computer 
system modeled on the human brain, is one of the 
methods of implementing artificial intelligence. In this 
paper, we have implemented a recurrent neural network 
methodology based text generation system called Story 
Scrambler. Our system aims to generate a new story
based on a series of inputted stories. For new story 
generation, we have considered two possibilities with 
respect to nature of inputted stories. Firstly, we have 
considered the stories with different storyline and 
characters. Secondly, we have worked with different 
volumes of the same stories where the storyline is in 
context with each other and characters are also similar. 
Results generated by the system are analyzed based on 
parameters like grammar correctness, linkage of events, 
interest level and uniqueness.
n this paper, an implementation of story scrambler 
system using RNN and LSTM is discussed. By increasing 
the values of different parameters such as the number of 
neurons, number of layers, batch size and sequence 
length, we have tried to minimize the train loss. The 
stories formed are also evaluated by humans and an 
accuracy of 63% is obtained. The accuracy of the system 
can be improved further by considering the contextual 
meaning of the words. Also, synonyms can be used to 
further improve the accuracy of the system. This system 
can be further extended for the automatic generation of 
messages or news articles or jokes or posts.
Uribe) Uva i Uy Uiolb) }rs as A ie uw a joint | aL joit Joi embedding} [embedding] [embedding] | embeddi LSTM }|——+| LSTM }-+| LSTM }+| LSTM |+| LSTM |» LSTM y y V 1 y log p(y) log p(y2) log p(ys) log p(ya) log p(ys)",Text generation,"The paper discusses the implementation of a story generation system called Story Scrambler using recurrent neural networks. The system aims to generate new stories based on inputted stories with different storylines and characters or similar storylines and characters. The generated stories are evaluated based on grammar correctness, linkage of events, interest level, and uniqueness. By increasing the values of different parameters, the system tries to minimize train loss, and human evaluation shows an accuracy of 63%. The system's accuracy can be improved by considering contextual meaning and using synonyms. The system can be extended for generating messages, news articles, jokes, or posts.",Natural Language Processing,Uribe) Uva i Uy Uiolb) }rs as A ie uw a joint | aL joit Joi embedding} [embedding] [embedding] | embeddi LSTM }|——+| LSTM }-+| LSTM }+| LSTM |+| LSTM |» LSTM y y V 1 y log p(y) log p(y2) log p(ys) log p(ya) log p(ys),Object Recognition
110,"Social Media Text Generation Based on Neural Network 
Model","Natural Language Generation; Neural Network Model; Social 
Media application ","The social media text is increasing rapidly in recent years. With 
this background, natural language generation technology is mature 
enough for implementing an NLG system in some general field, 
but the circumstance is difficult in the social media field because 
of the linguistic arbitrariness. This paper presents a neural 
network model building a social media NLG system. Compared 
with state-of-art model, our system outperforms the existing NLG 
system in the social media field.","This paper discusses a kind of Weibo text generating method 
based on neural network model. First of all, we collect a large
number of micro blog as raw data, and use the word embedding in
order to map the word to a low dimensional vector space, and 
trained a RNN model with LSTM cells and introduce the attention
mechanism. By comparing with the traditional Markov model and 
char-RNN model, it proves that the final result can improve 
performance. 
But there are still plenty of things that can be improved, such as 
increasing the data set and cleaning the data more carefully. 
Moreover, model structure and adjusting parameters also have a 
efficient impact on the performance. These work will be done in
the future research. ","Social Media Text Generation Based on Neural Network 
ModelNatural Language Generation; Neural Network Model; Social 
Media application The social media text is increasing rapidly in recent years. With 
this background, natural language generation technology is mature 
enough for implementing an NLG system in some general field, 
but the circumstance is difficult in the social media field because 
of the linguistic arbitrariness. This paper presents a neural 
network model building a social media NLG system. Compared 
with state-of-art model, our system outperforms the existing NLG 
system in the social media field.This paper discusses a kind of Weibo text generating method 
based on neural network model. First of all, we collect a large
number of micro blog as raw data, and use the word embedding in
order to map the word to a low dimensional vector space, and 
trained a RNN model with LSTM cells and introduce the attention
mechanism. By comparing with the traditional Markov model and 
char-RNN model, it proves that the final result can improve 
performance. 
But there are still plenty of things that can be improved, such as 
increasing the data set and cleaning the data more carefully. 
Moreover, model structure and adjusting parameters also have a 
efficient impact on the performance. These work will be done in
the future research. —__+—__ 200 i Zio =—_*—__, : rns Fao ANN 256 é a ANN 512 os oo 1 2 2 Number of Layers Fig.2. Train loss versus number of layers 200 a gi & 5 sateh sze_20 050 om RNN128 —-RNN.25G—RNN.S12 NN Size",Text generation,"The paper presents a neural network model for generating social media text using Weibo data. The model utilizes word embedding and RNN with LSTM cells, and introduces an attention mechanism to improve performance compared to traditional models. The study suggests that further improvements can be made by increasing and cleaning the dataset, and adjusting model structure and parameters. Overall, the system outperforms existing NLG systems in the social media field.",Natural Language Processing,"—__+—__ 200 i Zio =—_*—__, : rns Fao ANN 256 é a ANN 512 os oo 1 2 2 Number of Layers Fig.2. Train loss versus number of layers 200 a gi & 5 sateh sze_20 050 om RNN128 —-RNN.25G—RNN.S12 NN Size",Deep Learning and Machine Learning
111,Applying Automatic Text Summarization for Fake News Detection," Fake News Detection, Text Summarization, BERT, Ensemble","The distribution of fake news is not a new but a rapidly growing problem. The shift to news consumption via social media has
been one of the drivers for the spread of misleading and deliberately wrong information, as in addition to it of easy use there
is rarely any veracity monitoring. Due to the harmful effects of such fake news on society, the detection of these has become
increasingly important. We present an approach to the problem that combines the power of transformer-based language models
while simultaneously addressing one of their inherent problems. Our framework, CMTR-BERT, combines multiple text
representations, with the goal of circumventing sequential limits and related loss of information the underlying transformer
architecture typically suffers from. Additionally, it enables the incorporation of contextual information. Extensive experiments
on two very different, publicly available datasets demonstrates that our approach is able to set new state-of-the-art performance
benchmarks. Apart from the benefit of using automatic text summarization techniques we also find that the incorporation of
contextual information contributes to performance gains.
","We have presented an ensemble approach for fake news
detection that is based on the powerful paradigm of
transformer-based embeddings and utilizes text summarization as the main text transformation step before
classifying a document.
CMTR-BERT is able to achieve state-of-the-art results
for a common fake news benchmark collection and provides competitive results for a second one. Our results
indicate a measurable advantage of our architecture in
comparison to a standard BERT model.
Furthermore, our results emphasis the importance of
context information for fake news detection once more.
Not only do all context aware systems perform substantially better, it also seems feasible to not use content
information at all. While each text representation individually considered does not consistently bring advantages, the combination of all three seems to be the key.
It remains unclear to what extent our input transformation influences the performance, as the results here are
not decisive.
Overall, our results suggest that this is a worthwhile
direction of work, and we plan to explore this further.
Specifically, we are interested in using human summarizations, as arguably automatic summarization techniques are not on par with them yet and might negatively influence the system. Ideally, there would be an
additional dataset with context information as well as
aforementioned corresponding summarizations. This
might also be done with a small subset of, e.g. FakeNewsNet which gets manually annotated.
We would also like to see our approach used with
other datasets and different context information to get
a deeper understanding into which type of information
is key for effective fake news detection.
","Applying Automatic Text Summarization for Fake News Detection Fake News Detection, Text Summarization, BERT, EnsembleThe distribution of fake news is not a new but a rapidly growing problem. The shift to news consumption via social media has
been one of the drivers for the spread of misleading and deliberately wrong information, as in addition to it of easy use there
is rarely any veracity monitoring. Due to the harmful effects of such fake news on society, the detection of these has become
increasingly important. We present an approach to the problem that combines the power of transformer-based language models
while simultaneously addressing one of their inherent problems. Our framework, CMTR-BERT, combines multiple text
representations, with the goal of circumventing sequential limits and related loss of information the underlying transformer
architecture typically suffers from. Additionally, it enables the incorporation of contextual information. Extensive experiments
on two very different, publicly available datasets demonstrates that our approach is able to set new state-of-the-art performance
benchmarks. Apart from the benefit of using automatic text summarization techniques we also find that the incorporation of
contextual information contributes to performance gains.
We have presented an ensemble approach for fake news
detection that is based on the powerful paradigm of
transformer-based embeddings and utilizes text summarization as the main text transformation step before
classifying a document.
CMTR-BERT is able to achieve state-of-the-art results
for a common fake news benchmark collection and provides competitive results for a second one. Our results
indicate a measurable advantage of our architecture in
comparison to a standard BERT model.
Furthermore, our results emphasis the importance of
context information for fake news detection once more.
Not only do all context aware systems perform substantially better, it also seems feasible to not use content
information at all. While each text representation individually considered does not consistently bring advantages, the combination of all three seems to be the key.
It remains unclear to what extent our input transformation influences the performance, as the results here are
not decisive.
Overall, our results suggest that this is a worthwhile
direction of work, and we plan to explore this further.
Specifically, we are interested in using human summarizations, as arguably automatic summarization techniques are not on par with them yet and might negatively influence the system. Ideally, there would be an
additional dataset with context information as well as
aforementioned corresponding summarizations. This
might also be done with a small subset of, e.g. FakeNewsNet which gets manually annotated.
We would also like to see our approach used with
other datasets and different context information to get
a deeper understanding into which type of information
is key for effective fake news detection.
door reads S,_; and xj_1, and outputs a value C,_, to between 0 and 1 via the sigmoid function. one means “complete reservation"", zero means ""completely abandoned"" : fcr = 8(We X [hy Xe] + bp) Decoder Layer Encoder Layer Input Layer105.0% 99.8% 99.6% 100.0% sos seo ssn sows om seo son _. i _ HMM mChar-RNN m Our Model Figure 4. Different Top Similarities numbers Comparison 100.0% 95.0% 90.0% 85.0% 80.0% 10 50 100 OHM = —e=Char-RNN —®=Our model Figure 5. Different themes numbers comparison",Text summarization,"The distribution of fake news on social media is a growing problem, and detecting it has become important. The paper presents an ensemble approach called CMTR-BERT that combines multiple text representations to address the sequential limits of transformer-based language models and enable the incorporation of contextual information. CMTR-BERT achieves state-of-the-art results for two benchmark datasets and shows the importance of context information for fake news detection. The authors plan to further explore the use of human summarizations and other datasets to gain a deeper understanding of effective fake news detection.",Natural Language Processing,"door reads S,_; and xj_1, and outputs a value C,_, to between 0 and 1 via the sigmoid function. one means “complete reservation"", zero means ""completely abandoned"" : fcr = 8(We X [hy Xe] + bp) Decoder Layer Encoder Layer Input Layer105.0% 99.8% 99.6% 100.0% sos seo ssn sows om seo son _. i _ HMM mChar-RNN m Our Model Figure 4. Different Top Similarities numbers Comparison 100.0% 95.0% 90.0% 85.0% 80.0% 10 50 100 OHM = —e=Char-RNN —®=Our model Figure 5. Different themes numbers comparison",Deep Learning and Machine Learning
112,"Leveraging BERT for Extractive Text Summarization on 
Lectures","Lecture Summary; BERT; Deep Learning; Extractive 
Summarization","In the last two decades, automatic extractive text 
summarization on lectures has demonstrated to be a useful 
tool for collecting key phrases and sentences that best 
represent the content. However, many current approaches 
utilize dated approaches, producing sub-par outputs or 
requiring several hours of manual tuning to produce 
meaningful results. Recently, new machine learning 
architectures have provided mechanisms for extractive 
summarization through the clustering of output embeddings
from deep learning models. This paper reports on the project
called “lecture summarization service”, a python-based 
RESTful service that utilizes the BERT model for text 
embeddings and K-Means clustering to identify sentences 
closest to the centroid for summary selection. The purpose of 
the service was to provide student’s a utility that could 
summarize lecture content, based on their desired number of 
sentences. On top of summary work, the service also 
includes lecture and summary management, storing content 
on the cloud which can be used for collaboration. While the 
results of utilizing BERT for extractive text summarization 
were promising, there were still areas where the model 
struggled, providing future research opportunities for further 
improvement","Having the capability to properly summarize lectures is a 
powerful study and memory refreshing tool for university 
students. Automatic extractive summarization researchers
have attempted to solve this problem for the last several 
years, producing research with decent results. However, 
most of the approaches leave room for improvement as they 
utilize dated natural language processing models. 
Leveraging the most current deep learning NLP model called 
BERT, there is a steady improvement on dated approaches 
such as TextRank in the quality of summaries, combining 
context with the most important sentences. The lecture 
summarization service utilizes the BERT model to produce 
summaries for users, based on their specified configuration. 
While the service for automatic extractive summarization 
was not perfect, it provided the next step in quality when 
compared to dated approaches.","Leveraging BERT for Extractive Text Summarization on 
LecturesLecture Summary; BERT; Deep Learning; Extractive 
SummarizationIn the last two decades, automatic extractive text 
summarization on lectures has demonstrated to be a useful 
tool for collecting key phrases and sentences that best 
represent the content. However, many current approaches 
utilize dated approaches, producing sub-par outputs or 
requiring several hours of manual tuning to produce 
meaningful results. Recently, new machine learning 
architectures have provided mechanisms for extractive 
summarization through the clustering of output embeddings
from deep learning models. This paper reports on the project
called “lecture summarization service”, a python-based 
RESTful service that utilizes the BERT model for text 
embeddings and K-Means clustering to identify sentences 
closest to the centroid for summary selection. The purpose of 
the service was to provide student’s a utility that could 
summarize lecture content, based on their desired number of 
sentences. On top of summary work, the service also 
includes lecture and summary management, storing content 
on the cloud which can be used for collaboration. While the 
results of utilizing BERT for extractive text summarization 
were promising, there were still areas where the model 
struggled, providing future research opportunities for further 
improvementHaving the capability to properly summarize lectures is a 
powerful study and memory refreshing tool for university 
students. Automatic extractive summarization researchers
have attempted to solve this problem for the last several 
years, producing research with decent results. However, 
most of the approaches leave room for improvement as they 
utilize dated natural language processing models. 
Leveraging the most current deep learning NLP model called 
BERT, there is a steady improvement on dated approaches 
such as TextRank in the quality of summaries, combining 
context with the most important sentences. The lecture 
summarization service utilizes the BERT model to produce 
summaries for users, based on their specified configuration. 
While the service for automatic extractive summarization 
was not perfect, it provided the next step in quality when 
compared to dated approaches.sar io",Text summarization,"This paper discusses the development of a lecture summarization service using BERT and K-means clustering. The service utilizes deep learning models to produce high-quality summaries of lectures, and also includes lecture and summary management features. While the BERT model shows promise for extractive summarization, there is still room for improvement in certain areas. Overall, the lecture summarization service represents an improvement over dated natural language processing models and provides a useful tool for university students.",Natural Language Processing,sar io,Deep Learning and Machine Learning
113,"Adapting the Neural Encoder-Decoder Framework from Single to
Multi-Document Summarization","    multi-document summarization
    neural encoder-decoder
    maximal marginal relevance
    abstractive summarization
    automatic metrics","Generating a text abstract from a set of documents remains a challenging task. The neural
encoder-decoder framework has recently been
exploited to summarize single documents, but
its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data
for multi-document summarization are scarce
and costly to obtain. There is a pressing need
to adapt an encoder-decoder model trained on
single-document summarization data to work
with multiple-document input. In this paper,
we present an initial investigation into a novel
adaptation method. It exploits the maximal
marginal relevance method to select representative sentences from multi-document input,
and leverages an abstractive encoder-decoder
model to fuse disparate sentences to an abstractive summary. The adaptation method is
robust and itself requires no training data. Our
system compares favorably to state-of-the-art
extractive and abstractive approaches judged
by automatic metrics and human assessors.","We describe a novel adaptation method to generate abstractive summaries from multi-document
inputs. Our method combines an extractive summarization algorithm (MMR) for sentence extraction and a recent abstractive model (PG) for fusing
source sentences. The PG-MMR system demonstrates competitive results, outperforming strong
extractive and abstractive baselines.
","Adapting the Neural Encoder-Decoder Framework from Single to
Multi-Document Summarization    multi-document summarization
    neural encoder-decoder
    maximal marginal relevance
    abstractive summarization
    automatic metricsGenerating a text abstract from a set of documents remains a challenging task. The neural
encoder-decoder framework has recently been
exploited to summarize single documents, but
its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data
for multi-document summarization are scarce
and costly to obtain. There is a pressing need
to adapt an encoder-decoder model trained on
single-document summarization data to work
with multiple-document input. In this paper,
we present an initial investigation into a novel
adaptation method. It exploits the maximal
marginal relevance method to select representative sentences from multi-document input,
and leverages an abstractive encoder-decoder
model to fuse disparate sentences to an abstractive summary. The adaptation method is
robust and itself requires no training data. Our
system compares favorably to state-of-the-art
extractive and abstractive approaches judged
by automatic metrics and human assessors.We describe a novel adaptation method to generate abstractive summaries from multi-document
inputs. Our method combines an extractive summarization algorithm (MMR) for sentence extraction and a recent abstractive model (PG) for fusing
source sentences. The PG-MMR system demonstrates competitive results, outperforming strong
extractive and abstractive baselines.
",Text summarization,"Generating a summary from multiple documents is challenging due to the lack of parallel data. This paper presents a novel adaptation method that combines an extractive summarization algorithm with an abstractive encoder-decoder model to generate summaries from multiple documents. The method uses the maximal marginal relevance method to select representative sentences and requires no training data. The system performs well compared to state-of-the-art approaches, as judged by both automatic metrics and human assessors. The PG-MMR system outperforms strong extractive and abstractive baselines.",Natural Language Processing,,Deep Learning and Machine Learning
114,"Abstractive Text Summarization using Sequence-to-sequence RNNs and
Beyond
","    Abstractive Summarization
    Attentional Encoder-Decoder
    Recurrent Neural Networks
    Multi-Sentence Summarization
    Performance Benchmarks","In this work, we model abstractive text
summarization using Attentional EncoderDecoder Recurrent Neural Networks, and
show that they achieve state-of-the-art performance on two different corpora. We
propose several novel models that address
critical problems in summarization that
are not adequately modeled by the basic
architecture, such as modeling key-words,
capturing the hierarchy of sentence-toword structure, and emitting words that
are rare or unseen at training time. Our
work shows that many of our proposed
models contribute to further improvement
in performance. We also propose a new
dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.
"," this work, we apply the attentional encoderdecoder for the task of abstractive summarization
with very promising results, outperforming stateof-the-art results significantly on two different
datasets. Each of our proposed novel models addresses a specific problem in abstractive summarization, yielding further improvement in performance. We also propose a new dataset for multisentence summarization and establish benchmark
numbers on it. As part of our future work, we plan
to focus our efforts on this data and build more robust models for summaries consisting of multiple
sentences.","Abstractive Text Summarization using Sequence-to-sequence RNNs and
Beyond
    Abstractive Summarization
    Attentional Encoder-Decoder
    Recurrent Neural Networks
    Multi-Sentence Summarization
    Performance BenchmarksIn this work, we model abstractive text
summarization using Attentional EncoderDecoder Recurrent Neural Networks, and
show that they achieve state-of-the-art performance on two different corpora. We
propose several novel models that address
critical problems in summarization that
are not adequately modeled by the basic
architecture, such as modeling key-words,
capturing the hierarchy of sentence-toword structure, and emitting words that
are rare or unseen at training time. Our
work shows that many of our proposed
models contribute to further improvement
in performance. We also propose a new
dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.
 this work, we apply the attentional encoderdecoder for the task of abstractive summarization
with very promising results, outperforming stateof-the-art results significantly on two different
datasets. Each of our proposed novel models addresses a specific problem in abstractive summarization, yielding further improvement in performance. We also propose a new dataset for multisentence summarization and establish benchmark
numbers on it. As part of our future work, we plan
to focus our efforts on this data and build more robust models for summaries consisting of multiple
sentences.za tstsumm sent EAE athsumm sent [ES 2nd summ sent ESE sth summ sent IHRE 3rd summ sent (2) 2.7) 3,70) (10,58) PG-MMR (12.7) (12,62) os 6 m8 3 35 Location in the multi-document input‘Sent ‘Sent 2 ‘Sent 3 ‘Sent 4 ‘Sent 5 MMR = 0.62 MMR=02 © MMR=02 MMe MMR = 0.3 Fe Summ Sent 1 ef Summ Sent 2 mf Neural Decoder ———>",Text summarization,"The paper proposes an attentional encoder-decoder model for abstractive text summarization, which achieves state-of-the-art performance on two different corpora. Several novel models are proposed to address critical problems in summarization, leading to further improvements in performance. Additionally, a new dataset for multi-sentence summarization is introduced, and performance benchmarks are established. Future work will focus on building more robust models for multi-sentence summaries.",Natural Language Processing,"za tstsumm sent EAE athsumm sent [ES 2nd summ sent ESE sth summ sent IHRE 3rd summ sent (2) 2.7) 3,70) (10,58) PG-MMR (12.7) (12,62) os 6 m8 3 35 Location in the multi-document input‘Sent ‘Sent 2 ‘Sent 3 ‘Sent 4 ‘Sent 5 MMR = 0.62 MMR=02 © MMR=02 MMe MMR = 0.3 Fe Summ Sent 1 ef Summ Sent 2 mf Neural Decoder ———>",Deep Learning and Machine Learning
115,"Bidirectional Attentional Encoder-Decoder Model and Bidirectional 
Beam Search for Abstractive Summarization","sequence generative models, RNN variants, abstractive document summarization, bidirectional encoder-decoder architecture, bidirectional beam search.","Sequence generative models with RNN variants, such as 
LSTM, GRU, show promising performance on abstractive 
document summarization. However, they still have some issues that limit their performance, especially while dealing 
with long sequences. One of the issues is that, to the best of 
our knowledge, all current models employ a unidirectional
decoder, which reasons only about the past and still limited 
to retain future context while giving a prediction. This makes 
these models suffer on their own by generating unbalanced 
outputs. Moreover, unidirectional attention-based document 
summarization can only capture partial aspects of attentional 
regularities due to the inherited challenges in document summarization. To this end, we propose an end-to-end trainable 
bidirectional RNN model to tackle the aforementioned issues. 
The model has a bidirectional encoder-decoder architecture; 
in which the encoder and the decoder are bidirectional 
LSTMs. The forward decoder is initialized with the last hidden state of the backward encoder while the backward decoder is initialized with the last hidden state of the forward 
encoder. In addition, a bidirectional beam search mechanism 
is proposed as an approximate inference algorithm for generating the output summaries from the bidirectional model. 
This enables the model to reason about the past and future 
and to generate balanced outputs as a result. Experimental results on CNN / Daily Mail dataset show that the proposed
model outperforms the current abstractive state-of-the-art 
models by a considerable margin.","n this work, we used a bidirectional encoder-decoder architecture; each of which is a bidirectional recurrent neural network consists of two recurrent layers, one for learning history textual context and the other for learning future textual 
context. The output of the forward encoder was fed as input 
into the backward decoder while the output of the backward 
encoder was fed into the forward decoder. Then, a bidirectional beam search mechanism is used to generate tokens for 
the final summary one at a time. The experimental results 
have shown the effectiveness and the superiority of the proposed model compared to the-state-of-the-art models. Even 
though the pointer-generator network has alleviated the 
OOV problem, finding a way to tackle the problem while 
encouraging the model to generate summaries with more 
novelty and high level of abstraction is an exciting research 
problem. Furthermore, we believe that there is a real need to 
propose an evaluation metric besides ROUGE to optimize 
on summarization models, especially for long sequences.","Bidirectional Attentional Encoder-Decoder Model and Bidirectional 
Beam Search for Abstractive Summarizationsequence generative models, RNN variants, abstractive document summarization, bidirectional encoder-decoder architecture, bidirectional beam search.Sequence generative models with RNN variants, such as 
LSTM, GRU, show promising performance on abstractive 
document summarization. However, they still have some issues that limit their performance, especially while dealing 
with long sequences. One of the issues is that, to the best of 
our knowledge, all current models employ a unidirectional
decoder, which reasons only about the past and still limited 
to retain future context while giving a prediction. This makes 
these models suffer on their own by generating unbalanced 
outputs. Moreover, unidirectional attention-based document 
summarization can only capture partial aspects of attentional 
regularities due to the inherited challenges in document summarization. To this end, we propose an end-to-end trainable 
bidirectional RNN model to tackle the aforementioned issues. 
The model has a bidirectional encoder-decoder architecture; 
in which the encoder and the decoder are bidirectional 
LSTMs. The forward decoder is initialized with the last hidden state of the backward encoder while the backward decoder is initialized with the last hidden state of the forward 
encoder. In addition, a bidirectional beam search mechanism 
is proposed as an approximate inference algorithm for generating the output summaries from the bidirectional model. 
This enables the model to reason about the past and future 
and to generate balanced outputs as a result. Experimental results on CNN / Daily Mail dataset show that the proposed
model outperforms the current abstractive state-of-the-art 
models by a considerable margin.n this work, we used a bidirectional encoder-decoder architecture; each of which is a bidirectional recurrent neural network consists of two recurrent layers, one for learning history textual context and the other for learning future textual 
context. The output of the forward encoder was fed as input 
into the backward decoder while the output of the backward 
encoder was fed into the forward decoder. Then, a bidirectional beam search mechanism is used to generate tokens for 
the final summary one at a time. The experimental results 
have shown the effectiveness and the superiority of the proposed model compared to the-state-of-the-art models. Even 
though the pointer-generator network has alleviated the 
OOV problem, finding a way to tackle the problem while 
encouraging the model to generate summaries with more 
novelty and high level of abstraction is an exciting research 
problem. Furthermore, we believe that there is a real need to 
propose an evaluation metric besides ROUGE to optimize 
on summarization models, especially for long sequences.",Text summarization,"The use of sequence generative models with RNN variants has shown promise in abstractive document summarization, but they still have limitations when dealing with long sequences. Current models employ a unidirectional decoder which only reasons about the past, resulting in unbalanced outputs. To address these issues, an end-to-end trainable bidirectional RNN model is proposed with a bidirectional encoder-decoder architecture and bidirectional beam search mechanism. Experimental results on the CNN/Daily Mail dataset demonstrate the superiority of the proposed model over current state-of-the-art models. However, there is a need for further research to tackle the OOV problem and propose evaluation metrics beyond ROUGE.",Natural Language Processing,,Deep Learning and Machine Learning
116,"The Impact of Local Attention in LSTM for 
Abstractive Text Summarization","abstractive, local attention, LSTM, text 
summarization ","An attentional mechanism is very important to 
enhance a neural machine translation (NMT). There are two 
classes of attentions: global and local attentions. This paper 
focuses on comparing the impact of the local attention in Long 
Short-Term Memory (LSTM) model to generate an abstractive 
text summarization (ATS). Developing a model using a dataset 
of Amazon Fine Food Reviews and evaluating it using dataset 
of GloVe shows that the global attention-based model produces 
better ROUGE-1, where it generates more words contained in 
the actual summary. But, the local attention-based gives higher 
ROUGE-2, where it generates more pairs of words contained 
in the actual summary, since the mechanism of local attention 
considers the subset of input words instead of the whole input 
words","The global attention-based model produces better 
ROUGE-1, where it generates more words contained in the 
actual summary. But, the local attention-based gives higher 
ROUGE-2, where it generates more pairs of words 
contained in the actual summary, since the mechanism of 
local attention considers the subset of input words instead of 
the whole input words. Since the dataset is written using 
informal words, it contains a lot of symbols and unknown 
phrases those are not listed in the word embedding dataset. 
Therefore, the ROUGE score is not higher than the score 
from usual English text model. Resetting all parameters may 
give higher scores for both models. Some methods can be 
developed to improve the performance of both models, such 
us changing the dataset into any other containing article text 
instead of review text, rebuilding the model using more 
optimal parameters, or handling the OOV in data preprocessing. ","The Impact of Local Attention in LSTM for 
Abstractive Text Summarizationabstractive, local attention, LSTM, text 
summarization An attentional mechanism is very important to 
enhance a neural machine translation (NMT). There are two 
classes of attentions: global and local attentions. This paper 
focuses on comparing the impact of the local attention in Long 
Short-Term Memory (LSTM) model to generate an abstractive 
text summarization (ATS). Developing a model using a dataset 
of Amazon Fine Food Reviews and evaluating it using dataset 
of GloVe shows that the global attention-based model produces 
better ROUGE-1, where it generates more words contained in 
the actual summary. But, the local attention-based gives higher 
ROUGE-2, where it generates more pairs of words contained 
in the actual summary, since the mechanism of local attention 
considers the subset of input words instead of the whole input 
wordsThe global attention-based model produces better 
ROUGE-1, where it generates more words contained in the 
actual summary. But, the local attention-based gives higher 
ROUGE-2, where it generates more pairs of words 
contained in the actual summary, since the mechanism of 
local attention considers the subset of input words instead of 
the whole input words. Since the dataset is written using 
informal words, it contains a lot of symbols and unknown 
phrases those are not listed in the word embedding dataset. 
Therefore, the ROUGE score is not higher than the score 
from usual English text model. Resetting all parameters may 
give higher scores for both models. Some methods can be 
developed to improve the performance of both models, such 
us changing the dataset into any other containing article text 
instead of review text, rebuilding the model using more 
optimal parameters, or handling the OOV in data preprocessing. ",Text summarization,"This paper compares the impact of global and local attention mechanisms on the LSTM model for abstractive text summarization using a dataset of Amazon Fine Food Reviews and evaluating it using the GloVe dataset. The results show that the global attention-based model produces better ROUGE-1, while the local attention-based model gives higher ROUGE-2. However, the dataset contains a lot of symbols and unknown phrases, affecting the ROUGE score. Resetting all parameters may give higher scores for both models, and some methods can be developed to improve the performance, such as changing the dataset or handling OOV in data preprocessing.",Natural Language Processing,,Deep Learning and Machine Learning
117,"Gait-Based Person Recognition Using Arbitrary
View Transformation Model", Gait recognition.," Gait recognition is a useful biometric trait for
person authentication because it is usable even with low image
resolution. One challenge is robustness to a view change (crossview matching); view transformation models (VTMs) have been
proposed to solve this. The VTMs work well if the target
views are the same as their discrete training views. However,
the gait traits are observed from an arbitrary view in a real
situation. Thus, the target views may not coincide with discrete
training views, resulting in recognition accuracy degradation.
We propose an arbitrary VTM (AVTM) that accurately matches
a pair of gait traits from an arbitrary view. To realize an
AVTM, we first construct 3D gait volume sequences of training
subjects, disjoint from the test subjects in the target scene.
We then generate 2D gait silhouette sequences of the training
subjects by projecting the 3D gait volume sequences onto the
same views as the target views, and train the AVTM with
gait features extracted from the 2D sequences. In addition,
we extend our AVTM by incorporating a part-dependent view
selection scheme (AVTM_PdVS), which divides the gait feature
into several parts, and sets part-dependent destination views for
transformation. Because appropriate destination views may differ
for different body parts, the part-dependent destination view
selection can suppress transformation errors, leading to increased
recognition accuracy. Experiments using data sets collected in
different settings show that the AVTM improves the accuracy of
cross-view matching and that the AVTM_PdVS further improves
the accuracy in many cases, in particular, verification scenarios.","In this paper, we proposed an AVTM for cross-view gait
recognition. Using this arbitrary-view framework, we eliminated the discrete nature of previously proposed VTMs, and
generated an AVTM. Through the experiments, we showed
that the AVTM achieves higher accuracy than the DVTM,
woVTM, and RankSVM in verification tasks with score normalization and identification tasks. Moreover, we extended
our AVTM to the AVTM_PdVS by incorporating a PdVS
scheme based on the observation that transformation errors
are dependent not only on body parts, but also on destination
views. While the performance of the AVTM was sometimes
worse than that of RankSVM and woVTM in verification
tasks without score normalization because of the inhomogeneous subject-dependent bias of the dissimilarity scores, the
AVTM_PdVS achieved higher accuracy than the comparative
methods including the AVTM in most of the settings for all
tasks.
Moreover, we also showed that the proposed arbitraryview framework improves the accuracy of other approaches
such as RankSVM, which indicates wider applicability
of the arbitrary-view framework. Note that the proposed
AVTM_PdVS still achieves higher accuracy than the arbitraryview version of RankSVM in many settings, which confirms
that the proposed AVTM_PdVS is a promising approach for
cross-view gait recognition.","Gait-Based Person Recognition Using Arbitrary
View Transformation Model Gait recognition. Gait recognition is a useful biometric trait for
person authentication because it is usable even with low image
resolution. One challenge is robustness to a view change (crossview matching); view transformation models (VTMs) have been
proposed to solve this. The VTMs work well if the target
views are the same as their discrete training views. However,
the gait traits are observed from an arbitrary view in a real
situation. Thus, the target views may not coincide with discrete
training views, resulting in recognition accuracy degradation.
We propose an arbitrary VTM (AVTM) that accurately matches
a pair of gait traits from an arbitrary view. To realize an
AVTM, we first construct 3D gait volume sequences of training
subjects, disjoint from the test subjects in the target scene.
We then generate 2D gait silhouette sequences of the training
subjects by projecting the 3D gait volume sequences onto the
same views as the target views, and train the AVTM with
gait features extracted from the 2D sequences. In addition,
we extend our AVTM by incorporating a part-dependent view
selection scheme (AVTM_PdVS), which divides the gait feature
into several parts, and sets part-dependent destination views for
transformation. Because appropriate destination views may differ
for different body parts, the part-dependent destination view
selection can suppress transformation errors, leading to increased
recognition accuracy. Experiments using data sets collected in
different settings show that the AVTM improves the accuracy of
cross-view matching and that the AVTM_PdVS further improves
the accuracy in many cases, in particular, verification scenarios.In this paper, we proposed an AVTM for cross-view gait
recognition. Using this arbitrary-view framework, we eliminated the discrete nature of previously proposed VTMs, and
generated an AVTM. Through the experiments, we showed
that the AVTM achieves higher accuracy than the DVTM,
woVTM, and RankSVM in verification tasks with score normalization and identification tasks. Moreover, we extended
our AVTM to the AVTM_PdVS by incorporating a PdVS
scheme based on the observation that transformation errors
are dependent not only on body parts, but also on destination
views. While the performance of the AVTM was sometimes
worse than that of RankSVM and woVTM in verification
tasks without score normalization because of the inhomogeneous subject-dependent bias of the dissimilarity scores, the
AVTM_PdVS achieved higher accuracy than the comparative
methods including the AVTM in most of the settings for all
tasks.
Moreover, we also showed that the proposed arbitraryview framework improves the accuracy of other approaches
such as RankSVM, which indicates wider applicability
of the arbitrary-view framework. Note that the proposed
AVTM_PdVS still achieves higher accuracy than the arbitraryview version of RankSVM in many settings, which confirms
that the proposed AVTM_PdVS is a promising approach for
cross-view gait recognition.#ERROR!",Person recognition,"This paper proposes an arbitrary view transformation model (AVTM) for gait recognition in person authentication, which addresses the challenge of robustness to view changes. The AVTM is capable of accurately matching gait traits from an arbitrary view by constructing 3D gait volume sequences and generating 2D gait silhouette sequences of training subjects. The AVTM is further extended with a part-dependent view selection scheme (AVTM_PdVS) to improve recognition accuracy. Experimental results demonstrate that the AVTM improves cross-view matching accuracy, and the AVTM_PdVS achieves higher accuracy than comparative methods in most settings. The proposed arbitrary-view framework also improves the accuracy of other approaches such as RankSVM.",Object and Sentiment Recognition,#ERROR!,Deep Learning and Machine Learning
118,"T-BERTSum: Topic-Aware Text
Summarization Based on BERT"," Bidirectional Encoder Representations from
Transformers (BERTs), neural topic model (NTM), social network, text summarization.","In the era of social networks, the rapid growth
of data mining in information retrieval and natural language
processing makes automatic text summarization necessary. Currently, pretrained word embedding and sequence to sequence
models can be effectively adapted in social network summarization to extract significant information with strong encoding
capability. However, how to tackle the long text dependence
and utilize the latent topic mapping has become an increasingly
crucial challenge for these models. In this article, we propose
a topic-aware extractive and abstractive summarization model
named T-BERTSum, based on Bidirectional Encoder Representations from Transformers (BERTs). This is an improvement over
previous models, in which the proposed approach can simultaneously infer topics and generate summarization from social texts.
First, the encoded latent topic representation, through the neural
topic model (NTM), is matched with the embedded representation
of BERT, to guide the generation with the topic. Second,
the long-term dependencies are learned through the transformer
network to jointly explore topic inference and text summarization in an end-to-end manner. Third, the long short-term
memory (LSTM) network layers are stacked on the extractive
model to capture sequence timing information, and the effective
information is further filtered on the abstractive model through
a gated network. In addition, a two-stage extractive–abstractive
model is constructed to share the information. Compared with
the previous work, the proposed model T-BERTSum focuses
on pretrained external knowledge and topic mining to capture
more accurate contextual representations. Experimental results
on the CNN/Daily mail and XSum datasets demonstrate that our
proposed model achieves new state-of-the-art results while generating consistent topics compared with the most advanced method.","In this work, we propose a general model of extractive
and abstractive for text summarization, which is based on
BERT’s powerful architecture and additional topic embedding
information to guide contextual information capture. For a
good summary, an accurate representation is extremely important. This article introduces the representation of a powerful
pretraining language model (BERT) to lay the foundation of
the source text encoding and emphasizes the subjectivity of the
generated content. The fusion of topic embedding is a direct
and effective way to achieve high-quality generation through
NTM inferring. The combination of token embedding, segment
embedding, position embedding, and topic embedding can
more abundantly embed the information that the original text
should contain. Stacking the transformer layer in the encoding
stage is able to enhance the BERT’s ability to represent source
texts, make full use of self-attention, and judge the importance
of different components of the sentence through different focus
scores. The two-stage extractive–abstractive model can share
information and generate salient summaries, which reduces a
certain degree of redundancy. The experimental results show
that the model proposed in this article achieves the stateof-the-art results on the CNN/Daily Mail dataset and the
XSum dataset. The analysis shows that the model can generate
high-quality summaries with outstanding consistency for the
original text.","T-BERTSum: Topic-Aware Text
Summarization Based on BERT Bidirectional Encoder Representations from
Transformers (BERTs), neural topic model (NTM), social network, text summarization.In the era of social networks, the rapid growth
of data mining in information retrieval and natural language
processing makes automatic text summarization necessary. Currently, pretrained word embedding and sequence to sequence
models can be effectively adapted in social network summarization to extract significant information with strong encoding
capability. However, how to tackle the long text dependence
and utilize the latent topic mapping has become an increasingly
crucial challenge for these models. In this article, we propose
a topic-aware extractive and abstractive summarization model
named T-BERTSum, based on Bidirectional Encoder Representations from Transformers (BERTs). This is an improvement over
previous models, in which the proposed approach can simultaneously infer topics and generate summarization from social texts.
First, the encoded latent topic representation, through the neural
topic model (NTM), is matched with the embedded representation
of BERT, to guide the generation with the topic. Second,
the long-term dependencies are learned through the transformer
network to jointly explore topic inference and text summarization in an end-to-end manner. Third, the long short-term
memory (LSTM) network layers are stacked on the extractive
model to capture sequence timing information, and the effective
information is further filtered on the abstractive model through
a gated network. In addition, a two-stage extractive–abstractive
model is constructed to share the information. Compared with
the previous work, the proposed model T-BERTSum focuses
on pretrained external knowledge and topic mining to capture
more accurate contextual representations. Experimental results
on the CNN/Daily mail and XSum datasets demonstrate that our
proposed model achieves new state-of-the-art results while generating consistent topics compared with the most advanced method.In this work, we propose a general model of extractive
and abstractive for text summarization, which is based on
BERT’s powerful architecture and additional topic embedding
information to guide contextual information capture. For a
good summary, an accurate representation is extremely important. This article introduces the representation of a powerful
pretraining language model (BERT) to lay the foundation of
the source text encoding and emphasizes the subjectivity of the
generated content. The fusion of topic embedding is a direct
and effective way to achieve high-quality generation through
NTM inferring. The combination of token embedding, segment
embedding, position embedding, and topic embedding can
more abundantly embed the information that the original text
should contain. Stacking the transformer layer in the encoding
stage is able to enhance the BERT’s ability to represent source
texts, make full use of self-attention, and judge the importance
of different components of the sentence through different focus
scores. The two-stage extractive–abstractive model can share
information and generate salient summaries, which reduces a
certain degree of redundancy. The experimental results show
that the model proposed in this article achieves the stateof-the-art results on the CNN/Daily Mail dataset and the
XSum dataset. The analysis shows that the model can generate
high-quality summaries with outstanding consistency for the
original text.Enrollment phase Recognition phase Data acquisition Silhouette extraction I Feature extraction Pec) ered ~_ AVTM generation Part-dependent view selection Transformation uelucs Gait feature synthesis, Matching Independent training 3D gait volume sequences3D gait database Input gait feature Synthesized gait feature",Text summarization,"The article proposes a model for text summarization called T-BERTSum, which combines BERT's encoding capabilities with topic embedding to improve contextual representation and generate high-quality summaries. The model uses a neural topic model to infer topics and guide generation. The transformer network and LSTM layers capture long-term dependencies and timing information. The two-stage extractive-abstractive model reduces redundancy and achieves state-of-the-art results on CNN/Daily Mail and XSum datasets. The proposed model generates consistent and high-quality summaries.",Natural Language Processing,"Enrollment phase Recognition phase Data acquisition Silhouette extraction I Feature extraction Pec) ered ~_ AVTM generation Part-dependent view selection Transformation uelucs Gait feature synthesis, Matching Independent training 3D gait volume sequences3D gait database Input gait feature Synthesized gait feature",Object Recognition
119,"A Robust Predictive Model for Stock Price 
Prediction Using Deep Learning and Natural 
Language Processing","Stock Price Prediction, Classification, 
Regression, LSTM, Sentiment Analysis, Granger Causality, 
Cross-validation, Self-Organizing Fuzzy Neural Networks.
","Prediction of future movement of stock prices 
has been a subject matter of many research work. There is a 
gamut of literature of technical analysis of stock prices where 
the objective is to identify patterns in stock price movements 
and derive profit from it. Improving the prediction accuracy 
remains the single most challenge in this area of research. We 
propose a hybrid approach for stock price movement 
prediction using machine learning, deep learning, and natural 
language processing. We select the NIFTY 50 index values of 
the National Stock Exchange (NSE) of India, and collect its 
daily price movement over a period of three years (2015 –
2017). Based on the data of 2015 – 2017, we build various 
predictive models using machine learning, and then use those 
models to predict the closing value of NIFTY 50 for the period 
January 2018 till June 2019 with a prediction horizon of one 
week. For predicting the price movement patterns, we use a 
number of classification techniques, while for predicting the 
actual closing price of the stock, various regression models 
have been used. We also build a Long and Short-Term Memory
(LSTM)-based deep learning network for predicting the 
closing price of the stocks and compare the prediction 
accuracies of the machine learning models with the LSTM 
model. We further augment the predictive model by 
integrating a sentiment analysis module on twitter data to 
correlate the public sentiment of stock prices with the market
sentiment. This has been done using twitter sentiment and 
previous week closing values to predict stock price movement
for the next week. We tested our proposed scheme using a 
cross validation method based on Self Organizing Fuzzy Neural 
Networks (SOFNN) and found extremely interesting results.","In this paper, we have presented several approaches to 
stock price and movement prediction on a weekly forecast
horizon using eight regression and eight classification 
methods. These models are based on machine learning and 
deep learning approaches. We built, fine-tuned, and tested 
these models using daily historical data of NIFTY 50 during 
January 2, 2015 till June 28, 2019. The raw data is suitably
pre-processed and suitable variables are identified for
building predictive models. After designing and testing the 
machine learning and deep learning-based models, the 
predictive framework is further augmented by bringing in 
public sentiment in the social media in addition to the 
historical stock prices, as the two inputs a fuzzy neural 
network-based SOFNN algorithm. The performance of this 
sentiment analysis-enhanced model is found to be the best 
among all models in its ability to accurately forecast the 
stock price movement of NIFTY 50. The study has 
conclusively proved that public sentiments in the social 
media serve as a very significant input in predictive model 
building for stock price movement","A Robust Predictive Model for Stock Price 
Prediction Using Deep Learning and Natural 
Language ProcessingStock Price Prediction, Classification, 
Regression, LSTM, Sentiment Analysis, Granger Causality, 
Cross-validation, Self-Organizing Fuzzy Neural Networks.
Prediction of future movement of stock prices 
has been a subject matter of many research work. There is a 
gamut of literature of technical analysis of stock prices where 
the objective is to identify patterns in stock price movements 
and derive profit from it. Improving the prediction accuracy 
remains the single most challenge in this area of research. We 
propose a hybrid approach for stock price movement 
prediction using machine learning, deep learning, and natural 
language processing. We select the NIFTY 50 index values of 
the National Stock Exchange (NSE) of India, and collect its 
daily price movement over a period of three years (2015 –
2017). Based on the data of 2015 – 2017, we build various 
predictive models using machine learning, and then use those 
models to predict the closing value of NIFTY 50 for the period 
January 2018 till June 2019 with a prediction horizon of one 
week. For predicting the price movement patterns, we use a 
number of classification techniques, while for predicting the 
actual closing price of the stock, various regression models 
have been used. We also build a Long and Short-Term Memory
(LSTM)-based deep learning network for predicting the 
closing price of the stocks and compare the prediction 
accuracies of the machine learning models with the LSTM 
model. We further augment the predictive model by 
integrating a sentiment analysis module on twitter data to 
correlate the public sentiment of stock prices with the market
sentiment. This has been done using twitter sentiment and 
previous week closing values to predict stock price movement
for the next week. We tested our proposed scheme using a 
cross validation method based on Self Organizing Fuzzy Neural 
Networks (SOFNN) and found extremely interesting results.In this paper, we have presented several approaches to 
stock price and movement prediction on a weekly forecast
horizon using eight regression and eight classification 
methods. These models are based on machine learning and 
deep learning approaches. We built, fine-tuned, and tested 
these models using daily historical data of NIFTY 50 during 
January 2, 2015 till June 28, 2019. The raw data is suitably
pre-processed and suitable variables are identified for
building predictive models. After designing and testing the 
machine learning and deep learning-based models, the 
predictive framework is further augmented by bringing in 
public sentiment in the social media in addition to the 
historical stock prices, as the two inputs a fuzzy neural 
network-based SOFNN algorithm. The performance of this 
sentiment analysis-enhanced model is found to be the best 
among all models in its ability to accurately forecast the 
stock price movement of NIFTY 50. The study has 
conclusively proved that public sentiments in the social 
media serve as a very significant input in predictive model 
building for stock price movementAbstract: representation @ Neural Topic Mode! pd |O) Old) Transformer i=» Encoder_ours Transformer Topic-aware ‘Summarization|ooo Output i O00 Probabilities ‘&Softmax 1 I &, ' —o ' Sigmoid | Add&Norm ' ' | Feed Fi ' ' ! 1, [COO] [OOO)s SS x GateNN ‘Add&Norm Encoder Multi-Head Add&Norm Attention Feed Forward Add&Norm Add&Norm <— Masked 7 Multi-Head Multi-Head Attention Attention ch Decoder o ®Input Document Token Embedding Segment Embedding Position Embedding Topic Embedding Contextual Embedding HC} ow | He HB) | Hw | HHL | Hw HH AEE ann HRIEIE FG] tm toe | ser FG] ten two ser [EEG] tow tone nse (eS) cow |) com 81H) cow | Ome | sem AER] cm | ce | cer",Deep Learning and Machine Learning,"The paper proposes a hybrid approach for stock price movement prediction using machine learning, deep learning, and natural language processing. The study uses historical data of NIFTY 50 index values of the National Stock Exchange of India from 2015-2017 to build various predictive models using machine learning and deep learning techniques. The models are fine-tuned and tested to predict the closing value of NIFTY 50 for the period January 2018 till June 2019 with a prediction horizon of one week. The paper further augments the predictive model by integrating sentiment analysis of social media to correlate public sentiment with the market sentiment. The study concludes that public sentiments in social media serve as a significant input in predictive model building for stock price movement. The sentiment analysis-enhanced model is found to be the best among all models in accurately forecasting the stock price movement of NIFTY 50.",Deep Learning and Machine Learning,"Abstract: representation @ Neural Topic Mode! pd |O) Old) Transformer i=» Encoder_ours Transformer Topic-aware ‘Summarization|ooo Output i O00 Probabilities ‘&Softmax 1 I &, ' —o ' Sigmoid | Add&Norm ' ' | Feed Fi ' ' ! 1, [COO] [OOO)s SS x GateNN ‘Add&Norm Encoder Multi-Head Add&Norm Attention Feed Forward Add&Norm Add&Norm <— Masked 7 Multi-Head Multi-Head Attention Attention ch Decoder o ®Input Document Token Embedding Segment Embedding Position Embedding Topic Embedding Contextual Embedding HC} ow | He HB) | Hw | HHL | Hw HH AEE ann HRIEIE FG] tm toe | ser FG] ten two ser [EEG] tow tone nse (eS) cow |) com 81H) cow | Ome | sem AER] cm | ce | cer",Deep Learning and Machine Learning
120,"Unsupervised query‑focused multi‑document summarization based 
on transfer learning from sentence embedding models, BM25 model, 
and maximal marginal relevance criterion"," Query-focused multi-document summarization · Transfer learning · Sentence embedding models · BM25 
model · Semantic similarity · Maximal marginal relevance","Extractive query-focused multi-document summarization (QF-MDS) is the process of automatically generating an informative 
summary from a collection of documents that answers a pre-given query. Sentence and query representation is a fundamental 
cornerstone that afects the efectiveness of several QF-MDS methods. Transfer learning using pre-trained word embedding 
models has shown promising performance in many applications. However, most of these representations do not consider the 
order and the semantic relationships between words in a sentence, and thus they do not carry the meaning of a full sentence. 
In this paper, to deal with this issue, we propose to leverage transfer learning from pre-trained sentence embedding models 
to represent documents’ sentences and users’ queries using embedding vectors that capture the semantic and the syntactic 
relationships between their constituents (words, phrases). Furthermore, BM25 and semantic similarity function are linearly 
combined to retrieve a subset of sentences based on their relevance to the query. Finally, the maximal marginal relevance 
criterion is applied to re-rank the selected sentences by maintaining query relevance and minimizing redundancy. The proposed method is unsupervised, simple, efcient, and requires no labeled text summarization training data. Experiments are 
conducted using three standard datasets from the DUC evaluation campaign (DUC’2005–2007). The overall obtained results 
show that our method outperforms several state-of-the-art systems and achieves comparable results to the best performing 
systems, including supervised deep learning-based methods.","In this paper, we proposed an unsupervised method for queryfocused multi-document summarization based on transfer 
learning from pre-trained sentence embedding models. First, 
sentence embedding models are exploited to represent the 
cluster’s sentences and the users ’queries into fxed dense 
vectors, which are used to compute the semantic similarity 
between the cluster’s sentences and the user’s query. Then, 
the semantic similarity and the BM25 model are linearly 
combined to select the top-k ranked sentences based on their 
relevance to the input query. Finally, the maximal marginal relevance criterion is used to re-rank the top-k selected sentences to produce a query-relevant summary that maximizes 
relevant information and minimizes redundancy.
We performed an extensive experimental analysis to validate the robustness of the proposed query-focused multi-document summarization method. In particular, several experiments 
were conducted on DUC’2005–2007 datasets to evaluate each 
sentence embedding model exploited in our method and assess 
the impact of combining the semantic similarity function and 
the BM25 model. The experimental results showed that the use 
of sentence embedding representation and the combination of 
the semantic similarity and the BM25 model, have considerably improved the results for all evaluation measures (R-1, R-2, 
and R-SU4) in all benchmarks. We compared our method with 
several state-of-the-art query-focused multi- document summarization systems including recent supervised deep learning 
based systems. The obtained results show that our method has 
achieved promising results. In particular, it has outperformed 
several systems and achieved comparable results to the best 
performing unsupervised systems (CES and Dual-CES) and 
even outperformed them in terms of R-SU4 measure. Besides, 
its summarization quality can reach that of strong supervised 
systems (AttSum, CRSum-SF, and SRSum).
Transfer learning from pre-trained sentence embedding 
models has shown to be efective for boosting the performance of the query-focused multi-document summarization. 
Hence, in the future work, we plan to investigate the potential 
of the newest models T5 and GPT-3 for text summarization 
task. Furthermore, we also plan to explore transfer learning 
abilities from pre-trained models for summary generation.","Unsupervised query‑focused multi‑document summarization based 
on transfer learning from sentence embedding models, BM25 model, 
and maximal marginal relevance criterion Query-focused multi-document summarization · Transfer learning · Sentence embedding models · BM25 
model · Semantic similarity · Maximal marginal relevanceExtractive query-focused multi-document summarization (QF-MDS) is the process of automatically generating an informative 
summary from a collection of documents that answers a pre-given query. Sentence and query representation is a fundamental 
cornerstone that afects the efectiveness of several QF-MDS methods. Transfer learning using pre-trained word embedding 
models has shown promising performance in many applications. However, most of these representations do not consider the 
order and the semantic relationships between words in a sentence, and thus they do not carry the meaning of a full sentence. 
In this paper, to deal with this issue, we propose to leverage transfer learning from pre-trained sentence embedding models 
to represent documents’ sentences and users’ queries using embedding vectors that capture the semantic and the syntactic 
relationships between their constituents (words, phrases). Furthermore, BM25 and semantic similarity function are linearly 
combined to retrieve a subset of sentences based on their relevance to the query. Finally, the maximal marginal relevance 
criterion is applied to re-rank the selected sentences by maintaining query relevance and minimizing redundancy. The proposed method is unsupervised, simple, efcient, and requires no labeled text summarization training data. Experiments are 
conducted using three standard datasets from the DUC evaluation campaign (DUC’2005–2007). The overall obtained results 
show that our method outperforms several state-of-the-art systems and achieves comparable results to the best performing 
systems, including supervised deep learning-based methods.In this paper, we proposed an unsupervised method for queryfocused multi-document summarization based on transfer 
learning from pre-trained sentence embedding models. First, 
sentence embedding models are exploited to represent the 
cluster’s sentences and the users ’queries into fxed dense 
vectors, which are used to compute the semantic similarity 
between the cluster’s sentences and the user’s query. Then, 
the semantic similarity and the BM25 model are linearly 
combined to select the top-k ranked sentences based on their 
relevance to the input query. Finally, the maximal marginal relevance criterion is used to re-rank the top-k selected sentences to produce a query-relevant summary that maximizes 
relevant information and minimizes redundancy.
We performed an extensive experimental analysis to validate the robustness of the proposed query-focused multi-document summarization method. In particular, several experiments 
were conducted on DUC’2005–2007 datasets to evaluate each 
sentence embedding model exploited in our method and assess 
the impact of combining the semantic similarity function and 
the BM25 model. The experimental results showed that the use 
of sentence embedding representation and the combination of 
the semantic similarity and the BM25 model, have considerably improved the results for all evaluation measures (R-1, R-2, 
and R-SU4) in all benchmarks. We compared our method with 
several state-of-the-art query-focused multi- document summarization systems including recent supervised deep learning 
based systems. The obtained results show that our method has 
achieved promising results. In particular, it has outperformed 
several systems and achieved comparable results to the best 
performing unsupervised systems (CES and Dual-CES) and 
even outperformed them in terms of R-SU4 measure. Besides, 
its summarization quality can reach that of strong supervised 
systems (AttSum, CRSum-SF, and SRSum).
Transfer learning from pre-trained sentence embedding 
models has shown to be efective for boosting the performance of the query-focused multi-document summarization. 
Hence, in the future work, we plan to investigate the potential 
of the newest models T5 and GPT-3 for text summarization 
task. Furthermore, we also plan to explore transfer learning 
abilities from pre-trained models for summary generation.",Text summarization,"The paper proposes an unsupervised method for query-focused multi-document summarization using transfer learning from pre-trained sentence embedding models. The method combines semantic similarity and BM25 to select top-k sentences based on relevance to the query and uses maximal marginal relevance to re-rank them for query-relevant summary. Experimental analysis on DUC'2005-2007 datasets shows that the proposed method outperforms several state-of-the-art systems and achieves comparable results to the best performing systems, including supervised deep learning-based methods. The paper concludes by suggesting exploring the potential of newer models such as T5 and GPT-3 for text summarization and investigating transfer learning for summary generation.",Natural Language Processing,,Deep Learning and Machine Learning
121,Japanese abstractive text summarization using BERT,"abstractive text summarization; BERT; 
livedoor news corpus"," In         this         study,         we developed         an         automatic        
abstractive         text         summarization         algorithm         in         Japanese        
using a neural        network. We        used        a sequence-to-sequence
encoder-decoder        model        for        experimentation purposes.        The        
encoder        obtained a feature-based input vector        of        sentences        
using         BERT.         A         transformer-based decoder returned the
summary         sentence         from         the output         as         generated by         the        
encoder.         This experiment         was         performed         using         the        
livedoor news corpus         with         the         above         model. However,        
issues         arose         as         the         same         texts         were         repeated         in         the        
summary        sentence.","We conducted an experiment to demonstrate Japanese
abstractive text summarization with a neural network model 
using BERT. Our model comprised a BERT encoder and 
Transformer-based decoder. The dataset used in this paper 
was the livedoor news corpus consisting of 130,000 
datapoints, of which 100,000 were used for training.
 The results of the experiment revealed that the model was 
able to learn correctly as the summary sentence captured the 
key points of the text to some extent. However, the contents 
of the summary sentence were repeated, and the model was 
unable to handle unknown words. Additionally, there was a 
problem of simple word mistakes. We believe that the above 
problems could be solved by utilizing the coverage and copy 
mechanisms, and by improving the models.
In the future, we will explore these recommendations with
new experiments and compare the results.","Japanese abstractive text summarization using BERTabstractive text summarization; BERT; 
livedoor news corpus In         this         study,         we developed         an         automatic        
abstractive         text         summarization         algorithm         in         Japanese        
using a neural        network. We        used        a sequence-to-sequence
encoder-decoder        model        for        experimentation purposes.        The        
encoder        obtained a feature-based input vector        of        sentences        
using         BERT.         A         transformer-based decoder returned the
summary         sentence         from         the output         as         generated by         the        
encoder.         This experiment         was         performed         using         the        
livedoor news corpus         with         the         above         model. However,        
issues         arose         as         the         same         texts         were         repeated         in         the        
summary        sentence.We conducted an experiment to demonstrate Japanese
abstractive text summarization with a neural network model 
using BERT. Our model comprised a BERT encoder and 
Transformer-based decoder. The dataset used in this paper 
was the livedoor news corpus consisting of 130,000 
datapoints, of which 100,000 were used for training.
 The results of the experiment revealed that the model was 
able to learn correctly as the summary sentence captured the 
key points of the text to some extent. However, the contents 
of the summary sentence were repeated, and the model was 
unable to handle unknown words. Additionally, there was a 
problem of simple word mistakes. We believe that the above 
problems could be solved by utilizing the coverage and copy 
mechanisms, and by improving the models.
In the future, we will explore these recommendations with
new experiments and compare the results.D: Cluster of docs oe 1. Preprocessing ~ Tokenization 1. Preprocessing J Sentence Splitting ~ Set of N sentences -Tokenization = Special characters removal 2. Sentence & Query Representation ~ Query Q = Query Embedding @ - Sentences Embeddings 5, Summary Generation 3.1. Sentence Scoring 1. Compute BM25 Score (Eq.1) 2. Compute Semantic Similarity Score (Eq.2) 3. Combine Linearly BM25 and Semantic ‘Similarity Scores to get Sentence Query Relevance Score (Eq.3) | 3.2. Sentence Selection ~ Select Top-k ranked sentences ~ Apply Greedy Search ~ Apply Maximal Marginal Relevance to Algorithm | 4, Sentence Re-ranking re-rank Top-k sentences to ensure relevance and novelty",Text summarization,"The study developed an automatic abstractive text summarization algorithm in Japanese using a neural network with a BERT encoder and Transformer-based decoder. However, issues were encountered, such as repeated contents in the summary sentence and the model's inability to handle unknown words. The study suggests improvements such as utilizing coverage and copy mechanisms to solve these problems. Future experiments will explore these recommendations and compare results.",Natural Language Processing,"D: Cluster of docs oe 1. Preprocessing ~ Tokenization 1. Preprocessing J Sentence Splitting ~ Set of N sentences -Tokenization = Special characters removal 2. Sentence & Query Representation ~ Query Q = Query Embedding @ - Sentences Embeddings 5, Summary Generation 3.1. Sentence Scoring 1. Compute BM25 Score (Eq.1) 2. Compute Semantic Similarity Score (Eq.2) 3. Combine Linearly BM25 and Semantic ‘Similarity Scores to get Sentence Query Relevance Score (Eq.3) | 3.2. Sentence Selection ~ Select Top-k ranked sentences ~ Apply Greedy Search ~ Apply Maximal Marginal Relevance to Algorithm | 4, Sentence Re-ranking re-rank Top-k sentences to ensure relevance and novelty",Deep Learning and Machine Learning
122,"METHOD OF TEXT SUMMARIZATION USING LSA AND SENTENCE 
BASED TOPIC MODELLING WITH BERT","Text summarization, Extractive text summarization, 
Natural language processing, cosine similarity, TFIDF 
Vectorizer , BERT, Truncated SVD "," Document summarization is one such task of the 
natural language processing which deals with the long textual 
data to make its concise and fluent summaries that contains all 
of document relevant information. The Branch of NLP that 
deals with it, is automatic text summarizer. Automatic text 
summarizer does the task of converting the long textual 
document into short fluent summaries. There are generally two 
ways of summarizing text using automatic text summarizer, 
first is using extractive text summarizer and another 
abstractive text summarizer. This paper has demonstrated an 
experiment in contrast with the extractive text summarizer for 
summarizing the text. On the other hand topic modelling is a 
NLP task that extracts the relevant topic from the textual 
document. One such method is Latent semantic Analysis (LSA) 
using truncated SVD which extracts all the relevant topics 
from the text. This paper has demonstrated the experiment in 
which the proposed research work will be summarizing the 
long textual document using LSA topic modelling along with 
TFIDF keyword extractor for each sentence in a text document 
and also using BERT encoder model for encoding the sentences 
from textual document in order to retrieve the positional 
embedding of topics word vectors. The algorithm proposed 
algorithm in this paper is able to achieve the score greater than 
that of text summarization using Latent Dirichlet Allocation 
(LDA) topic modelling. ","This research explains the use of sentences keyword 
extractor and LSA topic modelling along with BERT on a 
text document results in extracting useful sentences from a 
text document that contains useful amount of information 
about the topic on which text document is based on. Recall 
score depicted that regardless of number of sentences of text 
document the relevant amount of information can still be 
extracted using proposed algorithm. The algorithm proposed 
were able to capture the semantic meaning of the topic word 
vectors in contrast with their semantic meaning to extract 
the sentences containing relevant number of sentences 
regarding those topics. LSA topic modelling using 
Truncated SVD when uses along TFIDF keyword extractor 
results in generating more accuracy for text summarization 
then LSA topic modelling alone. This tells us that each 
sentence contributes more to towards capturing semantic 
aspects of text than whole text alone. The proposed algorithm results in achieving higher scores in 
different phases of experiment than that of when used LDA 
topic modelling for text summarization alone. This depicts 
that rather than creating clusters from topics received to 
extract the sentences containing semantic and syntactic 
aspects of text, we could generate the more accurate 
summary by comparing semantic aspect of each sentence 
with that of the topics from LSA model to generate the 
fluent and coherent summary. The future scope of this 
model is generating summary in more accurately, furthermore using proposed algorithm in abstractive text 
summarizer where machine is generating summary in its 
own language will might results in achieving greater 
accuracy. ","METHOD OF TEXT SUMMARIZATION USING LSA AND SENTENCE 
BASED TOPIC MODELLING WITH BERTText summarization, Extractive text summarization, 
Natural language processing, cosine similarity, TFIDF 
Vectorizer , BERT, Truncated SVD  Document summarization is one such task of the 
natural language processing which deals with the long textual 
data to make its concise and fluent summaries that contains all 
of document relevant information. The Branch of NLP that 
deals with it, is automatic text summarizer. Automatic text 
summarizer does the task of converting the long textual 
document into short fluent summaries. There are generally two 
ways of summarizing text using automatic text summarizer, 
first is using extractive text summarizer and another 
abstractive text summarizer. This paper has demonstrated an 
experiment in contrast with the extractive text summarizer for 
summarizing the text. On the other hand topic modelling is a 
NLP task that extracts the relevant topic from the textual 
document. One such method is Latent semantic Analysis (LSA) 
using truncated SVD which extracts all the relevant topics 
from the text. This paper has demonstrated the experiment in 
which the proposed research work will be summarizing the 
long textual document using LSA topic modelling along with 
TFIDF keyword extractor for each sentence in a text document 
and also using BERT encoder model for encoding the sentences 
from textual document in order to retrieve the positional 
embedding of topics word vectors. The algorithm proposed 
algorithm in this paper is able to achieve the score greater than 
that of text summarization using Latent Dirichlet Allocation 
(LDA) topic modelling. This research explains the use of sentences keyword 
extractor and LSA topic modelling along with BERT on a 
text document results in extracting useful sentences from a 
text document that contains useful amount of information 
about the topic on which text document is based on. Recall 
score depicted that regardless of number of sentences of text 
document the relevant amount of information can still be 
extracted using proposed algorithm. The algorithm proposed 
were able to capture the semantic meaning of the topic word 
vectors in contrast with their semantic meaning to extract 
the sentences containing relevant number of sentences 
regarding those topics. LSA topic modelling using 
Truncated SVD when uses along TFIDF keyword extractor 
results in generating more accuracy for text summarization 
then LSA topic modelling alone. This tells us that each 
sentence contributes more to towards capturing semantic 
aspects of text than whole text alone. The proposed algorithm results in achieving higher scores in 
different phases of experiment than that of when used LDA 
topic modelling for text summarization alone. This depicts 
that rather than creating clusters from topics received to 
extract the sentences containing semantic and syntactic 
aspects of text, we could generate the more accurate 
summary by comparing semantic aspect of each sentence 
with that of the topics from LSA model to generate the 
fluent and coherent summary. The future scope of this 
model is generating summary in more accurately, furthermore using proposed algorithm in abstractive text 
summarizer where machine is generating summary in its 
own language will might results in achieving greater 
accuracy. BERT ‘Output Text ‘The movie “Smoke” the move “When a woman sleeps” that wes counted at the words tee lest nestle tts leased was elvays wating my performance Transformer decoder xN Input Text Dito Wayne Wang rom Hong Kong who has won the Sver Bea Ando te Barth ntemations ‘Fim Festal one of the worse largest tn festvals inthe fim Sine"", and apa swat famoue Seat Yeah Amovieed""When a Woman lens"" tHie'on he 1 month day) Storing out Ns on vin sce {UNK] “lao end Bones Takeshi who wa Tit orn st time yar Sl Figure 1. Overview of our text summarization model",Text summarization,"This paper discusses the use of automatic text summarization and topic modeling in natural language processing. The authors propose an algorithm that combines TFIDF keyword extraction, LSA topic modeling using truncated SVD, and BERT encoder model to extract useful sentences from a text document. The proposed algorithm is shown to achieve higher accuracy in summarization than using LDA topic modeling alone. The authors suggest that the proposed algorithm could be used in abstractive text summarization for even greater accuracy. Overall, the paper highlights the potential for combining different techniques in natural language processing to improve text summarization.",Natural Language Processing,"BERT ‘Output Text ‘The movie “Smoke” the move “When a woman sleeps” that wes counted at the words tee lest nestle tts leased was elvays wating my performance Transformer decoder xN Input Text Dito Wayne Wang rom Hong Kong who has won the Sver Bea Ando te Barth ntemations ‘Fim Festal one of the worse largest tn festvals inthe fim Sine"", and apa swat famoue Seat Yeah Amovieed""When a Woman lens"" tHie'on he 1 month day) Storing out Ns on vin sce {UNK] “lao end Bones Takeshi who wa Tit orn st time yar Sl Figure 1. Overview of our text summarization model",Deep Learning and Machine Learning
123,"Extractive Text Summarization using Dynamic 
Clustering and Co-Reference on BERT","Extractive Summarization, Abstractive 
Summarization, BERT, K-Means, Reference Resolution ","The process of picking sentences directly from the 
story to form the summary is extractive summarization. This 
process is aided by scoring functions and clustering algorithms 
to help choose the most suitable sentences. We use the existing 
BERT model which stands for Bidirectional Encoder 
Representations from Transformers, to produce extractive 
summarization by clustering the embeddings of sentences by Kmeans clustering, but introduce a dynamic method to decide the 
suitable number of sentences to pick from clusters. 
On top of that, the study is aimed at producing summaries 
of higher quality by incorporating reference resolution and 
dynamically producing summaries of suitable sizes depending 
on the text. This study aims to provide students with a 
summarizing service to help understand the content of lecture 
videos of long duration which would be vital in the process of 
revision. ","The main disadvantage of the existing model was that the 
entire context of the document to be summarized could not 
be represented in a smaller number of sentences. This could 
work in the case of abstractive summarization as the context 
of multiple sentences could be combined. However, in the 
case of extractive summarization, a greater number of 
sentences are required to represent a large document. We 
aimed to create summaries for large documents, and we have 
overcome the disadvantages of a short summary by 
improving the size of the summary depending on the size of 
the story. 
The CNN/DailyMail dataset had gold standard summaries of 
constant length irrespective of the size of the document to be 
summarized. Hence the linear regression model that was 
trained between summary length and WCSS/BCSS was not 
accurate. A dataset with summaries that were more 
representative of the document size would have fared better. 
In the proposed model, the base version of the pre-trained 
BERT model has been used. In future models, variations of 
BERT should be compared and tested; the BERT model can 
be finetuned and trained from scratch for obtaining results 
with more precision, which would again require larger 
datasets and time for training. 
Our work mainly revolved around CNN/Dailymail dataset 
and results were local to this dataset. In the future, the 
proposed model can be made to run on other standard datasets 
like document understanding conferences (DUC) and The 
New York times (NYT) news. One of the issues that we faced 
with CNN-Dailymail Dataset was that most of the 
reference/gold summary contained only 3-4 sentences which 
cannot be taken as a standard for predicting the number of 
sentences in the generated summary dynamically. To solve 
this, the proposed model must be run on various datasets 
available having a reference summary of varying lengths. Our 
main goal was to help students who have to go through pages of lectures by providing them with a reliable summary that 
captures the whole context and at the same time reducing the 
size of the document to the optimal length. Thus, the future 
work is to deploy the model on lectures from various massive 
open online courses (MOOC) platforms like Coursera and 
Udemy. 
To make this user friendly, a website can be developed to take 
the document as input from the user which then provides the 
user with the required summary. Also, the user may be 
prompted to give feedback on the generated summary which 
can be used in further improving the model. ","Extractive Text Summarization using Dynamic 
Clustering and Co-Reference on BERTExtractive Summarization, Abstractive 
Summarization, BERT, K-Means, Reference Resolution The process of picking sentences directly from the 
story to form the summary is extractive summarization. This 
process is aided by scoring functions and clustering algorithms 
to help choose the most suitable sentences. We use the existing 
BERT model which stands for Bidirectional Encoder 
Representations from Transformers, to produce extractive 
summarization by clustering the embeddings of sentences by Kmeans clustering, but introduce a dynamic method to decide the 
suitable number of sentences to pick from clusters. 
On top of that, the study is aimed at producing summaries 
of higher quality by incorporating reference resolution and 
dynamically producing summaries of suitable sizes depending 
on the text. This study aims to provide students with a 
summarizing service to help understand the content of lecture 
videos of long duration which would be vital in the process of 
revision. The main disadvantage of the existing model was that the 
entire context of the document to be summarized could not 
be represented in a smaller number of sentences. This could 
work in the case of abstractive summarization as the context 
of multiple sentences could be combined. However, in the 
case of extractive summarization, a greater number of 
sentences are required to represent a large document. We 
aimed to create summaries for large documents, and we have 
overcome the disadvantages of a short summary by 
improving the size of the summary depending on the size of 
the story. 
The CNN/DailyMail dataset had gold standard summaries of 
constant length irrespective of the size of the document to be 
summarized. Hence the linear regression model that was 
trained between summary length and WCSS/BCSS was not 
accurate. A dataset with summaries that were more 
representative of the document size would have fared better. 
In the proposed model, the base version of the pre-trained 
BERT model has been used. In future models, variations of 
BERT should be compared and tested; the BERT model can 
be finetuned and trained from scratch for obtaining results 
with more precision, which would again require larger 
datasets and time for training. 
Our work mainly revolved around CNN/Dailymail dataset 
and results were local to this dataset. In the future, the 
proposed model can be made to run on other standard datasets 
like document understanding conferences (DUC) and The 
New York times (NYT) news. One of the issues that we faced 
with CNN-Dailymail Dataset was that most of the 
reference/gold summary contained only 3-4 sentences which 
cannot be taken as a standard for predicting the number of 
sentences in the generated summary dynamically. To solve 
this, the proposed model must be run on various datasets 
available having a reference summary of varying lengths. Our 
main goal was to help students who have to go through pages of lectures by providing them with a reliable summary that 
captures the whole context and at the same time reducing the 
size of the document to the optimal length. Thus, the future 
work is to deploy the model on lectures from various massive 
open online courses (MOOC) platforms like Coursera and 
Udemy. 
To make this user friendly, a website can be developed to take 
the document as input from the user which then provides the 
user with the required summary. Also, the user may be 
prompted to give feedback on the generated summary which 
can be used in further improving the model. LSA Topic Modelling & Sentences TFIDF Keywords Preprocessing extractor BERT Encoder for sentences Embedding X= mean(PE(LSA Topics) Y() = Mean(PE(TFI keyword(sentence(i)) Fig 1. Flow of proposed algorithm Score(i) = Cosine similarity( & YO) sentences = MAX(score(i))",Text summarization,"The study describes an approach to extractive summarization using BERT model and clustering algorithms to generate summaries of suitable size depending on the text. The main goal is to provide students with a reliable summary of long lecture videos for revision. However, the existing model has the disadvantage of not representing the entire context of the document in a smaller number of sentences. The proposed model overcomes this by dynamically producing summaries of suitable sizes depending on the size of the story. The study mainly focuses on CNN/Dailymail dataset, but the proposed model can be made to run on other standard datasets like DUC and NYT news. Future work includes deploying the model on lectures from various MOOC platforms and developing a website to take the document as input and provide the user with the required summary.",Natural Language Processing,LSA Topic Modelling & Sentences TFIDF Keywords Preprocessing extractor BERT Encoder for sentences Embedding X= mean(PE(LSA Topics) Y() = Mean(PE(TFI keyword(sentence(i)) Fig 1. Flow of proposed algorithm Score(i) = Cosine similarity( & YO) sentences = MAX(score(i)),Deep Learning and Machine Learning
124,Sequence Level Contrastive Learning for Text Summarization,"    Contrastive learning model
    Text summarization
    Supervised abstractive summarization
    Sequence-to-sequence text generation model
    Faithfulness ratings","Contrastive learning models have achieved great success in
unsupervised visual representation learning, which maximize
the similarities between feature representations of different
views of the same image, while minimize the similarities between feature representations of views of different images.
In text summarization, the output summary is a shorter form
of the input document and they have similar meanings. In
this paper, we propose a contrastive learning model for supervised abstractive text summarization, where we view a
document, its gold summary and its model generated summaries as different views of the same mean representation
and maximize the similarities between them during training. We improve over a strong sequence-to-sequence text
generation model (i.e., BART) on three different summarization datasets. Human evaluation also shows that our model
achieves better faithfulness ratings compared to its counterpart without contrastive objectives.

","In text summarization, a document, its gold summary and
model generated summaries can be viewed as different views
of the same meaning representation. We propose SeqCo, a sequence level contrastive learning model for text summarization, which intends to minimize distances between the
document, its summary and its generated summaries during training. Experiments on three summarization datasets
(CNNDM, NYT and XSum) show that SeqCo consistantly
improves a strong Seq2Seq text generation model. In the
future, we plan to extend SeqCo in the multi-lingual or crosslingual text generation tasks. We observed in experiments
that using multiple contrastive objectives did not improve the
results. We are interested in developing methods for regularizing different contrastive objectives.","Sequence Level Contrastive Learning for Text Summarization    Contrastive learning model
    Text summarization
    Supervised abstractive summarization
    Sequence-to-sequence text generation model
    Faithfulness ratingsContrastive learning models have achieved great success in
unsupervised visual representation learning, which maximize
the similarities between feature representations of different
views of the same image, while minimize the similarities between feature representations of views of different images.
In text summarization, the output summary is a shorter form
of the input document and they have similar meanings. In
this paper, we propose a contrastive learning model for supervised abstractive text summarization, where we view a
document, its gold summary and its model generated summaries as different views of the same mean representation
and maximize the similarities between them during training. We improve over a strong sequence-to-sequence text
generation model (i.e., BART) on three different summarization datasets. Human evaluation also shows that our model
achieves better faithfulness ratings compared to its counterpart without contrastive objectives.

In text summarization, a document, its gold summary and
model generated summaries can be viewed as different views
of the same meaning representation. We propose SeqCo, a sequence level contrastive learning model for text summarization, which intends to minimize distances between the
document, its summary and its generated summaries during training. Experiments on three summarization datasets
(CNNDM, NYT and XSum) show that SeqCo consistantly
improves a strong Seq2Seq text generation model. In the
future, we plan to extend SeqCo in the multi-lingual or crosslingual text generation tasks. We observed in experiments
that using multiple contrastive objectives did not improve the
results. We are interested in developing methods for regularizing different contrastive objectives.el Gp Ga Gp Classification n Laver Fully-connected layer + ca Norm 3 2 Transformer encoder oe Tp T th ++ wi wi H",Text summarization,"The paper proposes a contrastive learning model called SeqCo for supervised abstractive text summarization. The model maximizes similarities between a document, its gold summary, and model-generated summaries during training, leading to improved performance on three summarization datasets. The authors plan to extend SeqCo to multilingual or cross-lingual text generation tasks and develop methods for regularizing different contrastive objectives. The paper also discusses experiments that found using multiple contrastive objectives did not improve results.",Natural Language Processing,el Gp Ga Gp Classification n Laver Fully-connected layer + ca Norm 3 2 Transformer encoder oe Tp T th ++ wi wi H,Deep Learning and Machine Learning
125,Abstractive Text Summarization Using BART," Text Summarization, BERT, BART, Roberta, 
Attention Mechanism, T5","In the last recent years, there's a huge amount of 
data available on the internet, and is generated very rapidly. It 
is very difficult for human beings to analyze and extract useful 
information from huge data especially when the text is large in 
size and longer documents which increases the time to process 
and analyze the data, further it also increases the time taken to 
summarize it. To address this issue automatic text 
summarization is used. Text summarization is defined as 
creating a short, accurate, and fluent summary of a longer 
document. It summarizes the larger text without any human 
intervention. This paper will provide a mechanism where it does 
the text summarization quickly and effectively even for large 
data. We can use this model to summarize and extract important 
information from a large document or text based on our input. 
Here a concept of the Deep Learning model is used for text 
summarization which is called BART (Bidirectional and AutoRegressive Transformer). it consists of both an encoder and 
decoder. The encoder and decoder are merged to form the 
BART algorithm. BERT is trained with a huge amount of 
unlabeled data to achieve the state of art results. The use of an 
attention mechanism in each layer of BERT makes the model 
much more popular as it highlights the important features of the 
input data. BERT performs the masked language modeling with 
the help of its several bidirectional transformer layers and 
predicts the missing values. On the other hand, decoder is used 
to predict the next token in a sentence. Merging both the 
encoder and decoder will form the BART model. Here we will 
compare the BART model with BERT, T5, and Roberta.","Due to the exponential growth of web textual data due 
to the Internet's quick expansion, tasks like paper records, text 
classification, and information retrieval have been
significantly hampered. Summarizing text automatically is a 
very essential way to address this issue. The main function of 
ATS is to automatically create a brief and understandable 
summary by extracting the main ideas from the source text. 
Recently, deep learning-based abstractive summarization 
models have been created in an effort to better balance and 
enhance these two characteristics. This study will present a 
system that can quickly and accurately summarize text, even 
for massive amounts of data. Using our input, utilize this 
method to summarize and extract crucial information from a 
lengthy document or text. Furthermore, a Deep Learning 
model approach known as BART is utilized for text 
summarization. The BART algorithm is created by 
combining the encoder and decoder. Additionally, the 
evaluation of the BART model using BERT, T5, and Roberta 
is shown in the result analysis. In future, this research can be 
extended by analyzing various learning models to improve 
the effectiveness. ","Abstractive Text Summarization Using BART Text Summarization, BERT, BART, Roberta, 
Attention Mechanism, T5In the last recent years, there's a huge amount of 
data available on the internet, and is generated very rapidly. It 
is very difficult for human beings to analyze and extract useful 
information from huge data especially when the text is large in 
size and longer documents which increases the time to process 
and analyze the data, further it also increases the time taken to 
summarize it. To address this issue automatic text 
summarization is used. Text summarization is defined as 
creating a short, accurate, and fluent summary of a longer 
document. It summarizes the larger text without any human 
intervention. This paper will provide a mechanism where it does 
the text summarization quickly and effectively even for large 
data. We can use this model to summarize and extract important 
information from a large document or text based on our input. 
Here a concept of the Deep Learning model is used for text 
summarization which is called BART (Bidirectional and AutoRegressive Transformer). it consists of both an encoder and 
decoder. The encoder and decoder are merged to form the 
BART algorithm. BERT is trained with a huge amount of 
unlabeled data to achieve the state of art results. The use of an 
attention mechanism in each layer of BERT makes the model 
much more popular as it highlights the important features of the 
input data. BERT performs the masked language modeling with 
the help of its several bidirectional transformer layers and 
predicts the missing values. On the other hand, decoder is used 
to predict the next token in a sentence. Merging both the 
encoder and decoder will form the BART model. Here we will 
compare the BART model with BERT, T5, and Roberta.Due to the exponential growth of web textual data due 
to the Internet's quick expansion, tasks like paper records, text 
classification, and information retrieval have been
significantly hampered. Summarizing text automatically is a 
very essential way to address this issue. The main function of 
ATS is to automatically create a brief and understandable 
summary by extracting the main ideas from the source text. 
Recently, deep learning-based abstractive summarization 
models have been created in an effort to better balance and 
enhance these two characteristics. This study will present a 
system that can quickly and accurately summarize text, even 
for massive amounts of data. Using our input, utilize this 
method to summarize and extract crucial information from a 
lengthy document or text. Furthermore, a Deep Learning 
model approach known as BART is utilized for text 
summarization. The BART algorithm is created by 
combining the encoder and decoder. Additionally, the 
evaluation of the BART model using BERT, T5, and Roberta 
is shown in the result analysis. In future, this research can be 
extended by analyzing various learning models to improve 
the effectiveness. ",Text summarization,"The exponential growth of web textual data has led to difficulty in analyzing and extracting useful information. Automatic text summarization is used to address this issue. This paper proposes a mechanism using the BART model for text summarization, which consists of an encoder and decoder. BERT, T5, and Roberta are compared with BART. The proposed system can quickly and accurately summarize text even for massive amounts of data. Deep learning-based abstractive summarization models have been created to better balance and enhance the summarization process. Future research can analyze various learning models to improve effectiveness.",Natural Language Processing,,Deep Learning and Machine Learning
126,Timeline Summarization  from Social Media with Life Cycle Models,Timeline Episode Detecting   ,"The popularity of social media shatters the barrier
for online users to create and share information at
any place at any time. As a consequence, it has become increasing difficult to locate relevance information about an entity. Timeline has been proven
to provide an effective and efficient access to understand an entity by displaying a list of episodes about the entity in chronological order. However,
summarizing the timeline about an entity with social media data faces new challenges. First, key
timeline episodes about the entity are typically unavailable in existing social media services. Second, the short, noisy and informal nature of social media posts determines that only content-based summarization could be insufficient. In this paper, we investigate the problem of timeline summarization and propose a novel framework Timeline-Sumy, which consists of episode detecting and summary ranking. In episode detecting, we explicitly model   temporal information with life cycle models to detect timeline episodes since episodes usually exhibit sudden-rise-and-heavy-tail patterns on timeseries. In summary ranking, we rank social media posts in each episode via a learning-to-rank approach. The experimental results on social media datasets demonstrate the effectiveness of the proposed framework.","In this paper, we study the problem of timeline summarization, which is to generate summary for a chain of episodes in a timeline about an entity from social media data. We introduce life cycle models to capture unique temporal patterns include life-cycle patterns and sudden-spike-and-heavy-tail patterns for timeline episodes, and propose a novel timeline summarization framework Timeline-Sumy with an episode detecting phase and a summary ranking phase. In the episode detecting phase, we propose a novel Bayesian nonparametric model which captures regular content, hashtag content and temporal information simultaneously. Gibbs sampling is employed to infer the model parameters, and a fast burn-in strategy based on temporal bursts is further introduced to speed up the model inference. In the summary ranking phase, we introduce a learning-to-rank based approach which is flexible to integrate various types of signals from social media data for timeline summarization. The experimental results demonstrate the effectiveness of Timeline-Sumy. In our current work we only consider each individual entity, and we will extend TimelineSumy for multiple correlated entities in our future work.","Timeline Summarization  from Social Media with Life Cycle ModelsTimeline Episode Detecting   The popularity of social media shatters the barrier
for online users to create and share information at
any place at any time. As a consequence, it has become increasing difficult to locate relevance information about an entity. Timeline has been proven
to provide an effective and efficient access to understand an entity by displaying a list of episodes about the entity in chronological order. However,
summarizing the timeline about an entity with social media data faces new challenges. First, key
timeline episodes about the entity are typically unavailable in existing social media services. Second, the short, noisy and informal nature of social media posts determines that only content-based summarization could be insufficient. In this paper, we investigate the problem of timeline summarization and propose a novel framework Timeline-Sumy, which consists of episode detecting and summary ranking. In episode detecting, we explicitly model   temporal information with life cycle models to detect timeline episodes since episodes usually exhibit sudden-rise-and-heavy-tail patterns on timeseries. In summary ranking, we rank social media posts in each episode via a learning-to-rank approach. The experimental results on social media datasets demonstrate the effectiveness of the proposed framework.In this paper, we study the problem of timeline summarization, which is to generate summary for a chain of episodes in a timeline about an entity from social media data. We introduce life cycle models to capture unique temporal patterns include life-cycle patterns and sudden-spike-and-heavy-tail patterns for timeline episodes, and propose a novel timeline summarization framework Timeline-Sumy with an episode detecting phase and a summary ranking phase. In the episode detecting phase, we propose a novel Bayesian nonparametric model which captures regular content, hashtag content and temporal information simultaneously. Gibbs sampling is employed to infer the model parameters, and a fast burn-in strategy based on temporal bursts is further introduced to speed up the model inference. In the summary ranking phase, we introduce a learning-to-rank based approach which is flexible to integrate various types of signals from social media data for timeline summarization. The experimental results demonstrate the effectiveness of Timeline-Sumy. In our current work we only consider each individual entity, and we will extend TimelineSumy for multiple correlated entities in our future work.",Text summarization,"The paper proposes a framework called Timeline-Sumy for summarizing social media data about an entity in chronological order. The framework consists of two phases: episode detecting and summary ranking. In the episode detecting phase, the framework uses life cycle models to detect timeline episodes, which exhibit sudden-rise-and-heavy-tail patterns on time series. The proposed Bayesian nonparametric model captures regular content, hashtag content, and temporal information simultaneously, and Gibbs sampling is employed to infer the model parameters. A fast burn-in strategy based on temporal bursts is further introduced to speed up the model inference. In the summary ranking phase, the framework uses a learning-to-rank approach to rank social media posts in each episode. The approach is flexible to integrate various types of signals from social media data for timeline summarization.",Natural Language Processing,,Deep Learning and Machine Learning
127,A survey on extractive text summarization,"Extractive text summarization,sentence Fusion,supervised and unsupervised learning methods","Text Summarization is the process of obtaining salient information from an authentic text document.                                                                                   In this technique, the extracted information is achieved as a summarized report and conferred as a concise summary to the user.                                                    It is very crucial for humans to understand and to describe the content of the text.                                                                                                                                           Text Summarization techniques are classified into abstractive and extractive summarization.                                                                                                     The extractive summarization technique focuses on choosing how paragraphs, important sentences, etc produces                                                                          the original documents in precise form. The implication of sentences is determined based on linguistic and statistical features.                                                                                                         In this work, a comprehensive review of extractive text summarization process methods has been ascertained.                                                                                In this paper, the various techniques, populous benchmarking datasets and challenges of extractive summarization have been reviewed.                                           This paper interprets extractive text summarization methods with a less redundant summary, highly adhesive,                                                                                    coherent and depth information.","Extractive summarization process is highly coherent, less redundant and cohesive (summary and information rich). The aim is to give a comprehensive review and comparison of distinctive approaches and techniques of extractive text summarization process. Although research on summarization started way long back, there is still a long way to go. Over the time, focused has drifted from summarizing scientific articles to advertisements, blogs, electronic mail messages and news articles. Simple eradication of sentences has composed satisfactory results in massive applications. Some trends in automatic evaluation of summary system have been focused. However, the work has not focused the different challenges of extractive text summarization process to its full intensity in premises of time and space complication.","A survey on extractive text summarizationExtractive text summarization,sentence Fusion,supervised and unsupervised learning methodsText Summarization is the process of obtaining salient information from an authentic text document.                                                                                   In this technique, the extracted information is achieved as a summarized report and conferred as a concise summary to the user.                                                    It is very crucial for humans to understand and to describe the content of the text.                                                                                                                                           Text Summarization techniques are classified into abstractive and extractive summarization.                                                                                                     The extractive summarization technique focuses on choosing how paragraphs, important sentences, etc produces                                                                          the original documents in precise form. The implication of sentences is determined based on linguistic and statistical features.                                                                                                         In this work, a comprehensive review of extractive text summarization process methods has been ascertained.                                                                                In this paper, the various techniques, populous benchmarking datasets and challenges of extractive summarization have been reviewed.                                           This paper interprets extractive text summarization methods with a less redundant summary, highly adhesive,                                                                                    coherent and depth information.Extractive summarization process is highly coherent, less redundant and cohesive (summary and information rich). The aim is to give a comprehensive review and comparison of distinctive approaches and techniques of extractive text summarization process. Although research on summarization started way long back, there is still a long way to go. Over the time, focused has drifted from summarizing scientific articles to advertisements, blogs, electronic mail messages and news articles. Simple eradication of sentences has composed satisfactory results in massive applications. Some trends in automatic evaluation of summary system have been focused. However, the work has not focused the different challenges of extractive text summarization process to its full intensity in premises of time and space complication.",Text summarization,"This paper provides an overview of extractive text summarization, which is the process of obtaining salient information from a text document and presenting it in a concise summary. Extractive summarization focuses on choosing important sentences and paragraphs from the original document based on linguistic and statistical features. The paper reviews various techniques, benchmarking datasets, and challenges associated with extractive summarization. The aim is to provide a less redundant, highly adhesive, and coherent summary with depth information. Although research on summarization has come a long way, there is still much to be done in terms of addressing the challenges of extractive text summarization.",Natural Language Processing,,Deep Learning and Machine Learning
128,"FGGAN: Feature-Guiding Generative Adversarial
Networks for Text Generation
","Generative adversarial networks, text generation, deep learning, reinforcement learning"," Text generation is a basic work of natural language processing, which plays an important role
in dialogue system and intelligent translation. As a kind of deep learning framework, Generative Adversarial
Networks (GAN) has been widely used in text generation. In combination with reinforcement learning, GAN
uses the output of discriminator as reward signal of reinforcement learning to guide generator training, but
the reward signal is a scalar and the guidance is weak. This paper proposes a text generation model named
Feature-Guiding Generative Adversarial Networks (FGGAN). To solve the problem of insufficient feedback
guidance from the discriminator network, FGGAN uses a feature guidance module to extract text features
from the discriminator network, convert them into feature guidance vectors and feed them into the generator
network for guidance. In addition, sampling is required to complete the sequence before feeding it into the
discriminator to get feedback signal in text generation. However, the randomness and insufficiency of the
sampling method lead to poor quality of generated text. This paper formulates text semantic rules to restrict
the token of the next time step in the sequence generation process and remove semantically unreasonable
tokens to improve the quality of generated text. Finally, text generation experiments are performed on
different datasets and the results verify the effectiveness and superiority of FGGAN","For text sequence generation, this paper proposes an
improved framework FGGAN. In order to solve the problem
that the feedback signal of the discriminator is not very
instructive, this paper proposes a feature guidance module
which obtains the text semantic feature feedback to the generator with more guidance. In addition, this paper proposes
a method to create a vocabulary mask based on semantic
rules which restricts the tokens during the generation to make
the sequence more realistic. The superiority of the improved
module is evaluated experimentally. In the synthetic experiment, the negative log-likelihood is used for the evaluation.
FGGAN proposed in this paper has higher ability of fitting data distribution. In the experiment on real data, BLEU
is used for the evaluation. Compared with other models,
FGGAN has a higher evaluation score and generates more
realistic text data.","FGGAN: Feature-Guiding Generative Adversarial
Networks for Text Generation
Generative adversarial networks, text generation, deep learning, reinforcement learning Text generation is a basic work of natural language processing, which plays an important role
in dialogue system and intelligent translation. As a kind of deep learning framework, Generative Adversarial
Networks (GAN) has been widely used in text generation. In combination with reinforcement learning, GAN
uses the output of discriminator as reward signal of reinforcement learning to guide generator training, but
the reward signal is a scalar and the guidance is weak. This paper proposes a text generation model named
Feature-Guiding Generative Adversarial Networks (FGGAN). To solve the problem of insufficient feedback
guidance from the discriminator network, FGGAN uses a feature guidance module to extract text features
from the discriminator network, convert them into feature guidance vectors and feed them into the generator
network for guidance. In addition, sampling is required to complete the sequence before feeding it into the
discriminator to get feedback signal in text generation. However, the randomness and insufficiency of the
sampling method lead to poor quality of generated text. This paper formulates text semantic rules to restrict
the token of the next time step in the sequence generation process and remove semantically unreasonable
tokens to improve the quality of generated text. Finally, text generation experiments are performed on
different datasets and the results verify the effectiveness and superiority of FGGANFor text sequence generation, this paper proposes an
improved framework FGGAN. In order to solve the problem
that the feedback signal of the discriminator is not very
instructive, this paper proposes a feature guidance module
which obtains the text semantic feature feedback to the generator with more guidance. In addition, this paper proposes
a method to create a vocabulary mask based on semantic
rules which restricts the tokens during the generation to make
the sequence more realistic. The superiority of the improved
module is evaluated experimentally. In the synthetic experiment, the negative log-likelihood is used for the evaluation.
FGGAN proposed in this paper has higher ability of fitting data distribution. In the experiment on real data, BLEU
is used for the evaluation. Compared with other models,
FGGAN has a higher evaluation score and generates more
realistic text data.",Text generation,"This paper proposes a new model called Feature-Guiding Generative Adversarial Networks (FGGAN) to improve text generation. FGGAN uses a feature guidance module to extract text features from the discriminator network and convert them into feature guidance vectors to better guide the generator network. Additionally, the paper formulates text semantic rules to restrict the tokens during the sequence generation process, leading to more realistic text. Experimental results demonstrate the effectiveness and superiority of FGGAN over other models in terms of fitting data distribution and generating more realistic text data.",Natural Language Processing,,Deep Learning and Machine Learning
129,"Text Generation Based on Generative
Adversarial Nets with Latent Variable"," Generative Adversarial Net; Variational Autoencoder; VGAN;
Text Generation","In this paper, we propose a model using generative adversarial net (GAN) to generate realistic text. Instead of using standard
GAN, we combine variational autoencoder (VAE) with generative adversarial net. The use of high-level latent random variables is helpful to
learn the data distribution and solve the problem that generative adversarial net always emits the similar data. We propose the VGAN model
where the generative model is composed of recurrent neural network and
VAE. The discriminative model is a convolutional neural network. We
train the model via policy gradient. We apply the proposed model to the
task of text generation and compare it to other recent neural network
based models, such as recurrent neural network language model and SeqGAN. We evaluate the performance of the model by calculating negative
log-likelihood and the BLEU score. We conduct experiments on three
benchmark datasets, and results show that our model outperforms other
previous models.
","In this paper, we proposed the VGAN model to generate realistic text based on
classical GAN model. The generative model of VGAN combines generative adversarial nets with variational autoencoder and can be applied to the sequences
of discrete tokens. In the process of training, we employ policy gradient to effectively train the generative model. Our results show that VGAN outperforms two
strong baseline models for text generation and behaves well on three benchmark
datasets. In the future, we plan to use deep deterministic policy gradient [11] to
train the generator better. In the addition, we will choose other models as the
discriminator such as recurrent convolutional neural network [16] and recurrent
neural network","Text Generation Based on Generative
Adversarial Nets with Latent Variable Generative Adversarial Net; Variational Autoencoder; VGAN;
Text GenerationIn this paper, we propose a model using generative adversarial net (GAN) to generate realistic text. Instead of using standard
GAN, we combine variational autoencoder (VAE) with generative adversarial net. The use of high-level latent random variables is helpful to
learn the data distribution and solve the problem that generative adversarial net always emits the similar data. We propose the VGAN model
where the generative model is composed of recurrent neural network and
VAE. The discriminative model is a convolutional neural network. We
train the model via policy gradient. We apply the proposed model to the
task of text generation and compare it to other recent neural network
based models, such as recurrent neural network language model and SeqGAN. We evaluate the performance of the model by calculating negative
log-likelihood and the BLEU score. We conduct experiments on three
benchmark datasets, and results show that our model outperforms other
previous models.
In this paper, we proposed the VGAN model to generate realistic text based on
classical GAN model. The generative model of VGAN combines generative adversarial nets with variational autoencoder and can be applied to the sequences
of discrete tokens. In the process of training, we employ policy gradient to effectively train the generative model. Our results show that VGAN outperforms two
strong baseline models for text generation and behaves well on three benchmark
datasets. In the future, we plan to use deep deterministic policy gradient [11] to
train the generator better. In the addition, we will choose other models as the
discriminator such as recurrent convolutional neural network [16] and recurrent
neural networka man riding skis on the snow covered slope Text Vector ConvolutionGenerator Feature guiding vector Discriminator Logistics classify Generated data | | Real data Unfinished Sequence Feature guiding network",Text generation,"This paper proposes the VGAN model, which combines generative adversarial nets (GAN) with variational autoencoder (VAE) to generate realistic text. The generative model uses a recurrent neural network and VAE, while the discriminative model uses a convolutional neural network. The model is trained using policy gradient and evaluated using negative log-likelihood and BLEU score. The results show that VGAN outperforms previous models on three benchmark datasets. The authors suggest future work using deep deterministic policy gradient and different discriminative models.",Natural Language Processing,a man riding skis on the snow covered slope Text Vector ConvolutionGenerator Feature guiding vector Discriminator Logistics classify Generated data | | Real data Unfinished Sequence Feature guiding network,Deep Learning and Machine Learning
130,"Affect-LM: A Neural Language Model for Customizable Affective Text
Generation","    Affective messages
    Neural language models
    LSTM
    Affect-LM
    Conversational text generation","Human verbal communication includes
affective messages which are conveyed
through use of emotionally colored words.
There has been a lot of research in this
direction but the problem of integrating state-of-the-art neural language models with affective information remains
an area ripe for exploration. In this
paper, we propose an extension to an
LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM
enables us to customize the degree of
emotional content in generated sentences
through an additional design parameter.
Perception studies conducted using Amazon Mechanical Turk show that AffectLM generates naturally looking emotional
sentences without sacrificing grammatical
correctness. Affect-LM also learns affectdiscriminative word representations, and
perplexity experiments show that additional affective information in conversational text can improve language model
prediction.
","In this paper, we have introduced a novel language
model Affect-LM for generating affective conversational text conditioned on context words, an affective category and an affective strength parameter. MTurk perception studies show that the model
can generate expressive text at varying degrees of
emotional strength without affecting grammatical
correctness. We also evaluate Affect-LM as a language model and show that it achieves lower perplexity than a baseline LSTM model when the affect category is obtained from the words in the
context. For future work, we wish to extend this
model by investigating language generation conditioned on other modalities such as facial images
and speech, and to applications such as dialogue
generation for virtual agents.","Affect-LM: A Neural Language Model for Customizable Affective Text
Generation    Affective messages
    Neural language models
    LSTM
    Affect-LM
    Conversational text generationHuman verbal communication includes
affective messages which are conveyed
through use of emotionally colored words.
There has been a lot of research in this
direction but the problem of integrating state-of-the-art neural language models with affective information remains
an area ripe for exploration. In this
paper, we propose an extension to an
LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM
enables us to customize the degree of
emotional content in generated sentences
through an additional design parameter.
Perception studies conducted using Amazon Mechanical Turk show that AffectLM generates naturally looking emotional
sentences without sacrificing grammatical
correctness. Affect-LM also learns affectdiscriminative word representations, and
perplexity experiments show that additional affective information in conversational text can improve language model
prediction.
In this paper, we have introduced a novel language
model Affect-LM for generating affective conversational text conditioned on context words, an affective category and an affective strength parameter. MTurk perception studies show that the model
can generate expressive text at varying degrees of
emotional strength without affecting grammatical
correctness. We also evaluate Affect-LM as a language model and show that it achieves lower perplexity than a baseline LSTM model when the affect category is obtained from the words in the
context. For future work, we wish to extend this
model by investigating language generation conditioned on other modalities such as facial images
and speech, and to applications such as dialogue
generation for virtual agents.START Var Vr. policy gradient . rewardiconvolution pooling Training data Discriminator GeneratorLSTI fea",Text generation,"The paper proposes Affect-LM, an LSTM language model for generating conversational text conditioned on affect categories. The model allows for customization of emotional content through a design parameter and produces naturally looking emotional sentences without sacrificing grammatical correctness. Additionally, Affect-LM learns affect-discriminative word representations and improves language model prediction. The paper suggests future work exploring language generation conditioned on other modalities and applications such as dialogue generation for virtual agents.",Natural Language Processing,START Var Vr. policy gradient . rewardiconvolution pooling Training data Discriminator GeneratorLSTI fea,Deep Learning and Machine Learning
131,A Hybrid Convolutional Variational Autoencoder for Text Generation,"    Variational Autoencoder (VAE)
    Hybrid architecture
    Convolutional and deconvolutional components
    KL-term collapsing
    Text generation.","In this paper we explore the effect of architectural choices on learning a Variational
Autoencoder (VAE) for text generation.
In contrast to the previously introduced
VAE model for text where both the encoder and decoder are RNNs, we propose
a novel hybrid architecture that blends
fully feed-forward convolutional and deconvolutional components with a recurrent
language model. Our architecture exhibits
several attractive properties such as faster
run time and convergence, ability to better
handle long sequences and, more importantly, it helps to avoid some of the major
difficulties posed by training VAE models
on textual data","We have introduced a novel generative model of
natural texts based on the VAE framework. Its
core components are a convolutional encoder and
a deconvolutional decoder combined with a recurrent layer. We have shown that the feed-forward
part of our model architecture makes it easier to
train a VAE and avoid the problem of KL-term collapsing to zero, where the decoder falls back to a
standard language model thus inhibiting the sampling ability of VAE. Additionally, we propose a
more natural way to encourage the model to rely
on the latent vector by introducing an additional
cost term in the training objective. We observe that
it works well on long sequences which is hard to
achieve with purely RNN-based VAEs using the
previously proposed tricks such as KL-term annealing and input dropout. Finally, we have extensively evaluated the trade-off between the KLterm and the reconstruction loss. In particular, we
investigated the effect of the receptive field size
on the ability of the model to respect the latent
vector which is crucial for being able to generate
realistic and diverse samples. In future work we
plan to apply our VAE model to semi-supervised
NLP tasks and experiment with conditioning generation on various text attributes such as sentiment
and writing style.
","A Hybrid Convolutional Variational Autoencoder for Text Generation    Variational Autoencoder (VAE)
    Hybrid architecture
    Convolutional and deconvolutional components
    KL-term collapsing
    Text generation.In this paper we explore the effect of architectural choices on learning a Variational
Autoencoder (VAE) for text generation.
In contrast to the previously introduced
VAE model for text where both the encoder and decoder are RNNs, we propose
a novel hybrid architecture that blends
fully feed-forward convolutional and deconvolutional components with a recurrent
language model. Our architecture exhibits
several attractive properties such as faster
run time and convergence, ability to better
handle long sequences and, more importantly, it helps to avoid some of the major
difficulties posed by training VAE models
on textual dataWe have introduced a novel generative model of
natural texts based on the VAE framework. Its
core components are a convolutional encoder and
a deconvolutional decoder combined with a recurrent layer. We have shown that the feed-forward
part of our model architecture makes it easier to
train a VAE and avoid the problem of KL-term collapsing to zero, where the decoder falls back to a
standard language model thus inhibiting the sampling ability of VAE. Additionally, we propose a
more natural way to encourage the model to rely
on the latent vector by introducing an additional
cost term in the training objective. We observe that
it works well on long sequences which is hard to
achieve with purely RNN-based VAEs using the
previously proposed tricks such as KL-term annealing and input dropout. Finally, we have extensively evaluated the trade-off between the KLterm and the reconstruction loss. In particular, we
investigated the effect of the receptive field size
on the ability of the model to respect the latent
vector which is crucial for being able to generate
realistic and diverse samples. In future work we
plan to apply our VAE model to semi-supervised
NLP tasks and experiment with conditioning generation on various text attributes such as sentiment
and writing style.
Context Words¢, | 2@> “good about this. “lfeelso ..."". -—>| Affect-LM .. great about this.” “... awesome about this.” Automatic Inference ~@ptional) ~ ] NI A"" y Affect Category @ t—1 Affect Strength B",Text generation,"The paper proposes a new architecture for Variational Autoencoder (VAE) for text generation, which blends feed-forward convolutional and deconvolutional components with a recurrent language model. The new architecture exhibits several advantages, such as faster run time and convergence, better handling of long sequences, and avoidance of some of the major difficulties posed by training VAE models on textual data. The authors have also shown that the feed-forward part of the model architecture makes it easier to train a VAE, and they have introduced a new cost term to encourage the model to rely on the latent vector. The paper evaluates the trade-off between the KL-term and the reconstruction loss and plans to apply the VAE model to semi-supervised NLP tasks in the future.",Natural Language Processing,"Context Words¢, | 2@> “good about this. “lfeelso ..."". -—>| Affect-LM .. great about this.” “... awesome about this.” Automatic Inference ~@ptional) ~ ] NI A"" y Affect Category @ t—1 Affect Strength B",Deep Learning and Machine Learning
132,"Text Generation Service Model Based on
Truth-Guided SeqGAN"," Text generation, generative adversarial networks, self-attention mechanism, truth-guided."," The Generative Adversarial Networks (GAN) has been successfully applied to the generation
of text content such as poetry and speech, and it is a hot topic in the field of text generation. However, GAN
has been facing the problem of training and convergence. For the generation model, this paper redefines
on the loss function. The truth-guided method has been added to make the generated text closer to the real
data. For the discriminant model, this paper designs a more suitable network structure. The self-attention
mechanism has been added to the discrimination network to obtain richer semantic information. Finally,
some experiments under different model structures and different parameters indicates the model with truthguided and self-attention mechanism gets better results.
","In this paper, the text generation task is studied and analyzed
and SeqGAN is used as the base model. To accelerate the
convergence of the network and guide the generation model to
generate more similar to real text, TG-SeqGAN chose to use
truth-guided. At the same time, the network structure of the
discriminant model is improved, and the semantic information of the whole sentence is fused by the self-attention mechanism. SeqGAN based on true value guidance has improved
the convergence speed of the text generation model, and the
text quality generated by the network has also been improved
after stabilization, both on NLL-test loss and Embedding
Similarity","Text Generation Service Model Based on
Truth-Guided SeqGAN Text generation, generative adversarial networks, self-attention mechanism, truth-guided. The Generative Adversarial Networks (GAN) has been successfully applied to the generation
of text content such as poetry and speech, and it is a hot topic in the field of text generation. However, GAN
has been facing the problem of training and convergence. For the generation model, this paper redefines
on the loss function. The truth-guided method has been added to make the generated text closer to the real
data. For the discriminant model, this paper designs a more suitable network structure. The self-attention
mechanism has been added to the discrimination network to obtain richer semantic information. Finally,
some experiments under different model structures and different parameters indicates the model with truthguided and self-attention mechanism gets better results.
In this paper, the text generation task is studied and analyzed
and SeqGAN is used as the base model. To accelerate the
convergence of the network and guide the generation model to
generate more similar to real text, TG-SeqGAN chose to use
truth-guided. At the same time, the network structure of the
discriminant model is improved, and the semantic information of the whole sentence is fused by the self-attention mechanism. SeqGAN based on true value guidance has improved
the convergence speed of the text generation model, and the
text quality generated by the network has also been improved
after stabilization, both on NLL-test loss and Embedding
SimilarityWO wi wit wi2 wt we w7 we wo wid wit_wi2 wid wid wis (c) Hybrid model with ByteNet “(a) Fully feed forward component of our VAE model decoder",Text generation,"This paper focuses on improving the training and convergence of Generative Adversarial Networks (GAN) for text generation, by redefining the loss function and designing a more suitable network structure for the discriminant model. The proposed model, TG-SeqGAN, incorporates a truth-guided method and self-attention mechanism to generate text closer to real data and obtain richer semantic information. Experiments show that the proposed model achieves better results in terms of convergence speed and text quality.",Natural Language Processing,WO wi wit wi2 wt we w7 we wo wid wit_wi2 wid wid wis (c) Hybrid model with ByteNet “(a) Fully feed forward component of our VAE model decoder,Deep Learning and Machine Learning
133,MULTI-VIEW GAIT RECOGNITION USING 3D CONVOLUTIONAL NEURAL NETWORKS,"Deep Learning, Convolutional Neural
Networks, Gait Recognition","In this work we present a deep convolutional neural network
using 3D convolutions for Gait Recognition in multiple views
capturing spatio-temporal features. A special input format,
consisting of the gray-scale image and optical flow enhance
color invaranice. The approach is evaluated on three different datasets, including variances in clothing, walking speeds
and the view angle. In contrast to most state-of-the-art Gait
Recognition systems the used neural network is able to generalize gait features across multiple large view angle changes.
The results show a comparable to better performance in comparison with previous approaches, especially for large view
differences.","A new approach has been presented to tackle the challenges
in the field of Gait Recognition. View, clothing and walking
speed invariance make Gait Recognition a versatile and difficult task. A modern state-of-the-art technique using Convolutional Neural Networks is proposed, extracting spatiotemporal features for classification. This representation results in a high accuracy across experiments on different popular databases pointing out the high potential of CNNs for Gait
Recognition. Nevertheless, due to the small amount variance
and the small database size overall, a special data split is applied and overfitting is a concern. Besides using better hardware and larger network structures, possible improvements
can also be seen in the growing amount of data with larger
databases to come. Using databases including thousands of
subjects and a large variance in walking behavior and appearance can further boost performance and reduce overfitting.","MULTI-VIEW GAIT RECOGNITION USING 3D CONVOLUTIONAL NEURAL NETWORKSDeep Learning, Convolutional Neural
Networks, Gait RecognitionIn this work we present a deep convolutional neural network
using 3D convolutions for Gait Recognition in multiple views
capturing spatio-temporal features. A special input format,
consisting of the gray-scale image and optical flow enhance
color invaranice. The approach is evaluated on three different datasets, including variances in clothing, walking speeds
and the view angle. In contrast to most state-of-the-art Gait
Recognition systems the used neural network is able to generalize gait features across multiple large view angle changes.
The results show a comparable to better performance in comparison with previous approaches, especially for large view
differences.A new approach has been presented to tackle the challenges
in the field of Gait Recognition. View, clothing and walking
speed invariance make Gait Recognition a versatile and difficult task. A modern state-of-the-art technique using Convolutional Neural Networks is proposed, extracting spatiotemporal features for classification. This representation results in a high accuracy across experiments on different popular databases pointing out the high potential of CNNs for Gait
Recognition. Nevertheless, due to the small amount variance
and the small database size overall, a special data split is applied and overfitting is a concern. Besides using better hardware and larger network structures, possible improvements
can also be seen in the growing amount of data with larger
databases to come. Using databases including thousands of
subjects and a large variance in walking behavior and appearance can further boost performance and reduce overfitting.",Person recognition,"This paper proposes a new approach for Gait Recognition using a deep convolutional neural network with 3D convolutions and a special input format including gray-scale image and optical flow to capture spatio-temporal features. The approach is evaluated on three different datasets, showing comparable to better performance in comparison with previous approaches, especially for large view differences. The proposed approach can handle view, clothing, and walking speed invariance, which are challenging factors in Gait Recognition. The results suggest the high potential of CNNs for Gait Recognition, and future improvements can be made by using larger databases and better hardware.",Object and Sentiment Recognition,,Deep Learning and Machine Learning
134,"Exploring Body Texture From mmW Images
for Person Recognition","mmW imaging, body texture information, border control security, hand-crafted features, deep learning features, CNN-level multimodal fusion, body parts.","Imaging using millimeter waves (mmWs) has many
advantages including the ability to penetrate obscurants, such as
clothes and polymers. After having explored shape information
retrieved from mmW images for person recognition, in this paper
we aim to gain some insight about the potential of using mmW
texture information for the same task, considering not only the
mmW face, but also mmW torso and mmW wholebody. We
report experimental results using the mmW TNO database consisting of 50 individuals based on both hand-crafted and learned
features from Alexnet and VGG-face pretrained convolutional
neural networks (CNNs) models. First, we analyze the individual performance of three mmW body parts, concluding that:
1) mmW torso region is more discriminative than mmW face
and the whole body; 2) CNN features produce better results
compared to hand-crafted features on mmW faces and the entire
body; and 3) hand-crafted features slightly outperform CNN features on mmW torso. In the second part of this paper, we analyze
different multi-algorithmic and multi-modal techniques, including a novel CNN-based fusion technique, improving verification
results to 2% EER and identification rank-1 results up to 99%.
Comparative analyses with mmW body shape information and
face recognition in the visible and NIR spectral bands are also
reported.","This work has presented one of the very first works addressing person recognition using texture extracted from mmW images. Different mmW body parts have been considered:
face, torso and whole body. We have carried out experiments
with several hand-crafted features and some state-of-the-art
deep learning features. Some of the findings from the experiments are: i) mmW torso is the most discriminative body
part, followed by whole body and face with average EER
of 6.30%, 23.25%, and 29.75%, respectively, ii) mmW face
recognition has very low performance, iii) CNN features overcome hand-crafted features with faces and whole body parts
and, iv) hand-crafted features achieve outstanding results for
torso-based person recognition.
Finding ii) may be explained by the low resolution of
mmW images. Comparative results of face recognition for
visible, NIR and mmW images reported show how the methods proposed in this paper achieve very high recognition
performance for high resolution face images in the visible
region. We believe that this problem could be overcome by
addressing it as a low resolution face recognition problem,
which has been studied deeply elsewhere [32]. The most common techniques are based on super resolution [33] or face
hallucination [34], [35].
The second experimental part of this work has addressed
fusion schemes for multimodal and multi-algorithm
approaches. In what concerns multi-algorithm fusion, in
general, it is convenient to fuse information from all feature
approaches only if individual performances are similar,
otherwise it is better to discard the feature approaches
with worse performance. Regarding multimodal approaches,
the best fusion approach for LBP and HOG features is at
score and feature level, respectively. However, none of the
fusion approaches is able to outperform the best individual
performance achieved with mmW torso. With learned features, CNN level fusion outperforms feature and score level
approaches. In this case, CNN level fusion is able to attain up
to 99% of R1 or 2.5% of EER, when using torso and whole
body mmW body parts.
The best verification results for multi-algorithm fusion is
obtained combining all different texture features extracted
from the torso with an EER of 2%. Regarding multimodal
fusion, the best result is obtained when fusing mmW torso
and mmW whole body at CNN level with Alexnet, reaching a 2.5% of EER. Also, for future work, we will consider
fusing texture with shape (see previous work [7] and texture
information jointly [10]). Also, it would be worth exploring
the possibilities of combining mmW and images from other
ranges of the spectrum [25]. In particular, the results reported
would suggest a fusion of mmW torso and/or the whole body
with visible face images would achieve improved recognition
results.","Exploring Body Texture From mmW Images
for Person RecognitionmmW imaging, body texture information, border control security, hand-crafted features, deep learning features, CNN-level multimodal fusion, body parts.Imaging using millimeter waves (mmWs) has many
advantages including the ability to penetrate obscurants, such as
clothes and polymers. After having explored shape information
retrieved from mmW images for person recognition, in this paper
we aim to gain some insight about the potential of using mmW
texture information for the same task, considering not only the
mmW face, but also mmW torso and mmW wholebody. We
report experimental results using the mmW TNO database consisting of 50 individuals based on both hand-crafted and learned
features from Alexnet and VGG-face pretrained convolutional
neural networks (CNNs) models. First, we analyze the individual performance of three mmW body parts, concluding that:
1) mmW torso region is more discriminative than mmW face
and the whole body; 2) CNN features produce better results
compared to hand-crafted features on mmW faces and the entire
body; and 3) hand-crafted features slightly outperform CNN features on mmW torso. In the second part of this paper, we analyze
different multi-algorithmic and multi-modal techniques, including a novel CNN-based fusion technique, improving verification
results to 2% EER and identification rank-1 results up to 99%.
Comparative analyses with mmW body shape information and
face recognition in the visible and NIR spectral bands are also
reported.This work has presented one of the very first works addressing person recognition using texture extracted from mmW images. Different mmW body parts have been considered:
face, torso and whole body. We have carried out experiments
with several hand-crafted features and some state-of-the-art
deep learning features. Some of the findings from the experiments are: i) mmW torso is the most discriminative body
part, followed by whole body and face with average EER
of 6.30%, 23.25%, and 29.75%, respectively, ii) mmW face
recognition has very low performance, iii) CNN features overcome hand-crafted features with faces and whole body parts
and, iv) hand-crafted features achieve outstanding results for
torso-based person recognition.
Finding ii) may be explained by the low resolution of
mmW images. Comparative results of face recognition for
visible, NIR and mmW images reported show how the methods proposed in this paper achieve very high recognition
performance for high resolution face images in the visible
region. We believe that this problem could be overcome by
addressing it as a low resolution face recognition problem,
which has been studied deeply elsewhere [32]. The most common techniques are based on super resolution [33] or face
hallucination [34], [35].
The second experimental part of this work has addressed
fusion schemes for multimodal and multi-algorithm
approaches. In what concerns multi-algorithm fusion, in
general, it is convenient to fuse information from all feature
approaches only if individual performances are similar,
otherwise it is better to discard the feature approaches
with worse performance. Regarding multimodal approaches,
the best fusion approach for LBP and HOG features is at
score and feature level, respectively. However, none of the
fusion approaches is able to outperform the best individual
performance achieved with mmW torso. With learned features, CNN level fusion outperforms feature and score level
approaches. In this case, CNN level fusion is able to attain up
to 99% of R1 or 2.5% of EER, when using torso and whole
body mmW body parts.
The best verification results for multi-algorithm fusion is
obtained combining all different texture features extracted
from the torso with an EER of 2%. Regarding multimodal
fusion, the best result is obtained when fusing mmW torso
and mmW whole body at CNN level with Alexnet, reaching a 2.5% of EER. Also, for future work, we will consider
fusing texture with shape (see previous work [7] and texture
information jointly [10]). Also, it would be worth exploring
the possibilities of combining mmW and images from other
ranges of the spectrum [25]. In particular, the results reported
would suggest a fusion of mmW torso and/or the whole body
with visible face images would achieve improved recognition
results.",Person recognition,"The paper explores the potential of using texture information extracted from millimeter wave (mmW) images for person recognition, focusing on three mmW body parts: face, torso, and whole body. The authors report experimental results using hand-crafted and learned features from convolutional neural networks (CNNs) models. They find that the mmW torso region is the most discriminative body part, followed by the whole body and face. CNN features outperform hand-crafted features on mmW faces and the entire body, but hand-crafted features achieve outstanding results for torso-based person recognition. The authors also analyze different multi-algorithmic and multi-modal fusion techniques, improving verification results to 2% EER and identification rank-1 results up to 99%. The paper suggests future work could explore fusing texture with shape and images from other ranges of the spectrum to achieve improved recognition results.",Object and Sentiment Recognition,,Object Recognition
135,Human-Centered Emotion Recognition in Animated GIFs,"Emotion recognition,Visualization,
Feature extraction
,
Task analysis
,
Videos
,
Heating systems
,
Social networking (online)
","As an intuitive way of expression emotion, the animated Graphical Interchange Format (GIF) images have been widely used on social media. Most previous studies on automated GIF emotion recognition fail to effectively utilize GIF's unique properties, and this potentially limits the recognition performance. In this study, we demonstrate the importance of human related information in GIFs and conduct human-centered GIF emotion recognition with a proposed Keypoint Attended Visual Attention Network (KAVAN). The framework consists of a facial attention module and a hierarchical segment temporal module. The facial attention module exploits the strong relationship between GIF contents and human characters, and extracts frame-level visual feature with a focus on human faces. The Hierarchical Segment LSTM (HS-LSTM) module is then proposed to better learn global GIF representations. Our proposed framework outperforms the state-of-the-art on the MIT GIFGIF dataset. Furthermore, the facial attention module provides reliable facial region mask predictions, which improves the model's interpretability.","This study demonstrates the significance of human-related information in GIFs for emotion recognition and proposes a human-centered approach utilizing a Keypoint Attended Visual Attention Network (KAVAN). The framework incorporates a facial attention module and a hierarchical segment temporal module to extract visual features and learn global GIF representations. The results of experiments conducted on the MIT GIFGIF dataset show that the proposed framework outperforms the existing state-of-the-art techniques. Additionally, the facial attention module provides reliable facial region mask predictions, enhancing the interpretability of the model. Overall, the findings of this study highlight the potential of utilizing GIF's unique properties for emotion recognition and can have significant implications for developing advanced applications in the field of computer vision and social media analysis","Human-Centered Emotion Recognition in Animated GIFsEmotion recognition,Visualization,
Feature extraction
,
Task analysis
,
Videos
,
Heating systems
,
Social networking (online)
As an intuitive way of expression emotion, the animated Graphical Interchange Format (GIF) images have been widely used on social media. Most previous studies on automated GIF emotion recognition fail to effectively utilize GIF's unique properties, and this potentially limits the recognition performance. In this study, we demonstrate the importance of human related information in GIFs and conduct human-centered GIF emotion recognition with a proposed Keypoint Attended Visual Attention Network (KAVAN). The framework consists of a facial attention module and a hierarchical segment temporal module. The facial attention module exploits the strong relationship between GIF contents and human characters, and extracts frame-level visual feature with a focus on human faces. The Hierarchical Segment LSTM (HS-LSTM) module is then proposed to better learn global GIF representations. Our proposed framework outperforms the state-of-the-art on the MIT GIFGIF dataset. Furthermore, the facial attention module provides reliable facial region mask predictions, which improves the model's interpretability.This study demonstrates the significance of human-related information in GIFs for emotion recognition and proposes a human-centered approach utilizing a Keypoint Attended Visual Attention Network (KAVAN). The framework incorporates a facial attention module and a hierarchical segment temporal module to extract visual features and learn global GIF representations. The results of experiments conducted on the MIT GIFGIF dataset show that the proposed framework outperforms the existing state-of-the-art techniques. Additionally, the facial attention module provides reliable facial region mask predictions, enhancing the interpretability of the model. Overall, the findings of this study highlight the potential of utilizing GIF's unique properties for emotion recognition and can have significant implications for developing advanced applications in the field of computer vision and social media analysisFeature Extraction TEXTURE won | lil pa a *|-3|_ mmW body Parts Ervaled_ ] CNNMbased fusion Templates | F | bi Accepted rN Matching scores Matching Le] Fusion t Decision : [> Verification (1 vs 1) Deep Leaming =!» identification (1 vs N) Rejected",Deep Learning and Machine Learning,"The study demonstrates the importance of human-related information in GIFs for emotion recognition and proposes a human-centered approach utilizing a Keypoint Attended Visual Attention Network (KAVAN). The framework incorporates a facial attention module and a hierarchical segment temporal module to extract visual features and learn global GIF representations. The results of experiments conducted on the MIT GIFGIF dataset show that the proposed framework outperforms the existing state-of-the-art techniques. Additionally, the facial attention module provides reliable facial region mask predictions, enhancing the interpretability of the model. Overall, the findings of this study highlight the potential of utilizing GIF's unique properties for emotion recognition and can have significant implications for developing advanced applications in the field of computer vision and social media analysis.",Object and Sentiment Recognition,Feature Extraction TEXTURE won | lil pa a *|-3|_ mmW body Parts Ervaled_ ] CNNMbased fusion Templates | F | bi Accepted rN Matching scores Matching Le] Fusion t Decision : [> Verification (1 vs 1) Deep Leaming =!» identification (1 vs N) Rejected,Object Recognition
136,"ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language
Generation","Vietnamese language, Transformer-based model, T5 self-supervised pretraining, text generation, Named Entity Recognition, state-of-the-art results, context length.","e present ViT5, a pretrained Transformerbased encoder-decoder model for the Vietnamese language. With T5-style selfsupervised pretraining, ViT5 is trained on
a large corpus of high-quality and diverse
Vietnamese texts. We benchmark ViT5 on
two downstream text generation tasks, Abstractive Text Summarization and Named Entity Recognition. Although Abstractive Text
Summarization has been widely studied for
the English language thanks to its rich and
large source of data, there has been minimal research into the same task in Vietnamese, a much lower resource language. In
this work, we perform exhaustive experiments
on both Vietnamese Abstractive Summarization and Named Entity Recognition, validating the performance of ViT5 against many
other pretrained Transformer-based encoderdecoder models. Our experiments show that
ViT5 significantly outperforms existing models and achieves state-of-the-art results on
Vietnamese Text Summarization. On the task
of Named Entity Recognition, ViT5 is competitive against previous best results from pretrained encoder-based Transformer models.
Further analysis shows the importance of context length during the self-supervised pretraining on downstream performance across different settings.
","e introduce ViT5, a pretrained sequence-tosequence Transformer model for the Vietnamese
language. Leveraging the T5 self-supervised pretraining formulation on massive and high-quality
Vietnamese corpora, we showed that finetuned
ViT5 models are performant on both generation
and classification tasks. We exhaustively compare ViT5 with other pretrained formulations on
both multilingual and monolingual corpora. Our
experiments show that ViT5 achieves state-of-theart results on summarization in both Wikilingua
and Vietnews corpus, and competitive results in
generating Named Entity Recognition (NER) on
the PhoNER COVID19 dataset. We also analyze
and discuss the importance of context length during the self-supervised pretraining stage, which
strongly influences and positively leads to better
downstream performance.
","ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language
GenerationVietnamese language, Transformer-based model, T5 self-supervised pretraining, text generation, Named Entity Recognition, state-of-the-art results, context length.e present ViT5, a pretrained Transformerbased encoder-decoder model for the Vietnamese language. With T5-style selfsupervised pretraining, ViT5 is trained on
a large corpus of high-quality and diverse
Vietnamese texts. We benchmark ViT5 on
two downstream text generation tasks, Abstractive Text Summarization and Named Entity Recognition. Although Abstractive Text
Summarization has been widely studied for
the English language thanks to its rich and
large source of data, there has been minimal research into the same task in Vietnamese, a much lower resource language. In
this work, we perform exhaustive experiments
on both Vietnamese Abstractive Summarization and Named Entity Recognition, validating the performance of ViT5 against many
other pretrained Transformer-based encoderdecoder models. Our experiments show that
ViT5 significantly outperforms existing models and achieves state-of-the-art results on
Vietnamese Text Summarization. On the task
of Named Entity Recognition, ViT5 is competitive against previous best results from pretrained encoder-based Transformer models.
Further analysis shows the importance of context length during the self-supervised pretraining on downstream performance across different settings.
e introduce ViT5, a pretrained sequence-tosequence Transformer model for the Vietnamese
language. Leveraging the T5 self-supervised pretraining formulation on massive and high-quality
Vietnamese corpora, we showed that finetuned
ViT5 models are performant on both generation
and classification tasks. We exhaustively compare ViT5 with other pretrained formulations on
both multilingual and monolingual corpora. Our
experiments show that ViT5 achieves state-of-theart results on summarization in both Wikilingua
and Vietnews corpus, and competitive results in
generating Named Entity Recognition (NER) on
the PhoNER COVID19 dataset. We also analyze
and discuss the importance of context length during the self-supervised pretraining stage, which
strongly influences and positively leads to better
downstream performance.
A Fixation Cross Expression Categorizing Intensity Rating BTID ed: bie e SN pd Fi iH EN BR RE RE PE _ 1 2 3 4 5 6 BS AWE Acedia oS OW) b--Tbeyed Std Until Response B Fixation Cross Stimulus Display Expression Categorizing Intensity Rating NO SC aie. acc aDMN tony Ei pap abe ah Edieba thd of re &E CE £3 GC) 7f EF bo a Re ee Sas OO Sc 8 MD TLL 1 2 3 4 5 6 7 TC, re te 500 ms 150 ms (A Until Response100 Accuracy (%) Expression intensity 80 60 40 20 10 co mo aN Face without feature HM Fullface EQ Eyebrows EA Eyes E49 Mouth Type: Cartoon Happy Sad NeutralAccuracy (%) 100 80 Oo © 40 20 Expression J Happy =] Sad = Neutral Cartoon Real Type Expression intensity 7.9 Or Oo N oO Expression HB Happy Hi Sad EM Neutral Cartoon Real Type",Text summarization,"The article introduces ViT5, a pretrained Transformer-based encoder-decoder model for the Vietnamese language that is trained on a large corpus of high-quality and diverse Vietnamese texts. ViT5 is benchmarked on two downstream tasks, Abstractive Text Summarization and Named Entity Recognition, and achieves state-of-the-art results on Vietnamese Text Summarization and competitive results on Named Entity Recognition. The article also discusses the importance of context length during the self-supervised pretraining stage, which positively influences downstream performance.",Natural Language Processing,"A Fixation Cross Expression Categorizing Intensity Rating BTID ed: bie e SN pd Fi iH EN BR RE RE PE _ 1 2 3 4 5 6 BS AWE Acedia oS OW) b--Tbeyed Std Until Response B Fixation Cross Stimulus Display Expression Categorizing Intensity Rating NO SC aie. acc aDMN tony Ei pap abe ah Edieba thd of re &E CE £3 GC) 7f EF bo a Re ee Sas OO Sc 8 MD TLL 1 2 3 4 5 6 7 TC, re te 500 ms 150 ms (A Until Response100 Accuracy (%) Expression intensity 80 60 40 20 10 co mo aN Face without feature HM Fullface EQ Eyebrows EA Eyes E49 Mouth Type: Cartoon Happy Sad NeutralAccuracy (%) 100 80 Oo © 40 20 Expression J Happy =] Sad = Neutral Cartoon Real Type Expression intensity 7.9 Or Oo N oO Expression HB Happy Hi Sad EM Neutral Cartoon Real Type",Sentiment Analysis
137,"A Hierarchical Representation Model Based on Longformer and
Transformer for Extractive Summarization", extractive summarization; transformer; longformer; deep learning,"Automatic text summarization is a method used to compress documents while preserving
the main idea of the original text, including extractive summarization and abstractive summarization.
Extractive text summarization extracts important sentences from the original document to serve
as the summary. The document representation method is crucial for the quality of the generated
summarization. To effectively represent the document, we propose a hierarchical document representation model Long-Trans-Extr for Extractive Summarization, which uses Longformer as the sentence
encoder and Transformer as the document encoder. The advantage of Longformer as sentence
encoder is that the model can input long document up to 4096 tokens with adding relative a little
calculation. The proposed model Long-Trans-Extr is evaluated on three benchmark datasets: CNN
(Cable News Network), DailyMail, and the combined CNN/DailyMail. It achieves 43.78 (Rouge-1)
and 39.71 (Rouge-L) on CNN/DailyMail and 33.75 (Rouge-1), 13.11 (Rouge-2), and 30.44 (Rouge-L)
on the CNN datasets. They are very competitive results, and furthermore, they show that our model
has better performance on long documents, such as the CNN corpus.","In this study, we propose a Long-Trans-Extr extractive summarization model, which
uses Longformer as a sentence encoder, Transformer as a document encoder, and finally,
an MLP classifier is used to decide whether a sentence in a document should be extracted
or not. This model solves the problem that it is difficult for previous models to deal
with long documents. It enables sentence representation and document representation
to notice longer text information without increasing too much computation and memory.
Experimental results show that, under the same decoder condition, our model is superior
to other models on the CNN/DailyMail dataset, and it achieves the best results on a long
CNN dataset","A Hierarchical Representation Model Based on Longformer and
Transformer for Extractive Summarization extractive summarization; transformer; longformer; deep learningAutomatic text summarization is a method used to compress documents while preserving
the main idea of the original text, including extractive summarization and abstractive summarization.
Extractive text summarization extracts important sentences from the original document to serve
as the summary. The document representation method is crucial for the quality of the generated
summarization. To effectively represent the document, we propose a hierarchical document representation model Long-Trans-Extr for Extractive Summarization, which uses Longformer as the sentence
encoder and Transformer as the document encoder. The advantage of Longformer as sentence
encoder is that the model can input long document up to 4096 tokens with adding relative a little
calculation. The proposed model Long-Trans-Extr is evaluated on three benchmark datasets: CNN
(Cable News Network), DailyMail, and the combined CNN/DailyMail. It achieves 43.78 (Rouge-1)
and 39.71 (Rouge-L) on CNN/DailyMail and 33.75 (Rouge-1), 13.11 (Rouge-2), and 30.44 (Rouge-L)
on the CNN datasets. They are very competitive results, and furthermore, they show that our model
has better performance on long documents, such as the CNN corpus.In this study, we propose a Long-Trans-Extr extractive summarization model, which
uses Longformer as a sentence encoder, Transformer as a document encoder, and finally,
an MLP classifier is used to decide whether a sentence in a document should be extracted
or not. This model solves the problem that it is difficult for previous models to deal
with long documents. It enables sentence representation and document representation
to notice longer text information without increasing too much computation and memory.
Experimental results show that, under the same decoder condition, our model is superior
to other models on the CNN/DailyMail dataset, and it achieves the best results on a long
CNN dataset‘wikilingua: Anh ay bat xe toi tham gia bua tiéc tai mot nha hang sang trong. Nhung trong budi tiéc, anh 4y nga quy ‘Anh dy d4 nhép vién sau khi tham gia bia tiéc. (He was hospitalized after attending the party.) xuéng va dug dua tai bénh vin. (He took the car to attend a party at a luxury restaurant. But at the party, he collapsed and was taken to the hospital.) pho_ner: Bénh nhan 75 la nir , 40 tui, dia chi 6 ‘Quan 2, TP. HCM (Patient No.75 is a female, 40 years old, and lives in District 2, HCM city) Bénh nhan PATIENT_ID* 75 PATIENT_1D* la GENDER* ni GENDER’ , AGE* 40 AGE* tuai , dia chi LOCATION* Quan 2 LOCATION* , LOCATION* TP. HCM LOCATION* (Patient PATIENT_ID* No.75 PATIENT_ID* is a GENDER* female GENDER* , AGE* 40 AGE* years old, and lives in LOCATION’ District 2 LOCATION"" , LOCATION"" HCM city LOCATION"")",Text summarization,"The article discusses automatic text summarization, which is used to compress documents while maintaining their main ideas. The proposed model, Long-Trans-Extr, uses Longformer as the sentence encoder, Transformer as the document encoder, and an MLP classifier to extract important sentences from long documents. The model is evaluated on three benchmark datasets and shows superior performance on long documents, achieving competitive results on the CNN/DailyMail and the best results on the long CNN dataset.",Natural Language Processing,"‘wikilingua: Anh ay bat xe toi tham gia bua tiéc tai mot nha hang sang trong. Nhung trong budi tiéc, anh 4y nga quy ‘Anh dy d4 nhép vién sau khi tham gia bia tiéc. (He was hospitalized after attending the party.) xuéng va dug dua tai bénh vin. (He took the car to attend a party at a luxury restaurant. But at the party, he collapsed and was taken to the hospital.) pho_ner: Bénh nhan 75 la nir , 40 tui, dia chi 6 ‘Quan 2, TP. HCM (Patient No.75 is a female, 40 years old, and lives in District 2, HCM city) Bénh nhan PATIENT_ID* 75 PATIENT_1D* la GENDER* ni GENDER’ , AGE* 40 AGE* tuai , dia chi LOCATION* Quan 2 LOCATION* , LOCATION* TP. HCM LOCATION* (Patient PATIENT_ID* No.75 PATIENT_ID* is a GENDER* female GENDER* , AGE* 40 AGE* years old, and lives in LOCATION’ District 2 LOCATION"" , LOCATION"" HCM city LOCATION"")",Deep Learning and Machine Learning
138,Emotion Residue in Neutral Faces: Implications for Impression Formation,"cartoon faces, emotion recognition, facial features, expression intensity, happy, sad","Cartoon faces are widely used in social media, animation production, and social robots because of their attractive ability to convey different emotional information. Despite their popular applications, the mechanisms of recognizing emotional expressions in cartoon faces are still unclear. Therefore, three experiments were conducted in this study to systematically explore a recognition process for emotional cartoon expressions (happy, sad, and neutral) and to examine the influence of key facial features (mouth, eyes, and eyebrows) on emotion recognition. Across the experiments, three presentation conditions were employed: (1) a full face; (2) individual feature only (with two other features concealed); and (3) one feature concealed with two other features presented. The cartoon face images used in this study were converted from a set of real faces acted by Chinese posers, and the observers were Chinese. The results show that happy cartoon expressions were recognized more accurately than neutral and sad expressions, which was consistent with the happiness recognition advantage revealed in real face studies. Compared with real facial expressions, sad cartoon expressions were perceived as sadder, and happy cartoon expressions were perceived as less happy, regardless of whether full-face or single facial features were viewed. For cartoon faces, the mouth was demonstrated to be a feature that is sufficient and necessary for the recognition of happiness, and the eyebrows were sufficient and necessary for the recognition of sadness. This study helps to clarify the perception mechanism underlying emotion recognition in cartoon faces and sheds some light on directions for future research on intelligent human-computer interactions.","To investigate facial emotional expression recognition in cartoon faces, three experiments were performed in this study. We found that the processing of emotion in cartoon faces showed a happiness advantage and that the highest recognition accuracy was obtained for happy expressions in cartoon faces. In terms of perceived intensity, cartoon faces with sad expressions were perceived as sadder than real faces with sad expressions. Furthermore, facial features showed a dissimilar impact on the recognition of emotional facial expressions, and we highlighted the role of the mouth in happiness recognition and the role of the eyebrows in sadness recognition. This study provides an important reference for extending existing facial emotion recognition studies, from real faces to cartoon faces, and the importance of features that was revealed in this study may shed light on the development of cartoon characters for emotional and social artificial intelligence.","Emotion Residue in Neutral Faces: Implications for Impression Formationcartoon faces, emotion recognition, facial features, expression intensity, happy, sadCartoon faces are widely used in social media, animation production, and social robots because of their attractive ability to convey different emotional information. Despite their popular applications, the mechanisms of recognizing emotional expressions in cartoon faces are still unclear. Therefore, three experiments were conducted in this study to systematically explore a recognition process for emotional cartoon expressions (happy, sad, and neutral) and to examine the influence of key facial features (mouth, eyes, and eyebrows) on emotion recognition. Across the experiments, three presentation conditions were employed: (1) a full face; (2) individual feature only (with two other features concealed); and (3) one feature concealed with two other features presented. The cartoon face images used in this study were converted from a set of real faces acted by Chinese posers, and the observers were Chinese. The results show that happy cartoon expressions were recognized more accurately than neutral and sad expressions, which was consistent with the happiness recognition advantage revealed in real face studies. Compared with real facial expressions, sad cartoon expressions were perceived as sadder, and happy cartoon expressions were perceived as less happy, regardless of whether full-face or single facial features were viewed. For cartoon faces, the mouth was demonstrated to be a feature that is sufficient and necessary for the recognition of happiness, and the eyebrows were sufficient and necessary for the recognition of sadness. This study helps to clarify the perception mechanism underlying emotion recognition in cartoon faces and sheds some light on directions for future research on intelligent human-computer interactions.To investigate facial emotional expression recognition in cartoon faces, three experiments were performed in this study. We found that the processing of emotion in cartoon faces showed a happiness advantage and that the highest recognition accuracy was obtained for happy expressions in cartoon faces. In terms of perceived intensity, cartoon faces with sad expressions were perceived as sadder than real faces with sad expressions. Furthermore, facial features showed a dissimilar impact on the recognition of emotional facial expressions, and we highlighted the role of the mouth in happiness recognition and the role of the eyebrows in sadness recognition. This study provides an important reference for extending existing facial emotion recognition studies, from real faces to cartoon faces, and the importance of features that was revealed in this study may shed light on the development of cartoon characters for emotional and social artificial intelligence.Ay hg hg JU Transformer I] Multi-Head Attention hy hy Ag Transformer Feed ForwardToken Embeddings | Ects | | Esent| | Eone | | Esep | | Ects | | Esent] | Ezna | | Esep | | Ects | | Esent] | Esra | | Esep - Position Embeddings | E, E2 E3 Ey Es, Ee | Es Ey | {Exo | | Esa | | E12 sentence FI Longformer encoder Ea ce cs document { ‘Transformer Layer encoder classifier {",Deep Learning and Machine Learning,"The study aimed to investigate the recognition of emotional expressions in cartoon faces and the impact of key facial features (mouth, eyes, and eyebrows) on emotion recognition. Three experiments were conducted, and the results showed a happiness recognition advantage in cartoon faces, with happy expressions being recognized more accurately than sad and neutral expressions. The study also found that the mouth was crucial for happiness recognition, while the eyebrows were essential for sadness recognition. The study provides insights into the perception mechanism underlying emotion recognition in cartoon faces and has implications for the development of emotional and social artificial intelligence.",Object and Sentiment Recognition,"Ay hg hg JU Transformer I] Multi-Head Attention hy hy Ag Transformer Feed ForwardToken Embeddings | Ects | | Esent| | Eone | | Esep | | Ects | | Esent] | Ezna | | Esep | | Ects | | Esent] | Esra | | Esep - Position Embeddings | E, E2 E3 Ey Es, Ee | Es Ey | {Exo | | Esa | | E12 sentence FI Longformer encoder Ea ce cs document { ‘Transformer Layer encoder classifier {",Deep Learning and Machine Learning
139,"PEGASUS: Pre-training with Extracted Gap-sentences for
Abstractive Summarization
","PEGASUS, abstractive text summarization, pre-training, self-supervised objective, downstream summarization tasks.","Recent work pre-training Transformers with
self-supervised objectives on large text corpora
has shown great success when fine-tuned on
downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have
not been explored. Furthermore there is a
lack of systematic evaluation across diverse domains. In this work, we propose pre-training
large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important
sentences are removed/masked from an input document and are generated together as one output
sequence from the remaining sentences, similar
to an extractive summary. We evaluated our best
PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured
by ROUGE scores. Our model also shows surprising performance on low-resource summarization,
surpassing previous state-of-the-art results on 6
datasets with only 1000 examples. Finally we
validated our results using human evaluation and
show that our model summaries achieve human
performance on multiple datasets.","n this work, we proposed PEGASUS, a sequence-tosequence model with gap-sentences generation as a pretraining objective tailored for abstractive text summarization. We studied several gap-sentence selection methods
and identified principle sentence selection as the optimal
strategy. We demonstrated the effects of the pre-training
corpora, gap-sentences ratios, vocabulary sizes and scaled
up the best configuration to achieve state-of-the-art results
on all 12 diverse downstream datasets considered. We also
showed that our model was able to adapt to unseen summarization datasets very quickly, achieving strong results in as
little as 1000 examples. We finally showed our model summaries achieved human performance on multiple datasets
using human evaluation.
","PEGASUS: Pre-training with Extracted Gap-sentences for
Abstractive Summarization
PEGASUS, abstractive text summarization, pre-training, self-supervised objective, downstream summarization tasks.Recent work pre-training Transformers with
self-supervised objectives on large text corpora
has shown great success when fine-tuned on
downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have
not been explored. Furthermore there is a
lack of systematic evaluation across diverse domains. In this work, we propose pre-training
large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important
sentences are removed/masked from an input document and are generated together as one output
sequence from the remaining sentences, similar
to an extractive summary. We evaluated our best
PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured
by ROUGE scores. Our model also shows surprising performance on low-resource summarization,
surpassing previous state-of-the-art results on 6
datasets with only 1000 examples. Finally we
validated our results using human evaluation and
show that our model summaries achieve human
performance on multiple datasets.n this work, we proposed PEGASUS, a sequence-tosequence model with gap-sentences generation as a pretraining objective tailored for abstractive text summarization. We studied several gap-sentence selection methods
and identified principle sentence selection as the optimal
strategy. We demonstrated the effects of the pre-training
corpora, gap-sentences ratios, vocabulary sizes and scaled
up the best configuration to achieve state-of-the-art results
on all 12 diverse downstream datasets considered. We also
showed that our model was able to adapt to unseen summarization datasets very quickly, achieving strong results in as
little as 1000 examples. We finally showed our model summaries achieved human performance on multiple datasets
using human evaluation.
15 x 15 pixel Rol in the Original Image Corresponding Region in Feature map of size 2.93 x 2.93 * .. RoiAlign Layer Original Image Feature Map 128 x 128 25 x 25No. of Images 1,100 1,000 gt 8 800 Distribution of Tom and Jerry images Surprise Emotion Hl Tom Mi JerryBox Seah Regression | ©!2ssification Head Fully Connected Layers Fixed Size Feature Map ROI Align Layer Feature Map Convolutional BackboneDataset collection & preparation Character Detection (Mask R-CNN) Identifying Region Of Interset (Ro!) Generating specific masks for both using VIA tool cartoon character faces Resized Masks Emotion Classification (VGG16, ResNet-50, InceptionV3, MobileNetv2) ‘Segmented Faces/Masks { Happy, Sad, Angry, Surprise }",Text summarization,"The paper proposes PEGASUS, a pre-trained sequence-to-sequence model with a self-supervised objective tailored for abstractive text summarization. The model generates summaries by filling in gaps left by selected sentences, similar to an extractive summary. The authors evaluate PEGASUS on 12 diverse downstream summarization tasks, achieving state-of-the-art performance and demonstrating the model's adaptability to new datasets. The paper also includes a study of different gap-sentence selection methods and the effects of various pre-training configurations.",Natural Language Processing,"15 x 15 pixel Rol in the Original Image Corresponding Region in Feature map of size 2.93 x 2.93 * .. RoiAlign Layer Original Image Feature Map 128 x 128 25 x 25No. of Images 1,100 1,000 gt 8 800 Distribution of Tom and Jerry images Surprise Emotion Hl Tom Mi JerryBox Seah Regression | ©!2ssification Head Fully Connected Layers Fixed Size Feature Map ROI Align Layer Feature Map Convolutional BackboneDataset collection & preparation Character Detection (Mask R-CNN) Identifying Region Of Interset (Ro!) Generating specific masks for both using VIA tool cartoon character faces Resized Masks Emotion Classification (VGG16, ResNet-50, InceptionV3, MobileNetv2) ‘Segmented Faces/Masks { Happy, Sad, Angry, Surprise }",Sentiment Analysis
140,Understanding cartoon emotion using integrated deep neural network on large dataset,"Animation
Cartoon
Character Detection
Convolutional Neural Network
Emotion
Face Segmentation
Mask R-CNN
VGG16","Emotion is an instinctive or intuitive feeling as distinguished from reasoning or knowledge. It varies over time, since it is a natural instinctive state of mind deriving from one’s circumstances, mood, or relationships with others. Since emotions vary over time, it is important to understand and analyze them appropriately. Existing works have mostly focused well on recognizing basic emotions from human faces. However, the emotion recognition from cartoon images has not been extensively covered. Therefore, in this paper, we present an integrated Deep Neural Network (DNN) approach that deals with recognizing emotions from cartoon images. Since state-of-works do not have large amount of data, we collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach, trained on a large dataset consisting of animations for both the characters (Tom and Jerry), correctly identifies the character, segments their face masks, and recognizes the consequent emotions with an accuracy score of 0.96. The approach utilizes Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16 for emotion classification. In our study, to classify emotions, VGG 16 outperforms others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN outperforms the state-of-the-art approaches.","Recognizing emotions from facial expressions of faces other than human beings is an interesting and challenging problem. Although the existing literature has endeavored to detect and recognize objects, however, recognizing emotions has not been extensively covered. Therefore, in this paper, we have presented an integrated Deep Neural Network (DNN) approach that has successfully recognized emotions from cartoon images. We have collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach has been trained on the large dataset and has correctly identified the character, segmented their face masks, and recognized the consequent emotions with an accuracy score of 0.96. The approach has utilized Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16, for emotion classification. The experimental analysis has depicted the outperformance of VGG 16 over others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN has also outperformed the state-of-the-art approaches.

The work would be beneficial to the animators, illustrators, and cartoonists. It can also be used to build a recommender system that allows users to associatively select emotion and cartoon pair. Studying emotions encased in cartoons also extracts other allied information, which if combined with artificial intelligence can open a plethora of opportunities, for instance, recognizing emotions from body gestures.","Understanding cartoon emotion using integrated deep neural network on large datasetAnimation
Cartoon
Character Detection
Convolutional Neural Network
Emotion
Face Segmentation
Mask R-CNN
VGG16Emotion is an instinctive or intuitive feeling as distinguished from reasoning or knowledge. It varies over time, since it is a natural instinctive state of mind deriving from one’s circumstances, mood, or relationships with others. Since emotions vary over time, it is important to understand and analyze them appropriately. Existing works have mostly focused well on recognizing basic emotions from human faces. However, the emotion recognition from cartoon images has not been extensively covered. Therefore, in this paper, we present an integrated Deep Neural Network (DNN) approach that deals with recognizing emotions from cartoon images. Since state-of-works do not have large amount of data, we collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach, trained on a large dataset consisting of animations for both the characters (Tom and Jerry), correctly identifies the character, segments their face masks, and recognizes the consequent emotions with an accuracy score of 0.96. The approach utilizes Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16 for emotion classification. In our study, to classify emotions, VGG 16 outperforms others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN outperforms the state-of-the-art approaches.Recognizing emotions from facial expressions of faces other than human beings is an interesting and challenging problem. Although the existing literature has endeavored to detect and recognize objects, however, recognizing emotions has not been extensively covered. Therefore, in this paper, we have presented an integrated Deep Neural Network (DNN) approach that has successfully recognized emotions from cartoon images. We have collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach has been trained on the large dataset and has correctly identified the character, segmented their face masks, and recognized the consequent emotions with an accuracy score of 0.96. The approach has utilized Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16, for emotion classification. The experimental analysis has depicted the outperformance of VGG 16 over others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN has also outperformed the state-of-the-art approaches.

The work would be beneficial to the animators, illustrators, and cartoonists. It can also be used to build a recommender system that allows users to associatively select emotion and cartoon pair. Studying emotions encased in cartoons also extracts other allied information, which if combined with artificial intelligence can open a plethora of opportunities, for instance, recognizing emotions from body gestures.Masked tokens Target text mythical} { names It is pure white. <eos> } t t {tt ff t Transformer Encoder Transformer Decoder ttt tttt ttt _ttt tt ( Pegasus is [MASK2] . [IV It [MASK2] the model . } { <s> It is pure white } Input ee Target text [Shifted Right] Pegasus is |mythical] . \It is pure white , It|names| the model }",Artificial Neural Network,"This paper presents an integrated Deep Neural Network (DNN) approach for recognizing emotions from cartoon images, which has not been extensively covered in existing works. The approach utilizes Mask R-CNN for character detection and state-of-the-art deep learning models for emotion classification. The proposed approach was trained on a dataset of size 8K from two cartoon characters ('Tom' and 'Jerry') with four different emotions, and achieved an accuracy score of 0.96. VGG 16 outperformed other deep learning models in emotion classification with an accuracy of 96% and F1 score of 0.85. This integrated DNN approach outperforms state-of-the-art approaches.",Object and Sentiment Recognition,"Masked tokens Target text mythical} { names It is pure white. <eos> } t t {tt ff t Transformer Encoder Transformer Decoder ttt tttt ttt _ttt tt ( Pegasus is [MASK2] . [IV It [MASK2] the model . } { <s> It is pure white } Input ee Target text [Shifted Right] Pegasus is |mythical] . \It is pure white , It|names| the model }",Deep Learning and Machine Learning
141,"A New Hybrid Approach for Efficient Emotion Recognition
using Deep Learning","Recurrent neural networks, Convolutional neural networks, Classification methods, PCA, ICA, EMOTICA, FER-13","""Facial emotion recognition has been very popular area for researchers in last few decades and it is found to
be very challenging and complex task due to large intra-class changes. Existing frameworks for this type of problem depends
mostly on techniques like Gabor filters, principle component analysis (PCA), and independent component analysis(ICA) followed
by some classification techniques trained by given videos and images. Most of these frameworks works significantly well image
database acquired in limited conditions but not perform well with the dynamic images having varying faces and images. In the
past years, various researches have been introduced framework for facial emotion recognition using deep learning methods.
Although they work well, but there is always some gap found in their research. In this research, we introduced hybrid approach
based on RNN and CNN which are able to retrieve some important parts in the given database and able to achieve very good
results on the given database like EMOTIC, FER-13 and FERG. We are also able to show that our hybrid framework is able to
accomplish promising accuracies with these datasets""","In this paper, a method is introduced to recognize emotion
from different facial images with pose, occlusion, and
illumination. From the past research, no such research has
been done for the facial emotion recognition based on hybrid
method. Despite of training is done in the dataset for still head
poses and illuminations, our model is able to adapt all the
variations like illumination, color, contrast, and head poses.
That is, our hybrid model is able to give better results than
traditional machine learning models. Our hybrid model is also
able to produce good results with less training datasets in the
publicly available datasets like EMOTIC, FER13, and FERG.
Our model is able to detect emotion recognition with high
accuracy and able to label each of them. The performance of
our model for FER13 dataset is best as compare to FERG and
EMOTIC datasets. In future, we will incorporate more deep
learning methods to improve the results and also try to conduct
some more experiments on other available datasets.
","A New Hybrid Approach for Efficient Emotion Recognition
using Deep LearningRecurrent neural networks, Convolutional neural networks, Classification methods, PCA, ICA, EMOTICA, FER-13""Facial emotion recognition has been very popular area for researchers in last few decades and it is found to
be very challenging and complex task due to large intra-class changes. Existing frameworks for this type of problem depends
mostly on techniques like Gabor filters, principle component analysis (PCA), and independent component analysis(ICA) followed
by some classification techniques trained by given videos and images. Most of these frameworks works significantly well image
database acquired in limited conditions but not perform well with the dynamic images having varying faces and images. In the
past years, various researches have been introduced framework for facial emotion recognition using deep learning methods.
Although they work well, but there is always some gap found in their research. In this research, we introduced hybrid approach
based on RNN and CNN which are able to retrieve some important parts in the given database and able to achieve very good
results on the given database like EMOTIC, FER-13 and FERG. We are also able to show that our hybrid framework is able to
accomplish promising accuracies with these datasets""In this paper, a method is introduced to recognize emotion
from different facial images with pose, occlusion, and
illumination. From the past research, no such research has
been done for the facial emotion recognition based on hybrid
method. Despite of training is done in the dataset for still head
poses and illuminations, our model is able to adapt all the
variations like illumination, color, contrast, and head poses.
That is, our hybrid model is able to give better results than
traditional machine learning models. Our hybrid model is also
able to produce good results with less training datasets in the
publicly available datasets like EMOTIC, FER13, and FERG.
Our model is able to detect emotion recognition with high
accuracy and able to label each of them. The performance of
our model for FER13 dataset is best as compare to FERG and
EMOTIC datasets. In future, we will incorporate more deep
learning methods to improve the results and also try to conduct
some more experiments on other available datasets.
LBP. Geome' Features HOG 91.7% 85% 92.56%Precision Recall ‘Accuracy F-score 92.6% 92.56% 92.56% 92.58%Anger | Disgust | Fear | Happy | Sad | Surprise Anger | 92.22 2.22 | 3.33 0 2.22 0 Disgust | 3.75 86.25 | 3.75 1.25 5 0 Fear 2.85 285 | 91.42 0 1.42 1.42 Happy 0 0 0 100 0 0 Sad 3.33 5 3.33 0 88.33 0 Surprise 0 0 2.85 0 0 97.14",Deep Learning and Machine Learning,"The paper discusses the challenges of facial emotion recognition and existing methods for solving it, including deep learning approaches. The authors propose a hybrid approach based on RNN and CNN for facial emotion recognition that performs well on datasets with varying faces, poses, occlusions, and illuminations. They compare their approach with traditional machine learning models and show that it outperforms them on publicly available datasets such as EMOTIC, FER-13, and FERG. The authors plan to incorporate more deep learning methods and conduct experiments on other available datasets in the future.",Object and Sentiment Recognition,LBP. Geome' Features HOG 91.7% 85% 92.56%Precision Recall ‘Accuracy F-score 92.6% 92.56% 92.56% 92.58%Anger | Disgust | Fear | Happy | Sad | Surprise Anger | 92.22 2.22 | 3.33 0 2.22 0 Disgust | 3.75 86.25 | 3.75 1.25 5 0 Fear 2.85 285 | 91.42 0 1.42 1.42 Happy 0 0 0 100 0 0 Sad 3.33 5 3.33 0 88.33 0 Surprise 0 0 2.85 0 0 97.14,Sentiment Analysis
142,Research on Animated GIFs Emotion Recognition Based on ResNet-ConvGRU,"GIF, emotion recognition, ResNet, ConvGRU, spatial-temporal features, sentiment classification, social media, public opinion trends.","Animated Graphics Interchange Format (GIF) images have become an important part of network information interaction, and are one of the main characteristics of analyzing social media emotions. At present, most of the research on GIF affection recognition fails to make full use of spatial-temporal characteristics of GIF images, which limits the performance of model recognition to a certain extent. A GIF emotion recognition algorithm based on ResNet-ConvGRU is proposed in this paper. First, GIF data is preprocessed, converting its image sequences to static image format for saving. Then, the spatial features of images and the temporal features of static image sequences are extracted with ResNet and ConvGRU networks, respectively. At last, the animated GIFs data features are synthesized and the seven emotional intensities of GIF data are calculated. The GIFGIF dataset is used to verify the experiment. From the experimental results, the proposed animated GIFs emotion recognition model based on ResNet-ConvGRU, compared with the classical emotion recognition algorithms such as VGGNet-ConvGRU, ResNet3D, CNN-LSTM, and C3D, has a stronger feature extraction ability, and sentiment classification performance. This method provides a finer-grained analysis for the study of public opinion trends and a new idea for affection recognition of GIF data in social media.","An animated GIFs emotion recognition method based on ResNet-ConvGRU is proposed in this paper. The model can improve further the training efficiency by converting GIF short videos into static image sequences and extracting the spatial and temporal features of image sequences using ResNet and ConvGRU networks, respectively. In the experiment, we first showed the frequency distribution of the number of animated GIF images in the data set. Then, the recognition efficiency of various emotion categories of the proposed method was verified. Finally, different emotion recognition models were compared by experiments to show that the proposed model has higher accuracy than the experimental results. The model was validated on only one data set in this paper. The effective performance of emotion recognition on different data sets and the robustness and universality of the model still need further research.","Research on Animated GIFs Emotion Recognition Based on ResNet-ConvGRUGIF, emotion recognition, ResNet, ConvGRU, spatial-temporal features, sentiment classification, social media, public opinion trends.Animated Graphics Interchange Format (GIF) images have become an important part of network information interaction, and are one of the main characteristics of analyzing social media emotions. At present, most of the research on GIF affection recognition fails to make full use of spatial-temporal characteristics of GIF images, which limits the performance of model recognition to a certain extent. A GIF emotion recognition algorithm based on ResNet-ConvGRU is proposed in this paper. First, GIF data is preprocessed, converting its image sequences to static image format for saving. Then, the spatial features of images and the temporal features of static image sequences are extracted with ResNet and ConvGRU networks, respectively. At last, the animated GIFs data features are synthesized and the seven emotional intensities of GIF data are calculated. The GIFGIF dataset is used to verify the experiment. From the experimental results, the proposed animated GIFs emotion recognition model based on ResNet-ConvGRU, compared with the classical emotion recognition algorithms such as VGGNet-ConvGRU, ResNet3D, CNN-LSTM, and C3D, has a stronger feature extraction ability, and sentiment classification performance. This method provides a finer-grained analysis for the study of public opinion trends and a new idea for affection recognition of GIF data in social media.An animated GIFs emotion recognition method based on ResNet-ConvGRU is proposed in this paper. The model can improve further the training efficiency by converting GIF short videos into static image sequences and extracting the spatial and temporal features of image sequences using ResNet and ConvGRU networks, respectively. In the experiment, we first showed the frequency distribution of the number of animated GIF images in the data set. Then, the recognition efficiency of various emotion categories of the proposed method was verified. Finally, different emotion recognition models were compared by experiments to show that the proposed model has higher accuracy than the experimental results. The model was validated on only one data set in this paper. The effective performance of emotion recognition on different data sets and the robustness and universality of the model still need further research.| ' ' ' ' : ' ' is 1 z ' S ' «th 0 |< ‘ 2a 1 gN ' =. ' =) 1 ' =] °|¢ ' a 1 Z 1 2 ' baud ' a 1 3 ' & | ' t t : Ht-1Residual Residual Residual Residual unit 1 unit 2 unit 3 unit 4 3x224x224 P1(maxpool) P2(avgpool)ore FeeEmotion classification layer A ConvGRU fg ConvGRru [ZT Converv ‘ : ConvGRU [g—*? ConvGRU [> -- ri ConvGRU t f t ResNet A Input layernumber",Deep Learning and Machine Learning,"The he paper proposes an animated GIF emotion recognition algorithm based on ResNet-ConvGRU, which extracts spatial and temporal features of GIF images for sentiment classification. The proposed method outperforms classical emotion recognition algorithms like VGGNet-ConvGRU, ResNet3D, CNN-LSTM, and C3D on the GIFGIF dataset, providing finer-grained analysis for studying public opinion trends. The model converts GIF short videos into static image sequences and extracts features using ResNet and ConvGRU networks, respectively, and achieves higher accuracy in emotion recognition. However, further research is needed to validate the performance on different datasets and the model's robustness and universality.",Object and Sentiment Recognition,| ' ' ' ' : ' ' is 1 z ' S ' «th 0 |< ‘ 2a 1 gN ' =. ' =) 1 ' =] °|¢ ' a 1 Z 1 2 ' baud ' a 1 3 ' & | ' t t : Ht-1Residual Residual Residual Residual unit 1 unit 2 unit 3 unit 4 3x224x224 P1(maxpool) P2(avgpool)ore FeeEmotion classification layer A ConvGRU fg ConvGRru [ZT Converv ‘ : ConvGRU [g—*? ConvGRU [> -- ri ConvGRU t f t ResNet A Input layernumber,Sentiment Analysis
143,"Facial recognition, emotion and race in animated social media","Facial recognition, animoji, racial identities, emotional expression, racialized logics, digital animation, design justice, data justice","Facial recognition systems are increasingly common components of commercial smart phones such as the iPhone X and the Samsung Galaxy S9. These technologies are also increasingly being put to use in consumer-facing social media video-sharing applications, such as Apple’s animoji and memoji, Facebook Messenger’s masks and filters and Samsung’s AR Emoji. These animations serve as technical phenomena translating moments of affective and emotional expression into mediated socially legible forms. Through an analysis of these objects and the broader literature on digital animation, this paper critiques the ways these facial recognition systems classify and categorize racial identities in human faces. The paper considers the potential dangers of both racializing logics as part of these systems of classification, and how data regarding emotional expression gathered through these systems might interact with identity-based forms of classification.","In uniting animation with human representation, animoji and memoji share some similarities with the animated Graphical Interface Format (GIF), or animated GIF (Eppink, 2014; Miltner and Highfield, 2017; Stark and Crawford, 2015). GIFs, too, are racialized: in a brilliant essay, Lauren Michele Jackson observes animated reaction GIFs disproportionately represent Black people, and suggests the mobilization of these expressive objects by white users in particular constitutes a form of “digital blackface” (Jackson, 2017). “Black people and black images are thus relied upon to perform a huge amount of emotional labor online on behalf of nonblack users,” Jackson notes. “We are your sass, your nonchalance, your fury, your delight, your annoyance, your happy dance, your diva, your shade, your ‘yaas’ moments. The weight of reaction GIFing, period, rests on our shoulders.”

Compounding this problem are the ways in which animated reaction GIFs are also codified and classified by businesses like GIF archive and purveyor Giphy, a site exemplary of how animated GIFs are now being monetized through more modulated control of the distribution channels which support the format’s circulation online. By categorizing reaction GIFs and suggesting them through its search function and widely-used API, Giphy is also profiting from animation’s racializing logics. The kind of classificatory mechanics a site like Giphy deploys are seemingly innocuous, but by training and habituating users to consider digital animations as everyday media of social and emotional exchange, Apple and other platforms like it risk codifying and reifying animation as a primary rhetoric of emotional self-expression.

Animated GIFs and animoji/memoji share many formal and aesthetic elements as personalizable forms of lively movement used for social purposes online (indeed, it is no surprise there are now many GIF images of animoji characters, some engaged in “animoji karaoke”). And as Jackson points out, users themselves are often guilty of participating in straightforward “digital blackface.” Yet there are formal qualities of the animated GIF that give the format the potential to be re-radicalized.

A format like the GIF was already widely accessible prior to the commercialization of the World Wide Web and its associated technologies, and so is resistant at a technical level to commodification and capture. Anyone can (and does) create an animated GIF, on any subject. In contrast, Apple and other platforms have technical control over the facial data they collect from users. With animoji and memoji, an animated character ventriloquizes a phone’s user directly, drawing on their attention, physiognomy, and sociality to train Apple’s facial recognition system and collect training data regarding the user’s emotional expressions. At a technical level, animoji and memoji are constrained by racializing classificatory logics in ways an animated GIF is not.

Digital animation — from the hearts and confetti which now appear automatically in the iMessage program on all iPhones to the avatars of games and virtual worlds like The Sims and Second Life — are pervasive across digital worlds. Yet by extension, the modes of emotional and affective labor these formats mediate are suffused with racializing logics, both generated and enabled by these technical formats. What makes such labor suffused with racializing difference is entangled with the history and technical affordances of digital animation as an aesthetic form, digital objects simultaneously interoperable and classifiable, open-ended, and recursive.

There is a resistant or emancipatory potential in animation: it comes in part from what Sianne Ngai terms animation’s “reanimation,” and thus subversion, of stereotypical representations, “images that are perversely both dead and alive” [41]. As facial recognition technologies become widespread, it is vital to turn to explicit philosophies of data justice (Hoffmann, 2018) and design justice (Costanza-Chock, 2018) in the world of digital animation as elsewhere. In accord with other theorists of digital discontinuity, heterodoxy, obfuscation, and queerness (Brunton and Nissenbaum, 2011; Cohen, 2012; Gaboury, 2013; Light, 2011), we need further study of these digital forms of emotional expression, in order to produce “new ways of understanding the technologization of the racialized body” [42]. The racializing tendencies of the digital animate can and must be subverted — but that subversion is unlikely to come from an animated panda calling out in the void. End of article","Facial recognition, emotion and race in animated social mediaFacial recognition, animoji, racial identities, emotional expression, racialized logics, digital animation, design justice, data justiceFacial recognition systems are increasingly common components of commercial smart phones such as the iPhone X and the Samsung Galaxy S9. These technologies are also increasingly being put to use in consumer-facing social media video-sharing applications, such as Apple’s animoji and memoji, Facebook Messenger’s masks and filters and Samsung’s AR Emoji. These animations serve as technical phenomena translating moments of affective and emotional expression into mediated socially legible forms. Through an analysis of these objects and the broader literature on digital animation, this paper critiques the ways these facial recognition systems classify and categorize racial identities in human faces. The paper considers the potential dangers of both racializing logics as part of these systems of classification, and how data regarding emotional expression gathered through these systems might interact with identity-based forms of classification.In uniting animation with human representation, animoji and memoji share some similarities with the animated Graphical Interface Format (GIF), or animated GIF (Eppink, 2014; Miltner and Highfield, 2017; Stark and Crawford, 2015). GIFs, too, are racialized: in a brilliant essay, Lauren Michele Jackson observes animated reaction GIFs disproportionately represent Black people, and suggests the mobilization of these expressive objects by white users in particular constitutes a form of “digital blackface” (Jackson, 2017). “Black people and black images are thus relied upon to perform a huge amount of emotional labor online on behalf of nonblack users,” Jackson notes. “We are your sass, your nonchalance, your fury, your delight, your annoyance, your happy dance, your diva, your shade, your ‘yaas’ moments. The weight of reaction GIFing, period, rests on our shoulders.”

Compounding this problem are the ways in which animated reaction GIFs are also codified and classified by businesses like GIF archive and purveyor Giphy, a site exemplary of how animated GIFs are now being monetized through more modulated control of the distribution channels which support the format’s circulation online. By categorizing reaction GIFs and suggesting them through its search function and widely-used API, Giphy is also profiting from animation’s racializing logics. The kind of classificatory mechanics a site like Giphy deploys are seemingly innocuous, but by training and habituating users to consider digital animations as everyday media of social and emotional exchange, Apple and other platforms like it risk codifying and reifying animation as a primary rhetoric of emotional self-expression.

Animated GIFs and animoji/memoji share many formal and aesthetic elements as personalizable forms of lively movement used for social purposes online (indeed, it is no surprise there are now many GIF images of animoji characters, some engaged in “animoji karaoke”). And as Jackson points out, users themselves are often guilty of participating in straightforward “digital blackface.” Yet there are formal qualities of the animated GIF that give the format the potential to be re-radicalized.

A format like the GIF was already widely accessible prior to the commercialization of the World Wide Web and its associated technologies, and so is resistant at a technical level to commodification and capture. Anyone can (and does) create an animated GIF, on any subject. In contrast, Apple and other platforms have technical control over the facial data they collect from users. With animoji and memoji, an animated character ventriloquizes a phone’s user directly, drawing on their attention, physiognomy, and sociality to train Apple’s facial recognition system and collect training data regarding the user’s emotional expressions. At a technical level, animoji and memoji are constrained by racializing classificatory logics in ways an animated GIF is not.

Digital animation — from the hearts and confetti which now appear automatically in the iMessage program on all iPhones to the avatars of games and virtual worlds like The Sims and Second Life — are pervasive across digital worlds. Yet by extension, the modes of emotional and affective labor these formats mediate are suffused with racializing logics, both generated and enabled by these technical formats. What makes such labor suffused with racializing difference is entangled with the history and technical affordances of digital animation as an aesthetic form, digital objects simultaneously interoperable and classifiable, open-ended, and recursive.

There is a resistant or emancipatory potential in animation: it comes in part from what Sianne Ngai terms animation’s “reanimation,” and thus subversion, of stereotypical representations, “images that are perversely both dead and alive” [41]. As facial recognition technologies become widespread, it is vital to turn to explicit philosophies of data justice (Hoffmann, 2018) and design justice (Costanza-Chock, 2018) in the world of digital animation as elsewhere. In accord with other theorists of digital discontinuity, heterodoxy, obfuscation, and queerness (Brunton and Nissenbaum, 2011; Cohen, 2012; Gaboury, 2013; Light, 2011), we need further study of these digital forms of emotional expression, in order to produce “new ways of understanding the technologization of the racialized body” [42]. The racializing tendencies of the digital animate can and must be subverted — but that subversion is unlikely to come from an animated panda calling out in the void. End of articleStimulus sets careon Mid-cartoon otoreoned ‘mig-rotoscoped Time course of a trial Which emotion? | rpisgusted — | + 8: Happy 8:Shocked | 0: Neutral 1000ms Until Responds:Accuracy (%) 100 50 40 30 20 10 Stimulus type by presentation time ~®Cartoon —#-Mid-Cartoon © Rotoscope —#-Mid-Rotoscope Photo 7 33 50 66 Stimulus presentation time (ms)Photo Shocked Neutral Mid- rotoscope Rotoscope| Mid- cartoon Cartoon OO ® @ @: OO® @ @: QOOD@®@ OO ® @@",Person recognition,"The use of facial recognition systems in social media video-sharing applications like animoji and memoji is critiqued in this paper due to the ways in which they classify and categorize racial identities. The paper explores the potential dangers of racializing logics in these systems of classification and how data collected through facial recognition systems may interact with identity-based forms of classification. Animated GIFs and animoji/memoji share many similarities as personalizable forms of lively movement used for social purposes online. However, animated reaction GIFs are also racialized and codified by businesses like Giphy, which profits from animation's racializing logics. The paper argues for explicit philosophies of data justice and design justice in the world of digital animation to subvert the racializing tendencies of the digital animate.",Object and Sentiment Recognition,Stimulus sets careon Mid-cartoon otoreoned ‘mig-rotoscoped Time course of a trial Which emotion? | rpisgusted — | + 8: Happy 8:Shocked | 0: Neutral 1000ms Until Responds:Accuracy (%) 100 50 40 30 20 10 Stimulus type by presentation time ~®Cartoon —#-Mid-Cartoon © Rotoscope —#-Mid-Rotoscope Photo 7 33 50 66 Stimulus presentation time (ms)Photo Shocked Neutral Mid- rotoscope Rotoscope| Mid- cartoon Cartoon OO ® @ @: OO® @ @: QOOD@®@ OO ® @@,Sentiment Analysis
144,"A Text Generation and Prediction System: 
Pre-training on New Corpora Using BERT and 
GPT-2
"," language model; text generation; OpenAI GPT-2;
BERT","Using a given starting word to make a sentence or filling 
in sentences is an important direction of natural language 
processing. From one aspect, it reflects whether the machine can 
have human thinking and creativity. We train the machine for 
specific tasks and then use it in natural language processing, which 
will help solve some sentence generation problems, especially for 
application scenarios such as summary generation, machine 
translation, and automatic question answering. The OpenAI GPT2 and BERT models are currently widely used language models 
for text generation and prediction. There have been many 
experiments to verify the outstanding performance of these two 
models in the field of text generation. This paper will use two new 
corpora to train OpenAI GPT-2 model, used to generate long 
sentences and articles, and finally perform a comparative analysis. 
At the same time, we will use the BERT model to complete the task 
of predicting intermediate words based on the context.","We have pre-trained the model by using different corpora, 
and used the trained model to complete the long sentence 
generation and masked word generation prediction task. The 
former mainly generates sentences by looping down from the 
start word, and the latter is based on the surroundings word to 
generate intermediate words. Through the experimental results, 
we can know that the GPT-2 and BERT models perform very 
well in text generation tasks. However, there are still some shortcomings, such as readability, corpora data, and training 
methods, which may cause generated sentences to be repeated, 
etc. In the future, we will try to find some ways to solve this 
defect.","A Text Generation and Prediction System: 
Pre-training on New Corpora Using BERT and 
GPT-2
 language model; text generation; OpenAI GPT-2;
BERTUsing a given starting word to make a sentence or filling 
in sentences is an important direction of natural language 
processing. From one aspect, it reflects whether the machine can 
have human thinking and creativity. We train the machine for 
specific tasks and then use it in natural language processing, which 
will help solve some sentence generation problems, especially for 
application scenarios such as summary generation, machine 
translation, and automatic question answering. The OpenAI GPT2 and BERT models are currently widely used language models 
for text generation and prediction. There have been many 
experiments to verify the outstanding performance of these two 
models in the field of text generation. This paper will use two new 
corpora to train OpenAI GPT-2 model, used to generate long 
sentences and articles, and finally perform a comparative analysis. 
At the same time, we will use the BERT model to complete the task 
of predicting intermediate words based on the context.We have pre-trained the model by using different corpora, 
and used the trained model to complete the long sentence 
generation and masked word generation prediction task. The 
former mainly generates sentences by looping down from the 
start word, and the latter is based on the surroundings word to 
generate intermediate words. Through the experimental results, 
we can know that the GPT-2 and BERT models perform very 
well in text generation tasks. However, there are still some shortcomings, such as readability, corpora data, and training 
methods, which may cause generated sentences to be repeated, 
etc. In the future, we will try to find some ways to solve this 
defect.",Text generation,"This paper explores the use of OpenAI GPT-2 and BERT models for text generation and prediction, particularly for summary generation, machine translation, and automatic question answering. The authors use two new corpora to pre-train the GPT-2 model and compare its performance to BERT in completing long sentence generation and masked word generation prediction tasks. While both models perform well, there are still some limitations such as readability and training methods. Future work will aim to address these shortcomings.",Natural Language Processing,,Sentiment Analysis
145,Adapting a Language Model for Controlled Affective Text Generation,"affective text generation, state-of-the-art language generation models, emotion as prior, grammatical correctness, flexibility.
","Human use language not just to convey information but also to express their inner feelings and
mental states. In this work, we adapt the state-of-the-art language generation models to generate
affective (emotional) text. We posit a model capable of generating affect-driven and topic focused
sentences without losing grammatical correctness as the affect intensity increases. We propose
to incorporate emotion as prior for the probabilistic state-of-the-art text generation model such
as GPT-2. The model gives a user the flexibility to control the category and intensity of emotion
as well as the topic of the generated text. Previous attempts at modelling fine-grained emotions
fall out on grammatical correctness at extreme intensities, but our model is resilient to this and
delivers robust results at all intensities. We conduct automated evaluations and human studies to
test the performance of our model, and provide a detailed comparison of the results with other
models. In all evaluations, our model outperforms existing affective text generation models.","In this paper, we present a novel model for affective text generation. The model augments SOTA text
generation models with affective information without compromising on the grammatical correctness.
As shown empirically, our model is able to successfully incorporate the chosen emotion category and
intensity, as well as the topic and does not fall out on grammatical correctness (unlike other models) at
high intensities. It also provides the flexibility for generating sentences in 8 emotion categories, which
is absent in all the existing models in related domains. Our model has wide applications in fields like
dialogue generation, therapy chatbots, story and advertisement completion.
","Adapting a Language Model for Controlled Affective Text Generationaffective text generation, state-of-the-art language generation models, emotion as prior, grammatical correctness, flexibility.
Human use language not just to convey information but also to express their inner feelings and
mental states. In this work, we adapt the state-of-the-art language generation models to generate
affective (emotional) text. We posit a model capable of generating affect-driven and topic focused
sentences without losing grammatical correctness as the affect intensity increases. We propose
to incorporate emotion as prior for the probabilistic state-of-the-art text generation model such
as GPT-2. The model gives a user the flexibility to control the category and intensity of emotion
as well as the topic of the generated text. Previous attempts at modelling fine-grained emotions
fall out on grammatical correctness at extreme intensities, but our model is resilient to this and
delivers robust results at all intensities. We conduct automated evaluations and human studies to
test the performance of our model, and provide a detailed comparison of the results with other
models. In all evaluations, our model outperforms existing affective text generation models.In this paper, we present a novel model for affective text generation. The model augments SOTA text
generation models with affective information without compromising on the grammatical correctness.
As shown empirically, our model is able to successfully incorporate the chosen emotion category and
intensity, as well as the topic and does not fall out on grammatical correctness (unlike other models) at
high intensities. It also provides the flexibility for generating sentences in 8 emotion categories, which
is absent in all the existing models in related domains. Our model has wide applications in fields like
dialogue generation, therapy chatbots, story and advertisement completion.
",Text generation,"This paper presents a novel model for affective text generation that incorporates emotion as a prior for the state-of-the-art text generation model. The model generates affect-driven and topic-focused sentences without compromising on grammatical correctness, and provides flexibility in controlling the category and intensity of the emotion. The model outperforms existing affective text generation models in automated evaluations and human studies. The model has wide applications in dialogue generation, therapy chatbots, story and advertisement completion.",Natural Language Processing,,Deep Learning and Machine Learning
146,NUBIA: NeUral Based Interchangeability Assessor for Text Generation,"NUBIA, automatic evaluation metrics, text generation, machine learning, neural feature extractor.","We present NUBIA, a methodology to build
automatic evaluation metrics for text generation using only machine learning models as
core components. A typical NUBIA model
is composed of three modules: a neural feature extractor, an aggregator and a calibrator.
We demonstrate an implementation of NUBIA
which outperforms metrics currently used to
evaluate machine translation, summaries and
slightly exceeds/matches state of the art metrics on correlation with human judgement on
the WMT segment-level Direct Assessment
task, sentence-level ranking and image captioning evaluation. The model implemented is
modular, explainable and set to continuously
improve over time.","In this work, we introduced NUBIA: a methodology to build automatic evaluation metrics for text
generation using machine learning models as core
components. This methodology achieves state-ofthe-art results across evaluation of machine translation and image captioning strongly building on
the successes of recent NLP architectures such as
RoBERTa and GPT-2. These strong results suggest
that using a neural networks to extract features and
combine them will be a key component of building
future automatic scoring metrics for text generation
with the promise of unifying evaluation of image
caption, machine translation and potentially other
text generation tasks.","NUBIA: NeUral Based Interchangeability Assessor for Text GenerationNUBIA, automatic evaluation metrics, text generation, machine learning, neural feature extractor.We present NUBIA, a methodology to build
automatic evaluation metrics for text generation using only machine learning models as
core components. A typical NUBIA model
is composed of three modules: a neural feature extractor, an aggregator and a calibrator.
We demonstrate an implementation of NUBIA
which outperforms metrics currently used to
evaluate machine translation, summaries and
slightly exceeds/matches state of the art metrics on correlation with human judgement on
the WMT segment-level Direct Assessment
task, sentence-level ranking and image captioning evaluation. The model implemented is
modular, explainable and set to continuously
improve over time.In this work, we introduced NUBIA: a methodology to build automatic evaluation metrics for text
generation using machine learning models as core
components. This methodology achieves state-ofthe-art results across evaluation of machine translation and image captioning strongly building on
the successes of recent NLP architectures such as
RoBERTa and GPT-2. These strong results suggest
that using a neural networks to extract features and
combine them will be a key component of building
future automatic scoring metrics for text generation
with the promise of unifying evaluation of image
caption, machine translation and potentially other
text generation tasks.Joy, Top Once upon a time, Episode 2, “Danger Room” (Season 6, \ episode 3), directed by Bryan Singer and... ,/ 0.6, Alfoct Joy, Topics Technolo, ‘ Qnce upon atime, } there was a.game that was so great 1.0, Acts Joy, Topic OMPT: / ‘Once upon a time, ‘ PR PT: Once upon \ there was a game that was so fun so much fun ! \..- mean so much FUN, I'm not exaggerating,’ Topics Legal Religion | Military Science Politics Space Monsters Technology| Topic BOW",Text generation,"NUBIA is a methodology for building automatic evaluation metrics for text generation using machine learning models. It consists of three modules: a neural feature extractor, an aggregator, and a calibrator. NUBIA outperforms existing metrics in machine translation and image captioning and matches state-of-the-art metrics in correlation with human judgement. It is modular, explainable, and has potential for unifying evaluation of various text generation tasks.",Natural Language Processing,"Joy, Top Once upon a time, Episode 2, “Danger Room” (Season 6, \ episode 3), directed by Bryan Singer and... ,/ 0.6, Alfoct Joy, Topics Technolo, ‘ Qnce upon atime, } there was a.game that was so great 1.0, Acts Joy, Topic OMPT: / ‘Once upon a time, ‘ PR PT: Once upon \ there was a game that was so fun so much fun ! \..- mean so much FUN, I'm not exaggerating,’ Topics Legal Religion | Military Science Politics Space Monsters Technology| Topic BOW",Deep Learning and Machine Learning
147,"Abstractive Text Summarization Using Hybrid 
Technique of Summarization","abstractive summarization, BERT2BERT, ParsBERT, Seq-to-Seq","Abstractive text summarization focuses at 
compress the document into a shorter form while keeping the 
connotation intact. The extractive summary can select chunks of 
sentences that are very related to the document, on the other hand, 
an abstractive summary can generate a summary based on 
extracted keywords. This research proposed an abstractive text 
summarization model, it gets data from source data (e.g., Daily 
Mail/CNN) or other documents and two summaries of this are 
generated. one from a philologist and the other by proposed model. 
The summary generated by the philologist kept as a model to 
compare with the machine-generated summary. The proposed 
model increased the accuracy and the readability of the summary. ","This research focused on the generation of single 
paper/document-based abstractive summarization. The main 
purpose of this study is to develop a deep learning model, the
proposed model's working is compared with two most common 
used machine learning algorithms: logistic regression (LR) and 
support vector machine (SVM) .The findings reveal that 
proposed deep learning model do better than the other two 
models in conditions of overall performance. The F1-measure 
approach was used to achieve this.
This research will expand the scope of the present procedure 
by training additional positions and bigger models at 
WebScaleText Corpora.","Abstractive Text Summarization Using Hybrid 
Technique of Summarizationabstractive summarization, BERT2BERT, ParsBERT, Seq-to-SeqAbstractive text summarization focuses at 
compress the document into a shorter form while keeping the 
connotation intact. The extractive summary can select chunks of 
sentences that are very related to the document, on the other hand, 
an abstractive summary can generate a summary based on 
extracted keywords. This research proposed an abstractive text 
summarization model, it gets data from source data (e.g., Daily 
Mail/CNN) or other documents and two summaries of this are 
generated. one from a philologist and the other by proposed model. 
The summary generated by the philologist kept as a model to 
compare with the machine-generated summary. The proposed 
model increased the accuracy and the readability of the summary. This research focused on the generation of single 
paper/document-based abstractive summarization. The main 
purpose of this study is to develop a deep learning model, the
proposed model's working is compared with two most common 
used machine learning algorithms: logistic regression (LR) and 
support vector machine (SVM) .The findings reveal that 
proposed deep learning model do better than the other two 
models in conditions of overall performance. The F1-measure 
approach was used to achieve this.
This research will expand the scope of the present procedure 
by training additional positions and bigger models at 
WebScaleText Corpora.| RoBERTa_STS (Reference moar) | RoBERTa_MNLI | Fully Connected >| Calibration Predicted Quality Score| Candidate “ne ) Neural Network [| or Candidate Sentence GPT-2",Text summarization,"This research proposes an abstractive text summarization model that generates summaries based on extracted keywords, and compares it with a summary generated by a philologist. The proposed model increases the accuracy and readability of the summary compared to the human-generated summary. The study develops a deep learning model and compares it with logistic regression and support vector machine algorithms, finding that the proposed model outperforms the other two models in overall performance using F1-measure. The research aims to expand the scope of the procedure by training additional positions and larger models at WebScaleText Corpora.",Natural Language Processing,| RoBERTa_STS (Reference moar) | RoBERTa_MNLI | Fully Connected >| Calibration Predicted Quality Score| Candidate “ne ) Neural Network [| or Candidate Sentence GPT-2,Deep Learning and Machine Learning
148,Discourse-Aware Neural Extractive Text Summarization,"DISCOBERT, discourse unit, summarization, structural discourse graphs, Graph Convolutional Networks.","Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based
extractive models often result in redundant
or uninformative phrases in the extracted
summaries. Also, long-range dependencies
throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address
these issues, we present a discourse-aware
neural summarization model - DISCOBERT1
.
DISCOBERT extracts sub-sentential discourse
units (instead of sentences) as candidates for
extractive selection on a finer granularity. To
capture the long-range dependencies among
discourse units, structural discourse graphs are
constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the
proposed model outperforms state-of-the-art
methods by a significant margin on popular
summarization benchmarks compared to other
BERT-base models.
","In this paper, we present DISCOBERT, which uses
discourse unit as the minimal selection basis to
reduce summarization redundancy and leverages
two types of discourse graphs as inductive bias to
capture long-range dependencies among discourse
units. We validate the proposed approach on two
popular summarization datasets, and observe consistent improvement over baseline models. For
future work, we will explore better graph encoding
methods, and apply discourse graphs to other tasks
that require long document encoding.
","Discourse-Aware Neural Extractive Text SummarizationDISCOBERT, discourse unit, summarization, structural discourse graphs, Graph Convolutional Networks.Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based
extractive models often result in redundant
or uninformative phrases in the extracted
summaries. Also, long-range dependencies
throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address
these issues, we present a discourse-aware
neural summarization model - DISCOBERT1
.
DISCOBERT extracts sub-sentential discourse
units (instead of sentences) as candidates for
extractive selection on a finer granularity. To
capture the long-range dependencies among
discourse units, structural discourse graphs are
constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the
proposed model outperforms state-of-the-art
methods by a significant margin on popular
summarization benchmarks compared to other
BERT-base models.
In this paper, we present DISCOBERT, which uses
discourse unit as the minimal selection basis to
reduce summarization redundancy and leverages
two types of discourse graphs as inductive bias to
capture long-range dependencies among discourse
units. We validate the proposed approach on two
popular summarization datasets, and observe consistent improvement over baseline models. For
future work, we will explore better graph encoding
methods, and apply discourse graphs to other tasks
that require long document encoding.
wane @ ©) @) & @ @ © @ @ @ @ & Tents [Bas [Es [Ene] Es fs) [Esa] [Ene) Ese) fc) [En] Ee] [Es] + ig CR) DC) CC) CC) postion embeddings CE: ][e ][ ] Ce) Ce) Ce] Ce) Ce) ee) [ee] [es] Le)Source Document Segmentation — Stop words Preprocessing K—, Removal Extractive Summary Key Word Selection Summary Generation Xe Summary Very Short Rich Information",Text summarization,"The paper proposes DISCOBERT, a discourse-aware neural summarization model that uses sub-sentential discourse units as selection basis to reduce redundancy in summaries and captures long-range dependencies using structural discourse graphs encoded with Graph Convolutional Networks. DISCOBERT outperforms state-of-the-art summarization models on popular benchmarks. Future work includes exploring better graph encoding methods and applying discourse graphs to other long document encoding tasks.",Natural Language Processing,"wane @ ©) @) & @ @ © @ @ @ @ & Tents [Bas [Es [Ene] Es fs) [Esa] [Ene) Ese) fc) [En] Ee] [Es] + ig CR) DC) CC) CC) postion embeddings CE: ][e ][ ] Ce) Ce) Ce] Ce) Ce) ee) [ee] [es] Le)Source Document Segmentation — Stop words Preprocessing K—, Removal Extractive Summary Key Word Selection Summary Generation Xe Summary Very Short Rich Information",Deep Learning and Machine Learning
149,Collecting emotional animated GIFs with clustered multi-task learning,"Emotion recognition
,
Visualization
,
Videos
,
Internet
,
Standards
,
Affective computing
,
Pipelines","Animated GIFs are widely used on the Internet to express emotions, but their automatic analysis is largely unexplored. Existing GIF datasets with emotion labels are too small for training contemporary machine learning models, so we propose a semi-automatic method to collect emotional animated GIFs from the Internet with the least amount of human labor. The method trains weak emotion recognizers on labeled data, and uses them to sort a large quantity of unlabeled GIFs. We found that by exploiting the clustered structure of emotions, the number of GIFs a labeler needs to check can be greatly reduced. Using the proposed method, a dataset called GIFGIF+ with 23,544 GIFs over 17 emotions was created, which provides a promising platform for affective computing research.","The article discusses how Animated GIFs are commonly used to express emotions on the Internet but their automatic analysis is largely unexplored due to small existing datasets with emotion labels. The authors propose a semi-automatic method to collect emotional animated GIFs from the internet with minimal human labor, which involves training weak emotion recognizers on labeled data and using them to sort through a large quantity of unlabeled GIFs. By exploiting the clustered structure of emotions, the number of GIFs a labeler needs to check can be greatly reduced. This method resulted in the creation of a new dataset called GIFGIF+ with 23,544 GIFs over 17 emotions, providing a promising platform for affective computing research.","Collecting emotional animated GIFs with clustered multi-task learningEmotion recognition
,
Visualization
,
Videos
,
Internet
,
Standards
,
Affective computing
,
PipelinesAnimated GIFs are widely used on the Internet to express emotions, but their automatic analysis is largely unexplored. Existing GIF datasets with emotion labels are too small for training contemporary machine learning models, so we propose a semi-automatic method to collect emotional animated GIFs from the Internet with the least amount of human labor. The method trains weak emotion recognizers on labeled data, and uses them to sort a large quantity of unlabeled GIFs. We found that by exploiting the clustered structure of emotions, the number of GIFs a labeler needs to check can be greatly reduced. Using the proposed method, a dataset called GIFGIF+ with 23,544 GIFs over 17 emotions was created, which provides a promising platform for affective computing research.The article discusses how Animated GIFs are commonly used to express emotions on the Internet but their automatic analysis is largely unexplored due to small existing datasets with emotion labels. The authors propose a semi-automatic method to collect emotional animated GIFs from the internet with minimal human labor, which involves training weak emotion recognizers on labeled data and using them to sort through a large quantity of unlabeled GIFs. By exploiting the clustered structure of emotions, the number of GIFs a labeler needs to check can be greatly reduced. This method resulted in the creation of a new dataset called GIFGIF+ with 23,544 GIFs over 17 emotions, providing a promising platform for affective computing research.Label 1 0 1 oO Prediction y Ms Ms t t t C Stacked Discourse Graph Encoders ) } ‘And today, the Pulitzer",Deep Learning and Machine Learning,"The article proposes a semi-automatic method for collecting emotional animated GIFs from the internet with minimal human labor, in order to create a larger dataset for machine learning models. The method involves training weak emotion recognizers on labeled data and using them to sort through a large quantity of unlabeled GIFs, while exploiting the clustered structure of emotions to reduce the number of GIFs a labeler needs to check. This resulted in the creation of a new dataset called GIFGIF+ with 23,544 GIFs over 17 emotions, providing a promising platform for affective computing research",Object and Sentiment Recognition,"Label 1 0 1 oO Prediction y Ms Ms t t t C Stacked Discourse Graph Encoders ) } ‘And today, the Pulitzer",Deep Learning and Machine Learning
150,"Iconic faces are not real faces: enhanced
emotion detection and altered neural
processing as faces become more iconic","Iconic faces, Face perception, Emotion, Expressions, P1, Event-related potentials","Iconic representations are ubiquitous; they fill children’s cartoons, add humor to newspapers, and bring emotional
tone to online communication. Yet, the communicative function they serve remains unaddressed by cognitive
psychology. Here, we examined the hypothesis that iconic representations communicate emotional information
more efficiently than their realistic counterparts. In Experiment 1, we manipulated low-level features of emotional
faces to create five sets of stimuli that ranged from photorealistic to fully iconic. Participants identified emotions on
briefly presented faces. Results showed that, at short presentation times, accuracy for identifying emotion on more
“cartoonized” images was enhanced. In addition, increasing contrast and decreasing featural complexity benefited
accuracy. In Experiment 2, we examined an event-related potential component, the P1, which is sensitive to low-level
visual stimulus features. Lower levels of contrast and complexity within schematic stimuli were also associated with
lower P1 amplitudes. These findings support the hypothesis that iconic representations differ from realistic images in
their ability to communicate specific information, including emotion, quickly and efficiently, and that this effect is
driven by changes in low-level visual features in the stimuli.","Iconic faces can be viewed either as analogous to realistic
images or as a distinct class of stimulus. Our findings
support the view that iconic representations serve a distinct
role – to impart specific information quickly and
efficiently – and highlight the advantages of simplifying
image features and increasing contrast to communicate
emotion. In addition, our data suggest that the effects of
iconization may not be specific to faces, but rather to
any stimulus that has these low-level featural changes. It
is thus important to consider that such features are not
just potential low-level confounds but contribute to specific
communicative functions. However, it is unknown if the discrimination of more subtle real-world types of
emotional expression would also benefit from iconic
representation (e.g., the ‘Duchenne’ smile, where genuine
happiness is expressed with the wrinkling of the corners
of the eyes) (Ekman, Davidson, & Friesen, 1990). It may
be that iconic images have a communicative advantage
only for simple visual information, a hypothesis that
invites future research.
The effective communicative role of iconic images may
underlie the ubiquity and popularity of iconic imagery and
cartoons in popular culture. Better understanding of the
factors that enhance their communicative role may help
improve their use in various real-world applications such
as emoticons, signs, and concept cartoons. We suggest
that the communicative role of iconic imagery is an important
area for further research, and its power would be
better exploited than ignored.","Iconic faces are not real faces: enhanced
emotion detection and altered neural
processing as faces become more iconicIconic faces, Face perception, Emotion, Expressions, P1, Event-related potentialsIconic representations are ubiquitous; they fill children’s cartoons, add humor to newspapers, and bring emotional
tone to online communication. Yet, the communicative function they serve remains unaddressed by cognitive
psychology. Here, we examined the hypothesis that iconic representations communicate emotional information
more efficiently than their realistic counterparts. In Experiment 1, we manipulated low-level features of emotional
faces to create five sets of stimuli that ranged from photorealistic to fully iconic. Participants identified emotions on
briefly presented faces. Results showed that, at short presentation times, accuracy for identifying emotion on more
“cartoonized” images was enhanced. In addition, increasing contrast and decreasing featural complexity benefited
accuracy. In Experiment 2, we examined an event-related potential component, the P1, which is sensitive to low-level
visual stimulus features. Lower levels of contrast and complexity within schematic stimuli were also associated with
lower P1 amplitudes. These findings support the hypothesis that iconic representations differ from realistic images in
their ability to communicate specific information, including emotion, quickly and efficiently, and that this effect is
driven by changes in low-level visual features in the stimuli.Iconic faces can be viewed either as analogous to realistic
images or as a distinct class of stimulus. Our findings
support the view that iconic representations serve a distinct
role – to impart specific information quickly and
efficiently – and highlight the advantages of simplifying
image features and increasing contrast to communicate
emotion. In addition, our data suggest that the effects of
iconization may not be specific to faces, but rather to
any stimulus that has these low-level featural changes. It
is thus important to consider that such features are not
just potential low-level confounds but contribute to specific
communicative functions. However, it is unknown if the discrimination of more subtle real-world types of
emotional expression would also benefit from iconic
representation (e.g., the ‘Duchenne’ smile, where genuine
happiness is expressed with the wrinkling of the corners
of the eyes) (Ekman, Davidson, & Friesen, 1990). It may
be that iconic images have a communicative advantage
only for simple visual information, a hypothesis that
invites future research.
The effective communicative role of iconic images may
underlie the ubiquity and popularity of iconic imagery and
cartoons in popular culture. Better understanding of the
factors that enhance their communicative role may help
improve their use in various real-world applications such
as emoticons, signs, and concept cartoons. We suggest
that the communicative role of iconic imagery is an important
area for further research, and its power would be
better exploited than ignored.",Deep Learning and Machine Learning,"The article explores the idea that iconic representations, such as cartoons, may be more efficient at communicating emotional information than realistic representations. The authors conducted two experiments, one measuring accuracy in identifying emotions on cartoonized images versus realistic images, and the other measuring event-related potentials in response to low-level visual features in schematic stimuli. The results support the hypothesis that iconic representations communicate specific information, including emotion, quickly and efficiently, and that this effect is driven by changes in low-level visual features in the stimuli. The authors suggest that the effective communicative role of iconic images may underlie their ubiquity and popularity in popular culture, and that further research is needed to better understand their communicative function and improve their use in various real-world applications.",Object and Sentiment Recognition,,Sentiment Analysis
151,"Collecting Large, Richly Annotated
Facial-Expression Databases from Movies","Facial expression recognition, large scale database, real-world conditions, emotion database","Creating large, richly annotated databases depicting real-world or simulated real-world conditions is
a challenging task. There has been a long understood need for recognition of human facial expressions in
realistic video scenarios. Although many expression databases are available, research has been restrained
by their limited scope due to their ‘lab controlled’ recording environment. This paper proposes a new
temporal facial expression database Acted Facial Expressions in the Wild (AFEW) and its static subset
Static Facial Expressions in the Wild (SFEW), extracted from movies. As creating databases is time
consuming and complex, a novel semi-automatic approach via a recommender system based on subtitles
is proposed. Further, experimental protocols based on varying levels of person dependency are defined.
AFEW is compared with the extended Cohn-Kanade CK+ database and SFEW with JAFFE and Multi-PIE
databases","Collecting richly annotated, large datasets representing real-world conditions is a non-trivial task. To
address this problem, we have collected two new facial expression databases derived from movies via a
semi-automatic recommender based method. The database contains videos showing natural head poses
and movements, close to real-world illumination, multiple subjects in the same frame, a large age range,
occlusions and searchable metadata. The datasets also cover toddler, child and teenager subjects, which are missing in other currently available temporal facial expression databases. AFEW also contains clips
with multiple subjects exhibiting similar or different expressions with respect to the scene and each other.
This will enable research on the effect of context/scene on human facial expressions. We compared our
AFEW database with the CK+ database using state-of-the-art descriptors and SFEW with the Multi-PIE
and JAFFE databases. We believe that these datasets will enable novel contributions to the advancement
of facial expression research and act as a benchmark for experimental validation of facial expression
analysis algorithms in real-world environments.

","Collecting Large, Richly Annotated
Facial-Expression Databases from MoviesFacial expression recognition, large scale database, real-world conditions, emotion databaseCreating large, richly annotated databases depicting real-world or simulated real-world conditions is
a challenging task. There has been a long understood need for recognition of human facial expressions in
realistic video scenarios. Although many expression databases are available, research has been restrained
by their limited scope due to their ‘lab controlled’ recording environment. This paper proposes a new
temporal facial expression database Acted Facial Expressions in the Wild (AFEW) and its static subset
Static Facial Expressions in the Wild (SFEW), extracted from movies. As creating databases is time
consuming and complex, a novel semi-automatic approach via a recommender system based on subtitles
is proposed. Further, experimental protocols based on varying levels of person dependency are defined.
AFEW is compared with the extended Cohn-Kanade CK+ database and SFEW with JAFFE and Multi-PIE
databasesCollecting richly annotated, large datasets representing real-world conditions is a non-trivial task. To
address this problem, we have collected two new facial expression databases derived from movies via a
semi-automatic recommender based method. The database contains videos showing natural head poses
and movements, close to real-world illumination, multiple subjects in the same frame, a large age range,
occlusions and searchable metadata. The datasets also cover toddler, child and teenager subjects, which are missing in other currently available temporal facial expression databases. AFEW also contains clips
with multiple subjects exhibiting similar or different expressions with respect to the scene and each other.
This will enable research on the effect of context/scene on human facial expressions. We compared our
AFEW database with the CK+ database using state-of-the-art descriptors and SFEW with the Multi-PIE
and JAFFE databases. We believe that these datasets will enable novel contributions to the advancement
of facial expression research and act as a benchmark for experimental validation of facial expression
analysis algorithms in real-world environments.

Photo Shocked Neutral Mid- rotoscope Rotoscope| Mid- cartoon Cartoon OO ® @ @: OO® @ @: QOOD@®@ OO ® @@Stimulus sets careon Mid-cartoon otoreoned ‘mig-rotoscoped Time course of a trial Which emotion? | rpisgusted — | + 8: Happy 8:Shocked | 0: Neutral 1000ms Until Responds:Accuracy (%) 100 50 40 30 20 10 Stimulus type by presentation time ~®Cartoon —#-Mid-Cartoon © Rotoscope —#-Mid-Rotoscope Photo 7 33 50 66 Stimulus presentation time (ms)",Deep Learning and Machine Learning,"The paper proposes a new facial expression database, Acted Facial Expressions in the Wild (AFEW), and its static subset, Static Facial Expressions in the Wild (SFEW), which are extracted from movies to provide a more realistic representation of human facial expressions. The database is created using a semi-automatic recommender system based on subtitles to make the process less time-consuming and complex. AFEW includes natural head poses, close to real-world illumination, multiple subjects, a large age range, and searchable metadata. It covers toddler, child, and teenager subjects, which are missing in other temporal facial expression databases. The paper compares AFEW with the extended Cohn-Kanade CK+ database and SFEW with JAFFE and Multi-PIE databases. The authors believe that these datasets will enable novel contributions to facial expression research and act as a benchmark for experimental validation of facial expression analysis algorithms in real-world environments.





",Object and Sentiment Recognition,Photo Shocked Neutral Mid- rotoscope Rotoscope| Mid- cartoon Cartoon OO ® @ @: OO® @ @: QOOD@®@ OO ® @@Stimulus sets careon Mid-cartoon otoreoned ‘mig-rotoscoped Time course of a trial Which emotion? | rpisgusted — | + 8: Happy 8:Shocked | 0: Neutral 1000ms Until Responds:Accuracy (%) 100 50 40 30 20 10 Stimulus type by presentation time ~®Cartoon —#-Mid-Cartoon © Rotoscope —#-Mid-Rotoscope Photo 7 33 50 66 Stimulus presentation time (ms),Sentiment Analysis
152,Gabor-Based Kernel Partial-Least-Squares Discrimination Features for Face Recognition,"Gabor features, Kernel partial-least-squares, face recognition, XM2VTS database, ORL database","The paper presents a novel method for the extraction of facial features based on the Gabor-wavelet representation of face images and the kernel partial-least-squares discrimination (KPLSD) algorithm. The proposed feature-extraction method, called the Gabor-based kernel partial-least-squares discrimination (GKPLSD), is performed in two consecutive steps. In the first step a set of forty Gabor wavelets is used to extract discriminative and robust facial features, while in the second step the kernel partial-least-squares discrimination technique is used to reduce the dimensionality of the Gabor feature vector and to further enhance its discriminatory power. For optimal performance, the KPLSD-based transformation is implemented using the recently proposed fractional-power-polynomial models. The experimental results based on the XM2VTS and ORL databases show that the GKPLSD approach outperforms feature-extraction methods such as principal component analysis (PCA), linear discriminant analysis (LDA), kernel principal component analysis (KPCA) or generalized discriminant analysis (GDA) as well as combinations of these methods with Gabor representations of the face images. Furthermore, as the KPLSD algorithm is derived from the kernel partial-least-squares regression (KPLSR) model it does not suffer from the small-sample-size problem, which is regularly encountered in the field of face recognition.","The paper proposes a new method called Gabor-based kernel partial-least-squares discrimination (GKPLSD) for extracting facial features from face images using Gabor wavelets and kernel partial-least-squares discrimination technique. The proposed method outperforms other feature-extraction methods such as PCA, LDA, KPCA, and GDA as well as their combinations with Gabor representations of face images. Additionally, the KPLSD algorithm used in the method does not suffer from the small-sample-size problem commonly encountered in face recognition. The experimental results based on the XM2VTS and ORL databases confirm the effectiveness of the proposed method.","Gabor-Based Kernel Partial-Least-Squares Discrimination Features for Face RecognitionGabor features, Kernel partial-least-squares, face recognition, XM2VTS database, ORL databaseThe paper presents a novel method for the extraction of facial features based on the Gabor-wavelet representation of face images and the kernel partial-least-squares discrimination (KPLSD) algorithm. The proposed feature-extraction method, called the Gabor-based kernel partial-least-squares discrimination (GKPLSD), is performed in two consecutive steps. In the first step a set of forty Gabor wavelets is used to extract discriminative and robust facial features, while in the second step the kernel partial-least-squares discrimination technique is used to reduce the dimensionality of the Gabor feature vector and to further enhance its discriminatory power. For optimal performance, the KPLSD-based transformation is implemented using the recently proposed fractional-power-polynomial models. The experimental results based on the XM2VTS and ORL databases show that the GKPLSD approach outperforms feature-extraction methods such as principal component analysis (PCA), linear discriminant analysis (LDA), kernel principal component analysis (KPCA) or generalized discriminant analysis (GDA) as well as combinations of these methods with Gabor representations of the face images. Furthermore, as the KPLSD algorithm is derived from the kernel partial-least-squares regression (KPLSR) model it does not suffer from the small-sample-size problem, which is regularly encountered in the field of face recognition.The paper proposes a new method called Gabor-based kernel partial-least-squares discrimination (GKPLSD) for extracting facial features from face images using Gabor wavelets and kernel partial-least-squares discrimination technique. The proposed method outperforms other feature-extraction methods such as PCA, LDA, KPCA, and GDA as well as their combinations with Gabor representations of face images. Additionally, the KPLSD algorithm used in the method does not suffer from the small-sample-size problem commonly encountered in face recognition. The experimental results based on the XM2VTS and ORL databases confirm the effectiveness of the proposed method.",Person recognition,"The paper presents the Gabor-based kernel partial-least-squares discrimination (GKPLSD) method for extracting facial features from face images, which involves using Gabor wavelets and kernel partial-least-squares discrimination technique. The GKPLSD approach outperforms other feature-extraction methods such as PCA, LDA, KPCA, and GDA, as well as their combinations with Gabor representations of face images. The KPLSD algorithm used in the method does not suffer from the small-sample-size problem commonly encountered in face recognition. Experimental results based on the XM2VTS and ORL databases confirm the effectiveness of the proposed method.",Object and Sentiment Recognition,,Sentiment Analysis
153,"EMOTION DETECTION FROM SPEECH TO ENRICH
MULTIMEDIA CONTENT","emotion detection, speech, avatars, virtual characters, SVM, KNN, NN, magnitude recognition.","The paper describes an experimental study on the detection of
emotion from speech. As computer based characters such as
avatars and virtual chat faces become more common, the use of
emotion to drive the expression of the virtual characters become
more important. The study utilizes a corpus containing emotional
speech with 721 short utterances expressing four emotions: anger,
happiness, sadness, and the neutral (unemotional) state, which
were captured manually from movies and teleplays. We introduce
a new concept to evaluate emotions in speech. Emotions are so
complex that most speech sentences cannot be precisely assigned
into a particular emotion category; however, most emotional
states nevertheless can be described as a mixture of multiple
emotions. Based on this concept we have trained SVMs (support
vector machines) to recognize utterances within these four
categories and developed an agent that can recognize and express
emotions.","Compared with KNN, training an SVM model gives a good
classifier without needing much training time. Even if we do not
know the exact pertinences between each feature, we still can
obtain good results. After we produce the SVM model by training
from training data sets, these training data sets are no longer needed since the SVM model contains all the useful information.
So classification does not need much time, and almost can be
applied within real-time rendering. The KNN rule relies on a
distance metric to perform classification, it is expected that
changing this metric will yield different and possibly better results.
Intuitively, one should weigh each feature according to how well
it correlates with the correct classification. But in our
investigation, those features are not irrelevant to each other. The
performance landscape in this metric space is quite rugged and
optimization is likely expensive. SVM can handle this problem
well. We need not know the relationships within each feature pair
and the dimensionality of each feature.
Compared with NNs, training a SVM model will require
much less time than training an NN classifier. And SVM is much
more robust than NN. In our application, the corpus is coming
from movies and teleplays. There are many speakers with various
backgrounds. In this kind of instance NNs do not work well.
The most important reason why we chose SVMs is that SVMs
give a magnitude for recognition. We need this magnitude for
synthesizing expressions with different degrees. For our future
work, we plan to study the effectiveness of our current approach
on data from different languages and cultures.","EMOTION DETECTION FROM SPEECH TO ENRICH
MULTIMEDIA CONTENTemotion detection, speech, avatars, virtual characters, SVM, KNN, NN, magnitude recognition.The paper describes an experimental study on the detection of
emotion from speech. As computer based characters such as
avatars and virtual chat faces become more common, the use of
emotion to drive the expression of the virtual characters become
more important. The study utilizes a corpus containing emotional
speech with 721 short utterances expressing four emotions: anger,
happiness, sadness, and the neutral (unemotional) state, which
were captured manually from movies and teleplays. We introduce
a new concept to evaluate emotions in speech. Emotions are so
complex that most speech sentences cannot be precisely assigned
into a particular emotion category; however, most emotional
states nevertheless can be described as a mixture of multiple
emotions. Based on this concept we have trained SVMs (support
vector machines) to recognize utterances within these four
categories and developed an agent that can recognize and express
emotions.Compared with KNN, training an SVM model gives a good
classifier without needing much training time. Even if we do not
know the exact pertinences between each feature, we still can
obtain good results. After we produce the SVM model by training
from training data sets, these training data sets are no longer needed since the SVM model contains all the useful information.
So classification does not need much time, and almost can be
applied within real-time rendering. The KNN rule relies on a
distance metric to perform classification, it is expected that
changing this metric will yield different and possibly better results.
Intuitively, one should weigh each feature according to how well
it correlates with the correct classification. But in our
investigation, those features are not irrelevant to each other. The
performance landscape in this metric space is quite rugged and
optimization is likely expensive. SVM can handle this problem
well. We need not know the relationships within each feature pair
and the dimensionality of each feature.
Compared with NNs, training a SVM model will require
much less time than training an NN classifier. And SVM is much
more robust than NN. In our application, the corpus is coming
from movies and teleplays. There are many speakers with various
backgrounds. In this kind of instance NNs do not work well.
The most important reason why we chose SVMs is that SVMs
give a magnitude for recognition. We need this magnitude for
synthesizing expressions with different degrees. For our future
work, we plan to study the effectiveness of our current approach
on data from different languages and cultures.",Natural Language Processing,"The paper presents an experimental study on detecting emotions from speech for use in driving the expression of computer-based characters such as avatars and virtual chat faces. The study uses a corpus of 721 short utterances expressing four emotions: anger, happiness, sadness, and neutrality. The authors introduce a new concept of emotions as a mixture of multiple emotions and train support vector machines (SVMs) to recognize these four categories. Compared to KNN and NN, SVMs require less training time and are more robust, making them suitable for complex emotional data. The SVM model contains all useful information, and classification can be done in real-time rendering. The authors plan to extend their approach to data from different languages and cultures in future work.",Object and Sentiment Recognition,,Object Recognition
154,Emotion Detection for Social Robots Based on NLP Transformers and an Emotion Ontology,social robots; natural language processing; ontology; emotion detection; text classification,"For social robots, knowledge regarding human emotional states is an essential part of adapting their behavior or associating emotions to other entities. Robots gather the information from which emotion detection is processed via different media, such as text, speech, images, or videos. The multimedia content is then properly processed to recognize emotions/sentiments, for example, by analyzing faces and postures in images/videos based on machine learning techniques or by converting speech into text to perform emotion detection with natural language processing (NLP) techniques. Keeping this information in semantic repositories offers a wide range of possibilities for implementing smart applications. We propose a framework to allow social robots to detect emotions and to store this information in a semantic repository, based on EMONTO (an EMotion ONTOlogy), and in the first figure or table caption. Please define if appropriate. an ontology to represent emotions. As a proof-of-concept, we develop a first version of this framework focused on emotion detection in text, which can be obtained directly as text or by converting speech to text. We tested the implementation with a case study of tour-guide robots for museums that rely on a speech-to-text converter based on the Google Application Programming Interface (API) and a Python library, a neural network to label the emotions in texts based on NLP transformers, and EMONTO integrated with an ontology for museums; thus, it is possible to register the emotions that artworks produce in visitors. We evaluate the classification model, obtaining equivalent results compared with a state-of-the-art transformer-based model and with a clear roadmap for improvement.","The need for interaction between machines and humans is becoming more common in people’s daily lives. The effort to improve these relationships through the interpretation of social behaviors are more and more frequent among research developed in the area of social robotics. The interpretation of the feelings of a human being is an important tool that a robot must know how to use to enact correct behavior in a social environment. Our framework is a clear contribution in this area, since it allows robots to interpret a person’s basic feelings and emotion recognition algorithms. Besides that, the information gathered by the robots and the outputs of the classification process can finally be organized in an ontology, leading to more complex analyses and uses of the data. The results obtained with a first implementation of our framework, as a proof-of-concept, show that the integration of speech-to-text and emotion detection algorithms with an ontology was a success despite the fact that there is still the possibility of being improved.
Our future research is focused on improvements of the framework, starting with the suggestions from Section 6 to provide a robust architecture in the community to develop third-party applications in social robotics. We realize the need to incorporate more interpretation characteristics that can complement the detection of a person’s feelings in our framework, such as detection of the face, posture, and context in which the person is involved. Thus, a new classification model that integrates all these characteristics to have more veracity in detecting the feeling will be developed. Once this extension is done, we could make another comparison to Megatron-LM to put in perspective how much value these new features add to our framework. In addition, the framework could be part of an autonomous navigation system applied to a social robot, which would complement decision-making in these processes as well as assuming better socially acceptable behavior towards humans.","Emotion Detection for Social Robots Based on NLP Transformers and an Emotion Ontologysocial robots; natural language processing; ontology; emotion detection; text classificationFor social robots, knowledge regarding human emotional states is an essential part of adapting their behavior or associating emotions to other entities. Robots gather the information from which emotion detection is processed via different media, such as text, speech, images, or videos. The multimedia content is then properly processed to recognize emotions/sentiments, for example, by analyzing faces and postures in images/videos based on machine learning techniques or by converting speech into text to perform emotion detection with natural language processing (NLP) techniques. Keeping this information in semantic repositories offers a wide range of possibilities for implementing smart applications. We propose a framework to allow social robots to detect emotions and to store this information in a semantic repository, based on EMONTO (an EMotion ONTOlogy), and in the first figure or table caption. Please define if appropriate. an ontology to represent emotions. As a proof-of-concept, we develop a first version of this framework focused on emotion detection in text, which can be obtained directly as text or by converting speech to text. We tested the implementation with a case study of tour-guide robots for museums that rely on a speech-to-text converter based on the Google Application Programming Interface (API) and a Python library, a neural network to label the emotions in texts based on NLP transformers, and EMONTO integrated with an ontology for museums; thus, it is possible to register the emotions that artworks produce in visitors. We evaluate the classification model, obtaining equivalent results compared with a state-of-the-art transformer-based model and with a clear roadmap for improvement.The need for interaction between machines and humans is becoming more common in people’s daily lives. The effort to improve these relationships through the interpretation of social behaviors are more and more frequent among research developed in the area of social robotics. The interpretation of the feelings of a human being is an important tool that a robot must know how to use to enact correct behavior in a social environment. Our framework is a clear contribution in this area, since it allows robots to interpret a person’s basic feelings and emotion recognition algorithms. Besides that, the information gathered by the robots and the outputs of the classification process can finally be organized in an ontology, leading to more complex analyses and uses of the data. The results obtained with a first implementation of our framework, as a proof-of-concept, show that the integration of speech-to-text and emotion detection algorithms with an ontology was a success despite the fact that there is still the possibility of being improved.
Our future research is focused on improvements of the framework, starting with the suggestions from Section 6 to provide a robust architecture in the community to develop third-party applications in social robotics. We realize the need to incorporate more interpretation characteristics that can complement the detection of a person’s feelings in our framework, such as detection of the face, posture, and context in which the person is involved. Thus, a new classification model that integrates all these characteristics to have more veracity in detecting the feeling will be developed. Once this extension is done, we could make another comparison to Megatron-LM to put in perspective how much value these new features add to our framework. In addition, the framework could be part of an autonomous navigation system applied to a social robot, which would complement decision-making in these processes as well as assuming better socially acceptable behavior towards humans.+ Haman ebtnwasion 2 ion cating (cae | ‘.Almagelvideo processing J\I\ i ‘an Gop pants Happy = ‘Angry = 0, {3.6 Text processing: Test obtained 5. Storage in semantic repository 4. Emotion detection ‘rectly or converted fom audio.optimism |. = conte",Natural Language Processing,"The paper describes a framework for social robots to detect and store emotions in a semantic repository, using an ontology called EMONTO. The framework focuses on emotion detection in text and uses a speech-to-text converter, a neural network for emotion labeling, and EMONTO integrated with an ontology for museums to register the emotions that artworks produce in visitors. The framework is a contribution to the interpretation of social behaviors in social robotics and provides a roadmap for improvement, including incorporating more interpretation characteristics and developing a new classification model. The framework could also be part of an autonomous navigation system for social robots, leading to better socially acceptable behavior towards humans.",Natural Language Processing,"+ Haman ebtnwasion 2 ion cating (cae | ‘.Almagelvideo processing J\I\ i ‘an Gop pants Happy = ‘Angry = 0, {3.6 Text processing: Test obtained 5. Storage in semantic repository 4. Emotion detection ‘rectly or converted fom audio.optimism |. = conte",Sentiment Analysis
155,"State-of-the-Arts Person Re-Identification using Deep 
Learning ","Person Re-Identification, deeply learned systems, 
Convolutional Neural Networks, Deep Learning","Person Re-Identification has become prominent 
because of various reasons majorly due to its high-performance 
methods based on deep-learning. It is the process of person 
recognition from various images captured by different cameras. 
Provided two set of images the purpose is to find that the given 
set of images are identical or not. Person Re-Id is often a 
challenging task due to the similarity in nature like people with 
identical features, color or clothes. Images are taken from 
various angles and distances of a given subject in order to 
achieve high accuracy, so it identifies correctly. Re-Identification 
has broadly two major categories: i) Image Re-ID and ii) Video 
Re-ID. Based on the category Re-ID has numerous applications 
like robotics, automated video surveillance, forensics and 
multimedia that are deployed using various public datasets like
Market1501, VIPeR, MARS, CUHK01, CUHK02, CUHK03, 
DukeMTMC-reID, MSMT17 etc. In this paper we aim to briefly 
discuss the process, datasets, recent work on Re-ID, challenges, 
its approaches and techniques that has been implemented using 
deep learning systems.","he paper presents the fundamental process of person reidentification, an outline of person re-ID with respect to Deep
learning methods and the current progress towards it. We have 
also discussed the ongoing researches, task and challenges of 
person re-identification. Available Datasets that are imagesbased as well as video-based for person re-ID reviewed. As 
applications starts to grow in wide variety of range, it turns out 
to be a complex task. Person re-Identification catching eyes in
not only academics but in industries also due to its various 
applications. Though models and methods have been utilized 
but there is still a strong need of in-depth exploration in real 
time problems.","State-of-the-Arts Person Re-Identification using Deep 
Learning Person Re-Identification, deeply learned systems, 
Convolutional Neural Networks, Deep LearningPerson Re-Identification has become prominent 
because of various reasons majorly due to its high-performance 
methods based on deep-learning. It is the process of person 
recognition from various images captured by different cameras. 
Provided two set of images the purpose is to find that the given 
set of images are identical or not. Person Re-Id is often a 
challenging task due to the similarity in nature like people with 
identical features, color or clothes. Images are taken from 
various angles and distances of a given subject in order to 
achieve high accuracy, so it identifies correctly. Re-Identification 
has broadly two major categories: i) Image Re-ID and ii) Video 
Re-ID. Based on the category Re-ID has numerous applications 
like robotics, automated video surveillance, forensics and 
multimedia that are deployed using various public datasets like
Market1501, VIPeR, MARS, CUHK01, CUHK02, CUHK03, 
DukeMTMC-reID, MSMT17 etc. In this paper we aim to briefly 
discuss the process, datasets, recent work on Re-ID, challenges, 
its approaches and techniques that has been implemented using 
deep learning systems.he paper presents the fundamental process of person reidentification, an outline of person re-ID with respect to Deep
learning methods and the current progress towards it. We have 
also discussed the ongoing researches, task and challenges of 
person re-identification. Available Datasets that are imagesbased as well as video-based for person re-ID reviewed. As 
applications starts to grow in wide variety of range, it turns out 
to be a complex task. Person re-Identification catching eyes in
not only academics but in industries also due to its various 
applications. Though models and methods have been utilized 
but there is still a strong need of in-depth exploration in real 
time problems.Image Cropping - as & ARotation CorrectionSyeprocesene [Only on Training Input ation a a) crieny AS k A See ee Output as Intensity Normalization x own-sampling = a = si Baines, age we [me] FY — oe re eal = zo] — oe Seat | —— [inion] | satin zs (a = “i ‘tapas:",Deep Learning and Machine Learning,"The paper discusses Person Re-Identification (Re-ID), which involves recognizing a person from various images captured by different cameras. Re-ID has two categories: Image Re-ID and Video Re-ID, and is applied in robotics, automated video surveillance, forensics, and multimedia using public datasets such as Market1501, VIPeR, and MARS. The paper outlines the process of Re-ID, its challenges, recent work, and deep learning techniques. Despite advancements, there is still a need for in-depth exploration in real-time problems.",Object and Sentiment Recognition,"Image Cropping - as & ARotation CorrectionSyeprocesene [Only on Training Input ation a a) crieny AS k A See ee Output as Intensity Normalization x own-sampling = a = si Baines, age we [me] FY — oe re eal = zo] — oe Seat | —— [inion] | satin zs (a = “i ‘tapas:",Deep Learning and Machine Learning
156,Object Identification form GPR Images by Deep Learning,"onvolutional neural network, ground penetrating radar, FDTD method, GPU, object identification.","We have developed an identification method of an
underground object form a ground penetrating radar (GPR)
image by the deep neural network. In this study, in order
to automatically detect an underground object from the GPR
image by the DNN, we have generated several hundred thousand
GPR images for training the DNN using a fast finite-differencetime-domain (FDTD) simulation with graphics processing units
(GPUs). Furthermore, Characteristics of underground objects
are extracted and learned from generated GPR images by a
9-layers convolutional neural network (CNN). It is shown that
the CNN can identify six materials with roughly 80% accuracy
in inhomogeneous underground media.","We have developed an identification method of the underground object form the GPR image by the deep neural
network. In this study, several hundred thousand GPR images
are generated by the FDTD simulation with 64 GPUs, and
the 9-layers CNN estimates the relative permittivity of the
underground object. As the results, the CNN can estimate roughly 80% in the inhomogeneous underground. For future
work, experimental GPR images will be tested.","Object Identification form GPR Images by Deep Learningonvolutional neural network, ground penetrating radar, FDTD method, GPU, object identification.We have developed an identification method of an
underground object form a ground penetrating radar (GPR)
image by the deep neural network. In this study, in order
to automatically detect an underground object from the GPR
image by the DNN, we have generated several hundred thousand
GPR images for training the DNN using a fast finite-differencetime-domain (FDTD) simulation with graphics processing units
(GPUs). Furthermore, Characteristics of underground objects
are extracted and learned from generated GPR images by a
9-layers convolutional neural network (CNN). It is shown that
the CNN can identify six materials with roughly 80% accuracy
in inhomogeneous underground media.We have developed an identification method of the underground object form the GPR image by the deep neural
network. In this study, several hundred thousand GPR images
are generated by the FDTD simulation with 64 GPUs, and
the 9-layers CNN estimates the relative permittivity of the
underground object. As the results, the CNN can estimate roughly 80% in the inhomogeneous underground. For future
work, experimental GPR images will be tested.Person Person Detection F™) Tracking Feature Feature Transform (9) Extraction J Person Re-Identification",Deep Learning and Machine Learning,"The study presents a method of identifying underground objects from GPR images using a deep neural network (DNN) trained on several hundred thousand images generated by a fast FDTD simulation with GPUs. The 9-layers CNN extracts and learns the characteristics of underground objects, enabling it to identify six materials with approximately 80% accuracy in inhomogeneous underground media. Future work involves testing the method on experimental GPR images.",Object and Sentiment Recognition,Person Person Detection F™) Tracking Feature Feature Transform (9) Extraction J Person Re-Identification,Object Recognition
157,"Deep Learning-based Object Classification on
Automotive Radar Spectra","    Person Re-Identification
    Deep Learning
    Ground Penetrating Radar
    Automotive Radar
    Scene Understanding","Scene understanding for automated driving requires
accurate detection and classification of objects and other traffic
participants. Automotive radar has shown great potential as a
sensor for driver assistance systems due to its robustness to
weather and light conditions, but reliable classification of object
types in real time has proved to be very challenging. Here we
propose a novel concept for radar-based classification, which
utilizes the power of modern Deep Learning methods to learn
favorable data representations and thereby replaces large parts
of the traditional radar signal processing chain. We propose
to apply deep Convolutional Neural Networks (CNNs) directly
to regions-of-interest (ROI) in the radar spectrum and thereby
achieve an accurate classification of different objects in a scene.
Experiments on a real-world dataset demonstrate the ability to
distinguish relevant objects from different viewpoints. We identify
deep learning challenges that are specific to radar classification
and introduce a set of novel mechanisms that lead to significant
improvements in object classification performance compared to
simpler classifiers. Our results demonstrate that Deep Learning
methods can greatly augment the classification capabilities of
automotive radar sensors.","This article has presented the first evaluation of deep learning methods applied directly to radar spectra for scene understanding under real-world conditions. The approach presents
a promising alternative to classical radar signal processing
methods and outperforms other machine learning approaches
on a novel dataset with realistic objects. The best results can
be obtained by combining state-of-the-art deep learning with
specific radar know-how and prior understanding of the task.
This suggests that a hybrid between data-driven and modelbased approaches may have the greatest chance for success,
in particular with limited available real-world training data","Deep Learning-based Object Classification on
Automotive Radar Spectra    Person Re-Identification
    Deep Learning
    Ground Penetrating Radar
    Automotive Radar
    Scene UnderstandingScene understanding for automated driving requires
accurate detection and classification of objects and other traffic
participants. Automotive radar has shown great potential as a
sensor for driver assistance systems due to its robustness to
weather and light conditions, but reliable classification of object
types in real time has proved to be very challenging. Here we
propose a novel concept for radar-based classification, which
utilizes the power of modern Deep Learning methods to learn
favorable data representations and thereby replaces large parts
of the traditional radar signal processing chain. We propose
to apply deep Convolutional Neural Networks (CNNs) directly
to regions-of-interest (ROI) in the radar spectrum and thereby
achieve an accurate classification of different objects in a scene.
Experiments on a real-world dataset demonstrate the ability to
distinguish relevant objects from different viewpoints. We identify
deep learning challenges that are specific to radar classification
and introduce a set of novel mechanisms that lead to significant
improvements in object classification performance compared to
simpler classifiers. Our results demonstrate that Deep Learning
methods can greatly augment the classification capabilities of
automotive radar sensors.This article has presented the first evaluation of deep learning methods applied directly to radar spectra for scene understanding under real-world conditions. The approach presents
a promising alternative to classical radar signal processing
methods and outperforms other machine learning approaches
on a novel dataset with realistic objects. The best results can
be obtained by combining state-of-the-art deep learning with
specific radar know-how and prior understanding of the task.
This suggests that a hybrid between data-driven and modelbased approaches may have the greatest chance for success,
in particular with limited available real-world training data",Deep Learning and Machine Learning,The article proposes a novel concept for radar-based object classification using deep learning methods. The traditional radar signal processing chain is replaced by applying deep Convolutional Neural Networks (CNNs) directly to regions-of-interest (ROI) in the radar spectrum. This approach outperforms other machine learning approaches on a novel dataset with realistic objects and demonstrates that deep learning methods can greatly augment the classification capabilities of automotive radar sensors. The article identifies deep learning challenges specific to radar classification and introduces a set of novel mechanisms that lead to significant improvements in object classification performance. The best results can be obtained by combining state-of-the-art deep learning with specific radar know-how and prior understanding of the task.,Object and Sentiment Recognition,,Object Recognition
158,"Applying the Convolutional Neural Network Deep Learning Technology to Behavioural
Recognition in Intelligent Video",convolutional neural network; deep learning technology; intelligent video; optical flow method ,"n order to improve the accuracy and real-time performance of abnormal behaviour identification in massive video monitoring data, the authors design intelligent 
video technology based on convolutional neural network deep learning and apply it to the smart city on the basis of summarizing video development technology. First, the 
technical framework of intelligent video monitoring algorithm is divided into bottom (object detection), middle (object identification) and high (behaviour analysis) layers. 
The object detection based on background modelling is applied to routine real-time detection and early warning. The object detection based on object modelling is applied 
to after-event data query and retrieval. The related optical flow algorithms are used to achieve the identification and detection of abnormal behaviours. In order to improve 
the accuracy, effectiveness and intelligence of identification, the deep learning technology based on convolutional neural network is applied to enhance the learning and 
identification ability of learning machine and realize the real-time upgrade of intelligence video’s ""brain"". This research has a good popularization value in the application 
field of intelligent video technology.","With rising requirements of cities on security and 
post disaster relief capability, the quantity of cameras and 
video capture systems are increasing, and surveillance 
coverage is expanding further. As traditional video 
management systems could no longer adapt to 
technological development, the intelligent cities need to 
adopt intelligent video technology to help administrators 
to accomplish various tasks. This article designed an 
algorithm frame based on bottom layers, middle layers 
and high layers of intelligent video surveillance, proposed 
object detection approaches based on background 
modelling and object modelling for different applications, 
designed identification and detection methods for 
abnormal behaviours based on optical flow approach, and 
in order to improve success rate of identification and 
intelligent type, it designed a deep learning approach 
based on convolutional neural networks, combined with video action identification. After simulation test in a city, 
it has demonstrated good performance and achieved the 
expected goal.","Applying the Convolutional Neural Network Deep Learning Technology to Behavioural
Recognition in Intelligent Videoconvolutional neural network; deep learning technology; intelligent video; optical flow method n order to improve the accuracy and real-time performance of abnormal behaviour identification in massive video monitoring data, the authors design intelligent 
video technology based on convolutional neural network deep learning and apply it to the smart city on the basis of summarizing video development technology. First, the 
technical framework of intelligent video monitoring algorithm is divided into bottom (object detection), middle (object identification) and high (behaviour analysis) layers. 
The object detection based on background modelling is applied to routine real-time detection and early warning. The object detection based on object modelling is applied 
to after-event data query and retrieval. The related optical flow algorithms are used to achieve the identification and detection of abnormal behaviours. In order to improve 
the accuracy, effectiveness and intelligence of identification, the deep learning technology based on convolutional neural network is applied to enhance the learning and 
identification ability of learning machine and realize the real-time upgrade of intelligence video’s ""brain"". This research has a good popularization value in the application 
field of intelligent video technology.With rising requirements of cities on security and 
post disaster relief capability, the quantity of cameras and 
video capture systems are increasing, and surveillance 
coverage is expanding further. As traditional video 
management systems could no longer adapt to 
technological development, the intelligent cities need to 
adopt intelligent video technology to help administrators 
to accomplish various tasks. This article designed an 
algorithm frame based on bottom layers, middle layers 
and high layers of intelligent video surveillance, proposed 
object detection approaches based on background 
modelling and object modelling for different applications, 
designed identification and detection methods for 
abnormal behaviours based on optical flow approach, and 
in order to improve success rate of identification and 
intelligent type, it designed a deep learning approach 
based on convolutional neural networks, combined with video action identification. After simulation test in a city, 
it has demonstrated good performance and achieved the 
expected goal.Input 1 @ 64x66 Convolutional Layer 1 32 @ 62x64 ‘Average Pooling 32 @ 31x32 Convolutional Layer 2 64 @ 29x30 64 @ 14x15 Convolutional Layer 3 128 @ 12x13 Average Pooling 128 @ 6x6 Dense Layers 512 units ] Oo 3 1 | | Il OQ} 9 oO 32 units [000-009 7 units [O | construction Barrier ‘Q| Motorbike ‘Q| Baby Carriage O| Bicycie }Q| Garbage Container Qlcer Stop Sign",Deep Learning and Machine Learning,"This article proposes an intelligent video technology based on deep learning for abnormal behavior identification in massive video monitoring data in smart cities. The technology utilizes a three-layer framework for object detection, identification, and behavior analysis. Object detection based on background and object modeling is used for real-time detection and data retrieval, while optical flow algorithms are used for abnormal behavior identification. The article also introduces deep learning technology based on convolutional neural networks to enhance the identification and real-time upgrade of intelligence video. The article highlights the importance of intelligent video technology for security and post-disaster relief in smart cities.",Deep Learning and Machine Learning,Input 1 @ 64x66 Convolutional Layer 1 32 @ 62x64 ‘Average Pooling 32 @ 31x32 Convolutional Layer 2 64 @ 29x30 64 @ 14x15 Convolutional Layer 3 128 @ 12x13 Average Pooling 128 @ 6x6 Dense Layers 512 units ] Oo 3 1 | | Il OQ} 9 oO 32 units [000-009 7 units [O | construction Barrier ‘Q| Motorbike ‘Q| Baby Carriage O| Bicycie }Q| Garbage Container Qlcer Stop Sign,Object Recognition
159,"A Vision-based Robotic Grasping System Using Deep Learning for 
Garbage Sorting ","Machine vision, Complex backgrounds, Deep Learning, Robotic grasping, Garbage sorting","This paper proposes a robotic grasping system for automatically sorting garbage based on machine vision. This 
system achieves the identification and positioning of target objects in complex background before using manipulator to 
automatically grab the sorting objects. The object identification in complex background is the key problem that machine vision 
algorithm is trying to solve. This paper uses the deep learning method to achieve the authenticity identification of target object in 
complex background. In order to achieve the accurate grabbing of target object, we apply the Region Proposal Generation (RPN) 
and the VGG-16 model for object recognition and pose estimation. The machine vision system sends the information of the 
geometric centre coordinates and the angle of the long side of the target object to the manipulator which completes the 
classification and grabbing of the target object. The results of sorting experiment of the bottles in the garbage show that the
vision algorithm and the manipulator control method of the proposed system can achieve the garbage sorting efficiently. ","In this paper, we present a vision-based robotic grasping 
system that is capable of object detection, recognition and 
grasping with different poses by using a deep learning model. The Region Proposal Generation (RPN) and the 
VGG-16 model for object recognition and pose estimation 
are employed to identify the target object in complex 
background. Finally, the results of sorting experiment of the 
bottles in the garbage show that the vision algorithm and the 
manipulator control method of the proposed system can 
achieve the garbage sorting efficiently. 
","A Vision-based Robotic Grasping System Using Deep Learning for 
Garbage Sorting Machine vision, Complex backgrounds, Deep Learning, Robotic grasping, Garbage sortingThis paper proposes a robotic grasping system for automatically sorting garbage based on machine vision. This 
system achieves the identification and positioning of target objects in complex background before using manipulator to 
automatically grab the sorting objects. The object identification in complex background is the key problem that machine vision 
algorithm is trying to solve. This paper uses the deep learning method to achieve the authenticity identification of target object in 
complex background. In order to achieve the accurate grabbing of target object, we apply the Region Proposal Generation (RPN) 
and the VGG-16 model for object recognition and pose estimation. The machine vision system sends the information of the 
geometric centre coordinates and the angle of the long side of the target object to the manipulator which completes the 
classification and grabbing of the target object. The results of sorting experiment of the bottles in the garbage show that the
vision algorithm and the manipulator control method of the proposed system can achieve the garbage sorting efficiently. In this paper, we present a vision-based robotic grasping 
system that is capable of object detection, recognition and 
grasping with different poses by using a deep learning model. The Region Proposal Generation (RPN) and the 
VGG-16 model for object recognition and pose estimation 
are employed to identify the target object in complex 
background. Finally, the results of sorting experiment of the 
bottles in the garbage show that the vision algorithm and the 
manipulator control method of the proposed system can 
achieve the garbage sorting efficiently. 
Test image Training image 1 ‘Sub window]! search ' I Feature 57 1 extraction extraction |! ' 1 ' r Object modeling | Object expression expression Object Model traini ing Primary testing result Post-processing Visual expression Window search of object Post-provessing Machine learning & model optimizationObject detection Object recognition Behaviour analysis I I I I Object |} Background Object |} Individual classification |} recognition + + Posture || Behaviour analysis |} | recognition Single scene |} Mult-scene {]_ moteting |] modeting (Whereis the object?) (Whats the object?) ' Event tracking ! analysis Batiom layer Mile layer High yer {What does the object do?)Deep learning machine ¥ Behaviour property Image reading reading Establish constraint ‘Sampling, calculation by layer equation ¥ Detection by optical Convolutional neural flow method networks Abnormal behaviour detection Surveillance analysis l Deep leaming",Text summarization,"This paper describes a robotic grasping system that can automatically sort garbage based on machine vision. The system uses deep learning to accurately identify and position target objects in complex backgrounds. The Region Proposal Generation (RPN) and VGG-16 model are used for object recognition and pose estimation, and the information is sent to the manipulator for object grabbing. The results of sorting experiments show that the system can achieve efficient garbage sorting.",Object and Sentiment Recognition,"Test image Training image 1 ‘Sub window]! search ' I Feature 57 1 extraction extraction |! ' 1 ' r Object modeling | Object expression expression Object Model traini ing Primary testing result Post-processing Visual expression Window search of object Post-provessing Machine learning & model optimizationObject detection Object recognition Behaviour analysis I I I I Object |} Background Object |} Individual classification |} recognition + + Posture || Behaviour analysis |} | recognition Single scene |} Mult-scene {]_ moteting |] modeting (Whereis the object?) (Whats the object?) ' Event tracking ! analysis Batiom layer Mile layer High yer {What does the object do?)Deep learning machine ¥ Behaviour property Image reading reading Establish constraint ‘Sampling, calculation by layer equation ¥ Detection by optical Convolutional neural flow method networks Abnormal behaviour detection Surveillance analysis l Deep leaming",Deep Learning and Machine Learning
160,Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order,"facial expression recognition, convolutional neural network, image pre-processing, public databases, accuracy, real-time, normalization procedures, competitive results","Facial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromarketing and sociable robots. The recognition of facial expressions is not an easy problem for machine learning methods, since people can vary significantly in the way they show their expressions. Even images of the same person in the same facial expression can vary in brightness, background and pose, and these variations are emphasized if considering different subjects (because of variations in shape, ethnicity among others). Although facial expression recognition is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging problem in computer vision. In this work, we propose a simple solution for facial expression recognition that uses a combination of Convolutional Neural Network and specific image pre-processing steps. Convolutional Neural Networks achieve better accuracy with big data. However, there are no publicly available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CK+, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the accuracy rate is presented. The proposed method: achieves competitive results when compared with other facial expression recognition methods – 96.76% of accuracy in the CK+ database – it is fast to train, and it allows for real time facial expression recognition with standard computers.","In this paper, we propose a facial expression recognition system that uses a combination of standard methods, like Convolutional Neural Network and specific image pre-processing steps. Experiments showed that the combination of the normalization procedures improve significantly the method's accuracy. As shown in the results, in comparison with the recent methods in the literature, that use the same facial expression database and experimental methodology, our method achieves competitive results","Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample orderfacial expression recognition, convolutional neural network, image pre-processing, public databases, accuracy, real-time, normalization procedures, competitive resultsFacial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromarketing and sociable robots. The recognition of facial expressions is not an easy problem for machine learning methods, since people can vary significantly in the way they show their expressions. Even images of the same person in the same facial expression can vary in brightness, background and pose, and these variations are emphasized if considering different subjects (because of variations in shape, ethnicity among others). Although facial expression recognition is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging problem in computer vision. In this work, we propose a simple solution for facial expression recognition that uses a combination of Convolutional Neural Network and specific image pre-processing steps. Convolutional Neural Networks achieve better accuracy with big data. However, there are no publicly available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CK+, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the accuracy rate is presented. The proposed method: achieves competitive results when compared with other facial expression recognition methods – 96.76% of accuracy in the CK+ database – it is fast to train, and it allows for real time facial expression recognition with standard computers.In this paper, we propose a facial expression recognition system that uses a combination of standard methods, like Convolutional Neural Network and specific image pre-processing steps. Experiments showed that the combination of the normalization procedures improve significantly the method's accuracy. As shown in the results, in comparison with the recent methods in the literature, that use the same facial expression database and experimental methodology, our method achieves competitive resultsConvl Conv2 FC6 FC7 Classfication Loss Conv3 ROT Poolins Regression Conv4 FC6_1 |} FC7_1 [>| Angle Loss Conv5sping _ g_ s 8 By g zglece| 2 &|;Eg 3 = = Robotic Gra: Machine vision (= ee] ounjdeo o8ewy Buissooo1g",Text summarization,"Facial expression recognition has many applications but is challenging for machine learning as people show expressions differently. Most facial expression recognition research mixes subjects during training and testing, leading to inaccurate results. A proposed solution uses a combination of Convolutional Neural Networks and image pre-processing to extract expression-specific features. The method achieved high accuracy rates, with competitive results compared to other facial expression recognition methods, and real-time recognition with standard computers. The proposed method's normalization procedures significantly improved accuracy rates, making it a promising solution for facial expression recognition.",Object and Sentiment Recognition,Convl Conv2 FC6 FC7 Classfication Loss Conv3 ROT Poolins Regression Conv4 FC6_1 |} FC7_1 [>| Angle Loss Conv5sping _ g_ s 8 By g zglece| 2 &|;Eg 3 = = Robotic Gra: Machine vision (= ee] ounjdeo o8ewy Buissooo1g,Object Recognition
161,"Emotion Recognition through Multiple Modalities:
Face, Body Gesture, Speech ","Affective body language, Affective speech, Emotion recognition,
Multimodal fusion.","In this paper we present a multimodal approach for the recognition of
eight emotions. Our approach integrates information from facial expressions,
body movement and gestures and speech. We trained and tested a model with a
Bayesian classifier, using a multimodal corpus with eight emotions and ten subjects. Firstly, individual classifiers were trained for each modality. Next, data
were fused at the feature level and the decision level. Fusing the multimodal
data resulted in a large increase in the recognition rates in comparison with the
unimodal systems: the multimodal approach gave an improvement of more than
10% when compared to the most successful unimodal system. Further, the fusion performed at the feature level provided better results than the one performed at the decision level","The challenge of endowing machines with emotional intelligence to create affective interactions with humans is of growing importance in human-computer interaction. Recognizing users' emotional state is a primary requirement for computers to successfully interact with humans. However, many works in affective computing consider different channels of information independently, such as facial expressions, speech, body movement, and gestures. This paper presents a multimodal approach for the recognition of eight acted emotional states using a Bayesian classifier trained on a multimodal corpus with ten subjects. The approach integrates information from facial expressions, body movement and gestures, and speech. Different strategies for data fusion in multimodal emotion recognition are compared.","Emotion Recognition through Multiple Modalities:
Face, Body Gesture, Speech Affective body language, Affective speech, Emotion recognition,
Multimodal fusion.In this paper we present a multimodal approach for the recognition of
eight emotions. Our approach integrates information from facial expressions,
body movement and gestures and speech. We trained and tested a model with a
Bayesian classifier, using a multimodal corpus with eight emotions and ten subjects. Firstly, individual classifiers were trained for each modality. Next, data
were fused at the feature level and the decision level. Fusing the multimodal
data resulted in a large increase in the recognition rates in comparison with the
unimodal systems: the multimodal approach gave an improvement of more than
10% when compared to the most successful unimodal system. Further, the fusion performed at the feature level provided better results than the one performed at the decision levelThe challenge of endowing machines with emotional intelligence to create affective interactions with humans is of growing importance in human-computer interaction. Recognizing users' emotional state is a primary requirement for computers to successfully interact with humans. However, many works in affective computing consider different channels of information independently, such as facial expressions, speech, body movement, and gestures. This paper presents a multimodal approach for the recognition of eight acted emotional states using a Bayesian classifier trained on a multimodal corpus with ten subjects. The approach integrates information from facial expressions, body movement and gestures, and speech. Different strategies for data fusion in multimodal emotion recognition are compared.val_acc 0.600 0.500 0.400 0.300 0.200 0.000 3000 6000 9000 #£=1.200kloss 2.00 1.60 1.20 0.800 0.400 0.00 0.000 300.0 600.0 9000.0 1.200k(6xSXD) 2 titers Simple convolution for Output 1212256 (6X5x1)3 fiters (11%) 256 fiters b Depthwise Separable Convolution for output 12X12X256Concat < Conv 3x3 Conv 5x5 Conv 1x1 ¥ Conv 1x1 | | Conv 1x1 Conv 1x1 MaxPool Input aaInput image Conv 1X1 + BN Sep Conv2D + SepConv2D+ Sep Conv2D +BN +Act Sep Conv2D + BN +MaxPool Global Average Pooling Activation function Output Class Conv2D + BN \ x2",Deep Learning and Machine Learning,"This paper presents a multimodal approach for recognizing eight emotions by integrating information from facial expressions, body movement and gestures, and speech using a Bayesian classifier. The approach fuses data at the feature and decision levels, resulting in a significant increase in recognition rates compared to unimodal systems. The paper highlights the importance of considering different channels of information in affective computing and compares different strategies for data fusion in multimodal emotion recognition.",Object and Sentiment Recognition,val_acc 0.600 0.500 0.400 0.300 0.200 0.000 3000 6000 9000 #£=1.200kloss 2.00 1.60 1.20 0.800 0.400 0.00 0.000 300.0 600.0 9000.0 1.200k(6xSXD) 2 titers Simple convolution for Output 1212256 (6X5x1)3 fiters (11%) 256 fiters b Depthwise Separable Convolution for output 12X12X256Concat < Conv 3x3 Conv 5x5 Conv 1x1 ¥ Conv 1x1 | | Conv 1x1 Conv 1x1 MaxPool Input aaInput image Conv 1X1 + BN Sep Conv2D + SepConv2D+ Sep Conv2D +BN +Act Sep Conv2D + BN +MaxPool Global Average Pooling Activation function Output Class Conv2D + BN \ x2,Sentiment Analysis
162,Emotion Detection from Speech to Enrich Multimedia Content,"Support Vector Machine
Support Vector Machine Classifier
Support Vector Machine Model
Emotion Category
Pitch Contour","This paper describes an experimental study on the detection of
emotion from speech. As computer-based characters such as avatars and virtual
chat faces become more common, the use of emotion to drive the expression of
the virtual characters becomes more important. This study utilizes a corpus
containing emotional speech with 721 short utterances expressing four
emotions: anger, happiness, sadness, and the neutral (unemotional) state, which
were captured manually from movies and teleplays. We introduce a new
concept to evaluate emotions in speech. Emotions are so complex that most
speech sentences cannot be precisely assigned to a particular emotion category;
however, most emotional states nevertheless can be described as a mixture of
multiple emotions. Based on this concept we have trained SVMs (support
vector machines) to recognize utterances within these four categories and
developed an agent that can recognize and express emotions. ","The paper discusses an experimental study on detecting emotions from speech to improve the naturalness of computer-human interactions. The study uses a corpus of emotional speech with four emotions: anger, happiness, sadness, and neutral state. The researchers have trained support vector machines (SVMs) to recognize utterances in these four categories and developed an agent that can recognize and express emotions. The study also focuses on speech-driven facial animation to produce a facial control model, which can generate various facial images based on different emotional state inputs. The paper discusses various classification algorithms used in recent studies about emotions in speech recognition.","Emotion Detection from Speech to Enrich Multimedia ContentSupport Vector Machine
Support Vector Machine Classifier
Support Vector Machine Model
Emotion Category
Pitch ContourThis paper describes an experimental study on the detection of
emotion from speech. As computer-based characters such as avatars and virtual
chat faces become more common, the use of emotion to drive the expression of
the virtual characters becomes more important. This study utilizes a corpus
containing emotional speech with 721 short utterances expressing four
emotions: anger, happiness, sadness, and the neutral (unemotional) state, which
were captured manually from movies and teleplays. We introduce a new
concept to evaluate emotions in speech. Emotions are so complex that most
speech sentences cannot be precisely assigned to a particular emotion category;
however, most emotional states nevertheless can be described as a mixture of
multiple emotions. Based on this concept we have trained SVMs (support
vector machines) to recognize utterances within these four categories and
developed an agent that can recognize and express emotions. The paper discusses an experimental study on detecting emotions from speech to improve the naturalness of computer-human interactions. The study uses a corpus of emotional speech with four emotions: anger, happiness, sadness, and neutral state. The researchers have trained support vector machines (SVMs) to recognize utterances in these four categories and developed an agent that can recognize and express emotions. The study also focuses on speech-driven facial animation to produce a facial control model, which can generate various facial images based on different emotional state inputs. The paper discusses various classification algorithms used in recent studies about emotions in speech recognition.",Deep Learning and Machine Learning,The paper describes an experimental study on detecting emotions from speech to improve computer-human interactions. It uses a corpus of emotional speech with four emotions and introduces a new concept to evaluate emotions in speech as a mixture of multiple emotions. The study trains SVMs to recognize utterances in four emotion categories and develops an agent that can recognize and express emotions. It also discusses speech-driven facial animation to produce a facial control model and classification algorithms used in recent studies about emotions in speech recognition.,Object and Sentiment Recognition,,Sentiment Analysis
163,Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network,convolutional neural network; attention mechanism; spatial transformer network; facial expression recognition,"Facial expression recognition has been an active area of research over the past few decades, and it is still challenging due to the high intra-class variation. Traditional approaches for this problem rely on hand-crafted features such as SIFT, HOG, and LBP, followed by a classifier trained on a database of images or videos. Most of these works perform reasonably well on datasets of images captured in a controlled condition but fail to perform as well on more challenging datasets with more image variation and partial faces. In recent years, several works proposed an end-to-end framework for facial expression recognition using deep learning models. Despite the better performance of these works, there are still much room for improvement. In this work, we propose a deep learning approach based on attentional convolutional network that is able to focus on important parts of the face and achieves significant improvement over previous models on multiple datasets, including FER-2013, CK+, FERG, and JAFFE. We also use a visualization technique that is able to find important facial regions to detect different emotions based on the classifier’s output. Through experimental results, we show that different emotions are sensitive to different parts of the face.","This paper proposes a new framework for facial expression recognition using an attentional convolutional network. We believe attention to special regions is important for detecting facial expressions, which can enable neural networks with less than 10 layers to compete with (and even outperform) much deeper networks in emotion recognition. We also provide an extensive experimental analysis of our work on four popular facial expression recognition databases and show some promising results. Additionally, we deployed a visualization method to highlight the salient regions of face images that are the most crucial parts thereof for detecting different facial expressions.","Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Networkconvolutional neural network; attention mechanism; spatial transformer network; facial expression recognitionFacial expression recognition has been an active area of research over the past few decades, and it is still challenging due to the high intra-class variation. Traditional approaches for this problem rely on hand-crafted features such as SIFT, HOG, and LBP, followed by a classifier trained on a database of images or videos. Most of these works perform reasonably well on datasets of images captured in a controlled condition but fail to perform as well on more challenging datasets with more image variation and partial faces. In recent years, several works proposed an end-to-end framework for facial expression recognition using deep learning models. Despite the better performance of these works, there are still much room for improvement. In this work, we propose a deep learning approach based on attentional convolutional network that is able to focus on important parts of the face and achieves significant improvement over previous models on multiple datasets, including FER-2013, CK+, FERG, and JAFFE. We also use a visualization technique that is able to find important facial regions to detect different emotions based on the classifier’s output. Through experimental results, we show that different emotions are sensitive to different parts of the face.This paper proposes a new framework for facial expression recognition using an attentional convolutional network. We believe attention to special regions is important for detecting facial expressions, which can enable neural networks with less than 10 layers to compete with (and even outperform) much deeper networks in emotion recognition. We also provide an extensive experimental analysis of our work on four popular facial expression recognition databases and show some promising results. Additionally, we deployed a visualization method to highlight the salient regions of face images that are the most crucial parts thereof for detecting different facial expressions.",Artificial Neural Network,"Facial expression recognition has been a challenging area of research for decades, with traditional methods relying on hand-crafted features and classifiers trained on databases of images or videos. While deep learning models have shown promise, there is still room for improvement. The authors propose a new approach using an attentional convolutional network that focuses on important parts of the face and achieves better results than previous models on multiple datasets. They also use a visualization technique to identify important facial regions for detecting different emotions, and show that different emotions are sensitive to different parts of the face. Overall, the authors believe that attention to special regions is important for detecting facial expressions and that their framework could enable neural networks with less than 10 layers to outperform much deeper networks in emotion recognition.





",Object and Sentiment Recognition,,Sentiment Analysis
164,"Emotion in user interface, voice interaction system","User interfaces
,
Humans
,
Facial animation
,
Databases
,
Speech analysis
,
Emotion recognition
,
Face
,
Engines
,
Production systems
,
Acoustical engineering","An approach towards a personalized voice-emotion user interface regardless of the speaker's age, sex or language is presented. An extensive set of carefully chosen utterances provided a speech database for investing acoustic similarities among eight emotional states: (unemotional) neutral, anger, sadness, happiness, disgust, surprised, stressed/troubled and scared. Based on those results, a voice interaction system (VIS) capable of sensing the user's emotional message was developed. In efforts to detect emotions, several primary parameters from human speech were analyzed: pitch, formants, tempo (rhythm) and power of human voice. First the individual basic speaker's voice characteristics were extracted (pitch or/and formants in neutral speech, normal speech rate, neutral speech power) and based on those parameters the emotional message of the subject's utterance was successfully extracted. The VIS interacts with the user while changing its response according to the user's utterances.","In conclusion, the paper presents an approach to create a personalized voice-emotion user interface that can detect emotions in the user's speech, regardless of their age, sex, or language. The approach used an extensive set of carefully chosen utterances to create a speech database for analyzing acoustic similarities among eight emotional states. Based on the results, a voice interaction system was developed that analyzes primary parameters of human speech to detect emotions in the user's speech. The system interacts with the user, changing its response based on the user's utterances. The proposed approach successfully extracts emotional messages from the subject's utterances by analyzing the basic speaker's voice characteristics, such as pitch, formants, tempo, and power in neutral speech. Overall, the approach presents a promising method for creating a personalized and intuitive voice-emotion user interface.","Emotion in user interface, voice interaction systemUser interfaces
,
Humans
,
Facial animation
,
Databases
,
Speech analysis
,
Emotion recognition
,
Face
,
Engines
,
Production systems
,
Acoustical engineeringAn approach towards a personalized voice-emotion user interface regardless of the speaker's age, sex or language is presented. An extensive set of carefully chosen utterances provided a speech database for investing acoustic similarities among eight emotional states: (unemotional) neutral, anger, sadness, happiness, disgust, surprised, stressed/troubled and scared. Based on those results, a voice interaction system (VIS) capable of sensing the user's emotional message was developed. In efforts to detect emotions, several primary parameters from human speech were analyzed: pitch, formants, tempo (rhythm) and power of human voice. First the individual basic speaker's voice characteristics were extracted (pitch or/and formants in neutral speech, normal speech rate, neutral speech power) and based on those parameters the emotional message of the subject's utterance was successfully extracted. The VIS interacts with the user while changing its response according to the user's utterances.In conclusion, the paper presents an approach to create a personalized voice-emotion user interface that can detect emotions in the user's speech, regardless of their age, sex, or language. The approach used an extensive set of carefully chosen utterances to create a speech database for analyzing acoustic similarities among eight emotional states. Based on the results, a voice interaction system was developed that analyzes primary parameters of human speech to detect emotions in the user's speech. The system interacts with the user, changing its response based on the user's utterances. The proposed approach successfully extracts emotional messages from the subject's utterances by analyzing the basic speaker's voice characteristics, such as pitch, formants, tempo, and power in neutral speech. Overall, the approach presents a promising method for creating a personalized and intuitive voice-emotion user interface.EY 6 Bis1204 Uonnienson onene Tse Burge A pusxoniexe Uorinjonuoy Te owny onsen Uornjonog t Ti Bue eT Localaation Network owoy ovsexe Uonnjonson ao euoy ovsexe ‘wonnjonon",Natural Language Processing,"The paper proposes a method for creating a personalized voice-emotion user interface that can detect emotions in the user's speech regardless of age, sex, or language. The approach involves creating a speech database for analyzing acoustic similarities among eight emotional states and developing a voice interaction system that analyzes primary parameters of human speech, including pitch, formants, tempo, and power, to detect emotions in the user's speech. The system interacts with the user, changing its response based on the user's utterances. The proposed approach successfully extracts emotional messages from the subject's utterances by analyzing the speaker's voice characteristics in neutral speech, and presents a promising method for creating a personalized and intuitive voice-emotion user interface.",Object and Sentiment Recognition,EY 6 Bis1204 Uonnienson onene Tse Burge A pusxoniexe Uorinjonuoy Te owny onsen Uornjonog t Ti Bue eT Localaation Network owoy ovsexe Uonnjonson ao euoy ovsexe ‘wonnjonon,Sentiment Analysis
165,"Categorical emotion recognition
from voice improves during
childhood and adolescence
","Emotion recognition, non-verbal cues, vocal expressions, childhood, adolescence, social interactions, developmental disorders.","Converging evidence demonstrates that emotion processing from facial expressions continues to
improve throughout childhood and part of adolescence. Here we investigated whether this is also
the case for emotions conveyed by non-linguistic vocal expressions, another key aspect of social
interactions. We tested 225 children and adolescents (age 5–17) and 30 adults in a forced-choice
labeling task using vocal bursts expressing four basic emotions (anger, fear, happiness and sadness).
Mixed-model logistic regressions revealed a small but highly signifcant change with age, mainly
driven by changes in the ability to identify anger and fear. Adult-level of performance was reached
between 14 and 15 years of age. Also, across ages, female participants obtained better scores than
male participants, with no signifcant interaction between age and sex efects. These results expand
the fndings showing that afective prosody understanding improves during childhood; they document,
for the frst time, continued improvement in vocal afect recognition from early childhood to midadolescence, a pivotal period for social maturation.","The ability to recognize emotions from non-verbal cues such as facial expression, body posture or tone of voice in speech is crucial for successful social interactions and for establishing intimate relationships throughout the lifespan. This ability emerges early on in infancy and continues to refine throughout childhood and adolescence. The ability to identify emotions from non-verbal behavior is related to academic achievement, peer-rated popularity, quality of relationship with adults, and is independent of IQ. Alteration of emotion processing in this age range has also been linked to a variety of developmental disorders such as autism, attention deficit and hyperactivity disorder, social anxiety, psychopathy, or conduct disorder. Most of the existing research has focused on the perception of static facial expressions, but some studies have explored the development of emotion recognition in the auditory modality. Infants can discriminate affect from tone of voice or from composite face-voice stimuli earlier than from faces alone, and studies focusing on childhood have shown that children as young as four years old can label the emotions expressed by unfamiliar adults from their tone of voice. However, it is unclear whether emotional voice processing is adult-like at the end of childhood or whether maturation still occurs during adolescence. More research is needed with adolescents to provide a complete picture of the development of emotion perception in the auditory modality.","Categorical emotion recognition
from voice improves during
childhood and adolescence
Emotion recognition, non-verbal cues, vocal expressions, childhood, adolescence, social interactions, developmental disorders.Converging evidence demonstrates that emotion processing from facial expressions continues to
improve throughout childhood and part of adolescence. Here we investigated whether this is also
the case for emotions conveyed by non-linguistic vocal expressions, another key aspect of social
interactions. We tested 225 children and adolescents (age 5–17) and 30 adults in a forced-choice
labeling task using vocal bursts expressing four basic emotions (anger, fear, happiness and sadness).
Mixed-model logistic regressions revealed a small but highly signifcant change with age, mainly
driven by changes in the ability to identify anger and fear. Adult-level of performance was reached
between 14 and 15 years of age. Also, across ages, female participants obtained better scores than
male participants, with no signifcant interaction between age and sex efects. These results expand
the fndings showing that afective prosody understanding improves during childhood; they document,
for the frst time, continued improvement in vocal afect recognition from early childhood to midadolescence, a pivotal period for social maturation.The ability to recognize emotions from non-verbal cues such as facial expression, body posture or tone of voice in speech is crucial for successful social interactions and for establishing intimate relationships throughout the lifespan. This ability emerges early on in infancy and continues to refine throughout childhood and adolescence. The ability to identify emotions from non-verbal behavior is related to academic achievement, peer-rated popularity, quality of relationship with adults, and is independent of IQ. Alteration of emotion processing in this age range has also been linked to a variety of developmental disorders such as autism, attention deficit and hyperactivity disorder, social anxiety, psychopathy, or conduct disorder. Most of the existing research has focused on the perception of static facial expressions, but some studies have explored the development of emotion recognition in the auditory modality. Infants can discriminate affect from tone of voice or from composite face-voice stimuli earlier than from faces alone, and studies focusing on childhood have shown that children as young as four years old can label the emotions expressed by unfamiliar adults from their tone of voice. However, it is unclear whether emotional voice processing is adult-like at the end of childhood or whether maturation still occurs during adolescence. More research is needed with adolescents to provide a complete picture of the development of emotion perception in the auditory modality.",Deep Learning and Machine Learning,"The ability to recognize emotions conveyed through non-verbal cues such as facial expressions and tone of voice is crucial for social interactions and relationships. While research has shown that emotion processing from facial expressions improves throughout childhood and adolescence, it is unclear if the same is true for emotions conveyed through vocal expressions. A study tested 225 children and adolescents and 30 adults in a forced-choice labeling task using vocal bursts expressing four basic emotions. Results showed that emotional vocal recognition improves throughout childhood and adolescence, with adult-level performance reached between 14 and 15 years of age. These findings expand on previous research and highlight the importance of continued improvement in emotional recognition skills during this pivotal period for social maturation.



",Object and Sentiment Recognition,,Sentiment Analysis
166,GA-SVM-Based Facial Emotion Recognition Using Facial Geometric Features,"Feature extraction
,
Support vector machines
,
Faces
,
Genetic algorithms
,
Sensors
,
Emotion recognition
,
Optimization","This paper presents a facial emotion recognition technique using two newly defined geometric features, landmark curvature and vectorized landmark. These features are extracted from facial landmarks associated with individual components of facial muscle movements. The presented method combines support vector machine (SVM) based classification with a genetic algorithm (GA) for a multi-attribute optimization problem of feature and parameter selection. Experimental evaluations were conducted on the extended Cohn-Kanade dataset (CK+) and the Multimedia Understanding Group (MUG) dataset. For 8-class CK+, 7-class CK+, and 7-class MUG, the validation accuracy was 93.57, 95.58, and 96.29%; and the test accuracy resulted in 95.85, 97.59, and 96.56%, respectively. Overall precision, recall, and F1-score were about 0.97, 0.95, and 0.96. For further evaluation, the presented technique was compared with a convolutional neural network (CNN), one of the widely adopted methods for facial emotion recognition. The presented method showed slightly higher test accuracy than CNN for 8-class CK+ (95.85% (SVM) vs. 95.43% (CNN)) and 7-class CK+ (97.59 vs. 97.34), while the CNN slightly outperformed on the 7-class MUG dataset (96.56 vs. 99.62). Compared to CNN-based approaches, this method employs less complicated models and thus shows potential for real-time machine vision applications in automated systems.","This thesis paper proposes a facial emotion recognition technique using two newly defined geometric features, landmark curvature, and vectorized landmark, extracted from facial landmarks associated with individual components of facial muscle movements. The presented method combines SVM-based classification with a GA for feature and parameter selection. The experimental evaluations were conducted on the CK+ and MUG datasets, and the results demonstrate high accuracy and precision in recognizing facial emotions. Additionally, this method shows potential for real-time machine vision applications in automated systems, as it employs less complicated models compared to CNN-based approaches. The presented technique provides a promising approach for automatic facial emotion recognition and has potential for use in various fields, including psychology, marketing, and human-computer interaction.","GA-SVM-Based Facial Emotion Recognition Using Facial Geometric FeaturesFeature extraction
,
Support vector machines
,
Faces
,
Genetic algorithms
,
Sensors
,
Emotion recognition
,
OptimizationThis paper presents a facial emotion recognition technique using two newly defined geometric features, landmark curvature and vectorized landmark. These features are extracted from facial landmarks associated with individual components of facial muscle movements. The presented method combines support vector machine (SVM) based classification with a genetic algorithm (GA) for a multi-attribute optimization problem of feature and parameter selection. Experimental evaluations were conducted on the extended Cohn-Kanade dataset (CK+) and the Multimedia Understanding Group (MUG) dataset. For 8-class CK+, 7-class CK+, and 7-class MUG, the validation accuracy was 93.57, 95.58, and 96.29%; and the test accuracy resulted in 95.85, 97.59, and 96.56%, respectively. Overall precision, recall, and F1-score were about 0.97, 0.95, and 0.96. For further evaluation, the presented technique was compared with a convolutional neural network (CNN), one of the widely adopted methods for facial emotion recognition. The presented method showed slightly higher test accuracy than CNN for 8-class CK+ (95.85% (SVM) vs. 95.43% (CNN)) and 7-class CK+ (97.59 vs. 97.34), while the CNN slightly outperformed on the 7-class MUG dataset (96.56 vs. 99.62). Compared to CNN-based approaches, this method employs less complicated models and thus shows potential for real-time machine vision applications in automated systems.This thesis paper proposes a facial emotion recognition technique using two newly defined geometric features, landmark curvature, and vectorized landmark, extracted from facial landmarks associated with individual components of facial muscle movements. The presented method combines SVM-based classification with a GA for feature and parameter selection. The experimental evaluations were conducted on the CK+ and MUG datasets, and the results demonstrate high accuracy and precision in recognizing facial emotions. Additionally, this method shows potential for real-time machine vision applications in automated systems, as it employs less complicated models compared to CNN-based approaches. The presented technique provides a promising approach for automatic facial emotion recognition and has potential for use in various fields, including psychology, marketing, and human-computer interaction.Anger 100 21035 3301109 3 2 = waoved payipal 155 2 95 ‘Age (years) 45Portrayed emotion Children (5-8) Pre-Ado (9-11) Mid-Ado (12-14) Late-Ado (15-17, Adults (218) AH FS Perceived emotion— Boys — Girls 3 3 3 (2109S 3201409 JUa2Jed peqIpeig 2 857 95. 2 85 4S 95 2 65 45 95 os 85 Ts Age (years) 7 45sasuodsad 19109 JUBII9d 169 143 19 94 69100 ot 7 a 3 2 1 ""A SH mF mS z= eb i Children Pre-Ado Early Ado Late Ado Adults.",Deep Learning and Machine Learning,"This paper proposes a facial emotion recognition technique that uses two new geometric features extracted from facial landmarks associated with individual components of facial muscle movements. The method combines SVM-based classification with a GA for feature and parameter selection and is evaluated on the CK+ and MUG datasets, demonstrating high accuracy and precision in recognizing facial emotions. The proposed method is compared to a CNN-based approach and shows slightly higher test accuracy for two datasets, making it a promising approach for automatic facial emotion recognition with potential for use in various fields, including psychology, marketing, and human-computer interaction.",Object and Sentiment Recognition,"Anger 100 21035 3301109 3 2 = waoved payipal 155 2 95 ‘Age (years) 45Portrayed emotion Children (5-8) Pre-Ado (9-11) Mid-Ado (12-14) Late-Ado (15-17, Adults (218) AH FS Perceived emotion— Boys — Girls 3 3 3 (2109S 3201409 JUa2Jed peqIpeig 2 857 95. 2 85 4S 95 2 65 45 95 os 85 Ts Age (years) 7 45sasuodsad 19109 JUBII9d 169 143 19 94 69100 ot 7 a 3 2 1 ""A SH mF mS z= eb i Children Pre-Ado Early Ado Late Ado Adults.",Sentiment Analysis
167,Cartoon image colorization based on emotion recognition and superpixel color resolution,"Image color analysis
,
Art
,
Emotion recognition
,
Semantics
,
Faces
,
Visualization
,
Rendering (computer graphics)","With the development of artificial intelligence technology, it is possible to automatically colorize the hand drawn sketch by machine. Researchers have conducted in-depth and meticulous study on hand-drawn manuscript recognition, generation, and retrieval. For the emotion-based line art colorization, facial expression should be also extracted from the image itself. To solve the recognition and colorization problem, this paper has proposed an algorithm with the DenseNet network for emotional recognition of anime faces and performed a two-stage interactive coloring method in view of superpixel color analysis features. Among them, the superpixel color analysis technology used a simple linear iterative clustering of SLIC algorithm. According to the experiment, the prompt color information of corresponding position could be generated through the emotion recognition result. After the prediction of superpixel color analysis by GAN (generative adversarial network), the original cartoon image could be rendered with suitable color scheme. The visualization results proved that our algorithm proposed would effectively realize the emotion-based line art colorization of high interactivity and reasonable color distribution.","this paper proposes an algorithm for emotion-based line art colorization using the DenseNet network for emotional recognition of anime faces and a two-stage interactive coloring method based on superpixel color analysis features. The superpixel color analysis technology uses a simple linear iterative clustering of SLIC algorithm to generate prompt color information. The GAN is used to predict the superpixel color analysis, and the original cartoon image is rendered with a suitable color scheme. The results demonstrate that the proposed algorithm effectively realizes emotion-based line art colorization with high interactivity and reasonable color distribution. The research on hand-drawn manuscript recognition, generation, and retrieval, as well as facial emotion recognition, has contributed to the development of AI technology for automatic colorization of hand-drawn sketches.","Cartoon image colorization based on emotion recognition and superpixel color resolutionImage color analysis
,
Art
,
Emotion recognition
,
Semantics
,
Faces
,
Visualization
,
Rendering (computer graphics)With the development of artificial intelligence technology, it is possible to automatically colorize the hand drawn sketch by machine. Researchers have conducted in-depth and meticulous study on hand-drawn manuscript recognition, generation, and retrieval. For the emotion-based line art colorization, facial expression should be also extracted from the image itself. To solve the recognition and colorization problem, this paper has proposed an algorithm with the DenseNet network for emotional recognition of anime faces and performed a two-stage interactive coloring method in view of superpixel color analysis features. Among them, the superpixel color analysis technology used a simple linear iterative clustering of SLIC algorithm. According to the experiment, the prompt color information of corresponding position could be generated through the emotion recognition result. After the prediction of superpixel color analysis by GAN (generative adversarial network), the original cartoon image could be rendered with suitable color scheme. The visualization results proved that our algorithm proposed would effectively realize the emotion-based line art colorization of high interactivity and reasonable color distribution.this paper proposes an algorithm for emotion-based line art colorization using the DenseNet network for emotional recognition of anime faces and a two-stage interactive coloring method based on superpixel color analysis features. The superpixel color analysis technology uses a simple linear iterative clustering of SLIC algorithm to generate prompt color information. The GAN is used to predict the superpixel color analysis, and the original cartoon image is rendered with a suitable color scheme. The results demonstrate that the proposed algorithm effectively realizes emotion-based line art colorization with high interactivity and reasonable color distribution. The research on hand-drawn manuscript recognition, generation, and retrieval, as well as facial emotion recognition, has contributed to the development of AI technology for automatic colorization of hand-drawn sketches.",Deep Learning and Machine Learning,"This paper proposes an algorithm for emotion-based line art colorization using the DenseNet network for emotional recognition of anime faces and a two-stage interactive coloring method based on superpixel color analysis features. The results demonstrate that the proposed algorithm effectively realizes emotion-based line art colorization with high interactivity and reasonable color distribution. The research on hand-drawn manuscript recognition, generation, and retrieval, as well as facial emotion recognition, has contributed to the development of AI technology for automatic colorization of hand-drawn sketches.",Object and Sentiment Recognition,,Sentiment Analysis
168,"Long Document Summarization in a Low Resource Setting
using Pretrained Language Models
","abstractive summarization, low-resource, legal briefs, BART, salience detection","Abstractive summarization is the task of compressing a long document into a coherent short
document while retaining salient information.
Modern abstractive summarization methods
are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive
and time-consuming task, practical industrial
settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with
an average source document length of 4268
words and only 120 available (document, summary) pairs. To account for data scarcity,
we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which
only achieves 17.9 ROUGE-L as it struggles
with long documents. We thus attempt to
compress these long documents by identifying salient sentences in the source which best
ground the summary, using a novel algorithm
based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates
within the low resource regime. On feeding the
compressed documents to BART, we observe a
6.0 ROUGE-L improvement. Our method also
beats several competitive salience detection
baselines. Furthermore, the identified salient
sentences tend to agree with an independent
human labeling by domain experts.","We tackle an important real-world problem of summarizing long domain-specific documents with
very less training data. We propose an extract-thenabstract pipeline which uses GPT-2 perplexity and
a BERT classifier to estimate sentence salience.
This sufficiently compresses a document, allowing
us to use a pretrained model (BART) to generate
coherent & fluent summaries.
","Long Document Summarization in a Low Resource Setting
using Pretrained Language Models
abstractive summarization, low-resource, legal briefs, BART, salience detectionAbstractive summarization is the task of compressing a long document into a coherent short
document while retaining salient information.
Modern abstractive summarization methods
are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive
and time-consuming task, practical industrial
settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with
an average source document length of 4268
words and only 120 available (document, summary) pairs. To account for data scarcity,
we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which
only achieves 17.9 ROUGE-L as it struggles
with long documents. We thus attempt to
compress these long documents by identifying salient sentences in the source which best
ground the summary, using a novel algorithm
based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates
within the low resource regime. On feeding the
compressed documents to BART, we observe a
6.0 ROUGE-L improvement. Our method also
beats several competitive salience detection
baselines. Furthermore, the identified salient
sentences tend to agree with an independent
human labeling by domain experts.We tackle an important real-world problem of summarizing long domain-specific documents with
very less training data. We propose an extract-thenabstract pipeline which uses GPT-2 perplexity and
a BERT classifier to estimate sentence salience.
This sufficiently compresses a document, allowing
us to use a pretrained model (BART) to generate
coherent & fluent summaries.
Anger Disgust Fear Joy Sadness Surprise Num. of Seq. M 37 34 62 33Video Eyebrow and Outer-Lip FAPs Extraction HMMLbased | Recognized FAPS | Automatic Facial | Expressions Expression > Recognition",Text summarization,"The paper presents a study on summarizing long legal briefs with only 120 available (document, summary) pairs, which is a low-resource setting. The authors propose a novel algorithm based on GPT-2 language model perplexity scores to identify salient sentences and compress the documents, which results in a 6.0 ROUGE-L improvement when fed to a modern pretrained abstractive summarizer BART. The method outperforms several salience detection baselines and is in agreement with independent human labeling by domain experts. The proposed extract-then-abstract pipeline provides a solution for summarizing domain-specific documents with limited training data.",Natural Language Processing,Anger Disgust Fear Joy Sadness Surprise Num. of Seq. M 37 34 62 33Video Eyebrow and Outer-Lip FAPs Extraction HMMLbased | Recognized FAPS | Automatic Facial | Expressions Expression > Recognition,Object Recognition
169,Research of Image Main Objects Detection Algorithm Based on Deep Learning ,"object detection; convolution neural network; 
scoring system; selective search; deep learning ","Images have many objects in complex background. 
How to identify these objects and identify the main objects 
therein and understand the relationship between the main 
objects and other objects are the focus of this parper. There 
are many ways to object recognition, but most of them cannot 
mark the main objects of the image.In this paper, we use 
improved RCNN [1] network to detect and recognize 
multi-object in the image. Then we put forward the main 
objects scoring system to mark the image main objects. The 
experimental results show that the algorithm not only 
maintains the superiority of RCNN, but also detects the main 
objects of the image.We found that the image main objects 
were related to the size of candidate region and the rarity of 
objects. ","In this paper, we proposed an image main objects 
scoring model. The model can extracte image main objects. 
And we found that there is no relationship between the 
image object extraction and the image detection model, only related to the size of the image candidate region and the 
rareness of the category. We reconstructed the network 
structure of RCNN and trained the ImageNet dataset with 
selective search, svm and bbox regression. Then we put 
forward the scoring system for recognizing image main 
objects. Finally, we use the scoring system to testing 
Flickr8k. 
However, our method still has the following problems. 
Although bregion regression is used to adjust the image's 
candidate region, the size of the candidate region does not 
completely replace the area of the target object. So we need 
to calculate the outline size of the object. We have no 
remedy for inaccurate images and no error alerts. Follow-up 
work will be carried out in accordance with these two 
issues. ","Research of Image Main Objects Detection Algorithm Based on Deep Learning object detection; convolution neural network; 
scoring system; selective search; deep learning Images have many objects in complex background. 
How to identify these objects and identify the main objects 
therein and understand the relationship between the main 
objects and other objects are the focus of this parper. There 
are many ways to object recognition, but most of them cannot 
mark the main objects of the image.In this paper, we use 
improved RCNN [1] network to detect and recognize 
multi-object in the image. Then we put forward the main 
objects scoring system to mark the image main objects. The 
experimental results show that the algorithm not only 
maintains the superiority of RCNN, but also detects the main 
objects of the image.We found that the image main objects 
were related to the size of candidate region and the rarity of 
objects. In this paper, we proposed an image main objects 
scoring model. The model can extracte image main objects. 
And we found that there is no relationship between the 
image object extraction and the image detection model, only related to the size of the image candidate region and the 
rareness of the category. We reconstructed the network 
structure of RCNN and trained the ImageNet dataset with 
selective search, svm and bbox regression. Then we put 
forward the scoring system for recognizing image main 
objects. Finally, we use the scoring system to testing 
Flickr8k. 
However, our method still has the following problems. 
Although bregion regression is used to adjust the image's 
candidate region, the size of the candidate region does not 
completely replace the area of the target object. So we need 
to calculate the outline size of the object. We have no 
remedy for inaccurate images and no error alerts. Follow-up 
work will be carried out in accordance with these two 
issues. ",Deep Learning and Machine Learning,"This paper proposes a scoring system for identifying main objects in complex background images using an improved RCNN network. The size of the candidate region and rarity of the object were found to be related to the image's main objects. However, the method still has some problems, such as inaccurate images and no error alerts. The authors suggest calculating the outline size of the object and working on these issues in future research. Keywords: object recognition, image main objects, RCNN network.",Object and Sentiment Recognition,,Deep Learning and Machine Learning
170,"AN AUTOMATIC CLASSIFICATION METHOD 
FOR ENVIRONMENT","Garbage segregation, Deep learning ","Recent enforcement of law by the Indian government 
for the welfare of sanitation workers has raised the need for an 
automated system in waste management. The existing garbage 
disposal system in India consists of unclassified waste collected 
from homes which are then segregated at a station manually. 
This segregation of solid waste done by manual labor can bring 
about many health hazards for the waste sorters in addition to 
being less efficient, time consuming and not completely feasible 
due to their large amount. In our paper, we have proposed an 
automated recognition system using Deep learning algorithm in 
Artificial Intelligence to classify objects as biodegradable and 
non-biodegradable, where the system once trained with an initial 
dataset, can identify objects real-time and classify them almost 
accurately. Biodegradable waste is used to generate power, 
enrich soil and act as food to animals. This process does not harm 
the earth making it valuable, ecologically safe and helps us to 
protect our environment, rich ecosystem and human inhabitants 
in future. ","Keeping our environment clean and eco friendly so 
that our next generation lives a disease free life is a priority. 
This project aims to support that thought. The Automatic waste 
management system is a step forward to the existing system to 
make the manual segregation of wastes easier. The developed 
system would pioneer the work for solid waste management 
process in the field of Artificial Intelligence. When properly 
trained, the system is highly efficient. The deep learning 
algorithm used here has better performance with comparatively 
less computation time. The emphasis of waste management 
policies in many countries around the world has transitioned from 
disposal to source segregation and recycling. The development of 
our project into a final product will help these countries in 
achieving their goal. 
This project proposal for the management of wastes is 
efficient and time saving than the currently employed method 
where the municipality employees perform. Though this 
system is simple in concept, it is very valuable and affordable. 
Hence to ensure being automated, a system which takes lots of 
dataset as input without human intervention and also has the 
capacity to think by itself offers the best solution. 
It acts as an aide for reducing pollution levels and in 
the long run focuses on the development of a nation and 
restoration of our ecosystem. We thus conclude that our 
project is an important asset to the society. ","AN AUTOMATIC CLASSIFICATION METHOD 
FOR ENVIRONMENTGarbage segregation, Deep learning Recent enforcement of law by the Indian government 
for the welfare of sanitation workers has raised the need for an 
automated system in waste management. The existing garbage 
disposal system in India consists of unclassified waste collected 
from homes which are then segregated at a station manually. 
This segregation of solid waste done by manual labor can bring 
about many health hazards for the waste sorters in addition to 
being less efficient, time consuming and not completely feasible 
due to their large amount. In our paper, we have proposed an 
automated recognition system using Deep learning algorithm in 
Artificial Intelligence to classify objects as biodegradable and 
non-biodegradable, where the system once trained with an initial 
dataset, can identify objects real-time and classify them almost 
accurately. Biodegradable waste is used to generate power, 
enrich soil and act as food to animals. This process does not harm 
the earth making it valuable, ecologically safe and helps us to 
protect our environment, rich ecosystem and human inhabitants 
in future. Keeping our environment clean and eco friendly so 
that our next generation lives a disease free life is a priority. 
This project aims to support that thought. The Automatic waste 
management system is a step forward to the existing system to 
make the manual segregation of wastes easier. The developed 
system would pioneer the work for solid waste management 
process in the field of Artificial Intelligence. When properly 
trained, the system is highly efficient. The deep learning 
algorithm used here has better performance with comparatively 
less computation time. The emphasis of waste management 
policies in many countries around the world has transitioned from 
disposal to source segregation and recycling. The development of 
our project into a final product will help these countries in 
achieving their goal. 
This project proposal for the management of wastes is 
efficient and time saving than the currently employed method 
where the municipality employees perform. Though this 
system is simple in concept, it is very valuable and affordable. 
Hence to ensure being automated, a system which takes lots of 
dataset as input without human intervention and also has the 
capacity to think by itself offers the best solution. 
It acts as an aide for reducing pollution levels and in 
the long run focuses on the development of a nation and 
restoration of our ecosystem. We thus conclude that our 
project is an important asset to the society. Data 224*224 conv3_I+relu3_I conv4_I+relud_I conv5_1+teluS_1 convl_2+relul_2 conv1_I+relul_1 cony2_I+relu2_1 conv3_2+relu3_2 conv4_2+relu4_2| conv5_2+relu5_2 cony2_2+relu2_2 conv3_3+relu3_3 conv4_3+relu4_3 conv5_3+reluS_3 pool pool2 pool3 poold roi_pools = jt cls_score (cls _score_) fe6+relu6 a — 6*6*256*P eis bbox_predict feT+relu7 drop7",Deep Learning and Machine Learning,"The Indian government's recent focus on the welfare of sanitation workers has led to a need for an automated waste management system. The existing system involves manual segregation of waste, which is inefficient and hazardous to the workers. The proposed solution is an automated recognition system that uses deep learning algorithms to classify objects as biodegradable or non-biodegradable. The system can accurately identify and classify objects in real-time, making the waste management process more efficient and environmentally friendly. The project aims to contribute to global waste management policies by transitioning from disposal to source segregation and recycling. The proposed system is simple, affordable, and valuable, making it an important asset to society.",Deep Learning and Machine Learning,Data 224*224 conv3_I+relu3_I conv4_I+relud_I conv5_1+teluS_1 convl_2+relul_2 conv1_I+relul_1 cony2_I+relu2_1 conv3_2+relu3_2 conv4_2+relu4_2| conv5_2+relu5_2 cony2_2+relu2_2 conv3_3+relu3_3 conv4_3+relu4_3 conv5_3+reluS_3 pool pool2 pool3 poold roi_pools = jt cls_score (cls _score_) fe6+relu6 a — 6*6*256*P eis bbox_predict feT+relu7 drop7,Object Recognition
171,The Object Detection Based on Deep Learning,"object detection; deep learning; framework design; 
model analysis; performance analysis ","The object detection based on deep learning is an 
important application in deep learning technology, which is 
characterized by its strong capability of feature learning and 
feature representation compared with the traditional object 
detection methods. The paper first makes an introduction of the 
classical methods in object detection, and expounds the relation 
and difference between the classical methods and the deep 
learning methods in object detection. Then it introduces the 
emergence of the object detection methods based on deep 
learning and elaborates the most typical methods nowadays in 
the object detection via deep learning. In the statement of the 
methods, the paper focuses on the framework design and the 
working principle of the models and analyzes the model 
performance in the real-time and the accuracy of detection. 
Eventually, it discusses the challenges in the object detection 
based on deep learning and offers some solutions for reference.","This paper firstly introduces the classical methodologies 
of object detection, discusses the relation and differences 
between the classic methodologies and the deep learning 
methodologies in object detection. Then it clarifies the ideas 
of model design and the limitations of deep learning method 
by overviewing the early object detection methods based on 
deep learning. Afterwards, it elaborates on the common object 
detection model based on deep learning, during whose 
process, it makes a detailed interpretation of the framework 
design and operation principle of the model and points out its 
innovation and performance assessment. Finally, this paper 
makes a further analysis of the challenges in object detection 
based on deep learning, and offers some solutions for 
reference. With the innovation of deep learning theories and 
computer hardware upgrading, the performance of object 
detection based on deep learning will be ceaselessly enhanced 
and the applications of it will be widely ranged. Spatially, the 
development and application of current embedded systems in 
deep learning will pave a promising prospect for object 
detection based on deep learning.","The Object Detection Based on Deep Learningobject detection; deep learning; framework design; 
model analysis; performance analysis The object detection based on deep learning is an 
important application in deep learning technology, which is 
characterized by its strong capability of feature learning and 
feature representation compared with the traditional object 
detection methods. The paper first makes an introduction of the 
classical methods in object detection, and expounds the relation 
and difference between the classical methods and the deep 
learning methods in object detection. Then it introduces the 
emergence of the object detection methods based on deep 
learning and elaborates the most typical methods nowadays in 
the object detection via deep learning. In the statement of the 
methods, the paper focuses on the framework design and the 
working principle of the models and analyzes the model 
performance in the real-time and the accuracy of detection. 
Eventually, it discusses the challenges in the object detection 
based on deep learning and offers some solutions for reference.This paper firstly introduces the classical methodologies 
of object detection, discusses the relation and differences 
between the classic methodologies and the deep learning 
methodologies in object detection. Then it clarifies the ideas 
of model design and the limitations of deep learning method 
by overviewing the early object detection methods based on 
deep learning. Afterwards, it elaborates on the common object 
detection model based on deep learning, during whose 
process, it makes a detailed interpretation of the framework 
design and operation principle of the model and points out its 
innovation and performance assessment. Finally, this paper 
makes a further analysis of the challenges in object detection 
based on deep learning, and offers some solutions for 
reference. With the innovation of deep learning theories and 
computer hardware upgrading, the performance of object 
detection based on deep learning will be ceaselessly enhanced 
and the applications of it will be widely ranged. Spatially, the 
development and application of current embedded systems in 
deep learning will pave a promising prospect for object 
detection based on deep learning.(—___""______ Image collection using camers ‘Analysis of emages r ‘Objed detection & localization ¥ ‘Object recognition tT ‘Object prediction t ‘Object classification ‘Stop",Deep Learning and Machine Learning,"This paper discusses the importance of object detection based on deep learning technology, which has stronger capabilities for feature learning and representation than traditional methods. It compares classical methods with deep learning methods in object detection and elaborates on the emergence of object detection methods based on deep learning. The paper then discusses the most typical deep learning methods used in object detection, including their framework design, working principles, and real-time detection accuracy. It concludes with a discussion of the challenges in object detection based on deep learning and offers some solutions for reference. The paper suggests that with the innovation of deep learning theories and computer hardware, object detection based on deep learning will continue to improve and have a wide range of applications, especially in current embedded systems.",Object and Sentiment Recognition,"(—___""______ Image collection using camers ‘Analysis of emages r ‘Objed detection & localization ¥ ‘Object recognition tT ‘Object prediction t ‘Object classification ‘Stop",Deep Learning and Machine Learning
172,"Super-High-Purity Seed Sorter Using Low-Latency
Image-Recognition Based on Deep Learning","Agricultural automation, deep learning in
robotics and automation, computer vision for automation.","Most commercial optical sorting systems are designed
to achieve high throughput, so they use a naive low-latency image
processing for object identification. These naive low-latency algorithms have difficulty in accurately identifying objects with various
shapes, textures, sizes, and colors, so the purity of sorted objects
is degraded. Current deep learning technology enables robust image detection and classification, but its inference latency requires
several milliseconds; thus, deep learning cannot be directly applied to such real-time high throughput applications. We therefore
developed a super-high purity seed sorting system that uses a lowlatency image-recognition based on a deep neural network and
removes the seeds of noxious weeds from mixed seed product at
high throughput with accuracy. The proposed system partitions
the detection task into localization and classification, and applies
batch inference only once strategy; it achieved 500-fps throughput
image-recognition including detection and tracking. Based on the
classified and tracked results, air ejectors expel the unwanted seeds.
This proposed system eliminates almost the whole weeds with small
loss of desired seeds,","In this letter, we developed an optical-seed-sorting system
that achieves both high throughput and high purity by exploiting deep learning using an inference only once strategy. The
batch inference only once method allows use of a CNN-based
classifier, which has high accuracy but is not easy to apply in
real-time systems because of its large inference latency. In quantitative evaluations our system achieved higher purity, classification accuracy and detection mAP, and lower loss and latency
than commercial systems and state-of-the-art detectors did. The
experiment results showed that the proposed image-recognition
system can be directly applied to actual optical sorting systems.
This system can be applied to acquire clean seed samples that
are contaminated by noxious weeds.
The proposed system can also be used to sort other objects
where results require high purity. Because this system exploits
characteristics such as shape and texture to classify objects,
it can detect inferior goods among a number of products. This
extensibility to other industries is a useful feature of this system","Super-High-Purity Seed Sorter Using Low-Latency
Image-Recognition Based on Deep LearningAgricultural automation, deep learning in
robotics and automation, computer vision for automation.Most commercial optical sorting systems are designed
to achieve high throughput, so they use a naive low-latency image
processing for object identification. These naive low-latency algorithms have difficulty in accurately identifying objects with various
shapes, textures, sizes, and colors, so the purity of sorted objects
is degraded. Current deep learning technology enables robust image detection and classification, but its inference latency requires
several milliseconds; thus, deep learning cannot be directly applied to such real-time high throughput applications. We therefore
developed a super-high purity seed sorting system that uses a lowlatency image-recognition based on a deep neural network and
removes the seeds of noxious weeds from mixed seed product at
high throughput with accuracy. The proposed system partitions
the detection task into localization and classification, and applies
batch inference only once strategy; it achieved 500-fps throughput
image-recognition including detection and tracking. Based on the
classified and tracked results, air ejectors expel the unwanted seeds.
This proposed system eliminates almost the whole weeds with small
loss of desired seeds,In this letter, we developed an optical-seed-sorting system
that achieves both high throughput and high purity by exploiting deep learning using an inference only once strategy. The
batch inference only once method allows use of a CNN-based
classifier, which has high accuracy but is not easy to apply in
real-time systems because of its large inference latency. In quantitative evaluations our system achieved higher purity, classification accuracy and detection mAP, and lower loss and latency
than commercial systems and state-of-the-art detectors did. The
experiment results showed that the proposed image-recognition
system can be directly applied to actual optical sorting systems.
This system can be applied to acquire clean seed samples that
are contaminated by noxious weeds.
The proposed system can also be used to sort other objects
where results require high purity. Because this system exploits
characteristics such as shape and texture to classify objects,
it can detect inferior goods among a number of products. This
extensibility to other industries is a useful feature of this system",Deep Learning and Machine Learning,"The letter discusses the limitations of naive low-latency algorithms used in commercial optical sorting systems, which can result in degraded purity of sorted objects due to difficulty in accurately identifying objects with various shapes, textures, sizes, and colors. The authors propose a super-high purity seed sorting system that utilizes deep learning technology for image recognition, but also addresses the inference latency issue by implementing a batch inference only once strategy. The proposed system partitions the detection task into localization and classification and achieves 500-fps throughput image-recognition. The system eliminates almost all weeds with minimal loss of desired seeds, demonstrating higher purity, classification accuracy, and detection mAP than commercial systems and state-of-the-art detectors. The system is applicable to actual optical sorting systems and has potential for use in other industries due to its ability to classify objects based on shape and texture.",Object and Sentiment Recognition,,Object Recognition
173,"Real-time Traffic Analysis Using Deep Learning Techniques 
And UAV Based Video
","urban traffic, UAV-based video, deep learning, traffic analysis, congestion, mobility metrics, real-time analysis","In urban environments there are daily issues of traffic 
congestion which city authorities need to address. Realtime analysis of traffic flow information is crucial for 
efficiently managing urban traffic. This paper aims to 
conduct traffic analysis using UAV-based videos and deep 
learning techniques. The road traffic video is collected by 
using a position-fixed UAV. The most recent deep learning 
methods are applied to identify the moving objects in 
videos. The relevant mobility metrics are calculated to 
conduct traffic analysis and measure the consequences of 
traffic congestion. The proposed approach is validated with 
the manual analysis results and the visualization results. 
The traffic analysis process is real-time in terms of the pretrained model used.","This paper presents a preliminary study on traffic 
surveillance and congestion using UAV based video and 
deep learning techniques. This initial research has raised 
issues surrounding traffic congestion which can be explored 
in future research offering the opportunity for improved 
traffic measurement metrics.
This study has established that complex traffic situations 
can result in inaccurate calculations. In Fig. 6, the traffic 
deadlock happens because a vehicle in the red box blocks 
the road due to its moving direction being orthogonal to the 
road direction. In this case, our approach can detect and 
recognize the vehicles in both the red box and the yellow 
box, however the proposed algorithm is unable to correctly 
count them. It is a challenging issue for future work. In this study, the UAV is fixed to collect the video data 
so its role is similar to a camera. We apply the geometric 
configuration and UAV’s location to estimate the spatial 
length pairing to a pixel in the image. In a future study, we 
will investigate traffic congestion in a large region with a 
mobile UAV. The pixel measurement could be more 
accurately calculated using the orthorectification with an 
appropriate mathematical model and the image 
georeferencing [14]. 
As shown in the paper, we focus on seeing the role of 
motorbike in the formation of traffic congestion in a given 
city of China. In different areas and with different 
geometric information, there will be different issues 
causing traffic congestion. Future studies will expand this 
initial approach to encompass a range of metropolitan areas 
to explore reasons for traffic congestion.
 ","Real-time Traffic Analysis Using Deep Learning Techniques 
And UAV Based Video
urban traffic, UAV-based video, deep learning, traffic analysis, congestion, mobility metrics, real-time analysisIn urban environments there are daily issues of traffic 
congestion which city authorities need to address. Realtime analysis of traffic flow information is crucial for 
efficiently managing urban traffic. This paper aims to 
conduct traffic analysis using UAV-based videos and deep 
learning techniques. The road traffic video is collected by 
using a position-fixed UAV. The most recent deep learning 
methods are applied to identify the moving objects in 
videos. The relevant mobility metrics are calculated to 
conduct traffic analysis and measure the consequences of 
traffic congestion. The proposed approach is validated with 
the manual analysis results and the visualization results. 
The traffic analysis process is real-time in terms of the pretrained model used.This paper presents a preliminary study on traffic 
surveillance and congestion using UAV based video and 
deep learning techniques. This initial research has raised 
issues surrounding traffic congestion which can be explored 
in future research offering the opportunity for improved 
traffic measurement metrics.
This study has established that complex traffic situations 
can result in inaccurate calculations. In Fig. 6, the traffic 
deadlock happens because a vehicle in the red box blocks 
the road due to its moving direction being orthogonal to the 
road direction. In this case, our approach can detect and 
recognize the vehicles in both the red box and the yellow 
box, however the proposed algorithm is unable to correctly 
count them. It is a challenging issue for future work. In this study, the UAV is fixed to collect the video data 
so its role is similar to a camera. We apply the geometric 
configuration and UAV’s location to estimate the spatial 
length pairing to a pixel in the image. In a future study, we 
will investigate traffic congestion in a large region with a 
mobile UAV. The pixel measurement could be more 
accurately calculated using the orthorectification with an 
appropriate mathematical model and the image 
georeferencing [14]. 
As shown in the paper, we focus on seeing the role of 
motorbike in the formation of traffic congestion in a given 
city of China. In different areas and with different 
geometric information, there will be different issues 
causing traffic congestion. Future studies will expand this 
initial approach to encompass a range of metropolitan areas 
to explore reasons for traffic congestion.
 Camera event Single-seed images Tracking information cPU1 cPU2 Localization and tracking Image acquisition Image fram CPUS GPU inference trigger -+-——>} (Batch inference) @Pu Classification cPUu4 Valve control Classification result 10 ms delay <>",Deep Learning and Machine Learning,"This paper proposes using UAV-based videos and deep learning techniques for real-time traffic analysis in urban environments. The study validates the approach by comparing it to manual analysis and visualizations. The paper acknowledges challenges in accurately calculating metrics in complex traffic situations and suggests future research to explore improved measurement methods and expand the approach to other metropolitan areas. The study focuses on the role of motorbikes in traffic congestion in a specific city in China, but the approach can be applied to different areas with different issues causing traffic congestion.",Deep Learning and Machine Learning,Camera event Single-seed images Tracking information cPU1 cPU2 Localization and tracking Image acquisition Image fram CPUS GPU inference trigger -+-——>} (Batch inference) @Pu Classification cPUu4 Valve control Classification result 10 ms delay <>,Object Recognition
174,Face analysis and synthesis,"Avatars
,
Mouth
,
Shape control
,
Neural networks
,
Speech
,
Muscles
,
Network synthesis
,
Humans
,
Signal generators
,
Communication system control","The author's goal is to generate a virtual space close to the real communication environment between network users or between humans and machines. There should be an avatar in cyberspace that projects the features of each user with a realistic texture-mapped face to generate facial expression and action controlled by a multimodal input signal. Users can also get a view in cyberspace through the avatar's eyes, so they can communicate with each other by gaze crossing. The face fitting tool from multi-view camera images is introduced to make a realistic three-dimensional (3-D) face model with texture and geometry very close to the original. This fitting tool is a GUI-based system using easy mouse operation to pick up each feature point on a face contour and the face parts, which can enable easy construction of a 3-D personal face model. When an avatar is speaking, the voice signal is essential in determining the mouth shape feature. Therefore, a real-time mouth shape control mechanism is proposed by using a neural network to convert speech parameters to lip shape parameters. This neural network can realize an interpolation between specific mouth shapes given as learning data. The emotional factor can sometimes be captured by speech parameters. This media conversion mechanism is described. For dynamic modeling of facial expression, a muscle structure constraint is introduced for making a facial expression naturally with few parameters. We also tried to obtain muscle parameters automatically from a local motion vector on the face calculated by the optical flow in a video sequence.","In this thesis paper, the author proposes a virtual space that mimics real communication environments between network users or between humans and machines. The author introduces an avatar that projects the features of each user with a realistic texture-mapped face to generate facial expressions and actions controlled by a multimodal input signal. A face fitting tool is introduced to create a realistic 3-D personal face model using easy mouse operation. The author proposes a real-time mouth shape control mechanism using a neural network to convert speech parameters to lip shape parameters. A muscle structure constraint is introduced for making facial expressions naturally with few parameters. The author also attempts to obtain muscle parameters automatically from a local motion vector on the face calculated by the optical flow in a video sequence. Overall, the proposed system aims to provide a more realistic and immersive communication experience in cyberspace.","Face analysis and synthesisAvatars
,
Mouth
,
Shape control
,
Neural networks
,
Speech
,
Muscles
,
Network synthesis
,
Humans
,
Signal generators
,
Communication system controlThe author's goal is to generate a virtual space close to the real communication environment between network users or between humans and machines. There should be an avatar in cyberspace that projects the features of each user with a realistic texture-mapped face to generate facial expression and action controlled by a multimodal input signal. Users can also get a view in cyberspace through the avatar's eyes, so they can communicate with each other by gaze crossing. The face fitting tool from multi-view camera images is introduced to make a realistic three-dimensional (3-D) face model with texture and geometry very close to the original. This fitting tool is a GUI-based system using easy mouse operation to pick up each feature point on a face contour and the face parts, which can enable easy construction of a 3-D personal face model. When an avatar is speaking, the voice signal is essential in determining the mouth shape feature. Therefore, a real-time mouth shape control mechanism is proposed by using a neural network to convert speech parameters to lip shape parameters. This neural network can realize an interpolation between specific mouth shapes given as learning data. The emotional factor can sometimes be captured by speech parameters. This media conversion mechanism is described. For dynamic modeling of facial expression, a muscle structure constraint is introduced for making a facial expression naturally with few parameters. We also tried to obtain muscle parameters automatically from a local motion vector on the face calculated by the optical flow in a video sequence.In this thesis paper, the author proposes a virtual space that mimics real communication environments between network users or between humans and machines. The author introduces an avatar that projects the features of each user with a realistic texture-mapped face to generate facial expressions and actions controlled by a multimodal input signal. A face fitting tool is introduced to create a realistic 3-D personal face model using easy mouse operation. The author proposes a real-time mouth shape control mechanism using a neural network to convert speech parameters to lip shape parameters. A muscle structure constraint is introduced for making facial expressions naturally with few parameters. The author also attempts to obtain muscle parameters automatically from a local motion vector on the face calculated by the optical flow in a video sequence. Overall, the proposed system aims to provide a more realistic and immersive communication experience in cyberspace.#ERROR!",Artificial Neural Network,"The author proposes a virtual space that mimics real communication environments between network users or between humans and machines, using avatars with realistic texture-mapped faces to generate facial expressions and actions controlled by a multimodal input signal. A face fitting tool is introduced to create a realistic 3-D personal face model, and a real-time mouth shape control mechanism is proposed using a neural network. A muscle structure constraint is introduced for making facial expressions naturally with few parameters, and an attempt is made to obtain muscle parameters automatically from a local motion vector on the face calculated by the optical flow in a video sequence. The goal is to provide a more realistic and immersive communication experience in cyberspace.",Deep Learning and Machine Learning,#ERROR!,Deep Learning and Machine Learning
175,Automatic facial expression recognition using facial animation parameters and multistream HMMs,"Face recognition
,
Facial animation
,
Hidden Markov models
,
Face detection
,
Financial advantage program
,
Human computer interaction
,
Performance analysis
,
Eyebrows
,
Facial features
,
Speech recognition","The performance of an automatic facial expression recognition system can be significantly improved by modeling the reliability of different streams of facial expression information utilizing multistream hidden Markov models (HMMs). In this paper, we present an automatic multistream HMM facial expression recognition system and analyze its performance. The proposed system utilizes facial animation parameters (FAPs), supported by the MPEG-4 standard, as features for facial expression classification. Specifically, the FAPs describing the movement of the outer-lip contours and eyebrows are used as observations. Experiments are first performed employing single-stream HMMs under several different scenarios, utilizing outer-lip and eyebrow FAPs individually and jointly. A multistream HMM approach is proposed for introducing facial expression and FAP group dependent stream reliability weights. The stream weights are determined based on the facial expression recognition results obtained when FAP streams are utilized individually. The proposed multistream HMM facial expression system, which utilizes stream reliability weights, achieves relative reduction of the facial expression recognition error of 44% compared to the single-stream HMM system.","This thesis paper proposes an automatic multistream hidden Markov model (HMM) facial expression recognition system that utilizes facial animation parameters (FAPs) to improve the performance of the system. The FAPs that describe the movement of the outer-lip contours and eyebrows are used as observations for facial expression classification. The proposed system introduces facial expression and FAP group dependent stream reliability weights to model the reliability of different streams of facial expression information. Experiments show that the multistream HMM approach with stream reliability weights achieves a significant reduction of the facial expression recognition error compared to the single-stream HMM system. This research provides a new method to improve the performance of automatic facial expression recognition systems and has practical applications in various fields, such as human-computer interaction, affective computing, and robotics.","Automatic facial expression recognition using facial animation parameters and multistream HMMsFace recognition
,
Facial animation
,
Hidden Markov models
,
Face detection
,
Financial advantage program
,
Human computer interaction
,
Performance analysis
,
Eyebrows
,
Facial features
,
Speech recognitionThe performance of an automatic facial expression recognition system can be significantly improved by modeling the reliability of different streams of facial expression information utilizing multistream hidden Markov models (HMMs). In this paper, we present an automatic multistream HMM facial expression recognition system and analyze its performance. The proposed system utilizes facial animation parameters (FAPs), supported by the MPEG-4 standard, as features for facial expression classification. Specifically, the FAPs describing the movement of the outer-lip contours and eyebrows are used as observations. Experiments are first performed employing single-stream HMMs under several different scenarios, utilizing outer-lip and eyebrow FAPs individually and jointly. A multistream HMM approach is proposed for introducing facial expression and FAP group dependent stream reliability weights. The stream weights are determined based on the facial expression recognition results obtained when FAP streams are utilized individually. The proposed multistream HMM facial expression system, which utilizes stream reliability weights, achieves relative reduction of the facial expression recognition error of 44% compared to the single-stream HMM system.This thesis paper proposes an automatic multistream hidden Markov model (HMM) facial expression recognition system that utilizes facial animation parameters (FAPs) to improve the performance of the system. The FAPs that describe the movement of the outer-lip contours and eyebrows are used as observations for facial expression classification. The proposed system introduces facial expression and FAP group dependent stream reliability weights to model the reliability of different streams of facial expression information. Experiments show that the multistream HMM approach with stream reliability weights achieves a significant reduction of the facial expression recognition error compared to the single-stream HMM system. This research provides a new method to improve the performance of automatic facial expression recognition systems and has practical applications in various fields, such as human-computer interaction, affective computing, and robotics.",Person recognition,"This thesis proposes an automatic facial expression recognition system that utilizes multistream hidden Markov models (HMMs) to improve performance by modeling the reliability of different streams of facial expression information. The system utilizes facial animation parameters (FAPs) to classify facial expressions and introduces stream reliability weights to model the reliability of different streams of facial expression information. Experiments show that the proposed system achieves a significant reduction of facial expression recognition error compared to the single-stream HMM system, with practical applications in fields such as human-computer interaction, affective computing, and robotics.



",Object and Sentiment Recognition,,Deep Learning and Machine Learning
176,Emotion Recognition in Cartoons Using Convolutional Neural Networks,"Estimation
,
Computer architecture
,
Covariance matrices
,
Cognitive systems
,
Cognitive science
,
Optimal control
,
Neuroscience","In psychology and neuroscience it is common to describe cognitive systems as input/output devices where perceptual and motor functions are implemented in a purely feedforward, open-loop fashion. On this view, perception and action are often seen as encapsulated modules with limited interaction between them. While embodied and enactive approaches to cognitive science have challenged the idealisation of the brain as an input/output device, we argue that even the more recent attempts to model systems using closed-loop architectures still heavily rely on a strong separation between motor and perceptual functions. Previously, we have suggested that the mainstream notion of modularity strongly resonates with the separation principle of control theory. In this work we present a minimal model of a sensorimotor loop implementing an architecture based on the separation principle. We link this to popular formulations of perception and action in the cognitive sciences, and show its limitations when, for instance, external forces are not modelled by an agent. These forces can be seen as variables that an agent cannot directly control, i.e., a perturbation from the environment or an interference caused by other agents. As an alternative approach inspired by embodied cognitive science, we then propose a nonmodular architecture based on active inference. We demonstrate the robustness of this architecture to unknown external inputs and show that the mechanism with which this is achieved in linear models is equivalent to integral control.","The traditional view of cognitive systems as input/output devices with limited interaction between perception and action has been challenged by embodied and enactive approaches. However, even recent closed-loop models still rely on a strong separation between motor and perceptual functions. This study presents a minimal model of a sensorimotor loop based on the separation principle, and highlights its limitations when faced with external forces or interference. An alternative non-modular architecture based on active inference is proposed, which is shown to be robust to unknown external inputs and equivalent to integral control in linear models.","Emotion Recognition in Cartoons Using Convolutional Neural NetworksEstimation
,
Computer architecture
,
Covariance matrices
,
Cognitive systems
,
Cognitive science
,
Optimal control
,
NeuroscienceIn psychology and neuroscience it is common to describe cognitive systems as input/output devices where perceptual and motor functions are implemented in a purely feedforward, open-loop fashion. On this view, perception and action are often seen as encapsulated modules with limited interaction between them. While embodied and enactive approaches to cognitive science have challenged the idealisation of the brain as an input/output device, we argue that even the more recent attempts to model systems using closed-loop architectures still heavily rely on a strong separation between motor and perceptual functions. Previously, we have suggested that the mainstream notion of modularity strongly resonates with the separation principle of control theory. In this work we present a minimal model of a sensorimotor loop implementing an architecture based on the separation principle. We link this to popular formulations of perception and action in the cognitive sciences, and show its limitations when, for instance, external forces are not modelled by an agent. These forces can be seen as variables that an agent cannot directly control, i.e., a perturbation from the environment or an interference caused by other agents. As an alternative approach inspired by embodied cognitive science, we then propose a nonmodular architecture based on active inference. We demonstrate the robustness of this architecture to unknown external inputs and show that the mechanism with which this is achieved in linear models is equivalent to integral control.The traditional view of cognitive systems as input/output devices with limited interaction between perception and action has been challenged by embodied and enactive approaches. However, even recent closed-loop models still rely on a strong separation between motor and perceptual functions. This study presents a minimal model of a sensorimotor loop based on the separation principle, and highlights its limitations when faced with external forces or interference. An alternative non-modular architecture based on active inference is proposed, which is shown to be robust to unknown external inputs and equivalent to integral control in linear models.200 150 100 ‘Velocity (m/s) 100 150 Action a (m/s*) 200 Double integrator - Active inference: @ Agent. @ Agent? @ Agent Agent 4 © Agent 5 Target 150-125-100 75-50 -25 6 25 50 Position (m) (a) ‘Action of double integrator - Active inference oi 234567 6 SON isis Time(s) (b)Spring-mass-damper system a,x m=tkgDouble integrator - LOG ws zontal Agent 2 0 fens Agent 4. _ fens E so > Eos : ° 2s 7 ta 50 get <9 SI) Fabien) (a) oi? 3 45 67 6 9 tO I isis Time(s) (b)Sliding block on a frictionless surface, target: x = 0, x’=0 Force applied50 Target Z 4 Trajectory E with ext. force > : é g -50 Agent 1 Agent 2 . Agent 3 100 Agent 4 Agent § =i0o 8360 380360 a0 poston im) (a) Agfjon of double integrator - LQG, no externa force in KBF —— Agent 1 —— Agent 2 350 —— Agent 3, — Agent 4 100 het 5 — ext fore so ° -s0 1005 123 4 5 6 7 8 9 10 11 12 13 14 15 Time(s) (b)",Deep Learning and Machine Learning,"The study argues that the traditional view of cognitive systems as input/output devices with limited interaction between perception and action is incomplete, even in more recent closed-loop models. The researchers present a minimal model of a sensorimotor loop that emphasizes the limitations of a separation principle-based architecture when faced with external forces or interference. An alternative non-modular architecture based on active inference is proposed, which is shown to be robust to unknown external inputs and equivalent to integral control in linear models.",Object and Sentiment Recognition,"200 150 100 ‘Velocity (m/s) 100 150 Action a (m/s*) 200 Double integrator - Active inference: @ Agent. @ Agent? @ Agent Agent 4 © Agent 5 Target 150-125-100 75-50 -25 6 25 50 Position (m) (a) ‘Action of double integrator - Active inference oi 234567 6 SON isis Time(s) (b)Spring-mass-damper system a,x m=tkgDouble integrator - LOG ws zontal Agent 2 0 fens Agent 4. _ fens E so > Eos : ° 2s 7 ta 50 get <9 SI) Fabien) (a) oi? 3 45 67 6 9 tO I isis Time(s) (b)Sliding block on a frictionless surface, target: x = 0, x’=0 Force applied50 Target Z 4 Trajectory E with ext. force > : é g -50 Agent 1 Agent 2 . Agent 3 100 Agent 4 Agent § =i0o 8360 380360 a0 poston im) (a) Agfjon of double integrator - LQG, no externa force in KBF —— Agent 1 —— Agent 2 350 —— Agent 3, — Agent 4 100 het 5 — ext fore so ° -s0 1005 123 4 5 6 7 8 9 10 11 12 13 14 15 Time(s) (b)",Sentiment Analysis
177,Real-time speech-driven face animation with expressions using neural networks,"Facial animation
,
Neural networks
,
Face
,
Humans
,
Real time systems
,
Audio databases
,
Collaboration
,
Deformable models
,
Motion analysis
,
Linear approximation","A real-time speech-driven synthetic talking face provides an effective multimodal communication interface in distributed collaboration environments. Nonverbal gestures such as facial expressions are important to human communication and should be considered by speech-driven face animation systems. In this paper, we present a framework that systematically addresses facial deformation modeling, automatic facial motion analysis, and real-time speech-driven face animation with expression using neural networks. Based on this framework, we learn a quantitative visual representation of the facial deformations, called the motion units (MUs). A facial deformation can be approximated by a linear combination of the MUs weighted by MU parameters (MUPs). We develop an MU-based facial motion tracking algorithm which is used to collect an audio-visual training database. Then, we construct a real-time audio-to-MUP mapping by training a set of neural networks using the collected audio-visual training database. The quantitative evaluation of the mapping shows the effectiveness of the proposed approach. Using the proposed method, we develop the functionality of real-time speech-driven face animation with expressions for the iFACE system. Experimental results show that the synthetic expressive talking face of the iFACE system is comparable with a real face in terms of the effectiveness of their influences on bimodal human emotion perception.","In summary, this paper presents a framework for real-time speech-driven face animation with expressions using neural networks. The framework includes facial deformation modeling, automatic facial motion analysis, and audio-to-MUP mapping. The study demonstrates the effectiveness of the proposed approach through quantitative evaluations and the development of the iFACE system. The synthetic expressive talking face of the iFACE system is shown to be comparable to a real face in terms of its influence on bimodal human emotion perception, making it an effective multimodal communication interface in distributed collaboration environments.



","Real-time speech-driven face animation with expressions using neural networksFacial animation
,
Neural networks
,
Face
,
Humans
,
Real time systems
,
Audio databases
,
Collaboration
,
Deformable models
,
Motion analysis
,
Linear approximationA real-time speech-driven synthetic talking face provides an effective multimodal communication interface in distributed collaboration environments. Nonverbal gestures such as facial expressions are important to human communication and should be considered by speech-driven face animation systems. In this paper, we present a framework that systematically addresses facial deformation modeling, automatic facial motion analysis, and real-time speech-driven face animation with expression using neural networks. Based on this framework, we learn a quantitative visual representation of the facial deformations, called the motion units (MUs). A facial deformation can be approximated by a linear combination of the MUs weighted by MU parameters (MUPs). We develop an MU-based facial motion tracking algorithm which is used to collect an audio-visual training database. Then, we construct a real-time audio-to-MUP mapping by training a set of neural networks using the collected audio-visual training database. The quantitative evaluation of the mapping shows the effectiveness of the proposed approach. Using the proposed method, we develop the functionality of real-time speech-driven face animation with expressions for the iFACE system. Experimental results show that the synthetic expressive talking face of the iFACE system is comparable with a real face in terms of the effectiveness of their influences on bimodal human emotion perception.In summary, this paper presents a framework for real-time speech-driven face animation with expressions using neural networks. The framework includes facial deformation modeling, automatic facial motion analysis, and audio-to-MUP mapping. The study demonstrates the effectiveness of the proposed approach through quantitative evaluations and the development of the iFACE system. The synthetic expressive talking face of the iFACE system is shown to be comparable to a real face in terms of its influence on bimodal human emotion perception, making it an effective multimodal communication interface in distributed collaboration environments.



Fae Fre\ Fre) Fae\ Fre\ Fae) Feel Fre\ Fe\4 —_—Labeled facial deformations ‘Learn the facial deformat ‘Train real-time audio- to-MUP mapping __) Speech stream ee",Artificial Neural Network,"This paper presents a framework for real-time speech-driven face animation with expressions using neural networks. The framework includes facial deformation modeling, automatic facial motion analysis, and audio-to-MUP mapping. The study demonstrates the effectiveness of the proposed approach through quantitative evaluations and the development of the iFACE system. The synthetic expressive talking face of the iFACE system is shown to be comparable to a real face in terms of its influence on bimodal human emotion perception, making it an effective multimodal communication interface in distributed collaboration environments.



",Object and Sentiment Recognition,Fae Fre\ Fre) Fae\ Fre\ Fae) Feel Fre\ Fe\4 —_—Labeled facial deformations ‘Learn the facial deformat ‘Train real-time audio- to-MUP mapping __) Speech stream ee,Sentiment Analysis
178,Deep learning-based personality recognition from text posts of online social networks,"Personality recognition,
Deep learning,
Online social networks,
Big Five personality","
Personality is an important psychological construct accounting for individual differences in people. Computational personality recognition from online social networks is gaining increased research attention in recent years. However, the majority of existing methodologies mainly focused on human-designed shallow statistical features and didn’t make full use of the rich semantic information in user-generated texts, while those texts are exactly the most direct way for people to translate their internal thoughts and emotions into a form that others can understand. This paper proposes a deep learning-based approach for personality recognition from text posts of online social network users. We first utilize a hierarchical deep neural network composed of our newly designed AttRCNN structure and a variant of the Inception structure to learn the deep semantic features of each user’s text posts. Then we concatenate the deep semantic features with the statistical linguistic features obtained directly from the text posts, and feed them into traditional regression algorithms to predict the real-valued Big Five personality scores. Experimental results show that the deep semantic feature vectors learned from our proposed neural network are more effective than the other four kinds of non-trivial baseline features; the approach that utilizes the concatenation of our deep semantic features and the statistical linguistic features as the input of the gradient boosting regression algorithm achieves the lowest average prediction error among all the approaches tested by us.","   Computational personality recognition is an emerging research field that consists of the automatic inference of users’ personality traits from publicly available information on online social platforms. In this paper, we present a two-level hierarchical neural network based on the newly designed AttRCNN structure and a variant of the CNN-based Inception structure to learn the deep semantic representations of online social network users’ text posts. Experimental evaluation shows that taking these kinds of deep semantic features as input of traditional regression algorithms contribute a lot to the performance improvement of Big Five personality recognition approaches. In future work, we will utilize these kind of deep semantic features as the input of some special designed regression algorithms so as to further improve the prediction accuracy of the personality recognition approaches.","Deep learning-based personality recognition from text posts of online social networksPersonality recognition,
Deep learning,
Online social networks,
Big Five personality
Personality is an important psychological construct accounting for individual differences in people. Computational personality recognition from online social networks is gaining increased research attention in recent years. However, the majority of existing methodologies mainly focused on human-designed shallow statistical features and didn’t make full use of the rich semantic information in user-generated texts, while those texts are exactly the most direct way for people to translate their internal thoughts and emotions into a form that others can understand. This paper proposes a deep learning-based approach for personality recognition from text posts of online social network users. We first utilize a hierarchical deep neural network composed of our newly designed AttRCNN structure and a variant of the Inception structure to learn the deep semantic features of each user’s text posts. Then we concatenate the deep semantic features with the statistical linguistic features obtained directly from the text posts, and feed them into traditional regression algorithms to predict the real-valued Big Five personality scores. Experimental results show that the deep semantic feature vectors learned from our proposed neural network are more effective than the other four kinds of non-trivial baseline features; the approach that utilizes the concatenation of our deep semantic features and the statistical linguistic features as the input of the gradient boosting regression algorithm achieves the lowest average prediction error among all the approaches tested by us.   Computational personality recognition is an emerging research field that consists of the automatic inference of users’ personality traits from publicly available information on online social platforms. In this paper, we present a two-level hierarchical neural network based on the newly designed AttRCNN structure and a variant of the CNN-based Inception structure to learn the deep semantic representations of online social network users’ text posts. Experimental evaluation shows that taking these kinds of deep semantic features as input of traditional regression algorithms contribute a lot to the performance improvement of Big Five personality recognition approaches. In future work, we will utilize these kind of deep semantic features as the input of some special designed regression algorithms so as to further improve the prediction accuracy of the personality recognition approaches.“J id oJLabeled facial deformations ‘Learn the facial deformat ‘Train real-time audio- to-MUP mapping __) Speech stream eeTraining set Neutral Smile Sad R 0.980 0.972 0.977 Va 0.197 0.205 0.209 Va 0.179 0.192 0.197 MSE | 0.0025 | 0.0031 | 0.0033",Person recognition,"The paper proposes a deep learning-based approach for personality recognition from text posts on online social networks. The method uses a hierarchical deep neural network to learn deep semantic features from user-generated text posts and combines them with statistical linguistic features to predict the Big Five personality scores. Experimental results show that the deep semantic feature vectors are more effective than other non-trivial baseline features, and the approach that utilizes the concatenation of deep semantic features and statistical linguistic features achieves the lowest prediction error. The proposed approach contributes significantly to the performance improvement of Big Five personality recognition approaches. Future work will focus on utilizing deep semantic features as input for specially designed regression algorithms to further improve prediction accuracy",Object and Sentiment Recognition,“J id oJLabeled facial deformations ‘Learn the facial deformat ‘Train real-time audio- to-MUP mapping __) Speech stream eeTraining set Neutral Smile Sad R 0.980 0.972 0.977 Va 0.197 0.205 0.209 Va 0.179 0.192 0.197 MSE | 0.0025 | 0.0031 | 0.0033,Object Recognition
179,"Toward Large-Scale Face
Recognition Using Social
Network Context"," Face recognition; graphical models; social
network context; structured prediction"," Personal photographs are being captured in
digital form at an accelerating rate, and our computational
tools for searching, browsing, and sharing these photos are
struggling to keep pace. One promising approach is automatic
face recognition, which would allow photos to be organized by
the identities of the individuals they contain. However,
achieving accurate recognition at the scale of the Web requires
discriminating among hundreds of millions of individuals and
would seem to be a daunting task. This paper argues that social
network context may be the key for large-scale face recognition to succeed. Many personal photographs are shared on the
Web through online social network sites, and we can leverage
the resources and structure of such social networks to improve
face recognition rates on the images shared. Drawing upon real
photo collections from volunteers who are members of a
popular online social network, we asses the availability of
resources to improve face recognition and discuss techniques
for applying these resources"," To advance the state of
the art in face recognition, the questions of how best to
apply these data and how to build scalable recognition
systems are worthy of attention. This paper argues that
social network context is an important tool for assembling
scalable recognition systems, and it provides an example of
a simple computational architecture for utilizing this contextual information.
We have only begun to consider the wide variety of
social signals that are readily available from Facebook and
other online social networks to improve recognition, and
additional sources of information will undoubtedly provide
a far bigger boost in recognition accuracy than we observed
in this small study. Photo timestamps, gender information,
individuals’ names [50] and positions within a photo [21],
scene context [51]–[55], and various sources of within album information [16]–[20], [56] are all immediate
possibilities.
In order to put all of this information to use, it will
likely be beneficial to move beyond the simple pairwise
MRF structure described in this paper. For example, one
might build graphs that span multiple photos to jointly
recognize individuals over a short stretch of time or an
entire event, and hierarchical models might capture
group effects caused by shared affiliations having salient
visual signatures (soccer teams, outdoor clubs, cultural
societies, etc.).
In all of these cases, the increased complexity of the
graphical model will make (approximate) inference and
learning more difficult, and this provides an intriguing
application for efficient techniques that have recently been
proposed (e.g., [38], [40], and [57]–[61]). Also, since the
size of the graphical model will often vary from one photo
(or event) to the next, one must explore whether it is
possible to learn a single set of parameters for variable sized graphs or whether a separate set of parameters must
be learned for each graph topology.
Ultimately, the growth of online social networks, the
development of improved social tagging systems, and the
increasing interconnectivity of the web have the potential
to enhance our ability to achieve face recognition at scale.
Exploring computational techniques that take advantage of
these trends seems a worthwhile endeavor. ","Toward Large-Scale Face
Recognition Using Social
Network Context Face recognition; graphical models; social
network context; structured prediction Personal photographs are being captured in
digital form at an accelerating rate, and our computational
tools for searching, browsing, and sharing these photos are
struggling to keep pace. One promising approach is automatic
face recognition, which would allow photos to be organized by
the identities of the individuals they contain. However,
achieving accurate recognition at the scale of the Web requires
discriminating among hundreds of millions of individuals and
would seem to be a daunting task. This paper argues that social
network context may be the key for large-scale face recognition to succeed. Many personal photographs are shared on the
Web through online social network sites, and we can leverage
the resources and structure of such social networks to improve
face recognition rates on the images shared. Drawing upon real
photo collections from volunteers who are members of a
popular online social network, we asses the availability of
resources to improve face recognition and discuss techniques
for applying these resources To advance the state of
the art in face recognition, the questions of how best to
apply these data and how to build scalable recognition
systems are worthy of attention. This paper argues that
social network context is an important tool for assembling
scalable recognition systems, and it provides an example of
a simple computational architecture for utilizing this contextual information.
We have only begun to consider the wide variety of
social signals that are readily available from Facebook and
other online social networks to improve recognition, and
additional sources of information will undoubtedly provide
a far bigger boost in recognition accuracy than we observed
in this small study. Photo timestamps, gender information,
individuals’ names [50] and positions within a photo [21],
scene context [51]–[55], and various sources of within album information [16]–[20], [56] are all immediate
possibilities.
In order to put all of this information to use, it will
likely be beneficial to move beyond the simple pairwise
MRF structure described in this paper. For example, one
might build graphs that span multiple photos to jointly
recognize individuals over a short stretch of time or an
entire event, and hierarchical models might capture
group effects caused by shared affiliations having salient
visual signatures (soccer teams, outdoor clubs, cultural
societies, etc.).
In all of these cases, the increased complexity of the
graphical model will make (approximate) inference and
learning more difficult, and this provides an intriguing
application for efficient techniques that have recently been
proposed (e.g., [38], [40], and [57]–[61]). Also, since the
size of the graphical model will often vary from one photo
(or event) to the next, one must explore whether it is
possible to learn a single set of parameters for variable sized graphs or whether a separate set of parameters must
be learned for each graph topology.
Ultimately, the growth of online social networks, the
development of improved social tagging systems, and the
increasing interconnectivity of the web have the potential
to enhance our ability to achieve face recognition at scale.
Exploring computational techniques that take advantage of
these trends seems a worthwhile endeavor. CNN-based Document Encoder FROLU Activation | EN 1005, BatchNormalization <N<100> Dropout (| <Nx100> (100) <n 100Fig. 2. Structure of the AMRCNN-based Sentence Encoder. The GRU layer and dropout layer are donated as “GRU (number of neurons)-sean : direction” and “Dropo MaxPooling an i <8x100> (dropout rate)”, respectively. AttRCNN-based ‘The output shape of each layer ntence En a ReLU Activation is shown within angle brackets Sentence Encode <SxWx 100> ‘TimeDistributed (FC (100)) <SxWx 100 > ‘Concatenation <8 xWx (100+€) > —_______¥ Gi ~ G2 | ” Dropout (dr) Dropout (dr) <SxWx50> i <SxWx50> AttentionLayer AttentionLayer <SxWx50> <SxWx50> ReLU Activation E ReLU Activation <SxWx50> <SxWXx50> BatchNormalization | ! | BatchNormalization <SxWXx50> <SxWx50>Deep learning-based personality recognition from text posts of online social networks 4239 Fig.3‘The process of obtaining final semantic representation the distributed semantic ‘of word representation of word wis by AMRCNN: (_ AuentionLayee | Le | 7 \ hy intermediate word 2 2 + intermediate word ‘annotaons of ti, |G ag a, — a, annotations of lefesde context right side context pre-trained word = MMe ee See =e rightside contextFig. 1. Schematic of our proposed hierarchical neural network for text posts modeling. ‘The fully-connected layer is, donated as “FC (number of ‘neurons)”. The shape of each object is shown within angle brackets, and so is the output shape of each layer eye) Ponca ie eo) Pes aed eee} Pocus ii cen eres) eons ae i",Person recognition,"This paper discusses the potential of leveraging social network context for scalable face recognition systems. The authors argue that social incentives, such as identity tags on Facebook, can provide significant quantities of labeled facial images of millions of individuals. They provide an example of a computational architecture that utilizes contextual information from social networks. The paper suggests that there are many additional sources of information that can be used to improve recognition accuracy, such as photo timestamps, gender information, and scene context. The authors propose exploring techniques that take advantage of the resources and structure of social networks to improve face recognition rates on shared images. Ultimately, the paper concludes that the growth of online social networks and improved tagging systems have the potential to enhance our ability to achieve face recognition at scale.",Object and Sentiment Recognition,"CNN-based Document Encoder FROLU Activation | EN 1005, BatchNormalization <N<100> Dropout (| <Nx100> (100) <n 100Fig. 2. Structure of the AMRCNN-based Sentence Encoder. The GRU layer and dropout layer are donated as “GRU (number of neurons)-sean : direction” and “Dropo MaxPooling an i <8x100> (dropout rate)”, respectively. AttRCNN-based ‘The output shape of each layer ntence En a ReLU Activation is shown within angle brackets Sentence Encode <SxWx 100> ‘TimeDistributed (FC (100)) <SxWx 100 > ‘Concatenation <8 xWx (100+€) > —_______¥ Gi ~ G2 | ” Dropout (dr) Dropout (dr) <SxWx50> i <SxWx50> AttentionLayer AttentionLayer <SxWx50> <SxWx50> ReLU Activation E ReLU Activation <SxWx50> <SxWXx50> BatchNormalization | ! | BatchNormalization <SxWXx50> <SxWx50>Deep learning-based personality recognition from text posts of online social networks 4239 Fig.3‘The process of obtaining final semantic representation the distributed semantic ‘of word representation of word wis by AMRCNN: (_ AuentionLayee | Le | 7 \ hy intermediate word 2 2 + intermediate word ‘annotaons of ti, |G ag a, — a, annotations of lefesde context right side context pre-trained word = MMe ee See =e rightside contextFig. 1. Schematic of our proposed hierarchical neural network for text posts modeling. ‘The fully-connected layer is, donated as “FC (number of ‘neurons)”. The shape of each object is shown within angle brackets, and so is the output shape of each layer eye) Ponca ie eo) Pes aed eee} Pocus ii cen eres) eons ae i",Sentiment Analysis
180,"Knowledge of words: An interpretable approach for personality
recognition from social media","Personality recognition,
Big five,
Lexicon,
Social media.","Personality is one of the fundamental and stable individual characteristics that can be detected from
human behavioral data. With the rise of social media, increasing attention has been paid to the ability
to recognize personality traits by analyzing the contents of user-generated text. Existing studies have
used general psychological lexicons or machine learning, and even deep learning models, to predict
personality, but their performance has been relatively poor or they have lacked the ability to interpret
personality. In this paper, we present a novel interpretable personality recognition model based on
a personality lexicon. First, we use word embedding techniques and prior-knowledge lexicons to
automatically construct a Chinese semantic lexicon suitable for personality analysis. Based on this
personality lexicon, we analyze the correlations between personality traits and semantic categories
of words, and extract the semantic features of users’ microblogs to construct personality recognition
models using classification algorithms. Extensive experiments are conducted to demonstrate that the
proposed model can achieve significantly better performances compared to previous approaches.","In this paper, we present an automatic approach to construct-
ing a personality lexicon suitable for personality recognition.
From users’ microblogs, we first extract representative words
using text-mining methods. We then use a clustering algorithm
to separate the representative words into various groups, each
of which is defined as a semantic category. The identified se-
mantic categories form a semantic lexicon as the first Chinese
personality lexicon. During the lexicon’s construction, a main
challenge pertains to how to accurately represent the words. We
use word embedding in a large corpus to obtain original word
vectors, and then use popular Chinese lexicons to refine word
vectors. Based on the personality lexicon, we analyze the correla-
tions between personality traits and word categories, and thereby
enable exploration and interpretation of personality traits using
knowledge of words. The personality lexicon is used to generate
semantic features of words, which are used to train personality
recognition models with the aid of machine learning classifiers.
A large number of experiments are carried out to verify that the
proposed personality recognition model can perform significantly
better compared to previous approaches.
In future, we will utilize distributional contextual representa-
tions of the keywords to obtain better word vectors. Furthermore,
the hierarchical clustering and information extraction approaches
S. Han, H. Huang and Y. Tang / Knowledge-Based Systems 194 (2020) 105550 19
will be presented to obtain a personality lexicon with more com-
plicated semantic relations, including hierarchical semantics and
fine-grained semantics of words. This will provide psychologists
with a vital tool to deeply study and interpret personality traits.","Knowledge of words: An interpretable approach for personality
recognition from social mediaPersonality recognition,
Big five,
Lexicon,
Social media.Personality is one of the fundamental and stable individual characteristics that can be detected from
human behavioral data. With the rise of social media, increasing attention has been paid to the ability
to recognize personality traits by analyzing the contents of user-generated text. Existing studies have
used general psychological lexicons or machine learning, and even deep learning models, to predict
personality, but their performance has been relatively poor or they have lacked the ability to interpret
personality. In this paper, we present a novel interpretable personality recognition model based on
a personality lexicon. First, we use word embedding techniques and prior-knowledge lexicons to
automatically construct a Chinese semantic lexicon suitable for personality analysis. Based on this
personality lexicon, we analyze the correlations between personality traits and semantic categories
of words, and extract the semantic features of users’ microblogs to construct personality recognition
models using classification algorithms. Extensive experiments are conducted to demonstrate that the
proposed model can achieve significantly better performances compared to previous approaches.In this paper, we present an automatic approach to construct-
ing a personality lexicon suitable for personality recognition.
From users’ microblogs, we first extract representative words
using text-mining methods. We then use a clustering algorithm
to separate the representative words into various groups, each
of which is defined as a semantic category. The identified se-
mantic categories form a semantic lexicon as the first Chinese
personality lexicon. During the lexicon’s construction, a main
challenge pertains to how to accurately represent the words. We
use word embedding in a large corpus to obtain original word
vectors, and then use popular Chinese lexicons to refine word
vectors. Based on the personality lexicon, we analyze the correla-
tions between personality traits and word categories, and thereby
enable exploration and interpretation of personality traits using
knowledge of words. The personality lexicon is used to generate
semantic features of words, which are used to train personality
recognition models with the aid of machine learning classifiers.
A large number of experiments are carried out to verify that the
proposed personality recognition model can perform significantly
better compared to previous approaches.
In future, we will utilize distributional contextual representa-
tions of the keywords to obtain better word vectors. Furthermore,
the hierarchical clustering and information extraction approaches
S. Han, H. Huang and Y. Tang / Knowledge-Based Systems 194 (2020) 105550 19
will be presented to obtain a personality lexicon with more com-
plicated semantic relations, including hierarchical semantics and
fine-grained semantics of words. This will provide psychologists
with a vital tool to deeply study and interpret personality traits.450 et with 9008 15.000 nds 1 2 8 4 5 6 7 10 Number of tags (N) Fig. 3. Fractions of individuals in our study who are associated with N ‘computer-detectable tagged face images. While most of the people ‘referenced in our data set only appear in photos a handful of times, ten ‘or more images are available for 44 000 people, and 300 or more images are available for 450 people. The number of human-assigned tags per individual is much higher; here we only count tags that could bbe assigned to computer-detected frontal faces with very high ‘confidence.Identification Performance vs. Rank Threshold Tce and context 03 => face only context only ° 10 20 30 40 50 60 Rank (R) Proportion with correct label in top R Fig. 5. Combining facial appearance and social network context for face recognition. The data set is split into training and testing sets (roughly 80%/20%) according to a time threshold t. raining images are used to learn models of facial appearance and social (pairwise) relationships, and these models are used to recognize individuals in the test set. The figure displays identification performance as a function of rank threshold: at each rank value Rit shows the proportion of all test samples for which the correct identity label appeared in the top f predictions. Results are shown for the facial appearance model (face), the social relationship model (context), and ‘combination.",Person recognition,"The paper presents an automatic approach for constructing a Chinese personality lexicon suitable for personality recognition using text-mining methods and clustering algorithms. The identified semantic categories form the first Chinese personality lexicon. Word embedding and prior-knowledge lexicons are used to refine word vectors and construct semantic features of words, which are used to train personality recognition models. The proposed model achieves significantly better performances compared to previous approaches. In future, distributional contextual representations and hierarchical clustering approaches will be used to obtain a more sophisticated personality lexicon. The paper provides a novel, interpretable personality recognition model that enables exploration and interpretation of personality traits using knowledge of words.",Object and Sentiment Recognition,"450 et with 9008 15.000 nds 1 2 8 4 5 6 7 10 Number of tags (N) Fig. 3. Fractions of individuals in our study who are associated with N ‘computer-detectable tagged face images. While most of the people ‘referenced in our data set only appear in photos a handful of times, ten ‘or more images are available for 44 000 people, and 300 or more images are available for 450 people. The number of human-assigned tags per individual is much higher; here we only count tags that could bbe assigned to computer-detected frontal faces with very high ‘confidence.Identification Performance vs. Rank Threshold Tce and context 03 => face only context only ° 10 20 30 40 50 60 Rank (R) Proportion with correct label in top R Fig. 5. Combining facial appearance and social network context for face recognition. The data set is split into training and testing sets (roughly 80%/20%) according to a time threshold t. raining images are used to learn models of facial appearance and social (pairwise) relationships, and these models are used to recognize individuals in the test set. The figure displays identification performance as a function of rank threshold: at each rank value Rit shows the proportion of all test samples for which the correct identity label appeared in the top f predictions. Results are shown for the facial appearance model (face), the social relationship model (context), and ‘combination.",Object Recognition
181,A survey of sentiment analysis in social media,"Sentiment analysis,
Social media,
Data mining,
Machine learning,
Survey.","Sentiments or opinions from social media provide the most up-to-date and inclusive information, due to the proliferation of social media and the low barrier for posting the message. Despite the growing importance of sentiment analysis, this area lacks a concise and systematic arrangement of prior efforts. It is essential to: (1) analyze its progress over the years, (2) provide an overview of the main advances achieved so far, and (3) outline remaining limitations. Several essential aspects, therefore, are addressed within the scope of this survey. On the one hand, this paper focuses on presenting typical methods from three different perspectives (task-oriented, granularity-oriented, methodology-oriented) in the area of sentiment analysis. Specifically, a large quantity of techniques and methods are categorized and compared. On the other hand, different types of data and advanced tools for research are introduced, as well as their limitations.","Sentiment analysis became a very popular research domain and a lot of excellent researches have been accomplished toward to this area. In this survey, a series of the state-of-the-art literatures have been reviewed. In particular, this survey categorized and classified sentiment analysis researches from multiple perspectives, i.e., task-oriented, granularity-oriented, and methodology-oriented. In addition, we explored different types of data and tools that can be used in sentiment analysis research and suggested their strength and limitations. Finally, we emphasized the prospects for future development, suggestions for possible extensions, and specially presented an overview of multimodal sentiment analysis (MSA). However, since MSA methods are, in general, not being used widely in sentiment analysis and related NLP research area, there are significant and timely opportunities for future research in the multi-disciplinary field of multimodal fusion.
This survey established a common terminology across various researches, enabling people from different background knowledge to easily understand, and laid a foundation for advanced research in sentiment analysis. Current studies are intended to pave the way for further researches and development activities by identifying weaknesses and deriving guidelines toward a holistic approach.","A survey of sentiment analysis in social mediaSentiment analysis,
Social media,
Data mining,
Machine learning,
Survey.Sentiments or opinions from social media provide the most up-to-date and inclusive information, due to the proliferation of social media and the low barrier for posting the message. Despite the growing importance of sentiment analysis, this area lacks a concise and systematic arrangement of prior efforts. It is essential to: (1) analyze its progress over the years, (2) provide an overview of the main advances achieved so far, and (3) outline remaining limitations. Several essential aspects, therefore, are addressed within the scope of this survey. On the one hand, this paper focuses on presenting typical methods from three different perspectives (task-oriented, granularity-oriented, methodology-oriented) in the area of sentiment analysis. Specifically, a large quantity of techniques and methods are categorized and compared. On the other hand, different types of data and advanced tools for research are introduced, as well as their limitations.Sentiment analysis became a very popular research domain and a lot of excellent researches have been accomplished toward to this area. In this survey, a series of the state-of-the-art literatures have been reviewed. In particular, this survey categorized and classified sentiment analysis researches from multiple perspectives, i.e., task-oriented, granularity-oriented, and methodology-oriented. In addition, we explored different types of data and tools that can be used in sentiment analysis research and suggested their strength and limitations. Finally, we emphasized the prospects for future development, suggestions for possible extensions, and specially presented an overview of multimodal sentiment analysis (MSA). However, since MSA methods are, in general, not being used widely in sentiment analysis and related NLP research area, there are significant and timely opportunities for future research in the multi-disciplinary field of multimodal fusion.
This survey established a common terminology across various researches, enabling people from different background knowledge to easily understand, and laid a foundation for advanced research in sentiment analysis. Current studies are intended to pave the way for further researches and development activities by identifying weaknesses and deriving guidelines toward a holistic approach.Test dataset Layer Model: Training dataset extreme -nonextrememodel Hieh| tow | — Middle Predicting : , I Extreme Nonextreme! : samples samples Extreme Nonextreme samples samples 2 Layer Model: high ~low model Training | Training Predicsne f L t Taye MEE 7 Layer Models high-lowmodel | | extreme -nonextrememodel High Low Middle (a) Model training (b) Model test Fig. 5. Two-layer classification model Review & Submit 1 [seldom feel blue ‘What is your Weibo nickname? ‘A Strongly disagree B. Disagree C. Neutral D. Agree E Strongly agree(Oe ERG ET EMAGIIS GING Be EGG | RMOMMCESE DESC OYSEEMNS LIE (EUAN III Step 1: Hyponym-hypernym word vector retrofitting Step 2: Synonym word vector retrofitting (b) Two-step word vector retrofitting algorithm",Person recognition,"This survey paper reviews the state-of-the-art research in sentiment analysis, categorizing and classifying it from multiple perspectives, including task-oriented, granularity-oriented, and methodology-oriented. The paper explores different types of data and tools used in sentiment analysis research, and suggests their strengths and limitations. The survey also highlights the prospects for future development and suggests possible extensions, including multimodal sentiment analysis. The paper establishes a common terminology, enabling people from different backgrounds to easily understand, and lays a foundation for advanced research in sentiment analysis. Overall, the survey addresses the progress, main advances, and remaining limitations in sentiment analysis.",Object and Sentiment Recognition,"Test dataset Layer Model: Training dataset extreme -nonextrememodel Hieh| tow | — Middle Predicting : , I Extreme Nonextreme! : samples samples Extreme Nonextreme samples samples 2 Layer Model: high ~low model Training | Training Predicsne f L t Taye MEE 7 Layer Models high-lowmodel | | extreme -nonextrememodel High Low Middle (a) Model training (b) Model test Fig. 5. Two-layer classification model Review & Submit 1 [seldom feel blue ‘What is your Weibo nickname? ‘A Strongly disagree B. Disagree C. Neutral D. Agree E Strongly agree(Oe ERG ET EMAGIIS GING Be EGG | RMOMMCESE DESC OYSEEMNS LIE (EUAN III Step 1: Hyponym-hypernym word vector retrofitting Step 2: Synonym word vector retrofitting (b) Two-step word vector retrofitting algorithm",Object Recognition
182,"Attention-based BiLSTM models for personality recognition
from user-generated content","Emojis,
Attention-based Bi-LSTM,
Personality traits,
User-generated content.","Emojis have been widely used in social media as a new way to express various emotions
and personalities. However, most previous research only focused on limited features from
textual information while neglecting rich emoji information in user-generated content.
This study presents two novel attention-based Bi-LSTM architectures to incorporate emoji
and textual information at different semantic levels, and investigate how the emoji information contributes to the performance of personality recognition tasks. Specifically, we
first extract emoji information from online user-generated content, and concatenate word
embedding and emoji embedding based on word and sentence perspectives. We then
obtain the document representations of all users from the word and sentence levels during
the training process and feed them into the attention-based Bi-LSTM architecture to predict
the Big Five personality traits. Experimental results show that the proposed methods
achieve state-of-the-art performance over the baseline models on the real dataset, demonstrating the usefulness and contribution of emoji information in personality recognition
tasks. The findings could help researchers and practitioners better understand the rich
semantics of emoji information and provide a new way to introduce emoji information into
personality recognition tasks.","This study proposed two attention-based Bi-LSTM models that incorporate textual and emoji information at different
semantic levels to predict the Big Five personality traits. The word-level attention-based Bi-LSTM model concatenated both
emoji and word embeddings while keeping the sequence information of each sentence. The sentence-level attention-based
Bi-LSTM model combined the emoji embeddings with sentence vectors, with the purpose of learning the emoji information
more effectively. The experiments on the baseline dataset indicated that our hierarchical models outperform the benchmark
approaches.
Unlike previous studies, the work utilized the attention-based Bi-LSTM network to highlight the value of emoji information on social media. This study is the first attempt to introduce emoji embedding for personality recognition tasks. Particularly, we proposed two hierarchical neural models that incorporate emoji and textual information at different semantic
levels. Our work can be further extended to handle tasks that require multisense embedding inputs. We add to the literature
by highlighting the rich semantics of emoji symbols, and improve the performance of personality recognition task.","Attention-based BiLSTM models for personality recognition
from user-generated contentEmojis,
Attention-based Bi-LSTM,
Personality traits,
User-generated content.Emojis have been widely used in social media as a new way to express various emotions
and personalities. However, most previous research only focused on limited features from
textual information while neglecting rich emoji information in user-generated content.
This study presents two novel attention-based Bi-LSTM architectures to incorporate emoji
and textual information at different semantic levels, and investigate how the emoji information contributes to the performance of personality recognition tasks. Specifically, we
first extract emoji information from online user-generated content, and concatenate word
embedding and emoji embedding based on word and sentence perspectives. We then
obtain the document representations of all users from the word and sentence levels during
the training process and feed them into the attention-based Bi-LSTM architecture to predict
the Big Five personality traits. Experimental results show that the proposed methods
achieve state-of-the-art performance over the baseline models on the real dataset, demonstrating the usefulness and contribution of emoji information in personality recognition
tasks. The findings could help researchers and practitioners better understand the rich
semantics of emoji information and provide a new way to introduce emoji information into
personality recognition tasks.This study proposed two attention-based Bi-LSTM models that incorporate textual and emoji information at different
semantic levels to predict the Big Five personality traits. The word-level attention-based Bi-LSTM model concatenated both
emoji and word embeddings while keeping the sequence information of each sentence. The sentence-level attention-based
Bi-LSTM model combined the emoji embeddings with sentence vectors, with the purpose of learning the emoji information
more effectively. The experiments on the baseline dataset indicated that our hierarchical models outperform the benchmark
approaches.
Unlike previous studies, the work utilized the attention-based Bi-LSTM network to highlight the value of emoji information on social media. This study is the first attempt to introduce emoji embedding for personality recognition tasks. Particularly, we proposed two hierarchical neural models that incorporate emoji and textual information at different semantic
levels. Our work can be further extended to handle tasks that require multisense embedding inputs. We add to the literature
by highlighting the rich semantics of emoji symbols, and improve the performance of personality recognition task.HIGH NEGATIVE AFFECT 1 distressed fearful hostile littery nervous oad SALLVDIN MOT GM ly se hy, %y Gs"", Fig. 2 The two-dimensional structure of emotionssubjective n-sentence Sentence? _-‘mesentence review extract(m<=n) . positive or negative? sit Tbs 2 | sz aS L subjective extraction | 1.5 Subjective extraction for improving polarity classificationPolarity lasification “The Level of Valence or Arousal ata Specific Seale “Task-oriented ‘Beyond Polarity SubjecivitylObjectvity Identification Feature/Aspet-based Sentiment Analysis ‘Document Level Sentimant Analysis ‘Sentiment AnalysisOpinion Mining |—L>[ Granularity-orented ‘Seatence Level Senimant Analysis Fig. 4 Sentiment analysis researches from multiple perspectives ‘Word Level Senimant Analysis ‘Supervised Leaming Approach ‘Unsupervised Learning Approach ‘Semi-supervised Leaning Approach",Person recognition,"This study explores the use of emojis in personality recognition tasks and presents two attention-based Bi-LSTM models that incorporate both textual and emoji information at different semantic levels. The proposed models achieve state-of-the-art performance on a real dataset, demonstrating the value of emoji information in personality recognition. The findings suggest that companies could improve their recommendation systems and innovation processes by considering users' personality traits. However, the study has some limitations and suggests future research on the inclusion of visual features and the application of other sequence learning models. Overall, the study highlights the rich semantics of emojis and their potential in NLP tasks.",Object and Sentiment Recognition,"HIGH NEGATIVE AFFECT 1 distressed fearful hostile littery nervous oad SALLVDIN MOT GM ly se hy, %y Gs"", Fig. 2 The two-dimensional structure of emotionssubjective n-sentence Sentence? _-‘mesentence review extract(m<=n) . positive or negative? sit Tbs 2 | sz aS L subjective extraction | 1.5 Subjective extraction for improving polarity classificationPolarity lasification “The Level of Valence or Arousal ata Specific Seale “Task-oriented ‘Beyond Polarity SubjecivitylObjectvity Identification Feature/Aspet-based Sentiment Analysis ‘Document Level Sentimant Analysis ‘Sentiment AnalysisOpinion Mining |—L>[ Granularity-orented ‘Seatence Level Senimant Analysis Fig. 4 Sentiment analysis researches from multiple perspectives ‘Word Level Senimant Analysis ‘Supervised Leaming Approach ‘Unsupervised Learning Approach ‘Semi-supervised Leaning Approach",Sentiment Analysis
183,"Comparison of machine learning algorithms for content based personality 
resolution of tweets","Machine learning (ML) ,
MBTI, 
BIG5, 
Twitter, 
Personality resolution.","The content of social media (SM) is expanding quickly with individuals sharing their feelings in a variety of ways, 
all of which depict their personalities to varying degrees. This study endeavored to build a system that could 
predict an individual’s personality through SM conversation. Four BIG5 personality items (i.e. Extraversion 
(EXT), Consciousness (CON), Agreeable (AGR) and Openness to Experiences (OPN) equivalent to the Myers–Briggs Type Indicator (MBTI)) were predicted using six supervised machine learning (SML) algorithms. In order 
to handle unstructured and unbalanced SM conversations, three feature extraction methods (i.e. term frequency 
and inverse document frequency (TF-IDF), the bag of words (BOW) and the global vector for word representation 
(GloVe)) were used. The TF-IDF method of feature extraction produces 2–9% higher accuracy than word2vec 
representation. GloVe is advocated as a better feature extractor because it maintains the spatial information of 
words. ","Many different approaches are available for identifying personalities, such as interview-based, rating scales, self-reports, personality 
inventories, projective techniques and behavioural observation. This 
study identifies personalities from self-reported content on Twitter as 
users express their inner feelings on the SM platform. The dataset has 
been formed by merging two datasets. The first is a standard dataset 
available on Kaggle for MBTI personality identification and the other is 
collected through the Twitter API. The MBTI personality indicators were 
then mapped to four BIG5 personality items. Personalities were dentified using six ML classifiers after extracting features by TF-IDF, 
BOW and the GloVe. The accuracy achieved by the TF-IDF feature 
extractor was highest. But the efficiency of a model cannot be assessed 
based on accuracy alone. GloVe is found to be the best feature extractor, 
as it is a dense representation of words with context information. The 
demerit of global vector representation is that it required a high 
computing resource. Thus, only a small GloVe vector is embedded in the 
present work. 
The present work can be extended for data collected from different 
SM platforms. A combination of different features can be applied 
because GloVe alone produces low accuracy. Thus, TF-IDF could be 
modified by providing context information of words. ","Comparison of machine learning algorithms for content based personality 
resolution of tweetsMachine learning (ML) ,
MBTI, 
BIG5, 
Twitter, 
Personality resolution.The content of social media (SM) is expanding quickly with individuals sharing their feelings in a variety of ways, 
all of which depict their personalities to varying degrees. This study endeavored to build a system that could 
predict an individual’s personality through SM conversation. Four BIG5 personality items (i.e. Extraversion 
(EXT), Consciousness (CON), Agreeable (AGR) and Openness to Experiences (OPN) equivalent to the Myers–Briggs Type Indicator (MBTI)) were predicted using six supervised machine learning (SML) algorithms. In order 
to handle unstructured and unbalanced SM conversations, three feature extraction methods (i.e. term frequency 
and inverse document frequency (TF-IDF), the bag of words (BOW) and the global vector for word representation 
(GloVe)) were used. The TF-IDF method of feature extraction produces 2–9% higher accuracy than word2vec 
representation. GloVe is advocated as a better feature extractor because it maintains the spatial information of 
words. Many different approaches are available for identifying personalities, such as interview-based, rating scales, self-reports, personality 
inventories, projective techniques and behavioural observation. This 
study identifies personalities from self-reported content on Twitter as 
users express their inner feelings on the SM platform. The dataset has 
been formed by merging two datasets. The first is a standard dataset 
available on Kaggle for MBTI personality identification and the other is 
collected through the Twitter API. The MBTI personality indicators were 
then mapped to four BIG5 personality items. Personalities were dentified using six ML classifiers after extracting features by TF-IDF, 
BOW and the GloVe. The accuracy achieved by the TF-IDF feature 
extractor was highest. But the efficiency of a model cannot be assessed 
based on accuracy alone. GloVe is found to be the best feature extractor, 
as it is a dense representation of words with context information. The 
demerit of global vector representation is that it required a high 
computing resource. Thus, only a small GloVe vector is embedded in the 
present work. 
The present work can be extended for data collected from different 
SM platforms. A combination of different features can be applied 
because GloVe alone produces low accuracy. Thus, TF-IDF could be 
modified by providing context information of words. STAT layer }->{_FCtayer_}—>{ Sostmare aver |» GEmanan) ATT — BiLSTM layer. “APE DILSTAM ays J —>_ AP BILSTA oer ‘model Fig. 2. Illustration of the word-levelattention-based Bi-LSTM model, 462 hang. L Zhao et al. Information Sciences 596 (2022) 460-471 a + ion ED armor |DecpMoji |DecpMoji | DeepMozi- — im vo orddvee Fig. 3 Illustration of the sentence-level attention-based Bi-LSTM model. ih A aEFig. 1. Architecture of attention-based B-1STM. = o(WeXs + Waal. +b) Fe =O (Woke + Woe + by) 01 = (Work + Werle. +be) = tanh Wears + Wear + Be) =f O61 tO hy = 0, @ tanh (c,) Attention Layer BiLSTM Layer",Person recognition,"The study aims to identify personalities by analyzing self-reported content on Twitter using six ML classifiers and three feature extraction methods: TF-IDF, BOW, and GloVe. The dataset was formed by merging a Kaggle dataset for MBTI personality identification and a Twitter API dataset. The MBTI personality indicators were mapped to four BIG5 personality items. The accuracy achieved by the TF-IDF feature extractor was highest, but GloVe is recommended as a better feature extractor as it maintains spatial information of words. The present work can be extended to data from different SM platforms and by combining different features. The study demonstrates the potential for predicting personality through SM conversation, and its findings could have implications for future research in this area.",Object and Sentiment Recognition,"STAT layer }->{_FCtayer_}—>{ Sostmare aver |» GEmanan) ATT — BiLSTM layer. “APE DILSTAM ays J —>_ AP BILSTA oer ‘model Fig. 2. Illustration of the word-levelattention-based Bi-LSTM model, 462 hang. L Zhao et al. Information Sciences 596 (2022) 460-471 a + ion ED armor |DecpMoji |DecpMoji | DeepMozi- — im vo orddvee Fig. 3 Illustration of the sentence-level attention-based Bi-LSTM model. ih A aEFig. 1. Architecture of attention-based B-1STM. = o(WeXs + Waal. +b) Fe =O (Woke + Woe + by) 01 = (Work + Werle. +be) = tanh Wears + Wear + Be) =f O61 tO hy = 0, @ tanh (c,) Attention Layer BiLSTM Layer",Object Recognition
184,"Kernel compositional embedding and its application in linguistic
structured data classification","Structured data representation
Kernel methods
Compositional embedding
Structured object classification","In many applications such as natural language processing, speech recognition, and computer vision,
there are inputs with hierarchical compositional structure and long relation among their subcomponents. Introducing this information in the definition of a model can improve its performance in dealing
with this type of data. On the other side, the high generalization power of kernel methods is proven
in traditional machine learning problems. If we can employ some idea of these methods on handling
structured objects, we can benefit from improving the performance and the generalization capability
of compositional models. Accordingly, a new approach is introduced in this paper to realize the idea
of simultaneously leveraging advantages of both kernel methods and compositional embedding to
provide powerful representations and classifiers for structured data. Based on this approach, which
is named Kernel Compositional Embedding (KCE), we propose two methods: Direct KCE (DKCE), and
Indirect KCE (IKCE). In the DKCE method, we directly deal with a potentially infinite dimensional
embedding space, i.e., the embeddings are used implicitly in the classification task. In IKCE method,
instead of implicitly performing all operations inside embedding space, only some elements of one or
more reproducing kernel Hilbert spaces are used to embed structured objects into low-dimensional
Euclidean spaces. To evaluate the performance of the proposed methods, we apply them on two
common computational linguistic tasks, i.e, sentiment analysis and natural language inference. The
experimental results illustrate that the classification performance of the proposed methods is higher
than or competitive to some well-known competing methods.","In this paper, we present two methods to leverage simultaneously the advantages of compositional embedding and kernel methods to embed and classify recursive structured objects.
In fact, both the methods operate based on a superior kernel
compositional embedding approach. In the first method, DKCE,
structured objects are embedded into a potentially infinite dimensional space and the necessary transpose and composition
H. Ganji, M.M. Ebadzadeh and S. Khadivi / Knowledge-Based Systems 194 (2020) 105553 11
operations are implicitly performed inside that space. The second method, IKCE, uses some elements of one or more kernel spaces to embed structured objects into low-dimensional
Euclidean spaces. The experimental results indicate that both
the proposed methods, especially IKCE, can provide structured
object classifiers whose classification performance is higher or
competitive compared to some well-known related methods. Particularly for the short phrases and sentences this superiority is
more prominent. In the future work, the KCE approach can be
extended to support non-linear composition and transpose operations. Also, applying the proposed methods on huge datasets
and other applications dealing with structured objects can be
considered for future work.
","Kernel compositional embedding and its application in linguistic
structured data classificationStructured data representation
Kernel methods
Compositional embedding
Structured object classificationIn many applications such as natural language processing, speech recognition, and computer vision,
there are inputs with hierarchical compositional structure and long relation among their subcomponents. Introducing this information in the definition of a model can improve its performance in dealing
with this type of data. On the other side, the high generalization power of kernel methods is proven
in traditional machine learning problems. If we can employ some idea of these methods on handling
structured objects, we can benefit from improving the performance and the generalization capability
of compositional models. Accordingly, a new approach is introduced in this paper to realize the idea
of simultaneously leveraging advantages of both kernel methods and compositional embedding to
provide powerful representations and classifiers for structured data. Based on this approach, which
is named Kernel Compositional Embedding (KCE), we propose two methods: Direct KCE (DKCE), and
Indirect KCE (IKCE). In the DKCE method, we directly deal with a potentially infinite dimensional
embedding space, i.e., the embeddings are used implicitly in the classification task. In IKCE method,
instead of implicitly performing all operations inside embedding space, only some elements of one or
more reproducing kernel Hilbert spaces are used to embed structured objects into low-dimensional
Euclidean spaces. To evaluate the performance of the proposed methods, we apply them on two
common computational linguistic tasks, i.e, sentiment analysis and natural language inference. The
experimental results illustrate that the classification performance of the proposed methods is higher
than or competitive to some well-known competing methods.In this paper, we present two methods to leverage simultaneously the advantages of compositional embedding and kernel methods to embed and classify recursive structured objects.
In fact, both the methods operate based on a superior kernel
compositional embedding approach. In the first method, DKCE,
structured objects are embedded into a potentially infinite dimensional space and the necessary transpose and composition
H. Ganji, M.M. Ebadzadeh and S. Khadivi / Knowledge-Based Systems 194 (2020) 105553 11
operations are implicitly performed inside that space. The second method, IKCE, uses some elements of one or more kernel spaces to embed structured objects into low-dimensional
Euclidean spaces. The experimental results indicate that both
the proposed methods, especially IKCE, can provide structured
object classifiers whose classification performance is higher or
competitive compared to some well-known related methods. Particularly for the short phrases and sentences this superiority is
more prominent. In the future work, the KCE approach can be
extended to support non-linear composition and transpose operations. Also, applying the proposed methods on huge datasets
and other applications dealing with structured objects can be
considered for future work.
Data balancing, stop word removal, lemmatization Mapping of personality traits N-S>OPN T-F CON FID ext LP DAGR GLOVE Vector Feature Extraction Pre-processing Dataset Creation and Mapping ML Classification Fig. 1. Workflow of present research.Pea PEIN atsNumber of Occurrences 3500 B22 eH g INFP INF] INTP INT) ENTP ENFP ISTP ISP ENT) IST] ENF) SF) ESTP ESFP Types Fig. 2. Personality distribution in merged dataset. type posts IE NS TF JP 0 INFJ ‘http:/ww.youtube.comwatch?v=qsXHewe3krw|||.. 1 1 0 14 4 ENTP ‘'m finding the lack of me in these posts ver. 0 1 1 0 2 INTP — ‘Goodone___httpsv/www.youtube.com/wat.. 1 1 1 0 3 INTJ ""Dear INTP, | enjoyed our conversation the o. 1 1 1 1 4 ENT ""You're fired,||[That's another silly misconce.. 0 1 1 14 Fig. 3. Mapping of personalities to LE, N-S, T-F and J-P. Distribution accoss types indicators Distribution accoss types indicators NS (a) (b) Fig. 4. Distribution of personality indicators (a) before resampling (b) after resampling. ESF) EST)",Person recognition,"The paper introduces a new approach called Kernel Compositional Embedding (KCE) to leverage the advantages of both kernel methods and compositional embedding to provide powerful representations and classifiers for structured data with hierarchical compositional structure and long relation among their subcomponents. The KCE approach is used to propose two methods: Direct KCE (DKCE), and Indirect KCE (IKCE), which are evaluated on two computational linguistic tasks: sentiment analysis and natural language inference. The experimental results show that both proposed methods can provide structured object classifiers with higher or competitive classification performance compared to some well-known related methods. The future work can extend the KCE approach to support non-linear composition and transpose operations and apply it to huge datasets and other applications dealing with structured objects.",Object and Sentiment Recognition,"Data balancing, stop word removal, lemmatization Mapping of personality traits N-S>OPN T-F CON FID ext LP DAGR GLOVE Vector Feature Extraction Pre-processing Dataset Creation and Mapping ML Classification Fig. 1. Workflow of present research.Pea PEIN atsNumber of Occurrences 3500 B22 eH g INFP INF] INTP INT) ENTP ENFP ISTP ISP ENT) IST] ENF) SF) ESTP ESFP Types Fig. 2. Personality distribution in merged dataset. type posts IE NS TF JP 0 INFJ ‘http:/ww.youtube.comwatch?v=qsXHewe3krw|||.. 1 1 0 14 4 ENTP ‘'m finding the lack of me in these posts ver. 0 1 1 0 2 INTP — ‘Goodone___httpsv/www.youtube.com/wat.. 1 1 1 0 3 INTJ ""Dear INTP, | enjoyed our conversation the o. 1 1 1 1 4 ENT ""You're fired,||[That's another silly misconce.. 0 1 1 14 Fig. 3. Mapping of personalities to LE, N-S, T-F and J-P. Distribution accoss types indicators Distribution accoss types indicators NS (a) (b) Fig. 4. Distribution of personality indicators (a) before resampling (b) after resampling. ESF) EST)",Object Recognition
185,"Multimodal assessment of apparent personality using feature attention
and error consistency constraint","Deep learning Apparent  personality Multimodal modeling Information fusion Feature attention,Error consistency","Personality computing and affective computing, where the recognition of personality traits is essential, have
gained increasing interest and attention in many research areas recently. We propose a novel approach to recognize the Big Five personality traits of people from videos. To this end, we use four different modalities, namely,
ambient appearance (scene), facial appearance, voice, and transcribed speech. Through a specialized subnetwork
for each of these modalities, our model learns reliable modality-specific representations and fuse them using an
attention mechanism that re-weights each dimension of these representations to obtain an optimal combination
of multimodal information. A novel loss function is employed to enforce the proposed model to give an equivalent importance for each of the personality traits to be estimated through a consistency constraint that keeps the
trait-specific errors as close as possible. To further enhance the reliability of our model, we employ (pre-trained)
state-of-the-art architectures (i.e., ResNet, VGGish, ELMo) as the backbones of the modality-specific subnetworks, which are complemented by multilayered Long Short-Term Memory networks to capture temporal dynamics. To minimize the computational complexity of multimodal optimization, we use two-stage modeling,
where the modality-specific subnetworks are first trained individually, and the whole network is then finetuned to jointly model multimodal data. On the large scale ChaLearn First Impressions V2 challenge dataset,
we evaluate the reliability of our model as well as investigating the informativeness of the considered modalities.
Experimental results show the effectiveness of the proposed attention mechanism and the error consistency constraint. While the best performance is obtained using facial information among individual modalities, with the
use of all four modalities, our model achieves a mean accuracy of 91.8%, improving the state of the art in automatic personality analysis.","We propose a novel multimodal approach for the estimation of apparent personality traits. Our method relies on four subnetworks, each
of which focuses on a specific modality, namely ambient appearance, facial appearance, voice, and transcribed speech. These subnetworks employ state-of-the-art deep architectures (e.g., ResNet-v2-101, VGGish,
ELMo) as backbones, and they are complemented with additional
LSTM layers to leverage temporal information. For more effective
modeling, first, each of the aforementioned subnetworks has been initialized with the (pre-trained) weight parameters of the corresponding
backbone network and trained (fine-tuned) in a modality-specific manner. Then, these subnetworks (after removing their regression layers)
have been combined and complemented by feature attention and regression layers. While the parameters of the subnetworks are kept frozen, new layers of the whole network have been trained to minimize
the average MAE of predicting the five personality traits as well as keeping the errors for each modality as close as possible to each other using
the proposed error consistency constraint. In this way, our model prevents overfitting to some specific traits due to joint multi-task optimization. Although the proposed architecture is end-to-end trainable, we
have followed a hierarchical training to minimize computational costs
while improving effectiveness.
Our framework has been thoroughly evaluated on the large-scale
ChaLearn First Impressions dataset. The effectiveness and reliability of
the proposed feature attention mechanism and the error consistency
constraint have been systematically assessed. Besides, the informativeness of different modalities and the added value of their joint use have
been investigated. Our results show that the proposed feature attention
and error consistency constraint are indeed useful and improve prediction accuracy. With the use of ambient appearance, facial appearance,
voice, and transcribed speech modalities, our proposed model achieves
a mean accuracy of 91.8%, improving the state of the art. As future
research directions, we envision that correlation between personality,
body movements, posture, eye-gaze, and emotion can be investigated.
","Multimodal assessment of apparent personality using feature attention
and error consistency constraintDeep learning Apparent  personality Multimodal modeling Information fusion Feature attention,Error consistencyPersonality computing and affective computing, where the recognition of personality traits is essential, have
gained increasing interest and attention in many research areas recently. We propose a novel approach to recognize the Big Five personality traits of people from videos. To this end, we use four different modalities, namely,
ambient appearance (scene), facial appearance, voice, and transcribed speech. Through a specialized subnetwork
for each of these modalities, our model learns reliable modality-specific representations and fuse them using an
attention mechanism that re-weights each dimension of these representations to obtain an optimal combination
of multimodal information. A novel loss function is employed to enforce the proposed model to give an equivalent importance for each of the personality traits to be estimated through a consistency constraint that keeps the
trait-specific errors as close as possible. To further enhance the reliability of our model, we employ (pre-trained)
state-of-the-art architectures (i.e., ResNet, VGGish, ELMo) as the backbones of the modality-specific subnetworks, which are complemented by multilayered Long Short-Term Memory networks to capture temporal dynamics. To minimize the computational complexity of multimodal optimization, we use two-stage modeling,
where the modality-specific subnetworks are first trained individually, and the whole network is then finetuned to jointly model multimodal data. On the large scale ChaLearn First Impressions V2 challenge dataset,
we evaluate the reliability of our model as well as investigating the informativeness of the considered modalities.
Experimental results show the effectiveness of the proposed attention mechanism and the error consistency constraint. While the best performance is obtained using facial information among individual modalities, with the
use of all four modalities, our model achieves a mean accuracy of 91.8%, improving the state of the art in automatic personality analysis.We propose a novel multimodal approach for the estimation of apparent personality traits. Our method relies on four subnetworks, each
of which focuses on a specific modality, namely ambient appearance, facial appearance, voice, and transcribed speech. These subnetworks employ state-of-the-art deep architectures (e.g., ResNet-v2-101, VGGish,
ELMo) as backbones, and they are complemented with additional
LSTM layers to leverage temporal information. For more effective
modeling, first, each of the aforementioned subnetworks has been initialized with the (pre-trained) weight parameters of the corresponding
backbone network and trained (fine-tuned) in a modality-specific manner. Then, these subnetworks (after removing their regression layers)
have been combined and complemented by feature attention and regression layers. While the parameters of the subnetworks are kept frozen, new layers of the whole network have been trained to minimize
the average MAE of predicting the five personality traits as well as keeping the errors for each modality as close as possible to each other using
the proposed error consistency constraint. In this way, our model prevents overfitting to some specific traits due to joint multi-task optimization. Although the proposed architecture is end-to-end trainable, we
have followed a hierarchical training to minimize computational costs
while improving effectiveness.
Our framework has been thoroughly evaluated on the large-scale
ChaLearn First Impressions dataset. The effectiveness and reliability of
the proposed feature attention mechanism and the error consistency
constraint have been systematically assessed. Besides, the informativeness of different modalities and the added value of their joint use have
been investigated. Our results show that the proposed feature attention
and error consistency constraint are indeed useful and improve prediction accuracy. With the use of ambient appearance, facial appearance,
voice, and transcribed speech modalities, our proposed model achieves
a mean accuracy of 91.8%, improving the state of the art. As future
research directions, we envision that correlation between personality,
body movements, posture, eye-gaze, and emotion can be investigated.
“historic church"" se ((<conpstion Con end from Adj. subspace = from N. subspace toNP subspace Tomsposttion to NP subspace from one or multiple > “= EEEEEE from one or multiple prior spaces toa , . prior spaces toa kernel space Intermediate mapping kernel space historic church¥P. ‘A small crowd ‘quietly enters Be Me fn the historic ‘Asmall i church crowd ee Ma = ese | Ges Fig. 1. An example of a natural language sentence with a corresponding parse ‘tree and embeddings ofits constituents. All the nodes are labeled using syntactic ‘categories of the grammar. The vectors of orange circles show the prior represen tation vectors for the words. The vectors of blue circles indicate the embeddings of structured objects, ie, phrases. These embeddings can be achieved using a ‘compositional embedding method. In this fashion, the embedding of each node is obtained by composing the embeddings ofits child nodes. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) oo",Person recognition,"The text describes a novel approach for recognizing the Big Five personality traits of people from videos using four modalities: ambient appearance, facial appearance, voice, and transcribed speech. The model learns modality-specific representations through subnetworks and uses an attention mechanism to fuse the information. A consistency constraint is used to ensure equal importance for each trait. State-of-the-art architectures and LSTM layers are employed for effective modeling. The model achieves a mean accuracy of 91.8% on the ChaLearn First Impressions dataset and improves the state of the art. The effectiveness and reliability of the proposed features are evaluated, and future research directions are suggested.",Object and Sentiment Recognition,"“historic church"" se ((<conpstion Con end from Adj. subspace = from N. subspace toNP subspace Tomsposttion to NP subspace from one or multiple > “= EEEEEE from one or multiple prior spaces toa , . prior spaces toa kernel space Intermediate mapping kernel space historic church¥P. ‘A small crowd ‘quietly enters Be Me fn the historic ‘Asmall i church crowd ee Ma = ese | Ges Fig. 1. An example of a natural language sentence with a corresponding parse ‘tree and embeddings ofits constituents. All the nodes are labeled using syntactic ‘categories of the grammar. The vectors of orange circles show the prior represen tation vectors for the words. The vectors of blue circles indicate the embeddings of structured objects, ie, phrases. These embeddings can be achieved using a ‘compositional embedding method. In this fashion, the embedding of each node is obtained by composing the embeddings ofits child nodes. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) oo",Object Recognition
186,Personality-based refinement for sentiment classification.,"Sentiment classification,
Social media analytics,
Personality prediction,
Big Five model.","Microblog has become one of the most widely used social media for people to share information and
express opinions. As information propagates fast in social network, understanding and analyzing public
sentiment implied in user-generated content is beneficial for many fields and has been applied to applications such as social management, business and public security. Most previous work on sentiment
analysis makes no distinctions of the tweets by different users and ignores the diverse word use of people. As some sentiment expressions are used by specific groups of people, the corresponding textual
sentiment features are often neglected in the analysis process. On the other hand, previous psychological
findings have shown that personality influences the ways people write and talk, suggesting that people
with same personality traits tend to choose similar sentiment expressions. Inspired by this, in this paper we propose a method to facilitate sentiment classification in microblog based on personality traits. To
this end, we first develop a rule-based method to predict users’ personality traits based on the most wellstudied personality model, the Big Five model. In order to leverage more effective but not widely used
sentiment features, we then extract those features grouped by different personality traits and construct
personality-based sentiment classifiers. Moreover, we adopt an ensemble learning strategy to integrate
traditional textual feature based and our personality-based sentiment classification. Experimental studies
on Chinese microblog dataset show the effectiveness of our method in refining the performance of both
the traditional and state-of-the-art sentiment classifiers. Our work is among the first to explicitly explore
the role of user’s personality in social media analytics and its application in sentiment classification","Microblog has become one of the major social media platforms
for people to express their opinions. It is both beneficial and critical for many applications to understand the sentiments implied
in user-generated contents. Most previous work on sentiment classification makes no distinctions of the texts published by different users. Thus personalized sentiment information is often neglected in the analysis process. Inspired by the psychological findings that personality influences the ways people write and talk,
this paper proposes a personality-based refinement method PbSC
to extract personalized features for sentiment classification in microblog based on the Big Five model. We first develop a rule-based
method to predict user’ personality traits with relatively high precision, which considers both the textual information and the microblog usage information. In order to leverage more personalized
textual features, we group tweets according to the predicted personality traits, and then extract sentiment features and train basic
classifiers for each personality group. On the basis of this, we employ ensemble learning and build a meta-classifier to make full use
of both the personality-based and general sentiment classifiers. Experimental studies using SinaWeibo dataset show the effectiveness
of our proposed method in refining the performances of both the
traditional and state-of-the-art sentiment classifiers.
Our work is among the first to explore the role of user’s personality in social media analytics, and we choose sentiment classification as an exemplar field for our investigation. In the future,
we shall make several further improvements for our work. First,
we shall explore the usage of other personality dimensions in sentiment classification. As we mainly use bag-of-words model to represent tweets, we shall also apply personality information to other
textual models, such as deep learning based model. The third direction is to extend PbSC for more fine-grained emotion classification. Finally, as PbSC involves an ensemble learning process, we can
apply parallel computing to accelerate this process","Personality-based refinement for sentiment classification.Sentiment classification,
Social media analytics,
Personality prediction,
Big Five model.Microblog has become one of the most widely used social media for people to share information and
express opinions. As information propagates fast in social network, understanding and analyzing public
sentiment implied in user-generated content is beneficial for many fields and has been applied to applications such as social management, business and public security. Most previous work on sentiment
analysis makes no distinctions of the tweets by different users and ignores the diverse word use of people. As some sentiment expressions are used by specific groups of people, the corresponding textual
sentiment features are often neglected in the analysis process. On the other hand, previous psychological
findings have shown that personality influences the ways people write and talk, suggesting that people
with same personality traits tend to choose similar sentiment expressions. Inspired by this, in this paper we propose a method to facilitate sentiment classification in microblog based on personality traits. To
this end, we first develop a rule-based method to predict users’ personality traits based on the most wellstudied personality model, the Big Five model. In order to leverage more effective but not widely used
sentiment features, we then extract those features grouped by different personality traits and construct
personality-based sentiment classifiers. Moreover, we adopt an ensemble learning strategy to integrate
traditional textual feature based and our personality-based sentiment classification. Experimental studies
on Chinese microblog dataset show the effectiveness of our method in refining the performance of both
the traditional and state-of-the-art sentiment classifiers. Our work is among the first to explicitly explore
the role of user’s personality in social media analytics and its application in sentiment classificationMicroblog has become one of the major social media platforms
for people to express their opinions. It is both beneficial and critical for many applications to understand the sentiments implied
in user-generated contents. Most previous work on sentiment classification makes no distinctions of the texts published by different users. Thus personalized sentiment information is often neglected in the analysis process. Inspired by the psychological findings that personality influences the ways people write and talk,
this paper proposes a personality-based refinement method PbSC
to extract personalized features for sentiment classification in microblog based on the Big Five model. We first develop a rule-based
method to predict user’ personality traits with relatively high precision, which considers both the textual information and the microblog usage information. In order to leverage more personalized
textual features, we group tweets according to the predicted personality traits, and then extract sentiment features and train basic
classifiers for each personality group. On the basis of this, we employ ensemble learning and build a meta-classifier to make full use
of both the personality-based and general sentiment classifiers. Experimental studies using SinaWeibo dataset show the effectiveness
of our proposed method in refining the performances of both the
traditional and state-of-the-art sentiment classifiers.
Our work is among the first to explore the role of user’s personality in social media analytics, and we choose sentiment classification as an exemplar field for our investigation. In the future,
we shall make several further improvements for our work. First,
we shall explore the usage of other personality dimensions in sentiment classification. As we mainly use bag-of-words model to represent tweets, we shall also apply personality information to other
textual models, such as deep learning based model. The third direction is to extend PbSC for more fine-grained emotion classification. Finally, as PbSC involves an ensemble learning process, we can
apply parallel computing to accelerate this processPretrained ResNet v2. 101 Multi-task CNN Face Detection and Alignment ResNet v2 101 Ambient Features (Frozen) Face Detection ResNet v2 101 and Alignment (Frozen) VGGish CNN (Frozen)",Person recognition,"The paper proposes a method called PbSC for sentiment classification in microblogs based on users' personality traits. The Big Five model is used to predict personality traits, and tweets are grouped by personality traits to extract personalized sentiment features and train personality-based classifiers. Ensemble learning is used to integrate personality-based and traditional textual features. The method is shown to be effective in refining the performance of sentiment classifiers on a Chinese microblog dataset. The paper also discusses future directions for improving the method, including exploring other personality dimensions, applying personality information to other textual models, extending PbSC for more fine-grained emotion classification, and accelerating the ensemble learning process with parallel computing",Object and Sentiment Recognition,Pretrained ResNet v2. 101 Multi-task CNN Face Detection and Alignment ResNet v2 101 Ambient Features (Frozen) Face Detection ResNet v2 101 and Alignment (Frozen) VGGish CNN (Frozen),Object Recognition
187,"User personality prediction based on topic preference and sentiment
analysis using LSTM model","Attention-based LSTM,
LDA,
Big-Five.","Based on the original text information, this paper converts the users’ theme preferences and text sentiment features into attention information and combines different forms with the LSTM (Long Short-Term
Memory) model to predict the personality characteristics of social network users. Finally, the experimental results of multiple groups’ show that the Attention-based LSTM model proposed in the paper can
achieve better results than the currently popular methods in the recognition of user personality traits
and that the model has good generalization ability.","he paper combined an attention mechanism and an LSTM network to propose a deep network model Attention-based LSTM for
user personality prediction. The feature information of the sentences extracted by other algorithms is used as the attention information of the LSTM network, which can make the model pay
close attention to a specific feature during the training process and
can pay close attention to the word information that is important
to a specific feature to mine out more hidden information. Then,
the LSTM network, which receives the sequential input of words,
acquired the dependency of the target entity in the sentence and
identified the differences between different users in the use of text,
which is helpful to better identify a user’s personality.","User personality prediction based on topic preference and sentiment
analysis using LSTM modelAttention-based LSTM,
LDA,
Big-Five.Based on the original text information, this paper converts the users’ theme preferences and text sentiment features into attention information and combines different forms with the LSTM (Long Short-Term
Memory) model to predict the personality characteristics of social network users. Finally, the experimental results of multiple groups’ show that the Attention-based LSTM model proposed in the paper can
achieve better results than the currently popular methods in the recognition of user personality traits
and that the model has good generalization ability.he paper combined an attention mechanism and an LSTM network to propose a deep network model Attention-based LSTM for
user personality prediction. The feature information of the sentences extracted by other algorithms is used as the attention information of the LSTM network, which can make the model pay
close attention to a specific feature during the training process and
can pay close attention to the word information that is important
to a specific feature to mine out more hidden information. Then,
the LSTM network, which receives the sequential input of words,
acquired the dependency of the target entity in the sentence and
identified the differences between different users in the use of text,
which is helpful to better identify a user’s personality.",Person recognition,This paper proposes an Attention-based LSTM model for predicting personality traits of social network users. The model combines the users' theme preferences and text sentiment features with an attention mechanism and an LSTM network. The attention mechanism is used to focus on specific features during the training process and identify important word information for mining hidden information. The LSTM network receives sequential input of words and identifies differences in the use of text to better identify a user's personality. Experimental results show that the model outperforms currently popular methods in personality trait recognition and has good generalization ability.,Object and Sentiment Recognition,,Object Recognition
188,Fake profile detection techniques in large-scale online social networks: A comprehensive review,"Fake profile detection,
Online social networks,
Sybil attacks,
Big data.","In the present era, online social networks are the most popular and rapid information propagation applications on the Internet. People of all ages spend most of their time on social networking sites. Huge volumes of data are being created and shared through social networks around the world. These interests have given rise to illegitimate users who engage in fraudulent activities against social network users. On social networks, fake profile creation is considered to cause more harm than any other form of cyber crime. This crime has to be detected even before the user is notified about the fake profile creation. Many algorithms and methods, most of which use the huge volume of unstructured data generated from social networks, have been proposed for the detection of fake profiles. This study presents a survey of the existing and latest technical work on fake profile detection.","This survey provides a comprehensive review of important techniques for fake profile detection in OSNs. The paper explores the most prominent historical approaches and focuses on state of the art works for detecting Sybil or fake accounts in social networks. The various approaches, along with their synthetic network type and dataset statistics, are compared and tabulated. We also focused on recently proposed schemes and their strengths and drawbacks. These schemes are compared based on their qualitative performance. The open issues in the domain of fake profile detection in OSNs are stated. We conclude that, despite numerous existing schemes, there is still no systematic solution for fake profile detection in OSNs that can provide efficient, fast and reliable recognition of user information. Furthermore, in this paper, it is asserted that fast big data technologies such as Hadoop and Spark will definitely be part of the solution for rapidly accessing large amounts of social network data for the security analysis of user profiles. Scalable algorithms need to be designed with concurrency so that they can run on fast data systems and stream input data at line speeds rather than carrying out batch analyses.","Fake profile detection techniques in large-scale online social networks: A comprehensive reviewFake profile detection,
Online social networks,
Sybil attacks,
Big data.In the present era, online social networks are the most popular and rapid information propagation applications on the Internet. People of all ages spend most of their time on social networking sites. Huge volumes of data are being created and shared through social networks around the world. These interests have given rise to illegitimate users who engage in fraudulent activities against social network users. On social networks, fake profile creation is considered to cause more harm than any other form of cyber crime. This crime has to be detected even before the user is notified about the fake profile creation. Many algorithms and methods, most of which use the huge volume of unstructured data generated from social networks, have been proposed for the detection of fake profiles. This study presents a survey of the existing and latest technical work on fake profile detection.This survey provides a comprehensive review of important techniques for fake profile detection in OSNs. The paper explores the most prominent historical approaches and focuses on state of the art works for detecting Sybil or fake accounts in social networks. The various approaches, along with their synthetic network type and dataset statistics, are compared and tabulated. We also focused on recently proposed schemes and their strengths and drawbacks. These schemes are compared based on their qualitative performance. The open issues in the domain of fake profile detection in OSNs are stated. We conclude that, despite numerous existing schemes, there is still no systematic solution for fake profile detection in OSNs that can provide efficient, fast and reliable recognition of user information. Furthermore, in this paper, it is asserted that fast big data technologies such as Hadoop and Spark will definitely be part of the solution for rapidly accessing large amounts of social network data for the security analysis of user profiles. Scalable algorithms need to be designed with concurrency so that they can run on fast data systems and stream input data at line speeds rather than carrying out batch analyses.Data collection { ¥ i Data processing i y ’ + {[earsreengineesnge | [emetion presicsons | [ Topic auantifiations t Analisis of experimental result Fig. 1. Framework of the user personality prediction.5 aia, 0, Zong ane ¥, iss of of / Patten Fig. 2. Structural diagram of the Attention-based LSTM model.",Person recognition,"The paper discusses the prevalence of fake profile creation on social networks and the need for effective detection methods. It surveys existing and latest technical work on fake profile detection in OSNs, comparing different approaches and their strengths and drawbacks. Despite the numerous schemes proposed, there is still no systematic solution for efficient and reliable detection. The paper suggests that big data technologies such as Hadoop and Spark can be used to rapidly access and analyze large amounts of social network data. Scalable algorithms with concurrency should be designed to run on fast data systems and stream input data in real-time for more effective detection.",Object and Sentiment Recognition,"Data collection { ¥ i Data processing i y ’ + {[earsreengineesnge | [emetion presicsons | [ Topic auantifiations t Analisis of experimental result Fig. 1. Framework of the user personality prediction.5 aia, 0, Zong ane ¥, iss of of / Patten Fig. 2. Structural diagram of the Attention-based LSTM model.",Object Recognition
189,A Review Paper on Face Recognition Techniques ," Principal Component Analysis (PCA), 
Linear Discriminant Analysis (LDA), 
Face Recognition, Independent 
Component Analysis (ICA), Artificial 
Neural Networks (ANN).","Abstract-Face recognition has been a fast growing, 
challenging and interesting area in real time 
applications. A large number of face recognition 
algorithms have been developed in last decades. In this 
paper an attempt is made to review a wide range of 
methods used for face recognition comprehensively. 
This include PCA, LDA, ICA, SVM, Gabor wavelet soft 
computing tool like ANN for recognition and various 
hybrid combination of this techniques. This review 
investigates all these methods with parameters that 
challenges face recognition like illumination, pose 
variation, facial expressions. ","This paper has attempted to review a significant 
number of papers to cover the recent development 
in the field of face recognition. Present study 
reveals that for enhanced face recognition new 
algorithm has to evolve using hybrid methods of 
soft computing tools such as ANN, SVM, SOM 
may yields better performance. The list of 
references to provide more detailed understanding 
of the approaches described is enlisted. We 
apologize to researchers whose important 
contributions may have been overlooked.","A Review Paper on Face Recognition Techniques  Principal Component Analysis (PCA), 
Linear Discriminant Analysis (LDA), 
Face Recognition, Independent 
Component Analysis (ICA), Artificial 
Neural Networks (ANN).Abstract-Face recognition has been a fast growing, 
challenging and interesting area in real time 
applications. A large number of face recognition 
algorithms have been developed in last decades. In this 
paper an attempt is made to review a wide range of 
methods used for face recognition comprehensively. 
This include PCA, LDA, ICA, SVM, Gabor wavelet soft 
computing tool like ANN for recognition and various 
hybrid combination of this techniques. This review 
investigates all these methods with parameters that 
challenges face recognition like illumination, pose 
variation, facial expressions. This paper has attempted to review a significant 
number of papers to cover the recent development 
in the field of face recognition. Present study 
reveals that for enhanced face recognition new 
algorithm has to evolve using hybrid methods of 
soft computing tools such as ANN, SVM, SOM 
may yields better performance. The list of 
references to provide more detailed understanding 
of the approaches described is enlisted. We 
apologize to researchers whose important 
contributions may have been overlooked.Ite‘ONLINE SOCIAL NETWORKS, | CT GE SECS Er SS (ey eee i User details & Activity logs Profile attributes, User activity values ‘Network Information with actors and their relationships DATA COLLECTION Machine Learning based classification Nodes, Edges, weights Graph based analysis, FEATURE EXTRACTION CLASSIFIER Fake ‘Threshold profiles based or arta or ybil region Ranking based DECISION MAKING Fig. 8. Generic process flow in identification of the real or fake profile.",Person recognition,"The paper reviews various face recognition methods, including PCA, LDA, ICA, SVM, Gabor wavelet, and soft computing tools like ANN. It investigates the challenges faced by face recognition, such as illumination, pose variation, and facial expressions. The paper suggests that the combination of soft computing tools like ANN, SVM, and SOM may yield better performance for face recognition. The paper provides a list of references for readers to gain a more detailed understanding of the discussed approaches. The authors apologize to researchers whose contributions may have been overlooked.",Object and Sentiment Recognition,"Ite‘ONLINE SOCIAL NETWORKS, | CT GE SECS Er SS (ey eee i User details & Activity logs Profile attributes, User activity values ‘Network Information with actors and their relationships DATA COLLECTION Machine Learning based classification Nodes, Edges, weights Graph based analysis, FEATURE EXTRACTION CLASSIFIER Fake ‘Threshold profiles based or arta or ybil region Ranking based DECISION MAKING Fig. 8. Generic process flow in identification of the real or fake profile.",Object Recognition
190,A Multimodal Deep Framework for Derogatory Social Media Post Identification of a Recognized Person," Social media analysis and security, deep learning, derogatory content,
transformer network, Indic languages, NLP","In today’s era of digitization, social media platforms play a significant role in networking and influencing the perception of the general population. Social network sites have recently been used to carry out harmful attacks against individuals, including political and theological figures, intellectuals, sports and movie stars, and other prominent dignitaries, which may or may not be intentional. However, the exchange of such information across the general population inevitably contributes to social-economic, socio-political turmoil, and even physical violence in society. By classifying the derogatory content of a social media post, this research work helps to eradicate and discourage the upsetting propagation of such hate campaigns. Social networking posts today often include the picture of Memes along with textual remarks and comments, which throw new challenges and opportunities to the research community while identifying the attacks. This article proposes a multimodal deep learning framework by utilizing ensembles of computer vision and natural language processing techniques to train an encapsulated transformer network for handling the classification problem. The proposed framework utilizes the fine-tuned state-of-the-art deep learning-based models (e.g., BERT, Electra) for multilingual text analysis along with face recognition and the optical character recognition model for Meme picture comprehension. For the study, a new Facebook meme-post dataset is created with recorded baseline results. The subject of the created dataset and context of the work is more geared toward multilingual Indian society. The findings demonstrate the efficacy of the proposed method in the identification of social media meme posts featuring derogatory content about a famous/recognized individual.
","
In our study, we examine the emerging area of research in social media and focus on the issue of derogatory post-identification of recognized/famous individuals in social media. The research aims to eradicate and deter the alarming dissemination of malicious campaigns against a country’s political, religious, and academic leaders, and other influential dignitaries. To evaluate a post, we consider three multimodal data forms: Memes (representing a person’s image along with the message), the post text (main text), and related comments. We discuss the concerns inherent in heterogeneous multimodal data processing and propose a new deep learning framework that incorporates state-of-the-art machine vision and NLP approaches. From pre-trained models for four sentence classification tasks (Hate Speech, Profane, Targeted Insult, and Negative Emotion Identification), we fine-tuned the deep learning methods. We proposed a transformer-based network to encapsulate multimodal learning and carry out the classification of derogatory post content after categorization of all sentences throughout a post. In addition, from the original Facebook post, we manually built an in-house dataset. We trained and assessed the dataset and provided the baseline results to verify the overall framework. The research could be used in multimodal social network research as a base point for several other use cases that could be solved using collections of deep learning models. Creating a huge dataset in real time might be a beneficial contribution to the task. It is also a promising field to take a multilingual approach by incorporating local languages in the new approach.","A Multimodal Deep Framework for Derogatory Social Media Post Identification of a Recognized Person Social media analysis and security, deep learning, derogatory content,
transformer network, Indic languages, NLPIn today’s era of digitization, social media platforms play a significant role in networking and influencing the perception of the general population. Social network sites have recently been used to carry out harmful attacks against individuals, including political and theological figures, intellectuals, sports and movie stars, and other prominent dignitaries, which may or may not be intentional. However, the exchange of such information across the general population inevitably contributes to social-economic, socio-political turmoil, and even physical violence in society. By classifying the derogatory content of a social media post, this research work helps to eradicate and discourage the upsetting propagation of such hate campaigns. Social networking posts today often include the picture of Memes along with textual remarks and comments, which throw new challenges and opportunities to the research community while identifying the attacks. This article proposes a multimodal deep learning framework by utilizing ensembles of computer vision and natural language processing techniques to train an encapsulated transformer network for handling the classification problem. The proposed framework utilizes the fine-tuned state-of-the-art deep learning-based models (e.g., BERT, Electra) for multilingual text analysis along with face recognition and the optical character recognition model for Meme picture comprehension. For the study, a new Facebook meme-post dataset is created with recorded baseline results. The subject of the created dataset and context of the work is more geared toward multilingual Indian society. The findings demonstrate the efficacy of the proposed method in the identification of social media meme posts featuring derogatory content about a famous/recognized individual.

In our study, we examine the emerging area of research in social media and focus on the issue of derogatory post-identification of recognized/famous individuals in social media. The research aims to eradicate and deter the alarming dissemination of malicious campaigns against a country’s political, religious, and academic leaders, and other influential dignitaries. To evaluate a post, we consider three multimodal data forms: Memes (representing a person’s image along with the message), the post text (main text), and related comments. We discuss the concerns inherent in heterogeneous multimodal data processing and propose a new deep learning framework that incorporates state-of-the-art machine vision and NLP approaches. From pre-trained models for four sentence classification tasks (Hate Speech, Profane, Targeted Insult, and Negative Emotion Identification), we fine-tuned the deep learning methods. We proposed a transformer-based network to encapsulate multimodal learning and carry out the classification of derogatory post content after categorization of all sentences throughout a post. In addition, from the original Facebook post, we manually built an in-house dataset. We trained and assessed the dataset and provided the baseline results to verify the overall framework. The research could be used in multimodal social network research as a base point for several other use cases that could be solved using collections of deep learning models. Creating a huge dataset in real time might be a beneficial contribution to the task. It is also a promising field to take a multilingual approach by incorporating local languages in the new approach.",Person recognition,"This article discusses the harmful effects of derogatory social media posts about famous individuals and proposes a multimodal deep learning framework for identifying such posts. The framework incorporates computer vision and natural language processing techniques and uses fine-tuned deep learning models for multilingual text analysis, face recognition, and optical character recognition. The proposed approach is tested on a new Facebook meme-post dataset created specifically for the study, with baseline results provided. The research aims to deter the dissemination of malicious campaigns against influential individuals and provides a basis for future research in multimodal social network analysis.",Object and Sentiment Recognition,,Object Recognition
191,"A comparative study of two models for celebrity
identification on Twitter."," Celebrity, Social media, Twitter, Influence","The concept of celebrities has shaped societies throughout
history. This work addresses the problem of celebrity identification from social media interactions. “Celebritiness” is
a characteristic assigned to persons that are initially based
on specific achievements or lineage. However, celebritiness
often transcends achievements and gets attached to the person itself, causing them to capture popular imagination and
create a public image that is bigger than life. The celebrity
identification problem is argued to be distinct from similar
problems of identifying influencers or of identification of experts. We develop two models for celebrity identification.
In this paper, we compare the two models on twitter data
and highlight the characteristics of each of the models.","Celebrity dynamics on social media is an interesting phenomenon, and the proposed algorithms provide promising
results to be practically applicable. By providing interpretations for acquaintance, affinity, identification, loyalty and
attention from appropriate signals, the proposed algorithms
can be easily ported to datasets from other forms of social
media. The distinction between AAI and AR distinguishes
between celebrities within the social media versus celebrities
in the real-world outside.
In the future, we plan to extend this model to identify
celebrities in a given domain and apply this model on other
forms of user generated content, in addition to social media
datasets.","A comparative study of two models for celebrity
identification on Twitter. Celebrity, Social media, Twitter, InfluenceThe concept of celebrities has shaped societies throughout
history. This work addresses the problem of celebrity identification from social media interactions. “Celebritiness” is
a characteristic assigned to persons that are initially based
on specific achievements or lineage. However, celebritiness
often transcends achievements and gets attached to the person itself, causing them to capture popular imagination and
create a public image that is bigger than life. The celebrity
identification problem is argued to be distinct from similar
problems of identifying influencers or of identification of experts. We develop two models for celebrity identification.
In this paper, we compare the two models on twitter data
and highlight the characteristics of each of the models.Celebrity dynamics on social media is an interesting phenomenon, and the proposed algorithms provide promising
results to be practically applicable. By providing interpretations for acquaintance, affinity, identification, loyalty and
attention from appropriate signals, the proposed algorithms
can be easily ported to datasets from other forms of social
media. The distinction between AAI and AR distinguishes
between celebrities within the social media versus celebrities
in the real-world outside.
In the future, we plan to extend this model to identify
celebrities in a given domain and apply this model on other
forms of user generated content, in addition to social media
datasets.Input Layer ‘Output Layer (128 Nodes) (St nodes) FaceNet Embedding -----» <> where, Mie number of known Fig. 2. The FCNN for known person identification.eee Poe 1 Represented Vane (Embengs) HER tenssormer ayers ia TE Teeesrmer reat ecsaings Self-trained layers from seratch | 0.00, 0.00, 0.0... 0.00 | 0.00, 0.00 0.0... 0.00 [087, 0.13, 0.60, 0.40, 0.97, 0.03, 0.4, 0.58[0.15, 0.46, 0:35, 0.17, 0.89, 0.15, 0.66, 0.12] Sentence 1 Text: ‘2a af stoma 7é %, 4 & att wi"" Sentence Type: Meme Text Sentence Type Embedding: | 0.15, 046, 0.35, .., 012 ‘Sentence Language: Hindi ‘seth Peceowe Modene (000; 0.00, 0100, 2, 0:00]) English Language Models: Probability of Four Tasks in Mixed Language Models: Probability of Four Tasks in Hindi Language Models: ‘Softmax form ‘Softmax form Softmax form Softmax form Hate Speech Targeted Insult Profane Langt ""Negative SentimentUser Post Textual remarks and comments Armee |e) labdenedm tel ro Ree yp et.al ah Te TI iaaaas ayaa big it a like that When you have completely destroyed the ‘whole economy and have nothing to do! ee Text and emojis Basie Prevproceesing Language type Identification | ampoyy Buypunssiopuy 224 DL Models (Total 12 Models) Post Selection and Feature Encapsulation Derogatory Post Probability Deep Learning Model ( Encapsulated Multi-model Transformer Network )- “Transformer Final Embeddings = After k number of steps (layers) Layer Normalization",Person recognition,"This paper discusses the problem of identifying celebrities from social media interactions. It highlights the difference between identifying celebrities versus influencers or experts, and proposes two models for celebrity identification. The paper compares the two models and provides interpretations for different signals such as acquaintance, affinity, identification, loyalty, and attention. The proposed algorithms can be applied to datasets from other forms of social media and can distinguish between celebrities within social media versus those in the real world. The authors plan to extend this model to identify celebrities in different domains and apply it to other forms of user-generated content in the future.",Object and Sentiment Recognition,"Input Layer ‘Output Layer (128 Nodes) (St nodes) FaceNet Embedding -----» <> where, Mie number of known Fig. 2. The FCNN for known person identification.eee Poe 1 Represented Vane (Embengs) HER tenssormer ayers ia TE Teeesrmer reat ecsaings Self-trained layers from seratch | 0.00, 0.00, 0.0... 0.00 | 0.00, 0.00 0.0... 0.00 [087, 0.13, 0.60, 0.40, 0.97, 0.03, 0.4, 0.58[0.15, 0.46, 0:35, 0.17, 0.89, 0.15, 0.66, 0.12] Sentence 1 Text: ‘2a af stoma 7é %, 4 & att wi"" Sentence Type: Meme Text Sentence Type Embedding: | 0.15, 046, 0.35, .., 012 ‘Sentence Language: Hindi ‘seth Peceowe Modene (000; 0.00, 0100, 2, 0:00]) English Language Models: Probability of Four Tasks in Mixed Language Models: Probability of Four Tasks in Hindi Language Models: ‘Softmax form ‘Softmax form Softmax form Softmax form Hate Speech Targeted Insult Profane Langt ""Negative SentimentUser Post Textual remarks and comments Armee |e) labdenedm tel ro Ree yp et.al ah Te TI iaaaas ayaa big it a like that When you have completely destroyed the ‘whole economy and have nothing to do! ee Text and emojis Basie Prevproceesing Language type Identification | ampoyy Buypunssiopuy 224 DL Models (Total 12 Models) Post Selection and Feature Encapsulation Derogatory Post Probability Deep Learning Model ( Encapsulated Multi-model Transformer Network )- “Transformer Final Embeddings = After k number of steps (layers) Layer Normalization",Object Recognition
192,Sentiment Analysis in Social Media Data for Depression Detection Using Artificial Intelligence," Sentiment analysis · Natural language processing · Social network analysis · Feature extraction · Multiclass 
classifcation · Emoticons & Emojis · Machine learning · Deep learning · Depression","Sentiment analysis is an emerging trend nowadays to understand people’s sentiments in multiple situations in their quotidian life. Social media data would be utilized for the entire process ie the analysis and classification processes and it consists of text data and emoticons, emojis, etc. Many experiments were conducted in the antecedent studies utilizing Binary and Ternary Classification whereas Multi-class Classification gives more precise and precise Classification. In Multi-class Classification, the data would be divided into multiple sub-classes predicated on the polarities. Machine Learning and Deep Learning Techniques would be utilized for the classification process. Utilizing Social media, sentiment levels can be monitored or analysed. This paper shows a review of the sentiment analysis on Social media data for apprehensiveness or dejection detection utilizing various artificial intelligence techniques. In the survey, it was optically canvassed that social media data which consists of texts,emoticons and emojis were utilized for the sentiment identification utilizing various artificial intelligence techniques. Multi Class Classification with Deep Learning Algorithm shows higher precision value during the sentiment analysis.","Sentiment analysis/Opinion Mining understands the feelings, replications as well as judgements amassed or extracted from texts or other data utilized in data analysis or mining, web mining, and convivial media analytics because sentiments are to judge human comportment. They can be categorized into positive, negative, or neutral. It discovers opinions,then convey the posture and categorize them division-sapient. The data amassed in the process, apperceiving their sentiments, culling features, relegating sentiments and conclusively calculating the sentiment polarity. It is very utilizable for business product reviews, stock markets up’s and down’s, mentality of people reading news, and views on political debates. There are different ways to relegate sentiments. In Machine Learning approach, it can be divided into supervised and unsupervised learning. Engendering a model from learned data and presaging the target class for the particular data is called as Supervised Learning whereas the technique of learning from unlabelled to identify the provided input data is called as unsupervised learning. Deep understanding is a paramount area in Machine Learning, to assimilate cognizance about multiple feature depiction levels by methods and actions.

In Multi class Classification, the data was classified into many subclasses predicated on the sentiment polarity where we can expect a precise or precise classification. Social media data additionally consists of emoticons and emojis, where they withal have sentiment score values as they additionally can be utilized for the sentiment analysis process or classification. The foremost paramount step is to ascertain that not to lose the emoticons during the pre-processing data stage since it contains the sentiment value. It can withal be utilized for sentiment analysis. To extract the features from the pre-processed data, feature extraction techniques are utilized. Utilizing classification procedure, the data was polarized into sentiment classes predicated on the sentiment values which was done by sundry machine learning and deep learning algorithms.","Sentiment Analysis in Social Media Data for Depression Detection Using Artificial Intelligence Sentiment analysis · Natural language processing · Social network analysis · Feature extraction · Multiclass 
classifcation · Emoticons & Emojis · Machine learning · Deep learning · DepressionSentiment analysis is an emerging trend nowadays to understand people’s sentiments in multiple situations in their quotidian life. Social media data would be utilized for the entire process ie the analysis and classification processes and it consists of text data and emoticons, emojis, etc. Many experiments were conducted in the antecedent studies utilizing Binary and Ternary Classification whereas Multi-class Classification gives more precise and precise Classification. In Multi-class Classification, the data would be divided into multiple sub-classes predicated on the polarities. Machine Learning and Deep Learning Techniques would be utilized for the classification process. Utilizing Social media, sentiment levels can be monitored or analysed. This paper shows a review of the sentiment analysis on Social media data for apprehensiveness or dejection detection utilizing various artificial intelligence techniques. In the survey, it was optically canvassed that social media data which consists of texts,emoticons and emojis were utilized for the sentiment identification utilizing various artificial intelligence techniques. Multi Class Classification with Deep Learning Algorithm shows higher precision value during the sentiment analysis.Sentiment analysis/Opinion Mining understands the feelings, replications as well as judgements amassed or extracted from texts or other data utilized in data analysis or mining, web mining, and convivial media analytics because sentiments are to judge human comportment. They can be categorized into positive, negative, or neutral. It discovers opinions,then convey the posture and categorize them division-sapient. The data amassed in the process, apperceiving their sentiments, culling features, relegating sentiments and conclusively calculating the sentiment polarity. It is very utilizable for business product reviews, stock markets up’s and down’s, mentality of people reading news, and views on political debates. There are different ways to relegate sentiments. In Machine Learning approach, it can be divided into supervised and unsupervised learning. Engendering a model from learned data and presaging the target class for the particular data is called as Supervised Learning whereas the technique of learning from unlabelled to identify the provided input data is called as unsupervised learning. Deep understanding is a paramount area in Machine Learning, to assimilate cognizance about multiple feature depiction levels by methods and actions.

In Multi class Classification, the data was classified into many subclasses predicated on the sentiment polarity where we can expect a precise or precise classification. Social media data additionally consists of emoticons and emojis, where they withal have sentiment score values as they additionally can be utilized for the sentiment analysis process or classification. The foremost paramount step is to ascertain that not to lose the emoticons during the pre-processing data stage since it contains the sentiment value. It can withal be utilized for sentiment analysis. To extract the features from the pre-processed data, feature extraction techniques are utilized. Utilizing classification procedure, the data was polarized into sentiment classes predicated on the sentiment values which was done by sundry machine learning and deep learning algorithms.Top 100 Celebrities AAI Score versus Follow Count (in log scale) i 1 AAI score in log scal¢ o-=neaao~ —_ it | t 1 1 ° 05 1 1.5 2 25 3 3.5 4 45 Follow count in log scale Figure 6: AAI score versus Number of Followers Celebrity Analysis - AAI versus AR model ‘Average statistics ‘Number of replies mNumber of mentions Number of retweets mNumber of followers im Number of tweets olen Mein — ole Exclusive to ‘Common for Exclusive to ‘AR mode! ‘AR and AAI mode! ‘AAI mode!",Person recognition,"The text discusses sentiment analysis on social media data using artificial intelligence techniques. Sentiment analysis is used to understand people's sentiments in various situations, and it can be categorized into positive, negative, or neutral. The paper reviews various techniques for sentiment analysis on social media data, including machine learning and deep learning algorithms. Multi-class classification is preferred as it gives more precise results. Emoticons and emojis in social media data are also utilized as they contain sentiment value. Feature extraction techniques are used to extract features from the pre-processed data, and various algorithms are used for sentiment classification. In future work, other data such as biometrics, facial expressions, speech signals, and EEG signals can also be used for depression detection. The combination of different algorithms can also be used to improve precision under different conditions and with different data.",Object and Sentiment Recognition,Top 100 Celebrities AAI Score versus Follow Count (in log scale) i 1 AAI score in log scal¢ o-=neaao~ —_ it | t 1 1 ° 05 1 1.5 2 25 3 3.5 4 45 Follow count in log scale Figure 6: AAI score versus Number of Followers Celebrity Analysis - AAI versus AR model ‘Average statistics ‘Number of replies mNumber of mentions Number of retweets mNumber of followers im Number of tweets olen Mein — ole Exclusive to ‘Common for Exclusive to ‘AR mode! ‘AR and AAI mode! ‘AAI mode!,Object Recognition
193,Violence Detection in Social Media-Review,"Machine learning, natural language processing, violence, social media, convolution neural network","Social media has become a vital part of humans’ day to day life. Different users engage with social media differently. With the increased usage of social media, many researchers have investigated different aspects of social media. Many examples in the recent past show, content in the social media can generate violence in the user community. Violence in social media can be categorised into aggregation in comments, cyber-bullying and incidents like protests, murders. Identifying violent content in social media is a challenging task: social media posts contain both the visual and text as well as these posts may contain hidden meaning according to the users’ context and other background information. This paper summarizes the different social media violent categories and existing methods to detect the violent content.","According to the literature, results differ depending on the datasets. But according to some of the 
research, deep learning methods have outperformed classical machine learning methods like SVM. 
Character n-gram method gives better performance when classifying text. As the literature suggests, 
most of the research are based on social media text or images. Although researchers have done research 
on multimodal content: caption of the images and image itself have been taken into consideration. 
Recently social media image posts which contain both text and objects are a popular way of 
communicating. We need to address this problem. Deep learning approaches with word embedding and 
object detection algorithms like YOLO can give better performance in this task. First task is the creation 
of balance (negative and positive) data corpus. Then text and object detection along with text embedding 
should be followed. Finally, classification algorithm should be decided.","Violence Detection in Social Media-ReviewMachine learning, natural language processing, violence, social media, convolution neural networkSocial media has become a vital part of humans’ day to day life. Different users engage with social media differently. With the increased usage of social media, many researchers have investigated different aspects of social media. Many examples in the recent past show, content in the social media can generate violence in the user community. Violence in social media can be categorised into aggregation in comments, cyber-bullying and incidents like protests, murders. Identifying violent content in social media is a challenging task: social media posts contain both the visual and text as well as these posts may contain hidden meaning according to the users’ context and other background information. This paper summarizes the different social media violent categories and existing methods to detect the violent content.According to the literature, results differ depending on the datasets. But according to some of the 
research, deep learning methods have outperformed classical machine learning methods like SVM. 
Character n-gram method gives better performance when classifying text. As the literature suggests, 
most of the research are based on social media text or images. Although researchers have done research 
on multimodal content: caption of the images and image itself have been taken into consideration. 
Recently social media image posts which contain both text and objects are a popular way of 
communicating. We need to address this problem. Deep learning approaches with word embedding and 
object detection algorithms like YOLO can give better performance in this task. First task is the creation 
of balance (negative and positive) data corpus. Then text and object detection along with text embedding 
should be followed. Finally, classification algorithm should be decided.DATA COLLECTION DATA CLEANING REQUIREMENTS, GATHERING DATA DATA DATA VISUALIZATION INTERPRETATION ANALYSISExpression Expression Expression Emoticons, Smiley Happy Rose @); Grin Batting Eyelashes Pig :@) Sad Love Struck Sick oh Crying ‘Smug Applause =D> Meh Big Grin Yawn cl Wink > Confused Good Luck %%- Embarassed =$ Cool Liar “0 Heart 3 Devil Daneing \D/ Angry >=0 Big Hug Nerd B Annoyed = ‘Shame on Raised ny You Eyebrows Surprised =0 Punch be Flag Hoe ‘Tounge =P Whistling ” No i Talking Party Alien Idea + Kiss Skull Peace Sign > Drooling Pig Monkey 0 Coffee Hypnotized Pumpkin eo Frustrated Bug Cowboy <) Praying Cow Chicken ~> ‘Thinking Don't tell Rolling Eyes 8-1 anyone ‘Talk to the hand Straight Face Angel 0-)Expression Expression Expression Emoticons, Smiley Happy Rose @); Grin Batting Eyelashes Pig :@) Sad Love Struck Sick oh Crying ‘Smug Applause =D> Meh Big Grin Yawn cl Wink > Confused Good Luck %%- Embarassed =$ Cool Liar “0 Heart 3 Devil Daneing \D/ Angry >=0 Big Hug Nerd B Annoyed = ‘Shame on Raised ny You Eyebrows Surprised =0 Punch be Flag Hoe ‘Tounge =P Whistling ” No i Talking Party Alien Idea + Kiss Skull Peace Sign > Drooling Pig Monkey 0 Coffee Hypnotized Pumpkin eo Frustrated Bug Cowboy <) Praying Cow Chicken ~> ‘Thinking Don't tell Rolling Eyes 8-1 anyone ‘Talk to the hand Straight Face Angel 0-)",sentimental analysis on social media review,"This paper discusses the challenge of identifying violent content in social media, which can be categorized into aggregation in comments, cyber-bullying, and incidents like protests and murders. Existing methods, such as deep learning and character n-gram methods, have shown promising results in detecting violent content. However, most research has been based on social media text or images, while social media image posts that contain both text and objects are becoming more prevalent. The paper suggests that deep learning approaches with word embedding and object detection algorithms can be effective in detecting violent content in these types of posts. The creation of a balanced data corpus, followed by text and object detection and embedding, and then classification algorithms are recommended steps for this task.",Natural Language Processing,"DATA COLLECTION DATA CLEANING REQUIREMENTS, GATHERING DATA DATA DATA VISUALIZATION INTERPRETATION ANALYSISExpression Expression Expression Emoticons, Smiley Happy Rose @); Grin Batting Eyelashes Pig :@) Sad Love Struck Sick oh Crying ‘Smug Applause =D> Meh Big Grin Yawn cl Wink > Confused Good Luck %%- Embarassed =$ Cool Liar “0 Heart 3 Devil Daneing \D/ Angry >=0 Big Hug Nerd B Annoyed = ‘Shame on Raised ny You Eyebrows Surprised =0 Punch be Flag Hoe ‘Tounge =P Whistling ” No i Talking Party Alien Idea + Kiss Skull Peace Sign > Drooling Pig Monkey 0 Coffee Hypnotized Pumpkin eo Frustrated Bug Cowboy <) Praying Cow Chicken ~> ‘Thinking Don't tell Rolling Eyes 8-1 anyone ‘Talk to the hand Straight Face Angel 0-)Expression Expression Expression Emoticons, Smiley Happy Rose @); Grin Batting Eyelashes Pig :@) Sad Love Struck Sick oh Crying ‘Smug Applause =D> Meh Big Grin Yawn cl Wink > Confused Good Luck %%- Embarassed =$ Cool Liar “0 Heart 3 Devil Daneing \D/ Angry >=0 Big Hug Nerd B Annoyed = ‘Shame on Raised ny You Eyebrows Surprised =0 Punch be Flag Hoe ‘Tounge =P Whistling ” No i Talking Party Alien Idea + Kiss Skull Peace Sign > Drooling Pig Monkey 0 Coffee Hypnotized Pumpkin eo Frustrated Bug Cowboy <) Praying Cow Chicken ~> ‘Thinking Don't tell Rolling Eyes 8-1 anyone ‘Talk to the hand Straight Face Angel 0-)",Sentiment Analysis
194,"Recent trends in deep learning based personality detection.
",Personality detection · Multimodal interaction · Deep learning,"Recently, the automatic prediction of personality traits has received a lot of attention. Specifically, personality trait prediction from multimodal data has emerged as a hot topic within the
field of affective computing. In this paper, we review significant machine learning models
which have been employed for personality detection, with an emphasis on deep learningbased methods. This review paper provides an overview of the most popular approaches
to automated personality detection, various computational datasets, its industrial applications, and state-of-the-art machine learning models for personality detection with specific
focus on multimodal approaches. Personality detection is a very broad and diverse topic: this
survey only focuses on computational approaches and leaves out psychological studies on
personality detection.","As discussed in this paper, there are many diverse applications of automated personality
detection which can be used in the industry, hence making it a very hot and upcoming field.
However, machine learning models are as powerful as the data used to train them. For a large
number of cases in this field, enough labelled data are not available to train huge neural networks. There is a dire need of larger, more accurate and more diverse datasets for personality
detection. Almost all of the current datasets focus on the Big-Five personality model and
very few for other personality measures such as the MBTI or PEN. Normally, personality is
measured by answering multiple questions in a survey. Assuming that everyone taking the
survey answers honestly, the credibility of this survey in correctly labelling an individual’s
personality is still in question. A more accurate and efficient way of labelling personality
traits needs to be explored. Most of the current methods for creating personality detection
datasets rely on manual annotation through crowd sourcing using Amazon Mechanical Turk.
Recent multimodal deep learning techniques have performed well and are starting to make
reliable personality predictions. Deep learning offers a way to harness the large amount of data
and computation power at our disposal with little engineering by hand. Various deep models
have become the new state-of-the-art methods not only for personality detection, but in other
fields as well. We expect this trend to continue with deeper models and new architectures
which are able to map very complex functions. We expect to see more personality detection
architectures that rely on efficient multimodal fusion.","Recent trends in deep learning based personality detection.
Personality detection · Multimodal interaction · Deep learningRecently, the automatic prediction of personality traits has received a lot of attention. Specifically, personality trait prediction from multimodal data has emerged as a hot topic within the
field of affective computing. In this paper, we review significant machine learning models
which have been employed for personality detection, with an emphasis on deep learningbased methods. This review paper provides an overview of the most popular approaches
to automated personality detection, various computational datasets, its industrial applications, and state-of-the-art machine learning models for personality detection with specific
focus on multimodal approaches. Personality detection is a very broad and diverse topic: this
survey only focuses on computational approaches and leaves out psychological studies on
personality detection.As discussed in this paper, there are many diverse applications of automated personality
detection which can be used in the industry, hence making it a very hot and upcoming field.
However, machine learning models are as powerful as the data used to train them. For a large
number of cases in this field, enough labelled data are not available to train huge neural networks. There is a dire need of larger, more accurate and more diverse datasets for personality
detection. Almost all of the current datasets focus on the Big-Five personality model and
very few for other personality measures such as the MBTI or PEN. Normally, personality is
measured by answering multiple questions in a survey. Assuming that everyone taking the
survey answers honestly, the credibility of this survey in correctly labelling an individual’s
personality is still in question. A more accurate and efficient way of labelling personality
traits needs to be explored. Most of the current methods for creating personality detection
datasets rely on manual annotation through crowd sourcing using Amazon Mechanical Turk.
Recent multimodal deep learning techniques have performed well and are starting to make
reliable personality predictions. Deep learning offers a way to harness the large amount of data
and computation power at our disposal with little engineering by hand. Various deep models
have become the new state-of-the-art methods not only for personality detection, but in other
fields as well. We expect this trend to continue with deeper models and new architectures
which are able to map very complex functions. We expect to see more personality detection
architectures that rely on efficient multimodal fusion.",Person recognition,"This paper reviews machine learning models used for automatic prediction of personality traits, with a focus on deep learning-based methods. The paper discusses the diverse applications of automated personality detection and the need for larger, more accurate, and more diverse datasets for personality detection. Current methods for creating personality detection datasets rely on manual annotation through crowd sourcing using Amazon Mechanical Turk. Multimodal deep learning techniques have performed well in predicting personality traits, and deep learning is expected to continue to advance in the field. The paper concludes with a prediction that there will be more personality detection architectures relying on efficient multimodal fusion.



",Object and Sentiment Recognition,,Sentiment Analysis
195,Emotion recognition and affective computing on vocal social media.,"Social media
Social network
Vocal data mining
Emotion recognition
Affective computing","Vocal media has become a popular method of communication in today's social networks. While conveying semantic information, vocal messages usually also contain abundant emotional information; this emotional information represents a new focus for data mining in social media analytics. This paper proposes a computational method for emotion recognition and affective computing on vocal social media to estimate complex emotion as well as its dynamic changes in a three-dimensional PAD (Position–Arousal–Dominance) space; furthermore, this paper analyzes the propagation characteristics of emotions on the vocal social media site WeChat.","The widespread use of emerging vocal social media has greatly facilitated communication as well as emotion propagation on social networks and is therefore having greater impacts on social psychological cognition and group behaviors than ever before. This issue of emotion recognition and affective computing on emerging vocal media has become a new hot area in social media analytics. The speech signal in vocal social media is human conversation using natural language and in most cases contains mixed emotions embedded with dynamic changes. Its complexity calls for further research addressing emotion recognition and computation.
Through the comprehensive analysis of previous research work, this paper proposed a computational method by turning the issue of emotion recognition and affective computing on vocal social media into a PAD value estimation exercise using 25 extracted acoustic feature parameters of speech signals based on a trained LS-SVR model. The choice of acoustic feature parameters, the acquisition of training samples, and the generalizability to real applications were discussed according to the characteristics of vocal social media under a research schema. The test result using the standard corpus and an experiment based on a real application showed that the proposed method can reach high accuracy independent of the semantic information and has good generalizability for different social media groups. It provides an effective approach for computing and analyzing the dynamic propagation of mixed emotions on vocal social media. However, the research findings in this paper indicated that training samples associated with the personalized features of social media group members have significant impact on the accuracy of the computational results, so the performance of the proposed method must be tested and verified through large samples and in different social media groups. Moreover, the precise relationship between acoustic feature parameters and PAD values as well as the optimization of acoustic feature parameters on the vector model are worthy of exploration in further studies.","Emotion recognition and affective computing on vocal social media.Social media
Social network
Vocal data mining
Emotion recognition
Affective computingVocal media has become a popular method of communication in today's social networks. While conveying semantic information, vocal messages usually also contain abundant emotional information; this emotional information represents a new focus for data mining in social media analytics. This paper proposes a computational method for emotion recognition and affective computing on vocal social media to estimate complex emotion as well as its dynamic changes in a three-dimensional PAD (Position–Arousal–Dominance) space; furthermore, this paper analyzes the propagation characteristics of emotions on the vocal social media site WeChat.The widespread use of emerging vocal social media has greatly facilitated communication as well as emotion propagation on social networks and is therefore having greater impacts on social psychological cognition and group behaviors than ever before. This issue of emotion recognition and affective computing on emerging vocal media has become a new hot area in social media analytics. The speech signal in vocal social media is human conversation using natural language and in most cases contains mixed emotions embedded with dynamic changes. Its complexity calls for further research addressing emotion recognition and computation.
Through the comprehensive analysis of previous research work, this paper proposed a computational method by turning the issue of emotion recognition and affective computing on vocal social media into a PAD value estimation exercise using 25 extracted acoustic feature parameters of speech signals based on a trained LS-SVR model. The choice of acoustic feature parameters, the acquisition of training samples, and the generalizability to real applications were discussed according to the characteristics of vocal social media under a research schema. The test result using the standard corpus and an experiment based on a real application showed that the proposed method can reach high accuracy independent of the semantic information and has good generalizability for different social media groups. It provides an effective approach for computing and analyzing the dynamic propagation of mixed emotions on vocal social media. However, the research findings in this paper indicated that training samples associated with the personalized features of social media group members have significant impact on the accuracy of the computational results, so the performance of the proposed method must be tested and verified through large samples and in different social media groups. Moreover, the precise relationship between acoustic feature parameters and PAD values as well as the optimization of acoustic feature parameters on the vector model are worthy of exploration in further studies.ve | Global Averge Pooling (GAP) a unit N + WN Class Activation Map (CAM)",Person recognition,"This paper proposes a method for emotion recognition and affective computing on vocal social media, which is becoming increasingly important for social media analytics due to its impact on social psychological cognition and group behaviors. The proposed method estimates complex emotion and its dynamic changes in a three-dimensional PAD space using 25 extracted acoustic feature parameters of speech signals based on a trained LS-SVR model. The method has been shown to be accurate and effective in analyzing the dynamic propagation of mixed emotions on vocal social media, but the performance is affected by the personalized features of social media group members. Further research is needed to optimize the acoustic feature parameters and explore the precise relationship between the parameters and PAD values.",Object and Sentiment Recognition,ve | Global Averge Pooling (GAP) a unit N + WN Class Activation Map (CAM),Object Recognition
196,"Assessing the Accuracy of Four Popular FaceRecognition Tools for Inferring Gender, Age, and Race.","Face Detection,Age Detection,Race Detection,Gender Detection","In this research, we evaluate four widely used face detec-tion tools, which are Face++, IBM Bluemix Visual Recog-nition, AWS Rekognition, and Microsoft Azure Face API,using multiple datasets to determine their accuracy in in-ferring user attributes, including gender, race, and age. Re-sults show that the tools are generally proficient at determin-ing gender, with accuracy rates greater than 90%, except forIBM Bluemix. Concerning race, only one of the four toolsprovides this capability, Face++, with an accuracy rate ofgreater than 90%, although the evaluation was performed ona high-quality dataset. Inferring age appears to be a challeng-ing problem, as all four tools performed poorly. The findingsof our quantitative evaluation are helpful for future computa-tional social science research using these tools, as their accu-racy needs to be taken into account when applied to classify-ing individuals on social media and other contexts. Triangu-lation and manual verification are suggested for researchersemploying these tools.","The results of our accuracy evaluation using multipledatasets for four popular facial recognition tools highlightthe need for triangulation as a crucial step for better compu-tational social science research, even for the relatively sim-ple task of face detection within an image. Reviewing ourspecific results we see a trend of high accuracy for gender,with three of the tools performing with an accuracy of above90 percent for all datasets. Concerning race, only one tooloffers this capability, Face++, and the accuracy is quite high,above 90 percent. However, this was evaluated by using ahigh-quality dataset of images. Future research is needed todetermine if such accuracy holds for noisy images.All of the tools performed poorly for age, even with therelaxed task of determining an age bin instead of exact age.Moreover, the average error regarding age for all tools wasquite high. We conjecture that an individual’s age may be adifficult attribute for facial recognition tools to discern, per-haps due to cosmetic surgeries, the use of make-up, hair col-oring, etc. As observed from crowdsourcing, age is difficultfor even humans to discern. In future research, we are con-sidering a more nuanced evaluation of these tools, includinglarger datasets and investigation into subgroups of gender,age, and race.","Assessing the Accuracy of Four Popular FaceRecognition Tools for Inferring Gender, Age, and Race.Face Detection,Age Detection,Race Detection,Gender DetectionIn this research, we evaluate four widely used face detec-tion tools, which are Face++, IBM Bluemix Visual Recog-nition, AWS Rekognition, and Microsoft Azure Face API,using multiple datasets to determine their accuracy in in-ferring user attributes, including gender, race, and age. Re-sults show that the tools are generally proficient at determin-ing gender, with accuracy rates greater than 90%, except forIBM Bluemix. Concerning race, only one of the four toolsprovides this capability, Face++, with an accuracy rate ofgreater than 90%, although the evaluation was performed ona high-quality dataset. Inferring age appears to be a challeng-ing problem, as all four tools performed poorly. The findingsof our quantitative evaluation are helpful for future computa-tional social science research using these tools, as their accu-racy needs to be taken into account when applied to classify-ing individuals on social media and other contexts. Triangu-lation and manual verification are suggested for researchersemploying these tools.The results of our accuracy evaluation using multipledatasets for four popular facial recognition tools highlightthe need for triangulation as a crucial step for better compu-tational social science research, even for the relatively sim-ple task of face detection within an image. Reviewing ourspecific results we see a trend of high accuracy for gender,with three of the tools performing with an accuracy of above90 percent for all datasets. Concerning race, only one tooloffers this capability, Face++, and the accuracy is quite high,above 90 percent. However, this was evaluated by using ahigh-quality dataset of images. Future research is needed todetermine if such accuracy holds for noisy images.All of the tools performed poorly for age, even with therelaxed task of determining an age bin instead of exact age.Moreover, the average error regarding age for all tools wasquite high. We conjecture that an individual’s age may be adifficult attribute for facial recognition tools to discern, per-haps due to cosmetic surgeries, the use of make-up, hair col-oring, etc. As observed from crowdsourcing, age is difficultfor even humans to discern. In future research, we are con-sidering a more nuanced evaluation of these tools, includinglarger datasets and investigation into subgroups of gender,age, and race.Experimental studies on the proposed method ‘ Choice of Test of acoustic Acquisition of Experiment in application acoustic feature feature Ly] training = | >| and examination on ‘parameters parameters samples generalization ability Historical Data from Wechat Experimental Data from ‘Wechat and QQ Standard Testing Corpus Fig. 4. Research schema.Input Signals | Pre-emphasis Mel filter group MFCC Coefficients <—| Fig. 5. Calculation process of the MFCC.° _ os ly 0 Voice waves Affective computing Emotion recognition 4 4 4 Acoustic r ; LS-SVR Estimated Percentage of Pre-processing [>] pas *! estimator [*] PADvalues [| typical emotions Training samples Fig. 3. Proposed computational method.scale of emotional polly (W Dat et ot /Ieformation & Management 52 (2015) 777-758 66 ef900 boo Wb oo Do 66 oo Chat No i 8. Dynamic proces of emotion propagation wth pose and neatie corte. valve Vale Vaio vate val",Person recognition,"This research evaluates the accuracy of four popular facial recognition tools - Face++, IBM Bluemix Visual Recognition, AWS Rekognition, and Microsoft Azure Face API - in determining user attributes such as gender, race, and age using multiple datasets. The study finds that the tools are generally accurate in determining gender, with accuracy rates above 90% except for IBM Bluemix. However, inferring age appears to be a challenging problem for all four tools. Regarding race, only Face++ provides this capability with an accuracy rate of above 90% but this was evaluated on a high-quality dataset. The study highlights the need for triangulation and manual verification for better computational social science research using these tools. The results suggest a trend of high accuracy for gender but poor performance for age. Future research is needed to determine if the accuracy holds for noisy images and investigate subgroups of gender, age, and race.",Object and Sentiment Recognition,Experimental studies on the proposed method ‘ Choice of Test of acoustic Acquisition of Experiment in application acoustic feature feature Ly] training = | >| and examination on ‘parameters parameters samples generalization ability Historical Data from Wechat Experimental Data from ‘Wechat and QQ Standard Testing Corpus Fig. 4. Research schema.Input Signals | Pre-emphasis Mel filter group MFCC Coefficients <—| Fig. 5. Calculation process of the MFCC.° _ os ly 0 Voice waves Affective computing Emotion recognition 4 4 4 Acoustic r ; LS-SVR Estimated Percentage of Pre-processing [>] pas *! estimator [*] PADvalues [| typical emotions Training samples Fig. 3. Proposed computational method.scale of emotional polly (W Dat et ot /Ieformation & Management 52 (2015) 777-758 66 ef900 boo Wb oo Do 66 oo Chat No i 8. Dynamic proces of emotion propagation wth pose and neatie corte. valve Vale Vaio vate val,Object Recognition
197,A Review Paper on FACIAL RECOGNITION ,Facial recognition at a glance.," A facial recognition system is a computer application for automatically identifying or verifying a person from a digital image or a 
video frame from a video source. One of the way is to do this is by comparing selected facial features from the image and a facial database. It 
is typically used in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems [1]. Recently 
face recognition is attracting much attention in the society of network multimedia information access. Areas such as network security, 
content indexing and retrieval, and video compression benefits from face recognition technology because ""people"" are the center of attention 
in a lot of video. Network access control via face recognition not only makes hackers virtually impossible to steal one's ""password"", but also 
increases the User friendliness in human-computer interaction. Indexing and/or retrieving video data based on the appearances of particular 
persons will be useful for users such as news reporters, political scientists, and moviegoers [2]. In this paper we focus on 3-D facial 
recognition system and biometric facial recognition system.","Face recognition technologies have been associated generally 
with very costly top secure applications. Today the core 
technologies have evolved and the cost of equipment is going 
down dramatically due to the integration and the increasing 
processing power. Certain applications of face recognition 
technology are now cost effective, reliable and highly 
accurate. As a result there are no technological or financial 
barriers for stepping from the pilot project to widespread 
deployment. 
Though there are some weaknesses of facial recognition 
system, there is a tremendous scope in India. This system can 
be effectively used in ATM’s ,identifying duplicate voters, 
passport and visa verification, driving license verification, in 
defense, competitive and other exams, in governments and 
private sectors. 
Government and NGOs should concentrate and promote 
applications of facial recognition system in India in various 
fields by giving economical support and appreciation. 
Face recognition is a both challenging and important 
recognition technique. Among all the biometric techniques, 
face recognition approach possesses one great advantage, 
which is its user-friendliness (or non-intrusiveness). In this 
paper, we have given an introductory survey for the face 
recognition technology. We hope this paper can provide the 
readers a better understanding about face recognition. ","A Review Paper on FACIAL RECOGNITION Facial recognition at a glance. A facial recognition system is a computer application for automatically identifying or verifying a person from a digital image or a 
video frame from a video source. One of the way is to do this is by comparing selected facial features from the image and a facial database. It 
is typically used in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems [1]. Recently 
face recognition is attracting much attention in the society of network multimedia information access. Areas such as network security, 
content indexing and retrieval, and video compression benefits from face recognition technology because ""people"" are the center of attention 
in a lot of video. Network access control via face recognition not only makes hackers virtually impossible to steal one's ""password"", but also 
increases the User friendliness in human-computer interaction. Indexing and/or retrieving video data based on the appearances of particular 
persons will be useful for users such as news reporters, political scientists, and moviegoers [2]. In this paper we focus on 3-D facial 
recognition system and biometric facial recognition system.Face recognition technologies have been associated generally 
with very costly top secure applications. Today the core 
technologies have evolved and the cost of equipment is going 
down dramatically due to the integration and the increasing 
processing power. Certain applications of face recognition 
technology are now cost effective, reliable and highly 
accurate. As a result there are no technological or financial 
barriers for stepping from the pilot project to widespread 
deployment. 
Though there are some weaknesses of facial recognition 
system, there is a tremendous scope in India. This system can 
be effectively used in ATM’s ,identifying duplicate voters, 
passport and visa verification, driving license verification, in 
defense, competitive and other exams, in governments and 
private sectors. 
Government and NGOs should concentrate and promote 
applications of facial recognition system in India in various 
fields by giving economical support and appreciation. 
Face recognition is a both challenging and important 
recognition technique. Among all the biometric techniques, 
face recognition approach possesses one great advantage, 
which is its user-friendliness (or non-intrusiveness). In this 
paper, we have given an introductory survey for the face 
recognition technology. We hope this paper can provide the 
readers a better understanding about face recognition. IBM Amazon MS 98% 99% 99% (98) 9) 9) IMDb-Wiki | 624% 79.7% 768% 712% (52.478) _ | 2,769) _ 41,811) (40,296) 37,343) Twitter-Age | 100% 915% 92.6% 04% 04) (4) (86) 7) (85) TwitterGender | 100% 936% 917% 90.1% G73) GB) G49) __—G42)_—336). dataset | 628% 798% 76.9% 71.3% (53.045) | (33,336)__(42.344) (40,824) (37,863) Table 6: Share of images where a face is detected. Dataset | Percentage 100 celebrities (100 imgs) | 97% (97) Twitter-Age 4) | 79.8% (75) ~Twitter-Gender (373) | 80.4% (300) _ All dataset (53,045) | 52.9% (28,113)| Face Attribute: | Free Quota & Pricing Faces | age with ange Timage second acer SO.0001 image ; etniity Ng’) j IBM | age (maximum/minimum) | 250 images / day 20 40 6 39 gender ‘$0,004 / image female male female male Amazon | age (high/low) ‘Sk images / month white white ‘black Asian gender 0.001 / image MS age 30k images / month Table 3: Examples from “100 celebrities’ with ground truth. sender 0.0018 image Table 1: Overview of four popular face detection tools using n tis research, Tr. ‘Clean Noisy Ground-truth _| (1) 100 Celebrities | _(2) IMDt No ground-truth | T9672 | 1921711703 T986/06/13 2009 1966 2011 male | @) Twitter profiles yee OO Oa ‘gender and age but no ethnicity information are available",Person recognition,"This paper discusses the concept of facial recognition technology, which is a computer application used for identifying or verifying individuals from digital images or video frames. The technology compares selected facial features from the image with a facial database and is commonly used in security systems. The paper also highlights the benefits of face recognition technology in various fields such as network security, content indexing, and retrieval, video compression, etc. The 3-D facial recognition system and biometric facial recognition system are also discussed. Although there are some weaknesses associated with facial recognition technology, the paper emphasizes the potential applications of this technology in India, including ATM security, identifying duplicate voters, passport and visa verification, driving license verification, competitive and other exams, and government and private sectors. The paper concludes by emphasizing the importance of promoting and supporting the applications of facial recognition technology in India through government and NGO initiatives.",Object and Sentiment Recognition,"IBM Amazon MS 98% 99% 99% (98) 9) 9) IMDb-Wiki | 624% 79.7% 768% 712% (52.478) _ | 2,769) _ 41,811) (40,296) 37,343) Twitter-Age | 100% 915% 92.6% 04% 04) (4) (86) 7) (85) TwitterGender | 100% 936% 917% 90.1% G73) GB) G49) __—G42)_—336). dataset | 628% 798% 76.9% 71.3% (53.045) | (33,336)__(42.344) (40,824) (37,863) Table 6: Share of images where a face is detected. Dataset | Percentage 100 celebrities (100 imgs) | 97% (97) Twitter-Age 4) | 79.8% (75) ~Twitter-Gender (373) | 80.4% (300) _ All dataset (53,045) | 52.9% (28,113)| Face Attribute: | Free Quota & Pricing Faces | age with ange Timage second acer SO.0001 image ; etniity Ng’) j IBM | age (maximum/minimum) | 250 images / day 20 40 6 39 gender ‘$0,004 / image female male female male Amazon | age (high/low) ‘Sk images / month white white ‘black Asian gender 0.001 / image MS age 30k images / month Table 3: Examples from “100 celebrities’ with ground truth. sender 0.0018 image Table 1: Overview of four popular face detection tools using n tis research, Tr. ‘Clean Noisy Ground-truth _| (1) 100 Celebrities | _(2) IMDt No ground-truth | T9672 | 1921711703 T986/06/13 2009 1966 2011 male | @) Twitter profiles yee OO Oa ‘gender and age but no ethnicity information are available",Object Recognition
198,A Review Paper on Facial Recognition Techniques.," Biometric, Face Detection, Feature Analysis, 
Support Vector Machine, Neural Network. ","With the top-notch enlargement in video and image facts set, there's a mind boggling want of programmed comprehension and assessment of data with the aid of the smart frameworks as physically it's far having the hazard to be naturally a long way off. Face assumes a big component in pleasant sex for passing on personal and sensations of a man or woman. Individuals do not have great capability to differentiate sudden countenances in comparison to machines. In this way, programmed face vicinity framework assumes a vast part in face acknowledgment, appearance acknowledgment, head-present assessment, human–PC collaboration and so on. Facial recognition is considered as biometric software which is used for face detection and authentication. This research maps the features of individual’s face in mathematics form and stores in the database as a face print. Comparison of digital image or instant captured image to the stored image (which are stored in the database) is performed by deep learning algorithm to authenticate individual’s identity. Face has important role in mutual communication for conveying identity. In digital image, size and location of a human face is determined by face recognition technology. Many applications of Facial recognition software have existed in security and surveillance industries and consumer markets. This paper presents the techniques, which are used in face recognition techniques with application and challenges.","Endless protection, and assessable applications requires the operation of face acknowledgment innovations. Among the whole kinds of biometric, face location and acknowledgment framework are the most exact. In this article, we have introduced an overview of face discovery strategies. It is energizing to see face discovery strategies are progressively utilized in genuine applications and items. Applications and difficulties of face recognition were likewise examined which roused us to do this exploration in face identification. The clearest future bearing is to additionally further develop the face recognition in presence of certain issues like face impediment and non-uniform brightening. Momentum research centers in field of face discovery and acknowledgment is the identification of countenances in presence of impediment and non-uniform light. A great deal of work has been done in face identification, however not in presence of issue of quality of impediment and non-uniform light. On the off chance that it occurs, it will assist a ton with confronting acknowledgment, face demeanor acknowledgment and so forth Right now many organizations are giving facial biometric in cell phone for access. In future it will be utilized for installments, security, medical services, promoting, criminal ID and so on may necessitate the need for such biometric systems.","A Review Paper on Facial Recognition Techniques. Biometric, Face Detection, Feature Analysis, 
Support Vector Machine, Neural Network. With the top-notch enlargement in video and image facts set, there's a mind boggling want of programmed comprehension and assessment of data with the aid of the smart frameworks as physically it's far having the hazard to be naturally a long way off. Face assumes a big component in pleasant sex for passing on personal and sensations of a man or woman. Individuals do not have great capability to differentiate sudden countenances in comparison to machines. In this way, programmed face vicinity framework assumes a vast part in face acknowledgment, appearance acknowledgment, head-present assessment, human–PC collaboration and so on. Facial recognition is considered as biometric software which is used for face detection and authentication. This research maps the features of individual’s face in mathematics form and stores in the database as a face print. Comparison of digital image or instant captured image to the stored image (which are stored in the database) is performed by deep learning algorithm to authenticate individual’s identity. Face has important role in mutual communication for conveying identity. In digital image, size and location of a human face is determined by face recognition technology. Many applications of Facial recognition software have existed in security and surveillance industries and consumer markets. This paper presents the techniques, which are used in face recognition techniques with application and challenges.Endless protection, and assessable applications requires the operation of face acknowledgment innovations. Among the whole kinds of biometric, face location and acknowledgment framework are the most exact. In this article, we have introduced an overview of face discovery strategies. It is energizing to see face discovery strategies are progressively utilized in genuine applications and items. Applications and difficulties of face recognition were likewise examined which roused us to do this exploration in face identification. The clearest future bearing is to additionally further develop the face recognition in presence of certain issues like face impediment and non-uniform brightening. Momentum research centers in field of face discovery and acknowledgment is the identification of countenances in presence of impediment and non-uniform light. A great deal of work has been done in face identification, however not in presence of issue of quality of impediment and non-uniform light. On the off chance that it occurs, it will assist a ton with confronting acknowledgment, face demeanor acknowledgment and so forth Right now many organizations are giving facial biometric in cell phone for access. In future it will be utilized for installments, security, medical services, promoting, criminal ID and so on may necessitate the need for such biometric systems.eC ear mae Pra RUE Sea cl nie} 7 HowStuthWorks:",Person recognition,"This paper highlights the increasing need for automated understanding and evaluation of data due to the massive growth in video and image datasets. Facial recognition technology plays a significant role in face detection, appearance recognition, and human-computer interaction. The technology maps the features of an individual's face in mathematical form and stores them in a database as a face print. Deep learning algorithms compare digital images or instantly captured images to the stored images to authenticate an individual's identity. Facial recognition technology has many applications in security and surveillance industries as well as in consumer markets. This paper provides an overview of the techniques used in face recognition and discusses the challenges and potential applications of this technology, including privacy concerns and assessable applications. The future direction of face recognition technology includes further improvement in the presence of obstacles and non-uniform lighting. Facial biometric technology is currently being used in smartphones for access and may have future applications in payments, healthcare, advertising, criminal identification, etc.",Object and Sentiment Recognition,eC ear mae Pra RUE Sea cl nie} 7 HowStuthWorks:,Object Recognition
199,Emotional face recognition in individuals with attention.,Facial emotion recognition. ,"This review focuses on facial emotion recognition (FER) in individuals with attention- deficit/hyperactivity disorder (ADHD). Behavioral studies of FER in ADHD have resulted in inconsistent findings. Here, we discuss the factors that vary across studies and the way that they influence FER processes in ADHD. Across reviewed studies, fear was the most deficient facial expression to be recognized. Our review suggested that FER deficit in ADHD does not alleviate across development and is partially distinct from ADHD symptoms. In conclusion, assessment of FER in ADHD and targeting that in interventional plans could lead to social skills improvement in ADHD.","There is evidence from studies that reviewed here to support the FER deficits in individuals with ADHD regardless of their age, and subtypes. Fear facial expression were found to be the most impaired emotion, while impairment in recognizing other facial expressions also was reported. It appears that ADHD symptoms are not at least entirely account for FER deficit and this potentially is a distinct problem that should be assessed and treated along with other cognitive impairments in ADHD. Further studies in large participant groups controlling the influence of impairment in executive functions will provide critical insights into the origins of FER in ADHD individuals.","Emotional face recognition in individuals with attention.Facial emotion recognition. This review focuses on facial emotion recognition (FER) in individuals with attention- deficit/hyperactivity disorder (ADHD). Behavioral studies of FER in ADHD have resulted in inconsistent findings. Here, we discuss the factors that vary across studies and the way that they influence FER processes in ADHD. Across reviewed studies, fear was the most deficient facial expression to be recognized. Our review suggested that FER deficit in ADHD does not alleviate across development and is partially distinct from ADHD symptoms. In conclusion, assessment of FER in ADHD and targeting that in interventional plans could lead to social skills improvement in ADHD.There is evidence from studies that reviewed here to support the FER deficits in individuals with ADHD regardless of their age, and subtypes. Fear facial expression were found to be the most impaired emotion, while impairment in recognizing other facial expressions also was reported. It appears that ADHD symptoms are not at least entirely account for FER deficit and this potentially is a distinct problem that should be assessed and treated along with other cognitive impairments in ADHD. Further studies in large participant groups controlling the influence of impairment in executive functions will provide critical insights into the origins of FER in ADHD individuals.Face Detection Feature base Image Base ! Neural Networks Active Shape | | Lowlevel | | Feature a Model ‘Analysis | | Analysis Linear +] Skin Viola |_| Subspace Snakes Color Jones _| Statistical Deformable Motion approach templates Gabor Feature Gray Seale POM Constellation Edge Fig 2: Face detection technologies",facial recognition,"This review explores facial emotion recognition (FER) in individuals with ADHD and discusses the inconsistent findings across studies. Fear facial expressions were found to be the most impaired emotion, but other facial expressions were also affected. The FER deficit in ADHD is not entirely accounted for by ADHD symptoms, and further studies controlling for executive functions are needed. FER in ADHD does not improve with age and may require assessment and intervention for social skills improvement.",Object and Sentiment Recognition,Face Detection Feature base Image Base ! Neural Networks Active Shape | | Lowlevel | | Feature a Model ‘Analysis | | Analysis Linear +] Skin Viola |_| Subspace Snakes Color Jones _| Statistical Deformable Motion approach templates Gabor Feature Gray Seale POM Constellation Edge Fig 2: Face detection technologies,Object Recognition
200,Face Recognition Systems: A Survey.," face recognition systems; person identification; biometric systems; survey
."," Over the past few decades, interest in theories and algorithms for face recognition has been
growing rapidly. Video surveillance, criminal identification, building access control, and unmanned
and autonomous vehicles are just a few examples of concrete applications that are gaining attraction
among industries. Various techniques are being developed including local, holistic, and hybrid
approaches, which provide a face image description using only a few face image features or the whole
facial features. The main contribution of this survey is to review some well-known techniques for
each approach and to give the taxonomy of their categories. In the paper, a detailed comparison
between these techniques is exposed by listing the advantages and the disadvantages of their schemes
in terms of robustness, accuracy, complexity, and discrimination. One interesting feature mentioned
in the paper is about the database used for face recognition. An overview of the most commonly
used databases, including those of supervised and unsupervised learning, is given. Numerical results
of the most interesting techniques are given along with the context of experiments and challenges
handled by these techniques. Finally, a solid discussion is given in the paper about future directions
in terms of techniques to be used for face recognition.","Face recognition system is a popular study task in the field of image processing and computer
vision, owing to its potentially enormous application as well as its theoretical value. This system is
widely deployed in many real-world applications such as security, surveillance, homeland security,
access control, image search, human-machine, and entertainment. However, these applications pose
different challenges such as lighting conditions and facial expressions. This paper highlights the recent
research on the 2D or 3D face recognition system, focusing mainly on approaches based on local,
holistic (subspace), and hybrid features. A comparative study between these approaches in terms of
processing time, complexity, discrimination, and robustness was carried out. We can conclude that local
feature techniques are the best choice concerning discrimination, rotation, translation, complexity, and
accuracy. We hope that this survey paper will further encourage researchers in this field to participate
and pay more attention to the use of local techniques for face recognition systems.
","Face Recognition Systems: A Survey. face recognition systems; person identification; biometric systems; survey
. Over the past few decades, interest in theories and algorithms for face recognition has been
growing rapidly. Video surveillance, criminal identification, building access control, and unmanned
and autonomous vehicles are just a few examples of concrete applications that are gaining attraction
among industries. Various techniques are being developed including local, holistic, and hybrid
approaches, which provide a face image description using only a few face image features or the whole
facial features. The main contribution of this survey is to review some well-known techniques for
each approach and to give the taxonomy of their categories. In the paper, a detailed comparison
between these techniques is exposed by listing the advantages and the disadvantages of their schemes
in terms of robustness, accuracy, complexity, and discrimination. One interesting feature mentioned
in the paper is about the database used for face recognition. An overview of the most commonly
used databases, including those of supervised and unsupervised learning, is given. Numerical results
of the most interesting techniques are given along with the context of experiments and challenges
handled by these techniques. Finally, a solid discussion is given in the paper about future directions
in terms of techniques to be used for face recognition.Face recognition system is a popular study task in the field of image processing and computer
vision, owing to its potentially enormous application as well as its theoretical value. This system is
widely deployed in many real-world applications such as security, surveillance, homeland security,
access control, image search, human-machine, and entertainment. However, these applications pose
different challenges such as lighting conditions and facial expressions. This paper highlights the recent
research on the 2D or 3D face recognition system, focusing mainly on approaches based on local,
holistic (subspace), and hybrid features. A comparative study between these approaches in terms of
processing time, complexity, discrimination, and robustness was carried out. We can conclude that local
feature techniques are the best choice concerning discrimination, rotation, translation, complexity, and
accuracy. We hope that this survey paper will further encourage researchers in this field to participate
and pay more attention to the use of local techniques for face recognition systems.
Records identified Additional records ‘through database identified through searching other sources (n= 58) (n=13) Screening Eligibility Included 1. PRISMA diagram. Records screened (n= 71) || Full-text articles assessed for eligibility [> (n= 63) Records excluded (n=8) Fulltext articles ‘excluded with reasons (n=37) Studies included in review (n= 26)",Person recognition,"This paper provides a survey of various techniques for face recognition, including local, holistic, and hybrid approaches. The authors compare the advantages and disadvantages of these techniques in terms of robustness, accuracy, complexity, and discrimination, and discuss the most commonly used databases for face recognition. The paper highlights the challenges posed by real-world applications such as lighting conditions and facial expressions. The authors conclude that local feature techniques are the best choice for face recognition systems in terms of discrimination, rotation, translation, complexity, and accuracy. The paper encourages researchers to pay more attention to the use of local techniques in face recognition systems.",Object and Sentiment Recognition,Records identified Additional records ‘through database identified through searching other sources (n= 58) (n=13) Screening Eligibility Included 1. PRISMA diagram. Records screened (n= 71) || Full-text articles assessed for eligibility [> (n= 63) Records excluded (n=8) Fulltext articles ‘excluded with reasons (n=37) Studies included in review (n= 26),Object Recognition
201,Deep Learning for Video Game Playing,"Algorithms, learning, machine learning algorithms, multilayer neural network, artificial intelligence, deep learning","In this paper, we review recent deep learning advances
in the context of how they have been applied to play different types
of video games such as first-person shooters, arcade games, and
real-time strategy games. We analyze the unique requirements that
different game genres pose to a deep learning system and highlight
important open challenges in the context of applying these machine
learning methods to video games, such as general game playing,
dealing with extremely large decision spaces and sparse rewards.","This paper reviewed DL methods applied to game playing
in video games of various genres including arcade, racing,
FPSs, open-world, RTS, team sports, physics, and text adventure
games. Most of the reviewed work is within end-to-end modelfree deep RL, where a CNN learns to play directly from raw
pixels by interacting with the game. Recent work demonstrates
that derivative-free ESs and genetic algorithms are competitive
alternatives. Some of the reviewed work apply supervised learning to imitate behaviors from game logs, while others are based
on methods that learn a model of the environment. For simple
games, such as most arcade games, the reviewed methods can
achieve above human-level performance, while there are many
open challenges in more complex games.","Deep Learning for Video Game PlayingAlgorithms, learning, machine learning algorithms, multilayer neural network, artificial intelligence, deep learningIn this paper, we review recent deep learning advances
in the context of how they have been applied to play different types
of video games such as first-person shooters, arcade games, and
real-time strategy games. We analyze the unique requirements that
different game genres pose to a deep learning system and highlight
important open challenges in the context of applying these machine
learning methods to video games, such as general game playing,
dealing with extremely large decision spaces and sparse rewards.This paper reviewed DL methods applied to game playing
in video games of various genres including arcade, racing,
FPSs, open-world, RTS, team sports, physics, and text adventure
games. Most of the reviewed work is within end-to-end modelfree deep RL, where a CNN learns to play directly from raw
pixels by interacting with the game. Recent work demonstrates
that derivative-free ESs and genetic algorithms are competitive
alternatives. Some of the reviewed work apply supervised learning to imitate behaviors from game logs, while others are based
on methods that learn a model of the environment. For simple
games, such as most arcade games, the reviewed methods can
achieve above human-level performance, while there are many
open challenges in more complex games.",Deep Learning and Machine Learning,"This paper reviews recent advances in deep learning methods applied to playing various types of video games, including first-person shooters, arcade games, and real-time strategy games. The authors analyze the unique challenges that different game genres pose to deep learning systems and highlight open challenges in applying these methods to video games, such as dealing with large decision spaces and sparse rewards. The reviewed work focuses on end-to-end model-free deep reinforcement learning, as well as supervised learning and methods that learn a model of the environment. The reviewed methods have achieved above-human-level performance in simpler games, but there are many challenges in more complex games.",Deep Learning and Machine Learning,,Deep Learning and Machine Learning
202,Real Time Face Detection and Facial Expression Recognition: Development and Applications to Human Computer Interaction.,"Face, Real time systems, Robots, Pixel, Training, Support vector machines, Gabor filters, Facial expression recognition","Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a a time scale in the order of 40 milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this paper we present progress on one such perceptual primitive. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [15, 2]. The expression recognizer receives image patches located by the face detector. A Gabor representation of the patch is formed and then processed by a bank of SVM classifiers. A novel combination of Adaboost and SVM's enhances performance. The system was tested on the Cohn-Kanade dataset of posed facial expressions [6]. The generalization performance to new subjects for a 7- way forced choice correct. Most interestingly the outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system has been deployed on a wide variety of platforms including Sony's Aibo pet robot, ATR's RoboVie, and CU animator, and is currently being evaluated for applications including automatic reading tutors, assessment of human-robot interaction.","Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a a time scale in the order of 40 milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this paper we present progress on one such perceptual primitive: Real time recognition of facial expressions.","Real Time Face Detection and Facial Expression Recognition: Development and Applications to Human Computer Interaction.Face, Real time systems, Robots, Pixel, Training, Support vector machines, Gabor filters, Facial expression recognitionComputer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a a time scale in the order of 40 milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this paper we present progress on one such perceptual primitive. The system automatically detects frontal faces in the video stream and codes them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [15, 2]. The expression recognizer receives image patches located by the face detector. A Gabor representation of the patch is formed and then processed by a bank of SVM classifiers. A novel combination of Adaboost and SVM's enhances performance. The system was tested on the Cohn-Kanade dataset of posed facial expressions [6]. The generalization performance to new subjects for a 7- way forced choice correct. Most interestingly the outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system has been deployed on a wide variety of platforms including Sony's Aibo pet robot, ATR's RoboVie, and CU animator, and is currently being evaluated for applications including automatic reading tutors, assessment of human-robot interaction.Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a a time scale in the order of 40 milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this paper we present progress on one such perceptual primitive: Real time recognition of facial expressions.",Deep Learning and Machine Learning,"This paper discusses how computer animated agents and robots enhance human-computer interaction and introduce a social dimension to daily life. It highlights the importance of real-time communication, which requires relying on sensory rich perceptual primitives rather than slow symbolic inference processes. The paper presents progress on one such primitive, which is the automatic detection and coding of facial expressions in real-time. The system employs a cascade of feature detectors and SVM classifiers to recognize facial expressions and has been tested on various platforms. The system's smooth output provides a valuable representation to code facial expression dynamics in an unobtrusive manner. The paper concludes by stating that the system has potential applications in areas such as automatic reading tutors and human-robot interaction assessment.",Object and Sentiment Recognition,,Sentiment Analysis
203,Emotion recognition in human–computer interaction,Emotions Emotion classification Attention control Sigma–pi neural networks Feedback learning Relaxation Emotion data sets Prosody Lexical content Face feature analysis,"In this paper, we outline the approach we have developed to construct an emotion-recognising system. It is based on guidance from psychological studies of emotion, as well as from the nature of emotion in its interaction with attention. A neural network architecture is constructed to be able to handle the fusion of different modalities (facial features, prosody and lexical content in speech). Results from the network are given and their implications discussed, as are implications for future direction for the research","In this paper we have introduced the framework of the EC project ERMIS. The aim of this project was to build an automatic emotion recognition system able to exploit multimodal emotional markers such as those embedded in the voice, face and words spoken. We discussed the numerous potential applications of such a system for industry as well as in academia. We then turned to the psychological literature to help lay the theoretical foundation of our system and make use of insights from the various emotion theories proposed in shaping the various components of our automatic emotion recognition system such as the input and output representations as well as the internal structure. We thus looked at the different proposals with respect to the size (basic emotions hypothesis) and nature (discrete or continuous) of the emotional space as well as its origin (evolution or social learning) and hierarchical structure (primary, secondary).

We then proceeded to discuss several issues that pertain to emotion recognition as suggested by psychological research. These were explored separately for the input and the output of the automatic recognition system. The input-related issues involved inconsistencies in the expression of emotion due to social norm, deceptive purposes as well as natural ambiguity of emotional expression. The output-related issues pertained to the nature of the representation of emotional states (discrete and categorical or continuous and non-categorical) as well as the time course of emotional states and the implications of different time scales for automatic recognition. We then proceeded to examine the features that can be used as emotional markers in the various modalities as well as the computational studies carried out based on those features. We also looked at the neural correlates of the perception of those features or markers. With respect to the word stream we reviewed the state-of-the-art in emotion extraction from text and presented our DAL system developed for ERMIS.

We then presented an artificial neural network called ANNA developed for the automatic classification of emotional states driven by a multimodal feature input. The novel feature of ANNA is the feedback attentional loop designed to exploit the attention-grabbing effect of emotional stimuli to further enhance and clarify the salient components of the input stream. Finally we presented the results obtained through the use of ANNA on training and testing material based on the SALAS scenario developed within the ERMIS framework.

The results obtained by using ANNA indicate that there can be crucial differences between subjects as to the clues they pick up from others about the emotional states of the latter. This was shown in the Table 1 of results, and discussed in 5.3 Analysis of results in, 5.4 Conclusions on the results. It is that feature, as well as to possible differences across what different human objects also ‘release’ to others about their inner emotional states, whose implications for constructing an artificial emotion state detector we need to consider carefully. Environmental conditions, such as poor lighting or a poor audio recorder, will influence the input features available to the system. Independently of that, we must take note of the further variability of the training data available for the system.","Emotion recognition in human–computer interactionEmotions Emotion classification Attention control Sigma–pi neural networks Feedback learning Relaxation Emotion data sets Prosody Lexical content Face feature analysisIn this paper, we outline the approach we have developed to construct an emotion-recognising system. It is based on guidance from psychological studies of emotion, as well as from the nature of emotion in its interaction with attention. A neural network architecture is constructed to be able to handle the fusion of different modalities (facial features, prosody and lexical content in speech). Results from the network are given and their implications discussed, as are implications for future direction for the researchIn this paper we have introduced the framework of the EC project ERMIS. The aim of this project was to build an automatic emotion recognition system able to exploit multimodal emotional markers such as those embedded in the voice, face and words spoken. We discussed the numerous potential applications of such a system for industry as well as in academia. We then turned to the psychological literature to help lay the theoretical foundation of our system and make use of insights from the various emotion theories proposed in shaping the various components of our automatic emotion recognition system such as the input and output representations as well as the internal structure. We thus looked at the different proposals with respect to the size (basic emotions hypothesis) and nature (discrete or continuous) of the emotional space as well as its origin (evolution or social learning) and hierarchical structure (primary, secondary).

We then proceeded to discuss several issues that pertain to emotion recognition as suggested by psychological research. These were explored separately for the input and the output of the automatic recognition system. The input-related issues involved inconsistencies in the expression of emotion due to social norm, deceptive purposes as well as natural ambiguity of emotional expression. The output-related issues pertained to the nature of the representation of emotional states (discrete and categorical or continuous and non-categorical) as well as the time course of emotional states and the implications of different time scales for automatic recognition. We then proceeded to examine the features that can be used as emotional markers in the various modalities as well as the computational studies carried out based on those features. We also looked at the neural correlates of the perception of those features or markers. With respect to the word stream we reviewed the state-of-the-art in emotion extraction from text and presented our DAL system developed for ERMIS.

We then presented an artificial neural network called ANNA developed for the automatic classification of emotional states driven by a multimodal feature input. The novel feature of ANNA is the feedback attentional loop designed to exploit the attention-grabbing effect of emotional stimuli to further enhance and clarify the salient components of the input stream. Finally we presented the results obtained through the use of ANNA on training and testing material based on the SALAS scenario developed within the ERMIS framework.

The results obtained by using ANNA indicate that there can be crucial differences between subjects as to the clues they pick up from others about the emotional states of the latter. This was shown in the Table 1 of results, and discussed in 5.3 Analysis of results in, 5.4 Conclusions on the results. It is that feature, as well as to possible differences across what different human objects also ‘release’ to others about their inner emotional states, whose implications for constructing an artificial emotion state detector we need to consider carefully. Environmental conditions, such as poor lighting or a poor audio recorder, will influence the input features available to the system. Independently of that, we must take note of the further variability of the training data available for the system.CET a “© Left O Stay eno Right Input Convolution Convolution Convolution | Fully connected | Recurrency OutputAgent reward action sampled from m= > Environment (video game) state2018 Govovin 2017 ‘Aforance Ae Pon aed Aeton Selection LsT#-00N ‘OPC Mixing Policy Targets rors cureutam Learning Scania Evolutionary Strategie, Progressive Natworks Ms Pac-Man Atari Montezuma’s Racing © Doom Minecraft StarCraft 20 Billiard Text © RoboCup. Revenge Adventure",Person recognition,"This paper outlines the development of an automatic emotion recognition system that combines various modalities including facial features, prosody, and lexical content in speech. The authors discuss the theoretical foundations of the system based on insights from psychological research on the nature of emotion and its interaction with attention. They also examine input and output-related issues related to emotion recognition, including social norms, deception, and the nature of emotional representation. The authors present an artificial neural network called ANNA that uses a feedback attentional loop to enhance the salient components of the input stream. The results obtained from ANNA indicate that there are differences in the clues individuals pick up from others about emotional states, and environmental conditions can influence input features. The authors highlight the importance of considering these factors when constructing an artificial emotion state detector.",Object and Sentiment Recognition,"CET a “© Left O Stay eno Right Input Convolution Convolution Convolution | Fully connected | Recurrency OutputAgent reward action sampled from m= > Environment (video game) state2018 Govovin 2017 ‘Aforance Ae Pon aed Aeton Selection LsT#-00N ‘OPC Mixing Policy Targets rors cureutam Learning Scania Evolutionary Strategie, Progressive Natworks Ms Pac-Man Atari Montezuma’s Racing © Doom Minecraft StarCraft 20 Billiard Text © RoboCup. Revenge Adventure",Deep Learning and Machine Learning
204,Emotion recognition from geometric facial features using self-organizing map,Facial expression analysisGeometric facial featuresSelf-organizing mapFeatures extractionSystem identificationRadial basis functionMulti-layer perceptronSupport vector machine,"This paper presents a novel emotion recognition model using the system identification approach. A comprehensive data driven model using an extended Kohonen self-organizing map (KSOM) has been developed whose input is a 26 dimensional facial geometric feature vector comprising eye, lip and eyebrow feature points. The analytical face model using this 26 dimensional geometric feature vector has been effectively used to describe the facial changes due to different expressions. This paper thus includes an automated generation scheme of this geometric facial feature vector. The proposed non-heuristic model has been developed using training data from MMI facial expression database. The emotion recognition accuracy of the proposed scheme has been compared with radial basis function network, multi-layered perceptron model and support vector machine based recognition schemes. The experimental results show that the proposed model is very efficient in recognizing six basic emotions while ensuring significant increase in average classification accuracy over radial basis function and multi-layered perceptron. It also shows that the average recognition rate of the proposed method is comparatively better than multi-class support vector machine.","A completely automated system for facial geometric features detection and facial expression classification is proposed. We introduce different techniques to detect eyebrow features, nose features, state of eyes and lip features. The proposed eye state detection method gives a clear distinction between different states of eye opening. The detection results for eyebrow feature points and lip feature points are compared against the ground truth. It is observed that for a neutral face having lips with height and width 33 and 65 pixels respectively, the average detection error is only 2.81 pixels. And for eyebrows with height and width 15 and 55 respectively, the average error is 1.45 pixels, which can be considered as very less.

A new mechanism is introduced based on 2D KSOM network to recognize facial expression that uses only a 26 dimensional geometric feature vector, containing directional displacement information about each feature point. The KSOM network parameters are updated simultaneously to train the model for six basic emotions as a function of 26 directional displacement data. Experimentation is carried out over 81 video clips taken from MMI database. An average recognition rate of 93.53% is achieved using the proposed KSOM based recognition method, with the highest recognition rate as 98.33%. The performance of the proposed method is compared with three widely used classifiers: RBFN, MLP3 and multi-class SVM. The average recognition accuracy using RBFN is found as 66.54%, which is much lower than the recognition rate found using our proposed method. MLP3 gives comparatively better performance than RBFN with average recognition rate 72.15%. However, MLP3 achieved highest recognition rate for happiness data (97.3%) among all the four classification methods. On the other hand, the KSOM based recognition method gives much better performance on average than RBFN and MLP3. The performance of KSOM is increased by 1% as compared to the multi-class SVM which is known to be the state of the art in classification. The relative increase of recognition accuracy using KSOM over MLP3 is 21.39% and over RBFN is 26.99% which is a significant improvement. Thus, the extensive experiment illustrates the effectiveness and accuracy of KSOM based facial expression recognition system using only geometric features.","Emotion recognition from geometric facial features using self-organizing mapFacial expression analysisGeometric facial featuresSelf-organizing mapFeatures extractionSystem identificationRadial basis functionMulti-layer perceptronSupport vector machineThis paper presents a novel emotion recognition model using the system identification approach. A comprehensive data driven model using an extended Kohonen self-organizing map (KSOM) has been developed whose input is a 26 dimensional facial geometric feature vector comprising eye, lip and eyebrow feature points. The analytical face model using this 26 dimensional geometric feature vector has been effectively used to describe the facial changes due to different expressions. This paper thus includes an automated generation scheme of this geometric facial feature vector. The proposed non-heuristic model has been developed using training data from MMI facial expression database. The emotion recognition accuracy of the proposed scheme has been compared with radial basis function network, multi-layered perceptron model and support vector machine based recognition schemes. The experimental results show that the proposed model is very efficient in recognizing six basic emotions while ensuring significant increase in average classification accuracy over radial basis function and multi-layered perceptron. It also shows that the average recognition rate of the proposed method is comparatively better than multi-class support vector machine.A completely automated system for facial geometric features detection and facial expression classification is proposed. We introduce different techniques to detect eyebrow features, nose features, state of eyes and lip features. The proposed eye state detection method gives a clear distinction between different states of eye opening. The detection results for eyebrow feature points and lip feature points are compared against the ground truth. It is observed that for a neutral face having lips with height and width 33 and 65 pixels respectively, the average detection error is only 2.81 pixels. And for eyebrows with height and width 15 and 55 respectively, the average error is 1.45 pixels, which can be considered as very less.

A new mechanism is introduced based on 2D KSOM network to recognize facial expression that uses only a 26 dimensional geometric feature vector, containing directional displacement information about each feature point. The KSOM network parameters are updated simultaneously to train the model for six basic emotions as a function of 26 directional displacement data. Experimentation is carried out over 81 video clips taken from MMI database. An average recognition rate of 93.53% is achieved using the proposed KSOM based recognition method, with the highest recognition rate as 98.33%. The performance of the proposed method is compared with three widely used classifiers: RBFN, MLP3 and multi-class SVM. The average recognition accuracy using RBFN is found as 66.54%, which is much lower than the recognition rate found using our proposed method. MLP3 gives comparatively better performance than RBFN with average recognition rate 72.15%. However, MLP3 achieved highest recognition rate for happiness data (97.3%) among all the four classification methods. On the other hand, the KSOM based recognition method gives much better performance on average than RBFN and MLP3. The performance of KSOM is increased by 1% as compared to the multi-class SVM which is known to be the state of the art in classification. The relative increase of recognition accuracy using KSOM over MLP3 is 21.39% and over RBFN is 26.99% which is a significant improvement. Thus, the extensive experiment illustrates the effectiveness and accuracy of KSOM based facial expression recognition system using only geometric features.",Person recognition,"The paper proposes a novel emotion recognition model using the system identification approach, which utilizes an extended Kohonen self-organizing map (KSOM) and a 26 dimensional facial geometric feature vector. The proposed model achieves an average recognition rate of 93.53% on six basic emotions, which is higher than other widely used classifiers such as RBFN and MLP3. The paper also introduces different techniques for automated detection of facial features and evaluates their accuracy. The proposed KSOM-based recognition method using only geometric features shows significant improvement over other classification methods, indicating its effectiveness and accuracy for facial expression recognition.",Object and Sentiment Recognition,,Object Recognition
205,An event-related potential comparison of facial expression processing between cartoon and real faces,"Facial expression recognition, event-related potentials, cartoon faces, real faces, emotion processing, N170, LPP, VPP","Faces play important roles in the social lives of humans. Besides real faces, people also encounter numerous cartoon faces in daily life which convey basic emotional states through facial expressions. Using event-related potentials (ERPs), we conducted a facial expression recognition experiment with 17 university students to compare the processing of cartoon faces with that of real faces. This study used face type (real vs. cartoon), emotion valence (happy vs. angry) and participant gender (male vs. female) as independent variables. Reaction time, recognition accuracy, and the amplitudes and latencies of emotion processing-related ERP components such as N170, VPP (vertex positive potential), and LPP (late positive potential) were used as dependent variables. The ERP results revealed that cartoon faces caused larger N170 and VPP amplitudes as well as a briefer N170 latency than did real faces; that real faces induced larger LPP amplitudes than did cartoon faces. In addition, the results showed a significant difference in the brain regions as reflected in a right hemispheric advantage. The behavioral results showed that the reaction times for happy faces were shorter than those for angry faces; that females showed a higher accuracy than did males; and that males showed a higher recognition accuracy for angry faces than happy faces. Due to the sample size, these results may suggestively but not rigorously demonstrate differences in facial expression recognition and neurological processing between cartoon faces and real faces. Cartoon faces showed a higher processing intensity and speed than real faces during the early processing stage. However, more attentional resources were allocated for real faces during the late processing stage.","We used ERPs to measure the brain activity responses induced by the facial expressions of cartoon and real faces. According to the neurophysiological evidence in this study, face type has a strong but heterogeneous effect on the N170, VPP, and LPP components. During the early processing stage, adults process cartoon faces faster than real faces. However, adults allocate more attentional resources for real face processing during late processing stage. Future research should use larger sample sizes to examine the interaction between face type (real vs. cartoon) and facial expression.","An event-related potential comparison of facial expression processing between cartoon and real facesFacial expression recognition, event-related potentials, cartoon faces, real faces, emotion processing, N170, LPP, VPPFaces play important roles in the social lives of humans. Besides real faces, people also encounter numerous cartoon faces in daily life which convey basic emotional states through facial expressions. Using event-related potentials (ERPs), we conducted a facial expression recognition experiment with 17 university students to compare the processing of cartoon faces with that of real faces. This study used face type (real vs. cartoon), emotion valence (happy vs. angry) and participant gender (male vs. female) as independent variables. Reaction time, recognition accuracy, and the amplitudes and latencies of emotion processing-related ERP components such as N170, VPP (vertex positive potential), and LPP (late positive potential) were used as dependent variables. The ERP results revealed that cartoon faces caused larger N170 and VPP amplitudes as well as a briefer N170 latency than did real faces; that real faces induced larger LPP amplitudes than did cartoon faces. In addition, the results showed a significant difference in the brain regions as reflected in a right hemispheric advantage. The behavioral results showed that the reaction times for happy faces were shorter than those for angry faces; that females showed a higher accuracy than did males; and that males showed a higher recognition accuracy for angry faces than happy faces. Due to the sample size, these results may suggestively but not rigorously demonstrate differences in facial expression recognition and neurological processing between cartoon faces and real faces. Cartoon faces showed a higher processing intensity and speed than real faces during the early processing stage. However, more attentional resources were allocated for real faces during the late processing stage.We used ERPs to measure the brain activity responses induced by the facial expressions of cartoon and real faces. According to the neurophysiological evidence in this study, face type has a strong but heterogeneous effect on the N170, VPP, and LPP components. During the early processing stage, adults process cartoon faces faster than real faces. However, adults allocate more attentional resources for real face processing during late processing stage. Future research should use larger sample sizes to examine the interaction between face type (real vs. cartoon) and facial expression.",Emotion detection using normal face,"The study compared the processing of cartoon faces and real faces in 17 university students using event-related potentials (ERPs). The study found that cartoon faces caused larger N170 and VPP amplitudes with briefer N170 latency than real faces, while real faces induced larger LPP amplitudes. The results suggested that cartoon faces showed higher processing intensity and speed than real faces during early processing stages, but more attentional resources were allocated for real face processing during the late processing stage. The study recommended future research with larger sample sizes to examine the interaction between face type and facial expression.",Object and Sentiment Recognition,,Sentiment Analysis
206,"Social behavior following traumatic brain injury and its association with emotion recognition, understanding of intentions, and cognitive flexibility","Traumatic brain injury
Social behavior
Emotion recognition
Theory of mind
Cognitive flexibility
Follow-up","Although the adverse consequences of changes in social behavior following traumatic brain injury (TBI) are well documented, relatively little is known about possible underlying neuropsychological deficits. Following a model originally developed for social behavior deficits in schizophrenia, we investigated whether impairments in emotion recognition, understanding of other people's intentions (“theory of mind”), and cognitive flexibility soon after first TBI or 1 year later were associated with self and proxy ratings of behavior following TBI. Each of the three functions was assessed with two separate tests, and ratings of behavior were collected on three questionnaires. Patients with TBI (n = 33) were impaired in emotion recognition, “theory of mind,” and cognitive flexibility compared with matched orthopedic controls (n = 34). Proxy ratings showed increases in behavioral problems 1 year following injury in the TBI group but not in the control group. However, test performance was not associated with questionnaire data. Severity of the impairments in emotion recognition, understanding intention, and flexibility were unrelated to the severity of behavioral problems following TBI. These findings failed to confirm the used model for social behavior deficits and may cast doubt on the alleged link between deficits in emotion recognition or theory of mind and social functioning. ","The aim of this study was to investigate whether deficits in emotion recognition, understanding intentions (ToM), or cognitive flexibility might underlie changes in social behavior following TBI. The study found that the TBI group was impaired on expression recognition, ToM, and cognitive flexibility soon after injury and at 1-year follow-up. Proxy ratings of behavior showed an increase in behavioral problems 1 year following TBI, but not following orthopedic injury. However, there was no association between test performance and post-injury behavior in the TBI group. The study discusses limitations, such as the small sample size, and the ambiguity of the relationship between impairments in executive functioning and social behavior following TBI. The study concludes that future studies could investigate more systematically the contribution of different aspects of executive functioning towards predicting emotional and social behavior following TBI.","Social behavior following traumatic brain injury and its association with emotion recognition, understanding of intentions, and cognitive flexibilityTraumatic brain injury
Social behavior
Emotion recognition
Theory of mind
Cognitive flexibility
Follow-upAlthough the adverse consequences of changes in social behavior following traumatic brain injury (TBI) are well documented, relatively little is known about possible underlying neuropsychological deficits. Following a model originally developed for social behavior deficits in schizophrenia, we investigated whether impairments in emotion recognition, understanding of other people's intentions (“theory of mind”), and cognitive flexibility soon after first TBI or 1 year later were associated with self and proxy ratings of behavior following TBI. Each of the three functions was assessed with two separate tests, and ratings of behavior were collected on three questionnaires. Patients with TBI (n = 33) were impaired in emotion recognition, “theory of mind,” and cognitive flexibility compared with matched orthopedic controls (n = 34). Proxy ratings showed increases in behavioral problems 1 year following injury in the TBI group but not in the control group. However, test performance was not associated with questionnaire data. Severity of the impairments in emotion recognition, understanding intention, and flexibility were unrelated to the severity of behavioral problems following TBI. These findings failed to confirm the used model for social behavior deficits and may cast doubt on the alleged link between deficits in emotion recognition or theory of mind and social functioning. The aim of this study was to investigate whether deficits in emotion recognition, understanding intentions (ToM), or cognitive flexibility might underlie changes in social behavior following TBI. The study found that the TBI group was impaired on expression recognition, ToM, and cognitive flexibility soon after injury and at 1-year follow-up. Proxy ratings of behavior showed an increase in behavioral problems 1 year following TBI, but not following orthopedic injury. However, there was no association between test performance and post-injury behavior in the TBI group. The study discusses limitations, such as the small sample size, and the ambiguity of the relationship between impairments in executive functioning and social behavior following TBI. The study concludes that future studies could investigate more systematically the contribution of different aspects of executive functioning towards predicting emotional and social behavior following TBI.‘1",Emotion detection using normal face,"The study aimed to investigate whether deficits in emotion recognition, understanding of other people's intentions (""theory of mind""), and cognitive flexibility might underlie changes in social behavior following traumatic brain injury (TBI). The study found that patients with TBI were impaired in all three functions compared to orthopedic controls, and there was an increase in behavioral problems 1 year following TBI. However, test performance was not associated with questionnaire data, and severity of impairments was unrelated to severity of behavioral problems. The study concludes that future studies could investigate the contribution of different aspects of executive functioning towards predicting emotional and social behavior following TBI.",Object and Sentiment Recognition,‘1,Sentiment Analysis
207,Review: Posed vs. Genuine Facial Emotion Recognition and Expression in Autism and Implications for Intervention,"Autism spectrum disorder, facial emotion recognition, facial emotion expression, social interaction, teaching, FER stimuli, intervention, research.","Different styles of social interaction are one of the core characteristics of autism spectrum disorder (ASD). Social differences among individuals with ASD often include difficulty in discerning the emotions of neurotypical people based on their facial expressions. This review first covers the rich body of literature studying differences in facial emotion recognition (FER) in those with ASD, including behavioral studies and neurological findings. In particular, we highlight subtle emotion recognition and various factors related to inconsistent findings in behavioral studies of FER in ASD. Then, we discuss the dual problem of FER – namely facial emotion expression (FEE) or the production of facial expressions of emotion. Despite being less studied, social interaction involves both the ability to recognize emotions and to produce appropriate facial expressions. How others perceive facial expressions of emotion in those with ASD has remained an under-researched area. Finally, we propose a method for teaching FER [FER teaching hierarchy (FERTH)] based on recent research investigating FER in ASD, considering the use of posed vs. genuine emotions and static vs. dynamic stimuli. We also propose two possible teaching approaches: (1) a standard method of teaching progressively from simple drawings and cartoon characters to more complex audio-visual video clips of genuine human expressions of emotion with context clues or (2) teaching in a field of images that includes posed and genuine emotions to improve generalizability before progressing to more complex audio-visual stimuli. Lastly, we advocate for autism interventionists to use FER stimuli developed primarily for research purposes to facilitate the incorporation of well-controlled stimuli to teach FER and bridge the gap between intervention and research in this area.","In conclusion, this article reviews the research on facial emotion recognition (FER) and facial emotion expression (FEE) in individuals with Autism Spectrum Disorder (ASD). It highlights the difficulties that individuals with ASD face in recognizing emotions of neurotypical people based on their facial expressions, as well as the less studied issue of producing appropriate facial expressions. The article proposes a method for teaching FER, called the FER teaching hierarchy (FERTH), which takes into account recent research on FER in ASD and suggests two possible teaching approaches. The article also advocates for the use of well-controlled FER stimuli in autism intervention to bridge the gap between intervention and research in this area.




","Review: Posed vs. Genuine Facial Emotion Recognition and Expression in Autism and Implications for InterventionAutism spectrum disorder, facial emotion recognition, facial emotion expression, social interaction, teaching, FER stimuli, intervention, research.Different styles of social interaction are one of the core characteristics of autism spectrum disorder (ASD). Social differences among individuals with ASD often include difficulty in discerning the emotions of neurotypical people based on their facial expressions. This review first covers the rich body of literature studying differences in facial emotion recognition (FER) in those with ASD, including behavioral studies and neurological findings. In particular, we highlight subtle emotion recognition and various factors related to inconsistent findings in behavioral studies of FER in ASD. Then, we discuss the dual problem of FER – namely facial emotion expression (FEE) or the production of facial expressions of emotion. Despite being less studied, social interaction involves both the ability to recognize emotions and to produce appropriate facial expressions. How others perceive facial expressions of emotion in those with ASD has remained an under-researched area. Finally, we propose a method for teaching FER [FER teaching hierarchy (FERTH)] based on recent research investigating FER in ASD, considering the use of posed vs. genuine emotions and static vs. dynamic stimuli. We also propose two possible teaching approaches: (1) a standard method of teaching progressively from simple drawings and cartoon characters to more complex audio-visual video clips of genuine human expressions of emotion with context clues or (2) teaching in a field of images that includes posed and genuine emotions to improve generalizability before progressing to more complex audio-visual stimuli. Lastly, we advocate for autism interventionists to use FER stimuli developed primarily for research purposes to facilitate the incorporation of well-controlled stimuli to teach FER and bridge the gap between intervention and research in this area.In conclusion, this article reviews the research on facial emotion recognition (FER) and facial emotion expression (FEE) in individuals with Autism Spectrum Disorder (ASD). It highlights the difficulties that individuals with ASD face in recognizing emotions of neurotypical people based on their facial expressions, as well as the less studied issue of producing appropriate facial expressions. The article proposes a method for teaching FER, called the FER teaching hierarchy (FERTH), which takes into account recent research on FER in ASD and suggests two possible teaching approaches. The article also advocates for the use of well-controlled FER stimuli in autism intervention to bridge the gap between intervention and research in this area.




fecaus) Facey, 4 eyel height ’ (20, Yoo) 1 Pose = M/S x Faceneign Detected nose 1 Expected nose region <t— ‘region H Nose width 4 (2-m) hy 1.5 X Ino Soeiere. aePropesnon Rae ‘Samples4440 0.875 0.80 0.5714 0.3846 0.2857Geomeiric Features | Number of | Description features Eyebrow 4x2] Twoextreme comers, 3 ""upper and lower mid points ee, Bes TI | Twoexteme comers, 2 ‘upper and lower mid points —<— Nose 3 ""Two nostrils and nose tip ~ Tip 4 | Twoextreme comers, NF ""upper and lower mid points",Person recognition,"This article discusses the social differences among individuals with Autism Spectrum Disorder (ASD), particularly in recognizing and producing facial expressions of emotion. The article reviews the research on facial emotion recognition (FER) and facial emotion expression (FEE) in individuals with ASD, proposes a method for teaching FER, and advocates for the use of well-controlled FER stimuli in autism intervention to bridge the gap between intervention and research in this area. The article highlights the difficulties that individuals with ASD face in recognizing emotions of neurotypical people based on their facial expressions and suggests two possible teaching approaches for FER.",Object and Sentiment Recognition,"fecaus) Facey, 4 eyel height ’ (20, Yoo) 1 Pose = M/S x Faceneign Detected nose 1 Expected nose region <t— ‘region H Nose width 4 (2-m) hy 1.5 X Ino Soeiere. aePropesnon Rae ‘Samples4440 0.875 0.80 0.5714 0.3846 0.2857Geomeiric Features | Number of | Description features Eyebrow 4x2] Twoextreme comers, 3 ""upper and lower mid points ee, Bes TI | Twoexteme comers, 2 ‘upper and lower mid points —<— Nose 3 ""Two nostrils and nose tip ~ Tip 4 | Twoextreme comers, NF ""upper and lower mid points",Sentiment Analysis
208,Facial Emotion Recognition in Children with High Functioning Autism and Children with Social Phobia,"facial affect recognition, High Functioning Autism, Social Phobia, typical development, emotion detection, social skills deficits, intensity valences, social skills training program.","Recognizing facial affect is essential for effective social functioning. This study examines emotion recognition abilities in children aged 7–13 years with High Functioning Autism (HFA = 19), Social Phobia (SP = 17), or typical development (TD = 21). Findings indicate that all children identified certain emotions more quickly (e.g., happy < anger, disgust, sad < fear) and more accurately (happy) than other emotions (disgust). No evidence was found for negative interpretation biases in children with HFA or SP (i.e., all groups showed similar ability to discriminate neutral from non-neutral facial expressions). However, distinct between-group differences emerged when considering facial expression intensity. Specifically, children with HFA detected mild affective expressions less accurately than TD peers. Behavioral ratings of social effectiveness or social anxiety were uncorrelated with facial affect recognition abilities across children. Findings have implications for social skills treatment programs targeting youth with skill deficits.","This study examined facial affect detection speed and accuracy, at two intensity valences, between children with clinically impairing social skills deficits (SP and HFA) relative to TD peers. Simple and choice reaction time measures were included for greater methodological control. All children were equal in their ability to detect certain emotions, and identified extreme expressions more quickly and accurately than mild expressions. Happiness was the easiest emotion to identify and disgust was the most difficult. Children did not differ in their ability to discriminate neutral facial affect from non-neutral emotions indicating no evidence for negative interpretation biases in children with HFA or SP. Importantly, intensity of the emotional affect displayed played a critical role in revealing unique facial affect recognition deficits. Specifically, children with HFA were less accurate than TD children at recognizing mild affective expressions. Third, simply recognizing facial affect appropriately does not assure effective social functioning for either clinical group, suggesting that this skill must be part of a comprehensive social skill training program.","Facial Emotion Recognition in Children with High Functioning Autism and Children with Social Phobiafacial affect recognition, High Functioning Autism, Social Phobia, typical development, emotion detection, social skills deficits, intensity valences, social skills training program.Recognizing facial affect is essential for effective social functioning. This study examines emotion recognition abilities in children aged 7–13 years with High Functioning Autism (HFA = 19), Social Phobia (SP = 17), or typical development (TD = 21). Findings indicate that all children identified certain emotions more quickly (e.g., happy < anger, disgust, sad < fear) and more accurately (happy) than other emotions (disgust). No evidence was found for negative interpretation biases in children with HFA or SP (i.e., all groups showed similar ability to discriminate neutral from non-neutral facial expressions). However, distinct between-group differences emerged when considering facial expression intensity. Specifically, children with HFA detected mild affective expressions less accurately than TD peers. Behavioral ratings of social effectiveness or social anxiety were uncorrelated with facial affect recognition abilities across children. Findings have implications for social skills treatment programs targeting youth with skill deficits.This study examined facial affect detection speed and accuracy, at two intensity valences, between children with clinically impairing social skills deficits (SP and HFA) relative to TD peers. Simple and choice reaction time measures were included for greater methodological control. All children were equal in their ability to detect certain emotions, and identified extreme expressions more quickly and accurately than mild expressions. Happiness was the easiest emotion to identify and disgust was the most difficult. Children did not differ in their ability to discriminate neutral facial affect from non-neutral emotions indicating no evidence for negative interpretation biases in children with HFA or SP. Importantly, intensity of the emotional affect displayed played a critical role in revealing unique facial affect recognition deficits. Specifically, children with HFA were less accurate than TD children at recognizing mild affective expressions. Third, simply recognizing facial affect appropriately does not assure effective social functioning for either clinical group, suggesting that this skill must be part of a comprehensive social skill training program.",Person recognition,"This study focused on facial affect recognition in children aged 7-13 with High Functioning Autism (HFA), Social Phobia (SP), and typical development (TD). Results showed that all children were equally able to detect certain emotions, but children with HFA had difficulty recognizing mild affective expressions compared to TD children. Additionally, no evidence was found for negative interpretation biases in children with HFA or SP. The study suggests that recognizing facial affect is important but not sufficient for effective social functioning, and it should be included in comprehensive social skill training programs for children with social skills deficits.",Object and Sentiment Recognition,,Sentiment Analysis
209,Hybrid Approaches Based Emotion Detection in Memes Sentiment Analysis,"Hybrid approaches, Sentiment analysis, Memes,
Emotions, Lexicon-based approaches.","A social network is a decent platform in which users share daily
emotions and opinions. As a consequence, they have become
an essential information source related to sentiment/opinion.
The purpose of Sentiment Analysis (SA) techniques are used to
extract sentiments, emotions, and opinions from texts. This data
is available by different data sources, such as social media, eblogs, e-sources, etc. The problem of previous research work
has done only text or image analysis using different techniques.
Most of the research work results have given low performance.
This research work presents a hybrid approach that is needed
for the computational processing of Internet memes. Internet
Memes provide different behavior of human expressions and
activities. The increasing prevalence of Internet memes on
social media sites such as Facebook, Instagram, and Twitter
also indicates that such multimodal content can no longer be
ignored. The present work is analyzing internet memes using
lexicon-based and machine learning approaches. In sentiment
analysis, every meme is denoted as different polarities (positive
or negative memes). The emotions contain six classes that are
anger, fear, happiness, sadness, disgust, and neutral. The
polarity contains positive (+1), neutral (0), and negative (-1)
values. Additionally, the results obtained from the proposed
work give better performance of sentiment analysis. The
emotion and polarity classes are predicted in both text and
image data","The study of feelings in Memes is comparatively more
complicated because the meme is more conceptual and
subjective. Meme analysis has two types of information that are
text and visual (image, video, gif, or flash files). This study
describes the method of emotional detection by identifying
sentiment and emotions from text and facial expressions using
the memes dataset. The Senti_Emotion_Hybrid model
improves the performance of existing methods. Also, classified
lexicon-based and machine learning techniques. In the future,
we will analyze different levels of texts and emoji’s in memes.
Additionally, we will try to analyze gif, cartoon, and animation
memes with different techniques and features.","Hybrid Approaches Based Emotion Detection in Memes Sentiment AnalysisHybrid approaches, Sentiment analysis, Memes,
Emotions, Lexicon-based approaches.A social network is a decent platform in which users share daily
emotions and opinions. As a consequence, they have become
an essential information source related to sentiment/opinion.
The purpose of Sentiment Analysis (SA) techniques are used to
extract sentiments, emotions, and opinions from texts. This data
is available by different data sources, such as social media, eblogs, e-sources, etc. The problem of previous research work
has done only text or image analysis using different techniques.
Most of the research work results have given low performance.
This research work presents a hybrid approach that is needed
for the computational processing of Internet memes. Internet
Memes provide different behavior of human expressions and
activities. The increasing prevalence of Internet memes on
social media sites such as Facebook, Instagram, and Twitter
also indicates that such multimodal content can no longer be
ignored. The present work is analyzing internet memes using
lexicon-based and machine learning approaches. In sentiment
analysis, every meme is denoted as different polarities (positive
or negative memes). The emotions contain six classes that are
anger, fear, happiness, sadness, disgust, and neutral. The
polarity contains positive (+1), neutral (0), and negative (-1)
values. Additionally, the results obtained from the proposed
work give better performance of sentiment analysis. The
emotion and polarity classes are predicted in both text and
image dataThe study of feelings in Memes is comparatively more
complicated because the meme is more conceptual and
subjective. Meme analysis has two types of information that are
text and visual (image, video, gif, or flash files). This study
describes the method of emotional detection by identifying
sentiment and emotions from text and facial expressions using
the memes dataset. The Senti_Emotion_Hybrid model
improves the performance of existing methods. Also, classified
lexicon-based and machine learning techniques. In the future,
we will analyze different levels of texts and emoji’s in memes.
Additionally, we will try to analyze gif, cartoon, and animation
memes with different techniques and features.CORRECT SEN a & & 8 += VA inital Afolowup Faces Voices FawPas Cartoons Fluency Brixton Bareioaa: «= Thonn et baad Flexibility~unERtcapmineanTable 2, Correlations between proxy ratings of postinjury behavior in the TBI group and test composite scores at initial assessment (A) a at 1-year follow-up (B) A Expressions KASR DEX ToM ase 010006 Expressions a Flexibility 02306 KASR oss! DEX @ Expressions KASR DEX ToM 00"" 008 oot Expressions ou 01s Flexibility 03 aR KAS.R 089 DEX NoaP 007 008 a7 sare BAP 003 oot 028, oar Nore. TBE Profile; DEX = Dysexccuti Revised: ToM = Theory of Mind p= 0. ne hel: traumatic brain injury: NBAP = Neuropsychology Behavior and Affe ‘Questionnaire: KAS R ~ Katz Adjustment Seale",sentiment analysis,"This article discusses how sentiment analysis techniques are used to extract emotions and opinions from texts, and how previous research has focused mainly on text and image analysis with low performance results. The study presents a hybrid approach to analyze internet memes using lexicon-based and machine learning approaches, with positive, neutral, and negative polarities and six emotion classes. The proposed method improves sentiment analysis performance and detects emotions from both text and facial expressions in memes. Future work includes analyzing different levels of texts and emojis, as well as gif, cartoon, and animation memes with various techniques and features.



",Object and Sentiment Recognition,"CORRECT SEN a & & 8 += VA inital Afolowup Faces Voices FawPas Cartoons Fluency Brixton Bareioaa: «= Thonn et baad Flexibility~unERtcapmineanTable 2, Correlations between proxy ratings of postinjury behavior in the TBI group and test composite scores at initial assessment (A) a at 1-year follow-up (B) A Expressions KASR DEX ToM ase 010006 Expressions a Flexibility 02306 KASR oss! DEX @ Expressions KASR DEX ToM 00"" 008 oot Expressions ou 01s Flexibility 03 aR KAS.R 089 DEX NoaP 007 008 a7 sare BAP 003 oot 028, oar Nore. TBE Profile; DEX = Dysexccuti Revised: ToM = Theory of Mind p= 0. ne hel: traumatic brain injury: NBAP = Neuropsychology Behavior and Affe ‘Questionnaire: KAS R ~ Katz Adjustment Seale",Sentiment Analysis
210,Impairments in Negative Facial Emotion Recognition in Chinese Schizophrenia Patients Detected With a Newly Designed Task,"Schizophrenia , Chinese , cognitive function , facial expression","Facial emotion recognition has been found to be impaired in schizophrenia, although overall results have been inconclusive. A new set of facial emotion stimuli with Chinese faces was developed, using static and dynamic avatars, the identification of which were subsequently validated in 562 healthy control subjects. This test was then used to identify facial emotion recognition accuracy in 44 patients with schizophrenia and 41 healthy controls. Overall, patients identified facial emotions significantly worse than healthy controls (p = 0.018) with a significant main effect for type of emotion (p = 0.016). Patients performed significantly worse in fear (p = 0.029) and sadness (p = 0.037), and marginally worse in anger (p = 0.052). No significant differences were evident in contempt (p = 0.254) or happiness (p = 0.943). Regarding error rates of misattribution, patients overidentified contempt (p = 0.035) and sadness (p = 0.01), but not anger, fear, or happiness. Conclusion, patients of Chinese ethnicity with schizophrenia may have significantly greater difficulties identifying negative, but not positive emotions.","The study aimed to investigate facial emotion recognition accuracy in Chinese patients with schizophrenia compared to healthy controls using a new set of facial emotion stimuli. The results showed that patients performed significantly worse than controls in identifying negative emotions, specifically fear and sadness, but not positive emotions like happiness. The study also found that patients had higher error rates in overidentifying contempt and sadness. Therefore, the study suggests that Chinese patients with schizophrenia may have greater difficulties recognizing negative emotions, which could have implications for the diagnosis and treatment of the disorder.","Impairments in Negative Facial Emotion Recognition in Chinese Schizophrenia Patients Detected With a Newly Designed TaskSchizophrenia , Chinese , cognitive function , facial expressionFacial emotion recognition has been found to be impaired in schizophrenia, although overall results have been inconclusive. A new set of facial emotion stimuli with Chinese faces was developed, using static and dynamic avatars, the identification of which were subsequently validated in 562 healthy control subjects. This test was then used to identify facial emotion recognition accuracy in 44 patients with schizophrenia and 41 healthy controls. Overall, patients identified facial emotions significantly worse than healthy controls (p = 0.018) with a significant main effect for type of emotion (p = 0.016). Patients performed significantly worse in fear (p = 0.029) and sadness (p = 0.037), and marginally worse in anger (p = 0.052). No significant differences were evident in contempt (p = 0.254) or happiness (p = 0.943). Regarding error rates of misattribution, patients overidentified contempt (p = 0.035) and sadness (p = 0.01), but not anger, fear, or happiness. Conclusion, patients of Chinese ethnicity with schizophrenia may have significantly greater difficulties identifying negative, but not positive emotions.The study aimed to investigate facial emotion recognition accuracy in Chinese patients with schizophrenia compared to healthy controls using a new set of facial emotion stimuli. The results showed that patients performed significantly worse than controls in identifying negative emotions, specifically fear and sadness, but not positive emotions like happiness. The study also found that patients had higher error rates in overidentifying contempt and sadness. Therefore, the study suggests that Chinese patients with schizophrenia may have greater difficulties recognizing negative emotions, which could have implications for the diagnosis and treatment of the disorder.% of Stimuli Correct 100.00 20.00 0.00 ild Anes ‘Dh Extreme wa Bee Sad",Person recognition,"This study aimed to investigate the accuracy of facial emotion recognition in Chinese patients with schizophrenia compared to healthy controls using a new set of facial emotion stimuli. The study found that patients had significantly greater difficulties identifying negative emotions, particularly fear and sadness, compared to healthy controls. Patients also had higher error rates in overidentifying contempt and sadness. However, no significant differences were observed in identifying positive emotions such as happiness. The study suggests that these findings could have implications for the diagnosis and treatment of Chinese patients with schizophrenia.",Object and Sentiment Recognition,% of Stimuli Correct 100.00 20.00 0.00 ild Anes ‘Dh Extreme wa Bee Sad,Sentiment Analysis
211,"Deep learning-based classification of the polar emotions of ""moe""-style cartoon pictures","Feature extraction, Face recognition, Animation, Image color analysis , Facial features, Forensics, Deep learning","The cartoon animation industry has developed into a huge industrial chain with a large potential market involving games, digital entertainment, and other industries. However, due to the coarse-grained classification of cartoon materials, cartoon animators can hardly find relevant materials during the process of creation. The polar emotions of cartoon materials are an important reference for creators as they can help them easily obtain the pictures they need. Some methods for obtaining the emotions of cartoon pictures have been proposed, but most of these focus on expression recognition. Meanwhile, other emotion recognition methods are not ideal for use as cartoon materials. We propose a deep learning-based method to classify the polar emotions of the cartoon pictures of the ""Moe"" drawing style. According to the expression feature of the cartoon characters of this drawing style, we recognize the facial expressions of cartoon characters and extract the scene and facial features of the cartoon images. Then, we correct the emotions of the pictures obtained by the expression recognition according to the scene features. Finally, we can obtain the polar emotions of corresponding picture. We designed a dataset and performed verification tests on it, achieving 81.9% experimental accuracy. The experimental results prove that our method is competitive.","In conclusion, the cartoon animation industry has become a vast industrial chain, but cartoon animators often struggle to find relevant materials during the creative process due to the coarse-grained classification of cartoon materials. The polar emotions of cartoon pictures play a crucial role in helping creators obtain the pictures they need. While various methods for emotion recognition have been proposed, most of them are not ideal for use with cartoon materials. This paper proposes a deep learning-based method for classifying the polar emotions of cartoon pictures in the ""Moe"" drawing style. The method uses facial expression recognition, scene features, and facial features of the cartoon images to correct the emotions of the pictures obtained. Experimental results demonstrate that this method achieves an accuracy of 81.9%, making it competitive for use in the cartoon animation industry.



","Deep learning-based classification of the polar emotions of ""moe""-style cartoon picturesFeature extraction, Face recognition, Animation, Image color analysis , Facial features, Forensics, Deep learningThe cartoon animation industry has developed into a huge industrial chain with a large potential market involving games, digital entertainment, and other industries. However, due to the coarse-grained classification of cartoon materials, cartoon animators can hardly find relevant materials during the process of creation. The polar emotions of cartoon materials are an important reference for creators as they can help them easily obtain the pictures they need. Some methods for obtaining the emotions of cartoon pictures have been proposed, but most of these focus on expression recognition. Meanwhile, other emotion recognition methods are not ideal for use as cartoon materials. We propose a deep learning-based method to classify the polar emotions of the cartoon pictures of the ""Moe"" drawing style. According to the expression feature of the cartoon characters of this drawing style, we recognize the facial expressions of cartoon characters and extract the scene and facial features of the cartoon images. Then, we correct the emotions of the pictures obtained by the expression recognition according to the scene features. Finally, we can obtain the polar emotions of corresponding picture. We designed a dataset and performed verification tests on it, achieving 81.9% experimental accuracy. The experimental results prove that our method is competitive.In conclusion, the cartoon animation industry has become a vast industrial chain, but cartoon animators often struggle to find relevant materials during the creative process due to the coarse-grained classification of cartoon materials. The polar emotions of cartoon pictures play a crucial role in helping creators obtain the pictures they need. While various methods for emotion recognition have been proposed, most of them are not ideal for use with cartoon materials. This paper proposes a deep learning-based method for classifying the polar emotions of cartoon pictures in the ""Moe"" drawing style. The method uses facial expression recognition, scene features, and facial features of the cartoon images to correct the emotions of the pictures obtained. Experimental results demonstrate that this method achieves an accuracy of 81.9%, making it competitive for use in the cartoon animation industry.



88 86 84 82 80 78 6 79.81 = Precision (6) Results 84.88 = Recall (%) 82.27 = F-Measure (%) 86.53 Accuracy (%)Sipe cpt Dee ores Coors eC RUC eet ia She said, ""You can get on the airplane.” | said, ""No thanks. \'m getting IN the airplane.”",Deep Learning and Machine Learning,"The cartoon animation industry has potential in various markets, but animators struggle to find relevant materials due to limited classification of cartoon materials. Polar emotions of cartoon pictures are important for creators to easily obtain the pictures they need. A deep learning-based method is proposed to classify polar emotions of ""Moe"" style cartoon pictures, using facial expression recognition, scene features, and facial features. The method achieves 81.9% accuracy and can be used in the cartoon animation industry.



",Deep Learning and Machine Learning,"88 86 84 82 80 78 6 79.81 = Precision (6) Results 84.88 = Recall (%) 82.27 = F-Measure (%) 86.53 Accuracy (%)Sipe cpt Dee ores Coors eC RUC eet ia She said, ""You can get on the airplane.” | said, ""No thanks. \'m getting IN the airplane.”",Sentiment Analysis
212,Basic emotion recognition of children on the autism spectrum is enhanced in music and typical for faces and voices,"Emotions 

Face 

Children 

Face recognition 

Fear 

Music therapy 

Music perception 

Music cognition","In contrast with findings of reduced facial and vocal emotional recognition (ER) accuracy, children on the autism spectrum (AS) demonstrate comparable ER skills to those of typically-developing (TD) children using music. To understand the specificity of purported ER differences, the goal of this study was to examine ER from music compared with faces and voices among children on the AS and TD children. Twenty-five children on the AS and 23 TD children (6–13 years) completed an ER task, using categorical (happy, sad, fear) and dimensional (valence, arousal) ratings, of emotions presented via music, faces, or voices. Compared to the TD group, the AS group showed a relative ER strength from music, and comparable performance from faces and voices. Although both groups demonstrated greater vocal ER accuracy, the children on the AS performed equally well with music and faces, whereas the TD children performed better with faces than with music. Both groups performed comparably with dimensional ratings, except for greater variability by the children on the AS in valence ratings for happy emotions. These findings highlight a need to re-examine ER of children on the AS, and to consider how facilitating strengths-based approaches can re-shape our thinking about and support for persons on the AS.","The findings from this study of ER across multiple basic emotions, stimulus types, and response options, help to dispel notions of deficits of the ability to explicitly recognize and label basic emotions of children on the AS relative to TD children. Instead, findings of comparable ER accuracy (in faces and voices) or relative strengths (in musical ER) depending on the stimulus type provides support for the use of music therapy, education, and interventions. Ultimately, our findings contribute to efforts to provide a nuanced understanding of the ways in which we can re-shape our thinking about persons on the AS by engaging their preferences and strengths.

","Basic emotion recognition of children on the autism spectrum is enhanced in music and typical for faces and voicesEmotions 

Face 

Children 

Face recognition 

Fear 

Music therapy 

Music perception 

Music cognitionIn contrast with findings of reduced facial and vocal emotional recognition (ER) accuracy, children on the autism spectrum (AS) demonstrate comparable ER skills to those of typically-developing (TD) children using music. To understand the specificity of purported ER differences, the goal of this study was to examine ER from music compared with faces and voices among children on the AS and TD children. Twenty-five children on the AS and 23 TD children (6–13 years) completed an ER task, using categorical (happy, sad, fear) and dimensional (valence, arousal) ratings, of emotions presented via music, faces, or voices. Compared to the TD group, the AS group showed a relative ER strength from music, and comparable performance from faces and voices. Although both groups demonstrated greater vocal ER accuracy, the children on the AS performed equally well with music and faces, whereas the TD children performed better with faces than with music. Both groups performed comparably with dimensional ratings, except for greater variability by the children on the AS in valence ratings for happy emotions. These findings highlight a need to re-examine ER of children on the AS, and to consider how facilitating strengths-based approaches can re-shape our thinking about and support for persons on the AS.The findings from this study of ER across multiple basic emotions, stimulus types, and response options, help to dispel notions of deficits of the ability to explicitly recognize and label basic emotions of children on the AS relative to TD children. Instead, findings of comparable ER accuracy (in faces and voices) or relative strengths (in musical ER) depending on the stimulus type provides support for the use of music therapy, education, and interventions. Ultimately, our findings contribute to efforts to provide a nuanced understanding of the ways in which we can re-shape our thinking about persons on the AS by engaging their preferences and strengths.

",Emotion detection using normal face,"Children on the autism spectrum (AS) show comparable emotional recognition (ER) skills to typically developing (TD) children when using music, despite reduced accuracy with facial and vocal emotions. A study examined ER from music, faces, and voices among 25 AS children and 23 TD children. The AS group showed relative strength in ER from music and comparable performance with faces and voices, whereas the TD group performed better with faces. Both groups had similar performance with dimensional ratings. The findings suggest a need to re-examine ER of AS children and consider strengths-based approaches such as music therapy and interventions.



",Object and Sentiment Recognition,,Sentiment Analysis
213,"The Effect of Behavioral Realism and Form Realism of Real-Time Avatar Faces on Verbal Disclosure, Nonverbal Disclosure, Emotion Recognition, and Copresence in Dyadic Interaction","Collaborative virtual environments (CVE), Facial expressions, Behavioral realism, Real time avatar, Identification of emotions, Hybrid realism
Distance learning
Therapy","The realism of avatars in terms of behavior and form is critical to the development of collaborative virtual environments. In the study we utilized state of the art, real-time face tracking technology to track and render facial expressions unobtrusively in a desktop CVE. Participants in dyads interacted with each other via either a video-conference (high behavioral realism and high form realism), voice only (low behavioral realism and low form realism), or an “emotibox” that rendered the dimensions of facial expressions abstractly in terms of color, shape, and orientation on a rectangular polygon (high behavioral realism and low form realism). Verbal and non-verbal self-disclosure were lowest in the videoconference condition while self-reported copresence and success of transmission and identification of emotions were lowest in the emotibox condition. Previous work demonstrates that avatar realism increases copresence while decreasing self-disclosure. We discuss the possibility of a hybrid realism solution that maintains high copresence without lowering self-disclosure, and the benefits of such an avatar on applications such as distance learning and therapy.","The study examined the importance of avatar realism in collaborative virtual environments, utilizing state-of-the-art face tracking technology to track and render facial expressions. Participants interacted with each other via video-conference, voice only, or an ""emotibox"" that rendered facial expressions abstractly. The results showed that verbal and non-verbal self-disclosure was lowest in the video-conference condition, while self-reported copresence and success of emotion transmission were lowest in the emotibox condition. The study suggests the need for a hybrid realism solution that maintains high copresence without lowering self-disclosure and highlights the potential benefits of such an avatar for applications such as distance learning and therapy.","The Effect of Behavioral Realism and Form Realism of Real-Time Avatar Faces on Verbal Disclosure, Nonverbal Disclosure, Emotion Recognition, and Copresence in Dyadic InteractionCollaborative virtual environments (CVE), Facial expressions, Behavioral realism, Real time avatar, Identification of emotions, Hybrid realism
Distance learning
TherapyThe realism of avatars in terms of behavior and form is critical to the development of collaborative virtual environments. In the study we utilized state of the art, real-time face tracking technology to track and render facial expressions unobtrusively in a desktop CVE. Participants in dyads interacted with each other via either a video-conference (high behavioral realism and high form realism), voice only (low behavioral realism and low form realism), or an “emotibox” that rendered the dimensions of facial expressions abstractly in terms of color, shape, and orientation on a rectangular polygon (high behavioral realism and low form realism). Verbal and non-verbal self-disclosure were lowest in the videoconference condition while self-reported copresence and success of transmission and identification of emotions were lowest in the emotibox condition. Previous work demonstrates that avatar realism increases copresence while decreasing self-disclosure. We discuss the possibility of a hybrid realism solution that maintains high copresence without lowering self-disclosure, and the benefits of such an avatar on applications such as distance learning and therapy.The study examined the importance of avatar realism in collaborative virtual environments, utilizing state-of-the-art face tracking technology to track and render facial expressions. Participants interacted with each other via video-conference, voice only, or an ""emotibox"" that rendered facial expressions abstractly. The results showed that verbal and non-verbal self-disclosure was lowest in the video-conference condition, while self-reported copresence and success of emotion transmission were lowest in the emotibox condition. The study suggests the need for a hybrid realism solution that maintains high copresence without lowering self-disclosure and highlights the potential benefits of such an avatar for applications such as distance learning and therapy.",Person recognition,"This study focused on the importance of avatar realism in collaborative virtual environments, using face tracking technology to render facial expressions. Participants interacted through video-conference, voice only, or an ""emotibox"" that abstracted facial expressions. Results showed that self-disclosure was lowest in the video-conference condition, while copresence and emotion transmission success were lowest in the emotibox condition. The study suggests a need for a hybrid realism solution and highlights potential benefits for distance learning and therapy.",Object and Sentiment Recognition,,Object Recognition
214,"Simulationist models of face-based
emotion recognition"," Emotion; Fear; Disgust; Anger; Theory of mind; Simulation theory; Theory theory; Facial feedback;
Mirror neuron","Recent studies of emotion mindreading reveal that for three emotions, fear, disgust, and anger,
deficits in face-based recognition are paired with deficits in the production of the same emotion.
What type of mindreading process would explain this pattern of paired deficits? The simulation
approach and the theorizing approach are examined to determine their compatibility with the
existing evidence. We conclude that the simulation approach offers the best explanation of the data.
What computational steps might be used, however, in simulation-style emotion detection? Four
alternative models are explored: a generate-and-test model, a reverse simulation model, a variant of
                               the reverse simulation model that employs an “as if” loop, and an unmediated resonance model.                                                                                                                                                ","Although the foregoing case for a simulational approach to face-based emotion
recognition strikes us as compelling, it leaves open the question of how the simulational
process proceeds. Those skeptical of our case for simulation may remain skeptical as long
as no plausible, sufficiently detailed story of the simulation process in these cases is
forthcoming. We get little help on this question from the existing literature. Articles
describing paired deficits often contain conclusions hinting at a simulational explanation,
but few pursue any details about the computational mechanisms. Exploring the options for
a simulational process is the next task we tackle.
There are several ways a simulation heuristic might be used to attribute a mental state,
depending on the nature of the causal link between the evidence events in the target
(known to the attributor) and the sought-after state of the target. The causal link might be
of two general kinds: (A) the evidence events cause the state, or (B) the state causes the
evidence events.","Simulationist models of face-based
emotion recognition Emotion; Fear; Disgust; Anger; Theory of mind; Simulation theory; Theory theory; Facial feedback;
Mirror neuronRecent studies of emotion mindreading reveal that for three emotions, fear, disgust, and anger,
deficits in face-based recognition are paired with deficits in the production of the same emotion.
What type of mindreading process would explain this pattern of paired deficits? The simulation
approach and the theorizing approach are examined to determine their compatibility with the
existing evidence. We conclude that the simulation approach offers the best explanation of the data.
What computational steps might be used, however, in simulation-style emotion detection? Four
alternative models are explored: a generate-and-test model, a reverse simulation model, a variant of
                               the reverse simulation model that employs an “as if” loop, and an unmediated resonance model.                                                                                                                                                Although the foregoing case for a simulational approach to face-based emotion
recognition strikes us as compelling, it leaves open the question of how the simulational
process proceeds. Those skeptical of our case for simulation may remain skeptical as long
as no plausible, sufficiently detailed story of the simulation process in these cases is
forthcoming. We get little help on this question from the existing literature. Articles
describing paired deficits often contain conclusions hinting at a simulational explanation,
but few pursue any details about the computational mechanisms. Exploring the options for
a simulational process is the next task we tackle.
There are several ways a simulation heuristic might be used to attribute a mental state,
depending on the nature of the causal link between the evidence events in the target
(known to the attributor) and the sought-after state of the target. The causal link might be
of two general kinds: (A) the evidence events cause the state, or (B) the state causes the
evidence events.",Emotion detection using normal face,"Recent studies have found that deficits in recognizing facial expressions of fear, disgust, and anger are associated with deficits in producing those emotions. The authors explore the simulation and theorizing approaches to explain this pattern and conclude that the simulation approach offers the best explanation. The authors then examine four alternative models of simulation-style emotion detection, but note that the computational process of simulation is not well understood. They suggest that the next step is to explore different options for a simulation heuristic to attribute a mental state, depending on the causal link between evidence events and the target state.",Object and Sentiment Recognition,,Object Recognition
215,Normal recognition of emotion in a prosopagnosic,"Facial perception, facial identity recognition, emotion recognition, separate mechanisms, prosopagnosic, impaired identity recognition, normal emotion recognition, variety of tests.","In the leading model of face perception, facial identity and facial expressions of
emotion are recognized by separate mechanisms. In this report, we provide evidence supporting
the independence of these processes by documenting an individual with severely impaired recognition of facial identity yet normal recognition of facial expressions of emotion. NM, a 40-year-old
prosopagnosic, showed severely impaired performance on five of six tests of facial identity recognition. In contrast, she performed in the normal range on four different tests of emotion recognition.
Because the tests of identity recognition and emotion recognition assessed her abilities in a
variety of ways, these results provide solid support for models in which identity recognition and
emotion recognition are performed by separate processes.","We used a variety of identity and expression tests in order to more firmly ground
our conclusions about her abilities. The tests of identity recognition varied in the
number of faces involved in the task (one/many), whether recognition of novel views
was required, and the time duration between memorization of a face and recognition
of the face (immediate/intermediate/weeks). Similarly, our tests of expression recognition differed in terms of the information presented (whole face/eye region), the nature
of the discrimination (between different emotions/within one emotion), the intensity of
the expression, and the expressions used. Given her consistent outcome on such a wide
variety of tests, we can confidently conclude that her recognition of facial identity is
impaired while her recognition of facial expressions of emotion is normal","Normal recognition of emotion in a prosopagnosicFacial perception, facial identity recognition, emotion recognition, separate mechanisms, prosopagnosic, impaired identity recognition, normal emotion recognition, variety of tests.In the leading model of face perception, facial identity and facial expressions of
emotion are recognized by separate mechanisms. In this report, we provide evidence supporting
the independence of these processes by documenting an individual with severely impaired recognition of facial identity yet normal recognition of facial expressions of emotion. NM, a 40-year-old
prosopagnosic, showed severely impaired performance on five of six tests of facial identity recognition. In contrast, she performed in the normal range on four different tests of emotion recognition.
Because the tests of identity recognition and emotion recognition assessed her abilities in a
variety of ways, these results provide solid support for models in which identity recognition and
emotion recognition are performed by separate processes.We used a variety of identity and expression tests in order to more firmly ground
our conclusions about her abilities. The tests of identity recognition varied in the
number of faces involved in the task (one/many), whether recognition of novel views
was required, and the time duration between memorization of a face and recognition
of the face (immediate/intermediate/weeks). Similarly, our tests of expression recognition differed in terms of the information presented (whole face/eye region), the nature
of the discrimination (between different emotions/within one emotion), the intensity of
the expression, and the expressions used. Given her consistent outcome on such a wide
variety of tests, we can confidently conclude that her recognition of facial identity is
impaired while her recognition of facial expressions of emotion is normalwert rg aeoa 50a oo ODDaD oo oo0o000",Emotion detection using normal face,"The report presents evidence supporting the idea that facial identity recognition and facial expression recognition are performed by separate mechanisms in the brain. The evidence comes from studying an individual named NM, who has severely impaired facial identity recognition but normal facial expression recognition. The study involved a variety of tests that assessed NM's abilities in different ways, and the results suggest that facial identity and expression recognition are independent processes.",Object and Sentiment Recognition,wert rg aeoa 50a oo ODDaD oo oo0o000,Object Recognition
216,Facial Emotion Recognition in Schizophrenia,"Facial emotion recognition, schizophrenia, Bruce-Young model, behavioral studies, event-related potential studies, positive emotions, negative emotions, stimulus material.","Deficits in facial emotion recognition are one of the most common cognitive impairments, and they have been extensively studied in various psychiatric disorders, especially in schizophrenia. However, there is still a lack of conclusive evidence about the factors associated with schizophrenia and impairment at each stage of the disease, which poses a challenge to the clinical management of patients. Based on this, we summarize facial emotion cognition among patients with schizophrenia, introduce the internationally recognized Bruce–Young face recognition model, and review the behavioral and event-related potential studies on the recognition of emotions at each stage of the face recognition process, including suggestions for the future direction of clinical research to explore the underlying mechanisms of schizophrenia.","n summary, studies on schizophrenia in facial emotion recognition have focused on differences in positive and negative emotion recognition and their relationship to psychotic symptoms. Most current studies use a facial emotion recognition task, but the specific stimuli and the form of the task differ across studies. Some use picture stimuli that are both static and dynamic, and others ask participants to directly identify the emotions expressed by the stimuli. Some studies ask participants to classify the emotions expressed by the stimuli as positive or negative, and others ask participants to rate the intensity of the emotions expressed by the stimuli. Such variability in tasks may account for the differential outcomes in patients' recognition of positive and negative emotions. However, most studies find that patients are more impaired in the recognition of negative emotions, leading to the wider use of only negative emotions as stimulus material, making the results unbalanced. Therefore, future experimental studies should include all six basic emotions.","Facial Emotion Recognition in SchizophreniaFacial emotion recognition, schizophrenia, Bruce-Young model, behavioral studies, event-related potential studies, positive emotions, negative emotions, stimulus material.Deficits in facial emotion recognition are one of the most common cognitive impairments, and they have been extensively studied in various psychiatric disorders, especially in schizophrenia. However, there is still a lack of conclusive evidence about the factors associated with schizophrenia and impairment at each stage of the disease, which poses a challenge to the clinical management of patients. Based on this, we summarize facial emotion cognition among patients with schizophrenia, introduce the internationally recognized Bruce–Young face recognition model, and review the behavioral and event-related potential studies on the recognition of emotions at each stage of the face recognition process, including suggestions for the future direction of clinical research to explore the underlying mechanisms of schizophrenia.n summary, studies on schizophrenia in facial emotion recognition have focused on differences in positive and negative emotion recognition and their relationship to psychotic symptoms. Most current studies use a facial emotion recognition task, but the specific stimuli and the form of the task differ across studies. Some use picture stimuli that are both static and dynamic, and others ask participants to directly identify the emotions expressed by the stimuli. Some studies ask participants to classify the emotions expressed by the stimuli as positive or negative, and others ask participants to rate the intensity of the emotions expressed by the stimuli. Such variability in tasks may account for the differential outcomes in patients' recognition of positive and negative emotions. However, most studies find that patients are more impaired in the recognition of negative emotions, leading to the wider use of only negative emotions as stimulus material, making the results unbalanced. Therefore, future experimental studies should include all six basic emotions.‘Classify one's current emotion state and attribute this. state tothe target Fig. 2. Reverse simulation.Generate an hypothesized emotion Produce a facial expression 9 Test this facial expression against that of the target 4 NO Is there a match? YES Classify one’s current emotion state and attribute this state to the target Fig. 1. Generate and test simulation.",Emotion detection using normal face,"Facial emotion recognition deficits are common in schizophrenia but factors associated with impairment at each stage of the disease are still unclear. This article summarizes studies on facial emotion recognition in schizophrenia, introduces the Bruce-Young face recognition model, and reviews studies on recognition of emotions at each stage of face recognition. Most studies find that patients are more impaired in recognizing negative emotions, but there is variability in tasks used and stimuli presented. Future studies should include all six basic emotions to balance the results.",Object and Sentiment Recognition,‘Classify one's current emotion state and attribute this. state tothe target Fig. 2. Reverse simulation.Generate an hypothesized emotion Produce a facial expression 9 Test this facial expression against that of the target 4 NO Is there a match? YES Classify one’s current emotion state and attribute this state to the target Fig. 1. Generate and test simulation.,Sentiment Analysis
217,"Enhancing Emotion Recognition in Children with Autism
Spectrum Conditions: An Intervention Using Animated Vehicles
with Real Emotional Faces","Autism spectrum  Children , Emotion recognition,  Intervention Animation, Intrinsic motivation","This study evaluated The Transporters, an
animated series designed to enhance emotion comprehension in children with autism spectrum conditions (ASC).
n = 20 children with ASC (aged 4–7) watched The Transporters everyday for 4 weeks. Participants were tested
before and after intervention on emotional vocabulary and
emotion recognition at three levels of generalization. Two
matched control groups of children (ASC group, n = 18 and
typically developing group, n = 18) were also assessed
twice without any intervention. The intervention group
improved significantly more than the clinical control group
on all task levels, performing comparably to typical controls
at Time 2. We conclude that using The Transporters significantly improves emotion recognition in children with ASC. Future research should evaluate the series’ effectiveness with lower-functioning individuals   ","After checking that all measures were normally distributed,2 we looked at the performance of the three groups on
the tasks at Time 1. Using four-one-way analyses of variance and Holm’s sequential rejective Bonferroni procedure
(Holm 1979) we found significant differences between
groups on the emotional vocabulary task (F[2,53] = 10.73,
p\0.001) and on the three Situation-Expression Matching
tasks (Level 1, F[2,53] = 9.88, p\0.001; Level 2,
F[2,53] = 8.24, p\0.002; Level 3, F[2,53] = 10.29,
p\0.001). Pre-planned comparisons using Bonferroni
corrections showed that these differences were due to the
significantly higher scores of the typical controls on all
tasks compared to the two clinical groups, which did not
differ from each other.","Enhancing Emotion Recognition in Children with Autism
Spectrum Conditions: An Intervention Using Animated Vehicles
with Real Emotional FacesAutism spectrum  Children , Emotion recognition,  Intervention Animation, Intrinsic motivationThis study evaluated The Transporters, an
animated series designed to enhance emotion comprehension in children with autism spectrum conditions (ASC).
n = 20 children with ASC (aged 4–7) watched The Transporters everyday for 4 weeks. Participants were tested
before and after intervention on emotional vocabulary and
emotion recognition at three levels of generalization. Two
matched control groups of children (ASC group, n = 18 and
typically developing group, n = 18) were also assessed
twice without any intervention. The intervention group
improved significantly more than the clinical control group
on all task levels, performing comparably to typical controls
at Time 2. We conclude that using The Transporters significantly improves emotion recognition in children with ASC. Future research should evaluate the series’ effectiveness with lower-functioning individuals   After checking that all measures were normally distributed,2 we looked at the performance of the three groups on
the tasks at Time 1. Using four-one-way analyses of variance and Holm’s sequential rejective Bonferroni procedure
(Holm 1979) we found significant differences between
groups on the emotional vocabulary task (F[2,53] = 10.73,
p\0.001) and on the three Situation-Expression Matching
tasks (Level 1, F[2,53] = 9.88, p\0.001; Level 2,
F[2,53] = 8.24, p\0.002; Level 3, F[2,53] = 10.29,
p\0.001). Pre-planned comparisons using Bonferroni
corrections showed that these differences were due to the
significantly higher scores of the typical controls on all
tasks compared to the two clinical groups, which did not
differ from each other.Identity recognition Emotion recognition 2 value for NM’s score poppet tt Face OIT |e SES Old/New 1 = Old/New 2 Famous Faces commis Different Views | cool ! Hexagon | a Eyes Test = Matching | 0-3-6 420249 6 8 w Impaired scores | Normal scores BB Response timeEE - i =< rds",Convolution Neural Network,"This study assessed the effectiveness of The Transporters, an animated series designed to enhance emotion comprehension in children with autism spectrum conditions (ASC). Twenty children with ASC aged 4-7 watched the series daily for four weeks and were tested on emotional vocabulary and emotion recognition at three levels of generalization before and after the intervention. Results showed that the intervention group significantly improved more than a clinical control group and performed comparably to typically developing children at Time 2. The study also found significant differences between the groups on the tasks at Time 1, with typical controls scoring higher than both clinical groups.",Object and Sentiment Recognition,Identity recognition Emotion recognition 2 value for NM’s score poppet tt Face OIT |e SES Old/New 1 = Old/New 2 Famous Faces commis Different Views | cool ! Hexagon | a Eyes Test = Matching | 0-3-6 420249 6 8 w Impaired scores | Normal scores BB Response timeEE - i =< rds,Sentiment Analysis
218,"Age Differences in Emotion Recognition Skills and
the Visual Scanning of Emotion Faces","Age-related decline, emotion recognition, eyes vs mouth, younger adults, older adults, eye tracking, brain changes, visual scanning","Research suggests that a person’s emotion recognition declines with advancing years. We examined whether or
not this age-related decline was attributable to a tendency to overlook emotion information in the eyes. In
Experiment 1, younger adults were significantly better than older adults at inferring emotions from full faces and
eyes, though not from mouths. Using an eye tracker in Experiment 2, we found young adults, in comparison with
older adults, to have superior emotion recognition performance and to look proportionately more to eyes than
mouths. However, although better emotion recognition performance was significantly correlated with more eye
looking in younger adults, the same was not true in older adults. We discuss these results in terms of brain changes
with age","There were no effects for emotion recognition for different
orders of the eyes, mouth, and full face conditions. Table 1
depicts performance on the six emotion types in the three
modalities. We log-transformed and analyzed data with a 2
(Age Group: young vs old) 3 3 (Region: eyes only vs mouth
only vs full face) 3 6 (Emotion) analysis of variance, with
number of correct responses (out of 36, with two trials for each
emotion in each region) as the dependent variable. Because
sphericity assumptions were violated, we report Greenhouse–
Geisser corrected values of probability and mean square error.
There was a main effect for age group, with young adults better
at identifying emotions","Age Differences in Emotion Recognition Skills and
the Visual Scanning of Emotion FacesAge-related decline, emotion recognition, eyes vs mouth, younger adults, older adults, eye tracking, brain changes, visual scanningResearch suggests that a person’s emotion recognition declines with advancing years. We examined whether or
not this age-related decline was attributable to a tendency to overlook emotion information in the eyes. In
Experiment 1, younger adults were significantly better than older adults at inferring emotions from full faces and
eyes, though not from mouths. Using an eye tracker in Experiment 2, we found young adults, in comparison with
older adults, to have superior emotion recognition performance and to look proportionately more to eyes than
mouths. However, although better emotion recognition performance was significantly correlated with more eye
looking in younger adults, the same was not true in older adults. We discuss these results in terms of brain changes
with ageThere were no effects for emotion recognition for different
orders of the eyes, mouth, and full face conditions. Table 1
depicts performance on the six emotion types in the three
modalities. We log-transformed and analyzed data with a 2
(Age Group: young vs old) 3 3 (Region: eyes only vs mouth
only vs full face) 3 6 (Emotion) analysis of variance, with
number of correct responses (out of 36, with two trials for each
emotion in each region) as the dependent variable. Because
sphericity assumptions were violated, we report Greenhouse–
Geisser corrected values of probability and mean square error.
There was a main effect for age group, with young adults better
at identifying emotionsSRT RE TELE Dare ee re prac g a een pretne) Co Aatitetd "" peed aarean BOs ray Btn Bsr oaNPeiEt erat r Eres onoeTt} Brose Poor object | of P100is non-emotio | of N170is z ors é Coo eno Crea E Crs Er ots Cre Poor er pa Bors Ng Bic ag rrcrc att aS Poe Ter} SET Need nosy sso is ate Ape ty CC y Sees TC eOE Cont eed FIGURE 3 | Bchavioral abies and corresponding ERP components in three stages of facial emotion recognition processing in schizophrenia,",Emotion detection using normal face,"Research investigated whether the age-related decline in emotion recognition is due to a tendency to overlook emotion information in the eyes. The study found that younger adults were better at inferring emotions from full faces and eyes than older adults, and young adults looked more to eyes than mouths. However, better emotion recognition performance was significantly correlated with more eye looking in younger adults, but not in older adults. The study highlights the effects of aging on emotion recognition and brain changes with age.",Object and Sentiment Recognition,"SRT RE TELE Dare ee re prac g a een pretne) Co Aatitetd "" peed aarean BOs ray Btn Bsr oaNPeiEt erat r Eres onoeTt} Brose Poor object | of P100is non-emotio | of N170is z ors é Coo eno Crea E Crs Er ots Cre Poor er pa Bors Ng Bic ag rrcrc att aS Poe Ter} SET Need nosy sso is ate Ape ty CC y Sees TC eOE Cont eed FIGURE 3 | Bchavioral abies and corresponding ERP components in three stages of facial emotion recognition processing in schizophrenia,",Sentiment Analysis
219,PREDICTIVE ANALYSIS OF HEART DISEASES WITH MACHINE LEARNING APPROACHES,"Heart Disease, Health Care, Medical Data, Heart Diseases.","Machine Learning (ML) is used in healthcare sectors worldwide. ML methods help in the protection of heart diseases, locomotor disorders in the medical data set. The discovery of such essential data helps researchers gain valuable insight into how to utilize their diagnosis and treatment for a particular patient. Researchers use various Machine Learning methods to examine massive amounts of complex healthcare data, which aids healthcare professionals in predicting diseases.  In this research, we are using an online UCI dataset with 303 rows and 76 properties. Approximately 14 of these 76 properties are selected for testing, which is necessary to validate the performances of different methods. The isolation forest approach uses the data set’s most essential qualities and metrics to standardize the information for better precision. This analysis is based on supervised learning methods, i.e., Naive Bayes, SVM, Logistic regression, Decision Tree Classifier, Random Forest, and K- Nearest Neighbor. The experimental results demonstrate the strength of KNN with eight neighbours order to test the effectiveness, sensitivity, precision, and accuracy, F1-score; as compared to other methods, i.e., Naive Bayes, SVM (Linear Kernel), Decision Tree Classifier with 4 or 18 features, and Random Forest classifiers.","FUTUREWORKThis  research  article  hasworked  on  four  different  methods  for  conducting  comparative  assessment  and obtainingtruepositiveperformance.WeconcludedthatMachine Learning methods significantlyoutperformedstatistical techniques in this research. This article proves the studies of various researchers which suggest that the  useofMLmodelsarethebestchoicetopredictand  classifyheartdiseaseevenifthedatabaseismore diminutive.Variousperformance  parameters,i.e.,precision,  F1 score,  accuracy,  and  recall,have  been compared for the entireMachine Learningclassification techniques on the UCI online heart disease data set. The KNN classification algorithm outperformed the fourteen available parameters.","PREDICTIVE ANALYSIS OF HEART DISEASES WITH MACHINE LEARNING APPROACHESHeart Disease, Health Care, Medical Data, Heart Diseases.Machine Learning (ML) is used in healthcare sectors worldwide. ML methods help in the protection of heart diseases, locomotor disorders in the medical data set. The discovery of such essential data helps researchers gain valuable insight into how to utilize their diagnosis and treatment for a particular patient. Researchers use various Machine Learning methods to examine massive amounts of complex healthcare data, which aids healthcare professionals in predicting diseases.  In this research, we are using an online UCI dataset with 303 rows and 76 properties. Approximately 14 of these 76 properties are selected for testing, which is necessary to validate the performances of different methods. The isolation forest approach uses the data set’s most essential qualities and metrics to standardize the information for better precision. This analysis is based on supervised learning methods, i.e., Naive Bayes, SVM, Logistic regression, Decision Tree Classifier, Random Forest, and K- Nearest Neighbor. The experimental results demonstrate the strength of KNN with eight neighbours order to test the effectiveness, sensitivity, precision, and accuracy, F1-score; as compared to other methods, i.e., Naive Bayes, SVM (Linear Kernel), Decision Tree Classifier with 4 or 18 features, and Random Forest classifiers.FUTUREWORKThis  research  article  hasworked  on  four  different  methods  for  conducting  comparative  assessment  and obtainingtruepositiveperformance.WeconcludedthatMachine Learning methods significantlyoutperformedstatistical techniques in this research. This article proves the studies of various researchers which suggest that the  useofMLmodelsarethebestchoicetopredictand  classifyheartdiseaseevenifthedatabaseismore diminutive.Variousperformance  parameters,i.e.,precision,  F1 score,  accuracy,  and  recall,have  been compared for the entireMachine Learningclassification techniques on the UCI online heart disease data set. The KNN classification algorithm outperformed the fourteen available parameters.it tondeten |G) 4. Charlie is going to get the pieces for the new special clock. () 6, The ncighbou’s dog as itn poop before, He is bing at Lovie ||) 8) ase seu e210",Medical Data Analysis,"This research article discusses the use of Machine Learning (ML) methods in healthcare to predict and classify heart diseases and locomotor disorders. The researchers used an online UCI dataset with 303 rows and 76 properties, and approximately 14 of these properties were selected for testing different ML methods such as Naive Bayes, SVM, Logistic Regression, Decision Tree Classifier, Random Forest, and K- Nearest Neighbor. The experimental results showed that the KNN algorithm outperformed the other methods in terms of precision, F1 score, accuracy, and recall. The researchers concluded that ML methods significantly outperformed statistical techniques and are the best choice for predicting and classifying heart disease even with a smaller database. Future work could involve exploring other ML methods and parameters for improving the accuracy of heart disease predictions.",Medical Data Analysis,"it tondeten |G) 4. Charlie is going to get the pieces for the new special clock. () 6, The ncighbou’s dog as itn poop before, He is bing at Lovie ||) 8) ase seu e210",Sentiment Analysis
220,Putting artificial intelligence (AI) on the spot: machine learning evaluation of pulmonary nodules.,"Pulmonary nodule, Lung cancer, Segmentation.
","Lung cancer remains the leading cause of cancer related death world-wide despite advances in treatment. This largely relates to the fact that many of these patients already have advanced diseases at the time of initial diagnosis. As most lung cancers present as nodules initially, an accurate classification of pulmonary nodules as early lung cancers is critical to reducing lung cancer morbidity and mortality. There have been significant recent advances in artificial intelligence (AI) for lung nodule evaluation. Deep learning (DL) and convolutional neural networks (CNNs) have shown promising results in pulmonary nodule detection and have also excelled in segmentation and classification of pulmonary nodules. This review aims to provide an overview of progress that has been made in AI recently for pulmonary nodule detection and characterization with the ultimate goal of lung cancer prediction and classification while outlining some of the pitfalls and challenges that remain to bring such advancements to routine clinical use.","There has been much progress in AI assisted nodule segmentation, detection and classification in the recent years. However, ML for nodule evaluation is at its infancy and there are still many limitations to overcome, including general acceptance of such disruptive innovation. Nonetheless, ML is here to stay and has demonstrated many promising potentials for pulmonary nodule evaluation and management. It is imperative for both radiologists and clinicians to be cognizant of these algorithms’ capabilities and limitations and play an active role in introducing these tools clinically to improve patient care.","Putting artificial intelligence (AI) on the spot: machine learning evaluation of pulmonary nodules.Pulmonary nodule, Lung cancer, Segmentation.
Lung cancer remains the leading cause of cancer related death world-wide despite advances in treatment. This largely relates to the fact that many of these patients already have advanced diseases at the time of initial diagnosis. As most lung cancers present as nodules initially, an accurate classification of pulmonary nodules as early lung cancers is critical to reducing lung cancer morbidity and mortality. There have been significant recent advances in artificial intelligence (AI) for lung nodule evaluation. Deep learning (DL) and convolutional neural networks (CNNs) have shown promising results in pulmonary nodule detection and have also excelled in segmentation and classification of pulmonary nodules. This review aims to provide an overview of progress that has been made in AI recently for pulmonary nodule detection and characterization with the ultimate goal of lung cancer prediction and classification while outlining some of the pitfalls and challenges that remain to bring such advancements to routine clinical use.There has been much progress in AI assisted nodule segmentation, detection and classification in the recent years. However, ML for nodule evaluation is at its infancy and there are still many limitations to overcome, including general acceptance of such disruptive innovation. Nonetheless, ML is here to stay and has demonstrated many promising potentials for pulmonary nodule evaluation and management. It is imperative for both radiologists and clinicians to be cognizant of these algorithms’ capabilities and limitations and play an active role in introducing these tools clinically to improve patient care.[=O > Young Mean [ove tesn Seconds atid ¢ alee Uifsilfitt? Figure 1. Dwell time at mouths and eyes for each emotion type in Experiment 2.Table Emotion Happy Sad “Angry Fear Disgust ‘Surprise Number of Correctly Recognized Emotion Faces in Experiment 2 Group Young Older Young Older Young Older Young Older Young Older Young Older M 3.00 3.00 248 243 2.96% 235"" 233 222 2.96 274 278 287 SD. 0.00 0.00 07s 059 019 O88 078 oT 09 0.82 042 0.4 ‘Notes: The maximum score possible is 3.0. SD = standard deviation. *p < 001,",Medical Data Analysis,"Lung cancer is a major cause of cancer-related deaths, often due to late diagnosis. Pulmonary nodules are the initial presentation in most lung cancers, making accurate classification critical for early detection. Recent advances in artificial intelligence (AI) and deep learning (DL) have shown promising results in pulmonary nodule detection, segmentation, and classification for lung cancer prediction. This review provides an overview of progress in AI-assisted nodule evaluation, but there are still limitations to overcome, including general acceptance of disruptive innovation. Nonetheless, ML has demonstrated promising potential for pulmonary nodule evaluation, and it is important for radiologists and clinicians to be aware of these capabilities and limitations to improve patient care.",Medical Data Analysis,"[=O > Young Mean [ove tesn Seconds atid ¢ alee Uifsilfitt? Figure 1. Dwell time at mouths and eyes for each emotion type in Experiment 2.Table Emotion Happy Sad “Angry Fear Disgust ‘Surprise Number of Correctly Recognized Emotion Faces in Experiment 2 Group Young Older Young Older Young Older Young Older Young Older Young Older M 3.00 3.00 248 243 2.96% 235"" 233 222 2.96 274 278 287 SD. 0.00 0.00 07s 059 019 O88 078 oT 09 0.82 042 0.4 ‘Notes: The maximum score possible is 3.0. SD = standard deviation. *p < 001,",Object Recognition
221,Accurate Prediction of Coronary Heart Disease for Patients With Hypertension From Electronic Health Records With Big Data and Machine-Learning Methods: Model Development and Performance Evaluation.,coronary heart disease ; machine learning ; electronic health records ; predictive algorithms ; hypertension. ,"Predictions of cardiovascular disease risks based on health records have long attracted broad research interests. Despite extensive efforts, the prediction accuracy has remained unsatisfactory. This raises the question as to whether the data insufficiency, statistical and machine-learning methods, or intrinsic noise have hindered the performance of previous approaches, and how these issues can be alleviated.","We demonstrated that accurate risk prediction of CHD from EHRs is possible given a sufficiently large population of training data. Sophisticated machine-learning methods played an important role in tackling the heterogeneity and nonlinear nature of disease prediction. Moreover, accumulated EHR data over multiple time points provided additional features that were valuable for risk prediction. Our study highlights the importance of accumulating big data from EHRs for accurate disease predictions.","Accurate Prediction of Coronary Heart Disease for Patients With Hypertension From Electronic Health Records With Big Data and Machine-Learning Methods: Model Development and Performance Evaluation.coronary heart disease ; machine learning ; electronic health records ; predictive algorithms ; hypertension. Predictions of cardiovascular disease risks based on health records have long attracted broad research interests. Despite extensive efforts, the prediction accuracy has remained unsatisfactory. This raises the question as to whether the data insufficiency, statistical and machine-learning methods, or intrinsic noise have hindered the performance of previous approaches, and how these issues can be alleviated.We demonstrated that accurate risk prediction of CHD from EHRs is possible given a sufficiently large population of training data. Sophisticated machine-learning methods played an important role in tackling the heterogeneity and nonlinear nature of disease prediction. Moreover, accumulated EHR data over multiple time points provided additional features that were valuable for risk prediction. Our study highlights the importance of accumulating big data from EHRs for accurate disease predictions.",Medical Data Analysis,"Despite extensive research efforts, predicting cardiovascular disease risks based on health records has remained unsatisfactory. An ensemble method, XGBoost, achieved high accuracy in predicting 3-year CHD onset by using sophisticated machine-learning methods to tackle the heterogeneity and nonlinear nature of disease prediction. Nonlinear models outperformed linear models on the same datasets, and machine-learning methods significantly surpassed traditional risk scales or fixed models. Using time-dependent features obtained from multiple records helped improve performance compared to using only static features. Accumulated EHR data over multiple time points provided additional valuable features for risk prediction. This study highlights the importance of accumulating big data from EHRs for accurate disease predictions.",Medical Data Analysis,,Medical Data Analysis
222,"Predicting EGFR mutation status in
lung adenocarcinoma on computed
tomography image using deep learning.","Epidermal growth factor receptor(EGFR), Computed tomography(CT), Adenocarcinoma.","Epidermal growth factor receptor (EGFR) genotyping is critical for treatment guidelines
such as the use of tyrosine kinase inhibitors in lung adenocarcinoma. Conventional identification of EGFR
genotype requires biopsy and sequence testing which is invasive and may suffer from the difficulty of
accessing tissue samples. Here, we propose a deep learning model to predict EGFR mutation status in lung
adenocarcinoma using non-invasive computed tomography (CT).
We retrospectively collected data from 844 lung adenocarcinoma patients with pre-operative CT images,
EGFR mutation and clinical information from two hospitals. An end-to-end deep learning model was
proposed to predict the EGFR mutation status by CT scanning.
By training in 14926 CT images, the deep learning model achieved encouraging predictive performance
in both the primary cohort (n=603; AUC 0.85, 95% CI 0.83–0.88) and the independent validation cohort
(n=241; AUC 0.81, 95% CI 0.79–0.83), which showed significant improvement over previous studies using
hand-crafted CT features or clinical characteristics ( p<0.001). The deep learning score demonstrated
significant differences in EGFR-mutant and EGFR-wild type tumours ( p<0.001).
Since CT is routinely used in lung cancer diagnosis, the deep learning model provides a non-invasive
and easy-to-use method for EGFR mutation status prediction."," we proposed a deep learning model using non-invasive CT images to predict EGFR
mutation status for patients with lung adenocarcinoma. We trained the deep learning model in 14926 CT
images from the primary cohort (603 patients), and validated its performance in an independent
validation cohort from another hospital (241 patients). The deep learning model showed encouraging
results in the primary cohort (AUC 0.85, 95% CI 0.83–0.88) and achieved strong performance in the
independent validation cohort (AUC 0.81, 95% CI 0.79–0.83). The deep learning model revealed that
there was a significant association between high-dimensional CT image features and EGFR genotype. Our
analysis provides an alternative method to non-invasively assess EGFR information for patients, and offers
a great supplement to biopsy. Meanwhile, our model can discover the suspicious tumour area that
dominates the prediction of EGFR mutation status. This analysis offered visual interpretation to clinicians
about understanding the prediction outcomes in CT data. Moreover, the deep learning model requires
only the raw tumour image as input and predicts the EGFR mutation status directly without further
human assistance, is easy to use and very fast.","Predicting EGFR mutation status in
lung adenocarcinoma on computed
tomography image using deep learning.Epidermal growth factor receptor(EGFR), Computed tomography(CT), Adenocarcinoma.Epidermal growth factor receptor (EGFR) genotyping is critical for treatment guidelines
such as the use of tyrosine kinase inhibitors in lung adenocarcinoma. Conventional identification of EGFR
genotype requires biopsy and sequence testing which is invasive and may suffer from the difficulty of
accessing tissue samples. Here, we propose a deep learning model to predict EGFR mutation status in lung
adenocarcinoma using non-invasive computed tomography (CT).
We retrospectively collected data from 844 lung adenocarcinoma patients with pre-operative CT images,
EGFR mutation and clinical information from two hospitals. An end-to-end deep learning model was
proposed to predict the EGFR mutation status by CT scanning.
By training in 14926 CT images, the deep learning model achieved encouraging predictive performance
in both the primary cohort (n=603; AUC 0.85, 95% CI 0.83–0.88) and the independent validation cohort
(n=241; AUC 0.81, 95% CI 0.79–0.83), which showed significant improvement over previous studies using
hand-crafted CT features or clinical characteristics ( p<0.001). The deep learning score demonstrated
significant differences in EGFR-mutant and EGFR-wild type tumours ( p<0.001).
Since CT is routinely used in lung cancer diagnosis, the deep learning model provides a non-invasive
and easy-to-use method for EGFR mutation status prediction. we proposed a deep learning model using non-invasive CT images to predict EGFR
mutation status for patients with lung adenocarcinoma. We trained the deep learning model in 14926 CT
images from the primary cohort (603 patients), and validated its performance in an independent
validation cohort from another hospital (241 patients). The deep learning model showed encouraging
results in the primary cohort (AUC 0.85, 95% CI 0.83–0.88) and achieved strong performance in the
independent validation cohort (AUC 0.81, 95% CI 0.79–0.83). The deep learning model revealed that
there was a significant association between high-dimensional CT image features and EGFR genotype. Our
analysis provides an alternative method to non-invasively assess EGFR information for patients, and offers
a great supplement to biopsy. Meanwhile, our model can discover the suspicious tumour area that
dominates the prediction of EGFR mutation status. This analysis offered visual interpretation to clinicians
about understanding the prediction outcomes in CT data. Moreover, the deep learning model requires
only the raw tumour image as input and predicts the EGFR mutation status directly without further
human assistance, is easy to use and very fast.",Medical Data Analysis,"The study proposes a deep learning model using non-invasive CT images to predict EGFR mutation status in lung adenocarcinoma patients. The model was trained on 14926 CT images from a primary cohort and validated on an independent validation cohort from another hospital. The model achieved encouraging predictive performance in both cohorts, with AUCs of 0.85 and 0.81, respectively. The deep learning model revealed a significant association between high-dimensional CT image features and EGFR genotype, offering an alternative method for non-invasive assessment of EGFR information for patients. The model requires only the raw tumour image as input and predicts EGFR mutation status directly without further human assistance, making it easy to use and very fast. The deep learning score demonstrated significant differences in EGFR-mutant and EGFR-wild type tumours. The model provides a non-invasive and easy-to-use method for EGFR mutation status prediction, which could supplement biopsy and offer a great visual interpretation to clinicians.",Medical Data Analysis,,Medical Data Analysis
223,"Blind separation of temporally correlated noncircular sources using
complex matrix joint diagonalization.","Blind source separation
Widely linear filter
Complex matrix joint diagonalization
Shear and Givens rotations.","In this paper, we introduce a new blind source separation (BSS) method for temporal correlated noncircular sources that uses widely linear filter (WLF) model to efficiently describe the temporal structure.
The algorithm consists of a WLF coefficients estimator followed by complex matrix joint diagonalization. In the derivation of the new BSS algorithm, an emerging matrix joint diagonalization issue needs to
be addressed. Subsequently, a complex matrix joint diagonalization algorithm based on successive Shear
and Givens rotations (SGR) is proposed to deal with the new joint diagonalization problem. Simulations
of SGR show that it converges fast and converges to excellent value, and it shows a satisfactory antiperturbation performance of matrix for the new BSS algorithm. We compare the performance of the new
BSS algorithm with several competing algorithms, and the simulation results demonstrate the superior
performance of the proposed BSS algorithm for both noncircular complex Gaussian and non-Gaussian
sources.","In this paper, the blind separation of stationary complex sources
was studied only using the dependency among samples. By modeling the sources as output of WLF systems driving by circular Gaussian noise, we propose a simple BSS algorithm which combine WLF
coefficients estimation with matrix joint diagonalization. In the
derivation of the new BSS algorithm, we encountered a new matrix
joint diagonalization problem. Then, we propose a new complex
matrix joint diagonalization algorithm SGR to deal with it. Simulations demonstrated that SGR converges fast and the contrast function gets excellent level when it converges, and also SGR shows
a strong anti-perturbation performance of the diagonalized matrices. The proposed BSS algorithm can be used to separate temporal
correlated noncircular Gaussian and non-Gaussian sources. For illuminating its performance, we design four experiments to compare
it with other various algorithms, which are based on either ICA
or matrix joint diagonalization of several time-delayed (pseudo)
covariance matrices of mixed signals. Simulation results demonstrate the superior performance of the proposed BSS algorithm. At
present, the studies of blind separation for complex-valued sources
mainly use the time structure and statistical characteristics of signals respectively, including this paper. The joint use of the two
characteristics may improve the separation performance.","Blind separation of temporally correlated noncircular sources using
complex matrix joint diagonalization.Blind source separation
Widely linear filter
Complex matrix joint diagonalization
Shear and Givens rotations.In this paper, we introduce a new blind source separation (BSS) method for temporal correlated noncircular sources that uses widely linear filter (WLF) model to efficiently describe the temporal structure.
The algorithm consists of a WLF coefficients estimator followed by complex matrix joint diagonalization. In the derivation of the new BSS algorithm, an emerging matrix joint diagonalization issue needs to
be addressed. Subsequently, a complex matrix joint diagonalization algorithm based on successive Shear
and Givens rotations (SGR) is proposed to deal with the new joint diagonalization problem. Simulations
of SGR show that it converges fast and converges to excellent value, and it shows a satisfactory antiperturbation performance of matrix for the new BSS algorithm. We compare the performance of the new
BSS algorithm with several competing algorithms, and the simulation results demonstrate the superior
performance of the proposed BSS algorithm for both noncircular complex Gaussian and non-Gaussian
sources.In this paper, the blind separation of stationary complex sources
was studied only using the dependency among samples. By modeling the sources as output of WLF systems driving by circular Gaussian noise, we propose a simple BSS algorithm which combine WLF
coefficients estimation with matrix joint diagonalization. In the
derivation of the new BSS algorithm, we encountered a new matrix
joint diagonalization problem. Then, we propose a new complex
matrix joint diagonalization algorithm SGR to deal with it. Simulations demonstrated that SGR converges fast and the contrast function gets excellent level when it converges, and also SGR shows
a strong anti-perturbation performance of the diagonalized matrices. The proposed BSS algorithm can be used to separate temporal
correlated noncircular Gaussian and non-Gaussian sources. For illuminating its performance, we design four experiments to compare
it with other various algorithms, which are based on either ICA
or matrix joint diagonalization of several time-delayed (pseudo)
covariance matrices of mixed signals. Simulation results demonstrate the superior performance of the proposed BSS algorithm. At
present, the studies of blind separation for complex-valued sources
mainly use the time structure and statistical characteristics of signals respectively, including this paper. The joint use of the two
characteristics may improve the separation performance.",Medical Data Analysis,"The paper introduces a new blind source separation (BSS) method for separating temporal correlated noncircular sources using a widely linear filter (WLF) model. The algorithm includes a WLF coefficients estimator and a complex matrix joint diagonalization step, which requires a new joint diagonalization algorithm called successive Shear and Givens rotations (SGR). The proposed BSS algorithm is shown to have superior performance compared to other algorithms for both noncircular complex Gaussian and non-Gaussian sources. The paper concludes by suggesting future work on improving separation performance by jointly using time structure and statistical characteristics of the signals.",Medical Data Analysis,,Medical Data Analysis
224,"Cascade learning from adversarial synthetic images for accurate pupil
detection.","Cascade regression
GANs
Pupil detection","mage-based pupil detection, which aims to find the pupil location in an image, has been an active research topic in computer vision community. Learning-based approaches can achieve preferable results
given large amounts of training data with eye center annotations. However, there are limited publicly
available datasets with accurate eye center annotations and it is unreliable and time-consuming for manually labeling large amounts of training data. In this paper, inspired by learning from synthetic data in
Parallel Vision framework, we introduce a step of parallel imaging built upon Generative Adversarial Networks (GANs) to generate adversarial synthetic images. In particular, we refine the synthetic eye images
by the improved SimGAN using adversarial training scheme. For the computational experiments, we further propose a coarse-to-fine pupil detection framework based on shape augmented cascade regression
models learning from the adversarial synthetic images. Experiments on benchmark databases of BioID,
GI4E, and LFW show that the proposed work performs significantly better over other state-of-the-art
methods by leveraging the power of cascade regression and adversarial image synthesis.","n this paper, we propose a unified framework to learn shape
augmented cascade regression models from adversarial synthetic
images for accurate pupil detection. We introduce a step of parallel imaging exploiting the adversarial training to refine the artificial
eyes with texture and appearance from real images and preserving
the detailed information like structural shape from synthetic images. For the step of computational experiments, we learn shape
augmented regression models based on the eye related shape,
texture, and appearance for pupil detection. Our proposed work
achieves the state-of-the-art performance on three public available
benchmark datasets.
Future work will focus on designing powerful nonlinear architectures (e.g. deep models) to map the appearance and target updates in the cascade level. And more efforts will be undertaken
to realize the parallel execution where it can achieve on-line optimization through real-time inputs of unlabeled images.","Cascade learning from adversarial synthetic images for accurate pupil
detection.Cascade regression
GANs
Pupil detectionmage-based pupil detection, which aims to find the pupil location in an image, has been an active research topic in computer vision community. Learning-based approaches can achieve preferable results
given large amounts of training data with eye center annotations. However, there are limited publicly
available datasets with accurate eye center annotations and it is unreliable and time-consuming for manually labeling large amounts of training data. In this paper, inspired by learning from synthetic data in
Parallel Vision framework, we introduce a step of parallel imaging built upon Generative Adversarial Networks (GANs) to generate adversarial synthetic images. In particular, we refine the synthetic eye images
by the improved SimGAN using adversarial training scheme. For the computational experiments, we further propose a coarse-to-fine pupil detection framework based on shape augmented cascade regression
models learning from the adversarial synthetic images. Experiments on benchmark databases of BioID,
GI4E, and LFW show that the proposed work performs significantly better over other state-of-the-art
methods by leveraging the power of cascade regression and adversarial image synthesis.n this paper, we propose a unified framework to learn shape
augmented cascade regression models from adversarial synthetic
images for accurate pupil detection. We introduce a step of parallel imaging exploiting the adversarial training to refine the artificial
eyes with texture and appearance from real images and preserving
the detailed information like structural shape from synthetic images. For the step of computational experiments, we learn shape
augmented regression models based on the eye related shape,
texture, and appearance for pupil detection. Our proposed work
achieves the state-of-the-art performance on three public available
benchmark datasets.
Future work will focus on designing powerful nonlinear architectures (e.g. deep models) to map the appearance and target updates in the cascade level. And more efforts will be undertaken
to realize the parallel execution where it can achieve on-line optimization through real-time inputs of unlabeled images.True positive rate EENECER-mutant Dd 1.07 ga corn. wid ype cl 06 treat none Quarties Treat at os —dtmedet 208 g 0b 2 ¢ 2° £03 5 & 204 202 é 0.00) 0.00 on 6 0.001 7 o2| ° = GinicatVabsoten AUC=O.11 00 02 04 06 08 10 Primary cohort Validation cohort mo 4 6 8 100 False positive rate ‘Threshold probability",Medical Data Analysis,This paper proposes a unified framework for pupil detection using shape augmented cascade regression models learned from adversarial synthetic images. The proposed method introduces a step of parallel imaging using Generative Adversarial Networks (GANs) to refine synthetic eye images with texture and appearance from real images while preserving structural shape from synthetic images. The computational experiments show that the proposed method achieves state-of-the-art performance on three benchmark datasets. Future work will focus on designing powerful nonlinear architectures for mapping appearance and target updates in cascade levels and realizing parallel execution for real-time optimization with unlabeled images. The proposed method addresses the limited availability of accurate eye center annotations and time-consuming manual labeling of training data by leveraging the power of cascade regression and adversarial image synthesis.,Medical Data Analysis,True positive rate EENECER-mutant Dd 1.07 ga corn. wid ype cl 06 treat none Quarties Treat at os —dtmedet 208 g 0b 2 ¢ 2° £03 5 & 204 202 é 0.00) 0.00 on 6 0.001 7 o2| ° = GinicatVabsoten AUC=O.11 00 02 04 06 08 10 Primary cohort Validation cohort mo 4 6 8 100 False positive rate ‘Threshold probability,Medical Data Analysis
225,"Using eye-tracker to compare search patterns between experienced and
novice workers for site hazard identification
.","Eye-tracking
Hazard identification
Construction safety
Knowledge extraction.","The construction industry accounts for a high number of accidents. Although identifying hazards prior to
commencing construction is widely employed to prevent accidents, it typically fails because of insufficient safety experience. The experience helps in training novice inspectors, although extracting and
describing tacit knowledge explicitly is difficult. This study created a digital building construction site,
and designed a hazard-identification experiment involving four workplaces featuring obvious and unobvious hazards (e.g., falls, collapses, and electric shocks), and an eye-tracker was used to compare the
search patterns of the experienced and novice workers. The results indicated that experience assisted
the experienced workers in assessing both obvious (p < 0.001) and unobvious hazards (p = 0.004) significantly faster than the novice workers could; however, it did not improve the accuracy with which they
identified hazards, indicating that general work experience is not equivalent to safety-specific experience, and may not necessarily improve workers’ accuracy in identifying hazards. Nevertheless, the experienced workers were more confident in identifying hazards, they exhibited fewer fixations, and their
scan paths for assessing hazards were more consistent. The experienced workers first assessed the
high-risk targets—laborers working at heights—and subsequently assessed those working on the ground,
followed by the equipment or environment. Furthermore, they typically inspected openings later than
novice workers did. The search strategies identified may be incorporated into the training courses to
improve the hazard awareness for novice workers.","The construction industry has high accident rates and fatalities, and hazard identification is a widely used approach for preventing accidents. However, inspectors often lack safety knowledge and experience. This study used an eye-tracker to compare the hazard identification performance and search patterns of experienced and novice workers. The experiment involved four images of workplaces with obvious and unobvious hazards. The results showed that experienced workers identified hazards faster than novice workers, but accuracy and miss rates were similar. Experienced workers also required less time and fewer fixations to identify hazards, and they exhibited more confidence in determining hazards. The search pattern analysis provided valuable information for safety trainers and educators. The findings suggest that improving safety training and hazard awareness about working at heights is crucial for construction workers. Eye-tracking technology may be used to improve safety training by analyzing search patterns and identifying insufficiencies in novice workers, as well as tracking the search patterns of experienced inspectors for hazard identification.","Using eye-tracker to compare search patterns between experienced and
novice workers for site hazard identification
.Eye-tracking
Hazard identification
Construction safety
Knowledge extraction.The construction industry accounts for a high number of accidents. Although identifying hazards prior to
commencing construction is widely employed to prevent accidents, it typically fails because of insufficient safety experience. The experience helps in training novice inspectors, although extracting and
describing tacit knowledge explicitly is difficult. This study created a digital building construction site,
and designed a hazard-identification experiment involving four workplaces featuring obvious and unobvious hazards (e.g., falls, collapses, and electric shocks), and an eye-tracker was used to compare the
search patterns of the experienced and novice workers. The results indicated that experience assisted
the experienced workers in assessing both obvious (p < 0.001) and unobvious hazards (p = 0.004) significantly faster than the novice workers could; however, it did not improve the accuracy with which they
identified hazards, indicating that general work experience is not equivalent to safety-specific experience, and may not necessarily improve workers’ accuracy in identifying hazards. Nevertheless, the experienced workers were more confident in identifying hazards, they exhibited fewer fixations, and their
scan paths for assessing hazards were more consistent. The experienced workers first assessed the
high-risk targets—laborers working at heights—and subsequently assessed those working on the ground,
followed by the equipment or environment. Furthermore, they typically inspected openings later than
novice workers did. The search strategies identified may be incorporated into the training courses to
improve the hazard awareness for novice workers.The construction industry has high accident rates and fatalities, and hazard identification is a widely used approach for preventing accidents. However, inspectors often lack safety knowledge and experience. This study used an eye-tracker to compare the hazard identification performance and search patterns of experienced and novice workers. The experiment involved four images of workplaces with obvious and unobvious hazards. The results showed that experienced workers identified hazards faster than novice workers, but accuracy and miss rates were similar. Experienced workers also required less time and fewer fixations to identify hazards, and they exhibited more confidence in determining hazards. The search pattern analysis provided valuable information for safety trainers and educators. The findings suggest that improving safety training and hazard awareness about working at heights is crucial for construction workers. Eye-tracking technology may be used to improve safety training by analyzing search patterns and identifying insufficiencies in novice workers, as well as tracking the search patterns of experienced inspectors for hazard identification.#ERROR!",Medical Data Analysis,"This study compared the hazard identification performance and search patterns of experienced and novice workers in the construction industry using eye-tracking technology. The experiment involved four images of workplaces with obvious and unobvious hazards, and the results showed that experienced workers identified hazards faster and with more confidence than novice workers, but accuracy and miss rates were similar. The study suggests that improving safety training and hazard awareness about working at heights is crucial for construction workers, and eye-tracking technology may be used to improve safety training by analyzing search patterns and identifying insufficiencies in novice workers. Overall, the study provides valuable information for safety trainers and educators in the construction industry.",Medical Data Analysis,#ERROR!,Medical Data Analysis
226,"Derivation and Validation of Machine Learning
Approaches to Predict Acute Kidney Injury after
Cardiac Surgery
."," acute kidney injury; cardiovascular surgery.
.","Machine learning approaches were introduced for better or comparable predictive ability
than statistical analysis to predict postoperative outcomes. We sought to compare the performance of
machine learning approaches with that of logistic regression analysis to predict acute kidney injury
after cardiac surgery. We retrospectively reviewed 2010 patients who underwent open heart surgery
and thoracic aortic surgery. Baseline medical condition, intraoperative anesthesia, and surgery-related
data were obtained. The primary outcome was postoperative acute kidney injury (AKI) defined
according to the Kidney Disease Improving Global Outcomes criteria. The following machine
learning techniques were used: decision tree, random forest, extreme gradient boosting, support
vector machine, neural network classifier, and deep learning. The performance of these techniques
was compared with that of logistic regression analysis regarding the area under the receiver-operating
characteristic curve (AUC). During the first postoperative week, AKI occurred in 770 patients (38.3%).
The best performance regarding AUC was achieved by the gradient boosting machine to predict the
AKI of all stages (0.78, 95% confidence interval (CI) 0.75–0.80) or stage 2 or 3 AKI. The AUC of logistic
regression analysis was 0.69 (95% CI 0.66–0.72). Decision tree, random forest, and support vector
machine showed similar performance to logistic regression. In our comprehensive comparison of
machine learning approaches with logistic regression analysis, gradient boosting technique showed
the best performance with the highest AUC and lower error rate. We developed an Internet–based
risk estimator which could be used for real-time processing of patient data to estimate the risk of AKI
at the end of surgery.
","In conclusion, our study demonstrated that the machine learning technique of extreme gradient
boosting showed significantly better performance than the traditional logistic regression analysis or
previous risk scores in predicting both AKI of all stages and stage 2 or 3 AKI after cardiac surgery.
Gradient boosting machine may be used for real-time processing of patient data to estimate the risk of
AKI after cardiac surgery at the end of surgery. Our Internet-based risk estimator may help to evaluate
the risk of AKI at the end of surgery. However, prospective multicenter trials are required to validate
the better prediction by gradient boosting. Further studies may apply extreme gradient boosting
machine to the other important clinical outcomes after cardiac surgeries and may prospectively validate
our results.","Derivation and Validation of Machine Learning
Approaches to Predict Acute Kidney Injury after
Cardiac Surgery
. acute kidney injury; cardiovascular surgery.
.Machine learning approaches were introduced for better or comparable predictive ability
than statistical analysis to predict postoperative outcomes. We sought to compare the performance of
machine learning approaches with that of logistic regression analysis to predict acute kidney injury
after cardiac surgery. We retrospectively reviewed 2010 patients who underwent open heart surgery
and thoracic aortic surgery. Baseline medical condition, intraoperative anesthesia, and surgery-related
data were obtained. The primary outcome was postoperative acute kidney injury (AKI) defined
according to the Kidney Disease Improving Global Outcomes criteria. The following machine
learning techniques were used: decision tree, random forest, extreme gradient boosting, support
vector machine, neural network classifier, and deep learning. The performance of these techniques
was compared with that of logistic regression analysis regarding the area under the receiver-operating
characteristic curve (AUC). During the first postoperative week, AKI occurred in 770 patients (38.3%).
The best performance regarding AUC was achieved by the gradient boosting machine to predict the
AKI of all stages (0.78, 95% confidence interval (CI) 0.75–0.80) or stage 2 or 3 AKI. The AUC of logistic
regression analysis was 0.69 (95% CI 0.66–0.72). Decision tree, random forest, and support vector
machine showed similar performance to logistic regression. In our comprehensive comparison of
machine learning approaches with logistic regression analysis, gradient boosting technique showed
the best performance with the highest AUC and lower error rate. We developed an Internet–based
risk estimator which could be used for real-time processing of patient data to estimate the risk of AKI
at the end of surgery.
In conclusion, our study demonstrated that the machine learning technique of extreme gradient
boosting showed significantly better performance than the traditional logistic regression analysis or
previous risk scores in predicting both AKI of all stages and stage 2 or 3 AKI after cardiac surgery.
Gradient boosting machine may be used for real-time processing of patient data to estimate the risk of
AKI after cardiac surgery at the end of surgery. Our Internet-based risk estimator may help to evaluate
the risk of AKI at the end of surgery. However, prospective multicenter trials are required to validate
the better prediction by gradient boosting. Further studies may apply extreme gradient boosting
machine to the other important clinical outcomes after cardiac surgeries and may prospectively validate
our results.Real Scene afc scene z g © observation and Coservation and) § | evaluation evaluation | 3 Perception and Experiment and Tearning and understanding evaluation training Fig. 2. Parallel vision framework based on ACP (7).Artificial Scene Simulate eyes with label } i Parallel imaging Computational experiments Refine artificial eyes & hy adversarial taining Cascade model learning Inference t 7 Real Scene i Fetes Unlabeled real eyes Fig. 1. Overview of the proposed pupil detection framework based on PV.",Medical Data Analysis,"The study compared machine learning approaches with logistic regression analysis to predict acute kidney injury (AKI) after cardiac surgery. They retrospectively reviewed 2010 patients and obtained baseline medical condition, intraoperative anesthesia, and surgery-related data. The following machine learning techniques were used: decision tree, random forest, extreme gradient boosting, support vector machine, neural network classifier, and deep learning. The performance of these techniques was compared with that of logistic regression analysis regarding the area under the receiver-operating characteristic curve (AUC). During the first postoperative week, AKI occurred in 38.3% of patients. The best performance regarding AUC was achieved by the gradient boosting machine to predict all stages of AKI or stage 2 or 3 AKI. The study demonstrated that the machine learning technique of extreme gradient boosting showed significantly better performance than the traditional logistic regression analysis or previous risk scores in predicting both AKI of all stages and stage 2 or 3 AKI after cardiac surgery. Gradient boosting machine may be used for real-time processing of patient data to estimate the risk of AKI after cardiac surgery at the end of surgery.",Medical Data Analysis,Real Scene afc scene z g © observation and Coservation and) § | evaluation evaluation | 3 Perception and Experiment and Tearning and understanding evaluation training Fig. 2. Parallel vision framework based on ACP (7).Artificial Scene Simulate eyes with label } i Parallel imaging Computational experiments Refine artificial eyes & hy adversarial taining Cascade model learning Inference t 7 Real Scene i Fetes Unlabeled real eyes Fig. 1. Overview of the proposed pupil detection framework based on PV.,Medical Data Analysis
227,AI in predicting COPD in the Canadian population. ,"Medical diagnosis , Text classification 
Bag of words model 
Extreme gradient boosting 
EMR data 
Feature importance 
COPD. ","Chronic obstructive pulmonary disease (COPD) is a progressive lung disease that produces non-reversible airflow 
limitations. Approximately 10% of Canadians aged 35 years or older are living with COPD. Primary care is often 
the first contact an individual will have with the healthcare system providing acute care, chronic disease 
management, and services aimed at health maintenance. This study used Electronic Medical Record (EMR) data 
from primary care clinics in seven provinces across Canada to develop predictive models to identify COPD in the 
Canadian population. The comprehensive nature of this primary care EMR data containing structured numeric, 
categorical, hybrid, and unstructured text data, enables the predictive models to capture symptoms of COPD and 
discriminate it from diseases with similar symptoms. We applied two supervised machine learning models, a 
Multilayer Neural Networks (MLNN) model and an Extreme Gradient Boosting (XGB) to identify COPD patients. 
The XGB model achieved an accuracy of 86% in the test dataset compared to 83% achieved by the MLNN. 
Utilizing feature importance, we identified a set of key symptoms from the EMR for diagnosing COPD, which 
included medications, health conditions, risk factors, and patient age. Application of this XGB model to primary 
care structured EMR data can identify patients with COPD from others having similar chronic conditions for 
disease surveillance, and improve evidence-based care delivery.","We developed machine learning models to identify COPD patients 
from control patients within community-based health care EMRs in 
Canada. Feature engineering was applied to group similar medications 
and diseases, and create a more accurate depiction of the EMR details. It 
ensured that the machine learning models were considered and learned 
from a variety of tables and variables available within the EMR. Feature 
importance provides an avenue to verify that the model is working 
correctly as well as identifies features that can improve or inform early 
detection of COPD. The XGB model applied on the mixed data was the 
most accurate model for classifying COPD patients from controls and 
enabled the assessment of features contributing to the model prediction. 
In future studies, we plan to include free-text EMR chart notes 
recorded by the physicians during patient encounters within the primary 
care systems. Identification of vulnerable patients suffering from chronic 
diseases such as COPD can not only improve care delivery but inform 
hospital readmission prediction of patients that can create improved 
efficiencies within the health care system.","AI in predicting COPD in the Canadian population. Medical diagnosis , Text classification 
Bag of words model 
Extreme gradient boosting 
EMR data 
Feature importance 
COPD. Chronic obstructive pulmonary disease (COPD) is a progressive lung disease that produces non-reversible airflow 
limitations. Approximately 10% of Canadians aged 35 years or older are living with COPD. Primary care is often 
the first contact an individual will have with the healthcare system providing acute care, chronic disease 
management, and services aimed at health maintenance. This study used Electronic Medical Record (EMR) data 
from primary care clinics in seven provinces across Canada to develop predictive models to identify COPD in the 
Canadian population. The comprehensive nature of this primary care EMR data containing structured numeric, 
categorical, hybrid, and unstructured text data, enables the predictive models to capture symptoms of COPD and 
discriminate it from diseases with similar symptoms. We applied two supervised machine learning models, a 
Multilayer Neural Networks (MLNN) model and an Extreme Gradient Boosting (XGB) to identify COPD patients. 
The XGB model achieved an accuracy of 86% in the test dataset compared to 83% achieved by the MLNN. 
Utilizing feature importance, we identified a set of key symptoms from the EMR for diagnosing COPD, which 
included medications, health conditions, risk factors, and patient age. Application of this XGB model to primary 
care structured EMR data can identify patients with COPD from others having similar chronic conditions for 
disease surveillance, and improve evidence-based care delivery.We developed machine learning models to identify COPD patients 
from control patients within community-based health care EMRs in 
Canada. Feature engineering was applied to group similar medications 
and diseases, and create a more accurate depiction of the EMR details. It 
ensured that the machine learning models were considered and learned 
from a variety of tables and variables available within the EMR. Feature 
importance provides an avenue to verify that the model is working 
correctly as well as identifies features that can improve or inform early 
detection of COPD. The XGB model applied on the mixed data was the 
most accurate model for classifying COPD patients from controls and 
enabled the assessment of features contributing to the model prediction. 
In future studies, we plan to include free-text EMR chart notes 
recorded by the physicians during patient encounters within the primary 
care systems. Identification of vulnerable patients suffering from chronic 
diseases such as COPD can not only improve care delivery but inform 
hospital readmission prediction of patients that can create improved 
efficiencies within the health care system.R-J. Dzeng et al, /Safety Science 82 (2016) 56-67 ad-D (1-2) hs ViewPoint EyeTracker Gig60 Pe tyecamera6OFS Ace =) © a |® Suis Window ae F Gazespace (SeneCam.. (5 | S| Unk: there Serve: 192.1681.250, Port 5000 (0:01:4.004: : Random: 142537906 a Bi} cq a fe Control sete eee Scone _|[ Ciera a, —— | a | oe | a - _ —tecamme| _etecomste | | eens ars —— — co = baer = 4 | men ens eon | =",Medical Data Analysis,"This study used Electronic Medical Record (EMR) data from primary care clinics in seven provinces across Canada to develop predictive models to identify patients with Chronic obstructive pulmonary disease (COPD). The study applied two supervised machine learning models, a Multilayer Neural Networks (MLNN) model and an Extreme Gradient Boosting (XGB) to identify COPD patients from control patients within community-based health care EMRs in Canada. The XGB model achieved an accuracy of 86% in the test dataset compared to 83% achieved by the MLNN. Feature importance analysis was used to identify a set of key symptoms from the EMR for diagnosing COPD, which included medications, health conditions, risk factors, and patient age. The study concludes that the application of the XGB model to primary care structured EMR data can identify patients with COPD from others having similar chronic conditions for disease surveillance and improve evidence-based care delivery. Future studies plan to include free-text EMR chart notes recorded by physicians during patient encounters within the primary care systems to improve care delivery and inform hospital readmission prediction of patients, thus creating improved efficiencies within the healthcare system.",Medical Data Analysis,"R-J. Dzeng et al, /Safety Science 82 (2016) 56-67 ad-D (1-2) hs ViewPoint EyeTracker Gig60 Pe tyecamera6OFS Ace =) © a |® Suis Window ae F Gazespace (SeneCam.. (5 | S| Unk: there Serve: 192.1681.250, Port 5000 (0:01:4.004: : Random: 142537906 a Bi} cq a fe Control sete eee Scone _|[ Ciera a, —— | a | oe | a - _ —tecamme| _etecomste | | eens ars —— — co = baer = 4 | men ens eon | =",Medical Data Analysis
228,"Machine Learning in Healthcare Data Analysis: A 
Survey."," Healthcare, Clinical Data, Sensor Data, Omics Data.","In recent years, healthcare data analysis is becoming one of the most promising research areas. Healthcare includes data in various types 
such as clinical data, Omics data, and Sensor data. Clinical data includes electronic health records which store patient records collected during 
ongoing treatment. Omics data is one of the high dimensional data comprising genome, transcriptome and proteome data types. Sensor data 
is collected from various wearable and wireless sensor devices. To handle this raw data manually is very difficult. For analysis of data, machine 
learning is emerged as a significant tool. Machine learning uses various statistical techniques and advanced algorithms to predict the results 
of healthcare data more precisely. In machine learning different types of algorithms like supervised, unsupervised and reinforcement are used 
for analysis. In this paper, different types of machine learning algorithms are described. Then use of machine learning algorithms for analyzing 
various healthcare data are surveyed..
","A different type of data is present in healthcare. To analyze 
this variety of data various Machine learning algorithms such 
assupervised, unsupervised and reinforced algorithms are used 
to improve prediction which can be analyzed using various 
performance parameters like accuracy, sensitivity, specificity, 
precision, F1 score, and Area under Curve. In this paper, 
machine learning algorithms are defined and use of machine 
learning algorithms for analyzing different types of healthcare 
data like clinical, omics and sensor data is done. From the 
survey, it is concluded that for analyzing different types of 
data in healthcare, various machine learning algorithms and 
feature extraction techniques are proposed by various authors 
for survival prediction of cancer patients.","Machine Learning in Healthcare Data Analysis: A 
Survey. Healthcare, Clinical Data, Sensor Data, Omics Data.In recent years, healthcare data analysis is becoming one of the most promising research areas. Healthcare includes data in various types 
such as clinical data, Omics data, and Sensor data. Clinical data includes electronic health records which store patient records collected during 
ongoing treatment. Omics data is one of the high dimensional data comprising genome, transcriptome and proteome data types. Sensor data 
is collected from various wearable and wireless sensor devices. To handle this raw data manually is very difficult. For analysis of data, machine 
learning is emerged as a significant tool. Machine learning uses various statistical techniques and advanced algorithms to predict the results 
of healthcare data more precisely. In machine learning different types of algorithms like supervised, unsupervised and reinforcement are used 
for analysis. In this paper, different types of machine learning algorithms are described. Then use of machine learning algorithms for analyzing 
various healthcare data are surveyed..
A different type of data is present in healthcare. To analyze 
this variety of data various Machine learning algorithms such 
assupervised, unsupervised and reinforced algorithms are used 
to improve prediction which can be analyzed using various 
performance parameters like accuracy, sensitivity, specificity, 
precision, F1 score, and Area under Curve. In this paper, 
machine learning algorithms are defined and use of machine 
learning algorithms for analyzing different types of healthcare 
data like clinical, omics and sensor data is done. From the 
survey, it is concluded that for analyzing different types of 
data in healthcare, various machine learning algorithms and 
feature extraction techniques are proposed by various authors 
for survival prediction of cancer patients.(eis) — {wo} g <0.75 @ ‘Surgery_time <7) Intraop_pRBC Total_intraop_Fluid 23 L Preop_Hct ” Preop_Cr Intraop_FFP- Mean, an 63 [ 35 G) Preop_£_of_e'_morethan_ wl eee",Medical Data Analysis,"Healthcare data analysis has become a promising research area due to the various types of data available, such as clinical, omics, and sensor data. To handle this raw data manually is difficult, so machine learning has emerged as a significant tool to predict results more accurately. Different types of machine learning algorithms, such as supervised, unsupervised, and reinforcement, are used for analysis. This paper surveys the use of machine learning algorithms for analyzing various healthcare data types, including clinical, omics, and sensor data. The performance parameters used to evaluate these algorithms include accuracy, sensitivity, specificity, precision, F1 score, and Area under Curve. From the survey, it is concluded that various machine learning algorithms and feature extraction techniques have been proposed by various authors for survival prediction of cancer patients.",Medical Data Analysis,"(eis) — {wo} g <0.75 @ ‘Surgery_time <7) Intraop_pRBC Total_intraop_Fluid 23 L Preop_Hct ” Preop_Cr Intraop_FFP- Mean, an 63 [ 35 G) Preop_£_of_e'_morethan_ wl eee",Medical Data Analysis
229,Role of machine learning in medical research: A survey.,"Medical research
Machine learning
Deep learning,Medical Data","Machine learning is one of the essential and effective tools in analyzing highly complex medical
data. With vast amounts of medical data being generated, there is an urgent need to effectively
use this data to benefit the medical and health care sectors all across the world. This survey paper
presents a systematic literature review for the investigation of various machine learning techniques
used for numerous medical applications which are published in highly reputable venues in recent
years. Considering only the recent work, we are able to survey the current machine learning and deep
learning models that are being used for medical data. This literature review identifies a clear shift
of artificial intelligence techniques used in the medical domain, with deep learning methods taking
precedence over machine learning methods.
","With the emergence of big data, machine learning techniques
are used to learn, analyze, and extrapolate details in the medical research. This survey provides a comprehensive overview of
the ML techniques including support vector machines, K-means
clustering, decision trees, random forests, Naïve Bayes, K nearest
neighbors, neural networks, and convolution neural networks,
that are being used for various types of medical data and applications in the recent years. A systemic literature review was
conducted to search and select research articles from highly
reputed and relevant journals in the recent years. The papers
selected to survey and document in this survey paper are from
journals with a high SJR and google scholar h-5 index. Other
criteria like year of publication and number of citations were also
taken into consideration in the selection process.
Further this survey paper discusses about the current influx
of medical data and the challenges that medical data faces with
respect to its analysis. Medical data, due to its complex nature,
needs intricate analysis techniques. ML techniques, but more
recently DL models, have proven to understand medical image,
and multi-variate numerical data. More recently, neural networks
(and convolution neural networks) have been used for medical
image segmentation and classification like brain lesion segmentation, lymph node detection, classification of nodules in human
chest from CT scans, brain tumor classification, among many
other applications. For other medical applications like medical
diagnosis, and dementia prognosis, DL models are also being used
more. In fact, for tabular datasets from UCI and KEEL, we have
seen applications being shifted from traditional ML techniques
to DL. Using empirical evidence, we have concluded that, in the
recent years, DL has been preferred more (19 out of 33 papers)
by the researchers to work with medical data. We believe that
this survey paper can be of high importance for the researchers
to observe relevant details and trends about the current research
in the computer science and the medical research.","Role of machine learning in medical research: A survey.Medical research
Machine learning
Deep learning,Medical DataMachine learning is one of the essential and effective tools in analyzing highly complex medical
data. With vast amounts of medical data being generated, there is an urgent need to effectively
use this data to benefit the medical and health care sectors all across the world. This survey paper
presents a systematic literature review for the investigation of various machine learning techniques
used for numerous medical applications which are published in highly reputable venues in recent
years. Considering only the recent work, we are able to survey the current machine learning and deep
learning models that are being used for medical data. This literature review identifies a clear shift
of artificial intelligence techniques used in the medical domain, with deep learning methods taking
precedence over machine learning methods.
With the emergence of big data, machine learning techniques
are used to learn, analyze, and extrapolate details in the medical research. This survey provides a comprehensive overview of
the ML techniques including support vector machines, K-means
clustering, decision trees, random forests, Naïve Bayes, K nearest
neighbors, neural networks, and convolution neural networks,
that are being used for various types of medical data and applications in the recent years. A systemic literature review was
conducted to search and select research articles from highly
reputed and relevant journals in the recent years. The papers
selected to survey and document in this survey paper are from
journals with a high SJR and google scholar h-5 index. Other
criteria like year of publication and number of citations were also
taken into consideration in the selection process.
Further this survey paper discusses about the current influx
of medical data and the challenges that medical data faces with
respect to its analysis. Medical data, due to its complex nature,
needs intricate analysis techniques. ML techniques, but more
recently DL models, have proven to understand medical image,
and multi-variate numerical data. More recently, neural networks
(and convolution neural networks) have been used for medical
image segmentation and classification like brain lesion segmentation, lymph node detection, classification of nodules in human
chest from CT scans, brain tumor classification, among many
other applications. For other medical applications like medical
diagnosis, and dementia prognosis, DL models are also being used
more. In fact, for tabular datasets from UCI and KEEL, we have
seen applications being shifted from traditional ML techniques
to DL. Using empirical evidence, we have concluded that, in the
recent years, DL has been preferred more (19 out of 33 papers)
by the researchers to work with medical data. We believe that
this survey paper can be of high importance for the researchers
to observe relevant details and trends about the current research
in the computer science and the medical research.",Medical Data Analysis,"This survey paper provides a comprehensive overview of machine learning techniques used for various medical applications. The paper identifies a shift towards the use of deep learning methods over traditional machine learning methods for medical data analysis. The authors conducted a systematic literature review of highly reputable journals in recent years and identified support vector machines, decision trees, random forests, neural networks, and convolution neural networks as the most commonly used techniques. The paper also discusses the challenges associated with analyzing medical data and the increasing use of deep learning models for medical image segmentation, medical diagnosis, and dementia prognosis. Overall, this survey paper provides important insights and trends in the current research of computer science and medical research.",Medical Data Analysis,,Medical Data Analysis
230,"An evolutionary framework for machine learning applied to medical
data.","Logical rule induction, Data mining, Supervised learning, Evolutionary computation, Genetic programming, Ensemble classifier, Medical data.
.","Supervised learning problems can be faced by using a wide variety of approaches supported in
machine learning. In recent years there has been an increasing interest in using the evolutionary
computation paradigm as a search method for classifiers, helping the applied machine learning
technique. In this context, the knowledge representation in the form of logical rules has been one
of the most accepted machine learning approaches, because of its level of expressiveness. This paper
proposes an evolutionary framework for rule-based classifier induction. Our proposal introduces
genetic programming to build a search method for classification-rules (IF/THEN). From this approach,
we deal with problems such as, maximum rule length and rule intersection. The experiments have
been carried out on our domain of interest, medical data. The achieved results define a methodology
to follow in the learning method evaluation for knowledge discovery from medical data. Moreover, the
results compared to other methods have shown that our proposal can be very useful in data analysis
and classification coming from the medical domain.","Machine learning, as a practical matter, deals with the extraction of the right features from the data to build the right models
achieving the right tasks [6,60]. We have also seen that the most
common approaches used in machine learning are classification
and regression. In that sense, we can say that machine learning processes aim to render classification expressions as simple
as possible for humans to understand [67]. The creation and
assessment of intelligent machines whose learning is based on
experience as well as being able to generalize learned concepts
are the main challenges faced by machine learning. Such challenges have resulted in the problem of choosing the machine
learning method that best fits the data (or the problem at hand)
since there is not a winner method in all aspects.
In this context, this research has introduced: the main concepts treated in machine learning, some of the learning models
aimed to classification and the data domain of our interest, clinical data. So that, we have given the aim pursued by this research
in the analysis of clinical data. In this sense, we have described a
framework based on an evolutionary algorithm capable of inducing a rule set to correctly classify a set of patterns. The algorithm
implements a sequential covering technique and has been validated through well-known data sets. We have also provided
a formal analysis for the problem of assessing the quality of
the solutions in the presence of class intersection. The results
achieved are very promising and competitive when compared
with other results obtained from other approaches. Moreover, the
methodology followed in the result presentation and validation of
this proposal can be used by other learning approaches.
The following results have been core for this research. Firstly,
identification of fitness functions allowing our evolutionary
method to reach the best performance on accuracy and rule
intersection from generated classifiers. Secondly, we have also
identified the combination of fitness functions providing classifiers with the smallest number of rules. Thirdly, an ensemble
classifier model has been proposed and tested to face the rule
intersection problem. Finally and from the theoretical point of
view, we have computed an upper bound for the maximum rule
size in relation to the training data set. This contribution has
practical implications such as, improvement of the runtime and
memory used by the learning method. Therefore, all previous
contributions prove that the evolutionary framework for rule induction, introduced in this work can be very useful in the analysis
and knowledge discovery process from medical databases.","An evolutionary framework for machine learning applied to medical
data.Logical rule induction, Data mining, Supervised learning, Evolutionary computation, Genetic programming, Ensemble classifier, Medical data.
.Supervised learning problems can be faced by using a wide variety of approaches supported in
machine learning. In recent years there has been an increasing interest in using the evolutionary
computation paradigm as a search method for classifiers, helping the applied machine learning
technique. In this context, the knowledge representation in the form of logical rules has been one
of the most accepted machine learning approaches, because of its level of expressiveness. This paper
proposes an evolutionary framework for rule-based classifier induction. Our proposal introduces
genetic programming to build a search method for classification-rules (IF/THEN). From this approach,
we deal with problems such as, maximum rule length and rule intersection. The experiments have
been carried out on our domain of interest, medical data. The achieved results define a methodology
to follow in the learning method evaluation for knowledge discovery from medical data. Moreover, the
results compared to other methods have shown that our proposal can be very useful in data analysis
and classification coming from the medical domain.Machine learning, as a practical matter, deals with the extraction of the right features from the data to build the right models
achieving the right tasks [6,60]. We have also seen that the most
common approaches used in machine learning are classification
and regression. In that sense, we can say that machine learning processes aim to render classification expressions as simple
as possible for humans to understand [67]. The creation and
assessment of intelligent machines whose learning is based on
experience as well as being able to generalize learned concepts
are the main challenges faced by machine learning. Such challenges have resulted in the problem of choosing the machine
learning method that best fits the data (or the problem at hand)
since there is not a winner method in all aspects.
In this context, this research has introduced: the main concepts treated in machine learning, some of the learning models
aimed to classification and the data domain of our interest, clinical data. So that, we have given the aim pursued by this research
in the analysis of clinical data. In this sense, we have described a
framework based on an evolutionary algorithm capable of inducing a rule set to correctly classify a set of patterns. The algorithm
implements a sequential covering technique and has been validated through well-known data sets. We have also provided
a formal analysis for the problem of assessing the quality of
the solutions in the presence of class intersection. The results
achieved are very promising and competitive when compared
with other results obtained from other approaches. Moreover, the
methodology followed in the result presentation and validation of
this proposal can be used by other learning approaches.
The following results have been core for this research. Firstly,
identification of fitness functions allowing our evolutionary
method to reach the best performance on accuracy and rule
intersection from generated classifiers. Secondly, we have also
identified the combination of fitness functions providing classifiers with the smallest number of rules. Thirdly, an ensemble
classifier model has been proposed and tested to face the rule
intersection problem. Finally and from the theoretical point of
view, we have computed an upper bound for the maximum rule
size in relation to the training data set. This contribution has
practical implications such as, improvement of the runtime and
memory used by the learning method. Therefore, all previous
contributions prove that the evolutionary framework for rule induction, introduced in this work can be very useful in the analysis
and knowledge discovery process from medical databases.‘Yash Dagi etal 16) Sa eee oe OT ‘Some Waang == (9) Tao Sheng aL Weaen bl Rovey oF a TOT ‘Aaden Worg oat HT Marcus Nguyen tat 25] Komardesp etal [261 [canya Sorex erat a7] Sr pot ea Babes Map erat 1917",Medical Data Analysis,"This paper proposes an evolutionary framework for rule-based classifier induction to address supervised learning problems, particularly in medical data analysis. The framework uses genetic programming to build a search method for classification rules, dealing with problems such as maximum rule length and rule intersection. The experiments show promising results and competitive performance when compared to other approaches. The paper identifies fitness functions, a combination of fitness functions, an ensemble classifier model, and an upper bound for the maximum rule size as significant contributions to the framework. Overall, the proposed framework can be useful in the analysis and knowledge discovery process from medical databases.",Medical Data Analysis,‘Yash Dagi etal 16) Sa eee oe OT ‘Some Waang == (9) Tao Sheng aL Weaen bl Rovey oF a TOT ‘Aaden Worg oat HT Marcus Nguyen tat 25] Komardesp etal [261 [canya Sorex erat a7] Sr pot ea Babes Map erat 1917,Medical Data Analysis
231,"Prediction of the development of acute
kidney injury following cardiac surgery by
machine learning
.","Cardiac surgery, Acute kidney injury, Prediction.
.","Cardiac surgery–associated acute kidney injury (CSA-AKI) is a major complication that results in
increased morbidity and mortality after cardiac surgery. Most established prediction models are limited to the
analysis of nonlinear relationships and fail to fully consider intraoperative variables, which represent the acute
response to surgery. Therefore, this study utilized an artificial intelligence–based machine learning approach
thorough perioperative data-driven learning to predict CSA-AKI.
A total of 671 patients undergoing cardiac surgery from August 2016 to August 2018 were enrolled. AKI
following cardiac surgery was defined according to criteria from Kidney Disease: Improving Global Outcomes (KDIGO).
The variables used for analysis included demographic characteristics, clinical condition, preoperative biochemistry data,
preoperative medication, and intraoperative variables such as time-series hemodynamic changes. The machine
learning methods used included logistic regression, support vector machine (SVM), random forest (RF), extreme
gradient boosting (XGboost), and ensemble (RF + XGboost). The performance of these models was evaluated using
the area under the receiver operating characteristic curve (AUC). We also utilized SHapley Additive exPlanation (SHAP)
values to explain the prediction model.
Development of CSA-AKI was noted in 163 patients (24.3%) during the first postoperative week. Regarding the
efficacy of the single model that most accurately predicted the outcome, RF exhibited the greatest AUC (0.839, 95%
confidence interval [CI] 0.772–0.898), whereas the AUC (0.843, 95% CI 0.778–0.899) of ensemble model (RF + XGboost)
was even greater than that of the RF model alone. The top 3 most influential features in the RF importance matrix plot
were intraoperative urine output, units of packed red blood cells (pRBCs) transfused during surgery, and preoperative
hemoglobin level. The SHAP summary plot was used to illustrate the positive or negative effects of the top 20 features
attributed to the RF. We also used the SHAP dependence plot to explain how a single feature affects the output of the
RF prediction model.","In conclusion, we successfully applied the machine
learning method to predict AKI after cardiac surgery,
which can be used to determine risks after surgery. We
demonstrated that the intraoperative time-series and
other features are crucial for AKI prediction. Further
software development is ongoing for the real-time adjustment of AKI risks following cardiac surgery, which
in turn will optimize treatment to improve prognosis.","Prediction of the development of acute
kidney injury following cardiac surgery by
machine learning
.Cardiac surgery, Acute kidney injury, Prediction.
.Cardiac surgery–associated acute kidney injury (CSA-AKI) is a major complication that results in
increased morbidity and mortality after cardiac surgery. Most established prediction models are limited to the
analysis of nonlinear relationships and fail to fully consider intraoperative variables, which represent the acute
response to surgery. Therefore, this study utilized an artificial intelligence–based machine learning approach
thorough perioperative data-driven learning to predict CSA-AKI.
A total of 671 patients undergoing cardiac surgery from August 2016 to August 2018 were enrolled. AKI
following cardiac surgery was defined according to criteria from Kidney Disease: Improving Global Outcomes (KDIGO).
The variables used for analysis included demographic characteristics, clinical condition, preoperative biochemistry data,
preoperative medication, and intraoperative variables such as time-series hemodynamic changes. The machine
learning methods used included logistic regression, support vector machine (SVM), random forest (RF), extreme
gradient boosting (XGboost), and ensemble (RF + XGboost). The performance of these models was evaluated using
the area under the receiver operating characteristic curve (AUC). We also utilized SHapley Additive exPlanation (SHAP)
values to explain the prediction model.
Development of CSA-AKI was noted in 163 patients (24.3%) during the first postoperative week. Regarding the
efficacy of the single model that most accurately predicted the outcome, RF exhibited the greatest AUC (0.839, 95%
confidence interval [CI] 0.772–0.898), whereas the AUC (0.843, 95% CI 0.778–0.899) of ensemble model (RF + XGboost)
was even greater than that of the RF model alone. The top 3 most influential features in the RF importance matrix plot
were intraoperative urine output, units of packed red blood cells (pRBCs) transfused during surgery, and preoperative
hemoglobin level. The SHAP summary plot was used to illustrate the positive or negative effects of the top 20 features
attributed to the RF. We also used the SHAP dependence plot to explain how a single feature affects the output of the
RF prediction model.In conclusion, we successfully applied the machine
learning method to predict AKI after cardiac surgery,
which can be used to determine risks after surgery. We
demonstrated that the intraoperative time-series and
other features are crucial for AKI prediction. Further
software development is ongoing for the real-time adjustment of AKI risks following cardiac surgery, which
in turn will optimize treatment to improve prognosis.Telntroduction | (2, Methods —eeE—————s Support vector machines ~ Clustering. = Kemeans lustering Survey structure, Search Strategy EDecklon er | SIM a} Random torn ‘Selection Strategy = K nearest neighbors Naive bayes = Deep learning = Neural networks = Convolutional neural networks “Other recent medical applications of ML and DL Fig. 2. Structure of this article.Fig. 8 Support Vector Machine producing a linear solution [19]Computer Science Review 40 (2021) 100370 Dataset of human brain mages with traces of ‘cancer cells, Selecting the features. ‘Select xm, where xm is the set ot Selecting the features ‘important features to be used ode Cac Data Preperation Models being trained to Identity cancer co wrt. pixels from brain Images Train ML algorithm with xm and y J ‘Model selection and training Mode! Evaluation and ‘tuning Model Deployment",Medical Data Analysis,"
This study aimed to predict cardiac surgery-associated acute kidney injury (CSA-AKI) using artificial intelligence-based machine learning. The study included 671 patients who underwent cardiac surgery, and AKI was defined according to KDIGO criteria. The variables analyzed included demographic characteristics, clinical condition, preoperative biochemistry data, preoperative medication, and intraoperative variables such as time-series hemodynamic changes. Logistic regression, support vector machine, random forest, extreme gradient boosting, and ensemble models were used for analysis. The random forest model exhibited the greatest area under the receiver operating characteristic curve (AUC) of 0.839, and the ensemble model (random forest + extreme gradient boosting) had an even greater AUC of 0.843. The top 3 most influential features in the random forest importance matrix plot were intraoperative urine output, units of packed red blood cells transfused during surgery, and preoperative hemoglobin level. The study concluded that machine learning methods can successfully predict AKI after cardiac surgery, and intraoperative time-series and other features are crucial for AKI prediction. Further software development is ongoing for real-time adjustment of AKI risks following cardiac surgery.",Medical Data Analysis,"Telntroduction | (2, Methods —eeE—————s Support vector machines ~ Clustering. = Kemeans lustering Survey structure, Search Strategy EDecklon er | SIM a} Random torn ‘Selection Strategy = K nearest neighbors Naive bayes = Deep learning = Neural networks = Convolutional neural networks “Other recent medical applications of ML and DL Fig. 2. Structure of this article.Fig. 8 Support Vector Machine producing a linear solution [19]Computer Science Review 40 (2021) 100370 Dataset of human brain mages with traces of ‘cancer cells, Selecting the features. ‘Select xm, where xm is the set ot Selecting the features ‘important features to be used ode Cac Data Preperation Models being trained to Identity cancer co wrt. pixels from brain Images Train ML algorithm with xm and y J ‘Model selection and training Mode! Evaluation and ‘tuning Model Deployment",Medical Data Analysis
232,"A collaborative computer aided diagnosis (C-CAD) system with
eye-tracking, sparse attentional model, and deep learning.","Graph sparsification, Eye-tracking, Lung cancer screening, Prostate cancer screening, Attention
.","Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening
to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in
this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop
a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking
systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies
eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologists’ search efficiency
by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a
newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously.
The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability
of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework
is generalizable to more complex applications such as prostate cancer screening with multi-parametric
magnetic resonance imaging (mp-MRI).","Our study offers a new perspective on eye-tracking studies in
radiology because of its seamless integration into the real radiology
rooms and collaborative nature of image analysis methods. First,
the proposed system models the raw gaze data from eye-tracker as
a graph. Then, a novel attention based spectral graph sparsification
method is proposed to extract global search pattern of radiologist
as well as attention regions. Later, we propose a 3D deep multitask learning based CNN to perform diagnosis and segmentation
tasks jointly inside ROIs. Our proposed sparsification method reduced 90% of data within seconds while keeping mean the square
error under 0.1. The segmentation algorithm achieved the average
Dice Similarity Coefficient of 91% and classification accuracy for
nodule vs. non-nodule was 97%.
As can be interpreted from the lung screening experiment,
the less experienced participant had more crowded visual search
patterns and examined the most lung volume.
Our work has some limitations that should be noted. One of the
limitations is the lack of large amount of data for training more
sophisticated deep learning models as well as conducting scanpath
analysis with several other radiologists. In our extended study, we
plan to address this limitation and explore the validity of the proposed methods in different settings, incorporating the behavioral
patterns into screening experiments such as cognitive fatigue of
the radiologists.
For lung cancer screening with C-CAD, our system has the assumption that the radiologists are examining only lung regions and
the ROIs fall into the lung regions. If the radiologist starts focusing
on some other areas, outside the lungs, the segmentation results
might not be as desired, because of non-lung regions. To solve this
problem, one may include a simple segmentation step into the CCAD to restricts the ROI definition into the lungs only. However,
this procedure may affect analysis of incidental findings too.
In conclusion, CAD systems are often prone to high number
of false positives findings, which is one of the main drawbacks
in such systems. Missing tumors, especially in their early stages,
is also very common in screening. To increase the efficacy of the
lung cancer screening process, we propose a novel computer algorithm, namely collaborative CAD (C-CAD). Our proposed method
takes into account the gaze data of radiologists during the screening process, and incorporates this information into the CAD system
for better accuracy in reducing false positive findings in particular. In return, C-CAD has the capability of improving true positive
findings as well as reducing missing cases. With our proposed attention based graph sparsification method, qualitative comparison
and analysis of different radiologists’ visual search patterns (both
locally and globally) has become feasible. It is worth noting that
we are doing a local image analysis in the regions of interest and
not solving the detection problem per se. It is also worth noting
that, although deep learning has improved the accuracy and performance of detection methods, the efficiency of search is still a
big issue in state of the art detection methods (Khosravan and
Bagci, 2018a). Our proposed system is a promising step toward
combining the efficiency of searching strategy from the expert radiologist and accuracy of analysis from deep learning methods.
Since our framework is capable of integrating any image analysis
block as well as a detection block, having a detection task on top of
diagnosis and segmentation is a promising future direction which
enables the framework to handle cases that are missed by radiologists.","A collaborative computer aided diagnosis (C-CAD) system with
eye-tracking, sparse attentional model, and deep learning.Graph sparsification, Eye-tracking, Lung cancer screening, Prostate cancer screening, Attention
.Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening
to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in
this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop
a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking
systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies
eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologists’ search efficiency
by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a
newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously.
The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability
of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework
is generalizable to more complex applications such as prostate cancer screening with multi-parametric
magnetic resonance imaging (mp-MRI).Our study offers a new perspective on eye-tracking studies in
radiology because of its seamless integration into the real radiology
rooms and collaborative nature of image analysis methods. First,
the proposed system models the raw gaze data from eye-tracker as
a graph. Then, a novel attention based spectral graph sparsification
method is proposed to extract global search pattern of radiologist
as well as attention regions. Later, we propose a 3D deep multitask learning based CNN to perform diagnosis and segmentation
tasks jointly inside ROIs. Our proposed sparsification method reduced 90% of data within seconds while keeping mean the square
error under 0.1. The segmentation algorithm achieved the average
Dice Similarity Coefficient of 91% and classification accuracy for
nodule vs. non-nodule was 97%.
As can be interpreted from the lung screening experiment,
the less experienced participant had more crowded visual search
patterns and examined the most lung volume.
Our work has some limitations that should be noted. One of the
limitations is the lack of large amount of data for training more
sophisticated deep learning models as well as conducting scanpath
analysis with several other radiologists. In our extended study, we
plan to address this limitation and explore the validity of the proposed methods in different settings, incorporating the behavioral
patterns into screening experiments such as cognitive fatigue of
the radiologists.
For lung cancer screening with C-CAD, our system has the assumption that the radiologists are examining only lung regions and
the ROIs fall into the lung regions. If the radiologist starts focusing
on some other areas, outside the lungs, the segmentation results
might not be as desired, because of non-lung regions. To solve this
problem, one may include a simple segmentation step into the CCAD to restricts the ROI definition into the lungs only. However,
this procedure may affect analysis of incidental findings too.
In conclusion, CAD systems are often prone to high number
of false positives findings, which is one of the main drawbacks
in such systems. Missing tumors, especially in their early stages,
is also very common in screening. To increase the efficacy of the
lung cancer screening process, we propose a novel computer algorithm, namely collaborative CAD (C-CAD). Our proposed method
takes into account the gaze data of radiologists during the screening process, and incorporates this information into the CAD system
for better accuracy in reducing false positive findings in particular. In return, C-CAD has the capability of improving true positive
findings as well as reducing missing cases. With our proposed attention based graph sparsification method, qualitative comparison
and analysis of different radiologists’ visual search patterns (both
locally and globally) has become feasible. It is worth noting that
we are doing a local image analysis in the regions of interest and
not solving the detection problem per se. It is also worth noting
that, although deep learning has improved the accuracy and performance of detection methods, the efficiency of search is still a
big issue in state of the art detection methods (Khosravan and
Bagci, 2018a). Our proposed system is a promising step toward
combining the efficiency of searching strategy from the expert radiologist and accuracy of analysis from deep learning methods.
Since our framework is capable of integrating any image analysis
block as well as a detection block, having a detection task on top of
diagnosis and segmentation is a promising future direction which
enables the framework to handle cases that are missed by radiologists.Ctessitcaton Made asitcaton ModeCLASSIFIER ENSEMBLE MODEL TO IMPROVE RIM-GP (CLASSIFICATION MODEL-II) Distance-based Classifier ia) (pattern rule) TORTS. Intersection Classifier Distance to the rule boundary TORENT",Medical Data Analysis,"The text describes a new computer-aided diagnosis (CAD) system called Collaborative CAD (C-CAD) that uses eye-tracking technology and a deep learning algorithm to assist radiologists in reducing diagnostic errors in lung and prostate cancer screening. C-CAD unifies CAD and eye-tracking systems in realistic radiology room settings and incorporates radiologists' search efficiency by processing their gaze patterns. The proposed attention-based graph sparsification method extracts global search patterns and attention regions, while the 3D deep multi-task learning-based CNN performs diagnosis and segmentation tasks jointly inside ROIs. The system has been tested in lung and prostate cancer screening experiments with multiple radiologists, demonstrating its efficiency, accuracy, and applicability in real radiology room settings. The system has the potential to improve true positive findings and reduce missing cases, as well as reduce false positive findings. However, the system has limitations and requires further validation and exploration in different settings.",Medical Data Analysis,Ctessitcaton Made asitcaton ModeCLASSIFIER ENSEMBLE MODEL TO IMPROVE RIM-GP (CLASSIFICATION MODEL-II) Distance-based Classifier ia) (pattern rule) TORTS. Intersection Classifier Distance to the rule boundary TORENT,Medical Data Analysis
233,"Explainable deep learning based medical diagnostic system
.","medical diagnosis, Heterogeneous representation, Knowledge extraction, Query processing.
.","Recently, many researchers have conducted data mining over medical data to uncover hidden
patterns and use them to learn prediction models for clinical decision making and personalized
medicine. While such healthcare learning models can achieve encouraging results, they seldom
incorporate existing expert knowledge into their frameworks and hence prediction accuracy for
individual patients can still be improved. However, expert knowledge spans across various websites and multiple databases with heterogeneous representations and hence is difficult to harness
for improving learning models. In addition, patients' queries at medical consult websites are often
ambiguous in their specified terms and hence the returned responses may not contain the information they seek. To tackle these problems, we first design a knowledge extraction framework that
can generate an aggregated dataset to characterize diseases by integrating heterogeneous medical
data sources. Then, based on the integrated dataset, we propose an end-to-end deep learning based
medical diagnosis system (DL-MDS) to provide disease diagnosis for authorized users. We also
provide explanations for the diagnose results. Evaluations on real-world data demonstrate that our
proposed system achieves good performance on diseases diagnosis with a diverse set of patients’
queries.","In this paper, we have proposed a deep learning based medical diagnosis system (DL-MDS), which can be used to aid efficient clinical
care, where authorized users can conduct searches for medical diagnosis. One of our major contributions is that we have designed a
medical knowledge extraction framework to collect useful data from multiple sources, so that we can further generate medical diagnosis
models. The other is that we have generated a deep learning based model, which allows authorized users to conduct searches for medical
diagnosis based on their personalized queries and also provides explanations for the predicted results.
As for future work, instead of training separate query processing model and disease diagnosis model, we will integrate them and train
an end-to-end model. In addition, although the results indicate that a machine learning-based predictive model generated using
aggregated collected data could aid in clinical care, improvement can be made if we can include additional information such as lab tests
results since some diseases (e.g., cancer) cannot be diagnosed accurately only based on symptoms and risk factors. Furthermore, we also
intend to improve our DL-MDS to support multiple languages. For example, conduct NLP sentence processing and train the learning
models for Chinese users.
","Explainable deep learning based medical diagnostic system
.medical diagnosis, Heterogeneous representation, Knowledge extraction, Query processing.
.Recently, many researchers have conducted data mining over medical data to uncover hidden
patterns and use them to learn prediction models for clinical decision making and personalized
medicine. While such healthcare learning models can achieve encouraging results, they seldom
incorporate existing expert knowledge into their frameworks and hence prediction accuracy for
individual patients can still be improved. However, expert knowledge spans across various websites and multiple databases with heterogeneous representations and hence is difficult to harness
for improving learning models. In addition, patients' queries at medical consult websites are often
ambiguous in their specified terms and hence the returned responses may not contain the information they seek. To tackle these problems, we first design a knowledge extraction framework that
can generate an aggregated dataset to characterize diseases by integrating heterogeneous medical
data sources. Then, based on the integrated dataset, we propose an end-to-end deep learning based
medical diagnosis system (DL-MDS) to provide disease diagnosis for authorized users. We also
provide explanations for the diagnose results. Evaluations on real-world data demonstrate that our
proposed system achieves good performance on diseases diagnosis with a diverse set of patients’
queries.In this paper, we have proposed a deep learning based medical diagnosis system (DL-MDS), which can be used to aid efficient clinical
care, where authorized users can conduct searches for medical diagnosis. One of our major contributions is that we have designed a
medical knowledge extraction framework to collect useful data from multiple sources, so that we can further generate medical diagnosis
models. The other is that we have generated a deep learning based model, which allows authorized users to conduct searches for medical
diagnosis based on their personalized queries and also provides explanations for the predicted results.
As for future work, instead of training separate query processing model and disease diagnosis model, we will integrate them and train
an end-to-end model. In addition, although the results indicate that a machine learning-based predictive model generated using
aggregated collected data could aid in clinical care, improvement can be made if we can include additional information such as lab tests
results since some diseases (e.g., cancer) cannot be diagnosed accurately only based on symptoms and risk factors. Furthermore, we also
intend to improve our DL-MDS to support multiple languages. For example, conduct NLP sentence processing and train the learning
models for Chinese users.
‘PRBC transfusion during surgery (units) = 1.0 ‘ini = 0.5 samples = 468 value = [234.0, 234.0) class = Yes gini = 0.499 Lv Ejection Fraction =60.5 samples = 74 gini= 05 value’? samples = 141 un = (97 572, 96-204) value = (71.189, 68.336] Fig. 3 Simple decision tree model illustrating the classification of patients with (class = yes) and without (class = no) acute kidney injury. Each box has the following components: selected variables for classification, Gini index, number of samples classified to the box according to the previous variable, the average number of patients for each classification with S-cr0ss validation, and the majority of classes at the split node. Blue land orange represent the yes class and the no class, respectively, and the color densities increase when the Gini indexes decrease. Abbreviations: PREC, packed red blood call; BMI, body mass index; CCS, Canadian Cardiovascular Society; LV, left ventricular; HGB, hemoglobin",Medical Data Analysis,"The article discusses the limitations of existing healthcare learning models and the difficulty in incorporating expert knowledge from heterogeneous medical data sources. The authors propose a knowledge extraction framework that integrates multiple sources to generate an aggregated dataset for disease characterization, and an end-to-end deep learning based medical diagnosis system (DL-MDS) that provides disease diagnosis for authorized users with personalized queries and explanations for the results. The system shows promising results on real-world data, but future work includes integrating query processing and disease diagnosis models, incorporating additional information like lab test results, and improving the system to support multiple languages.",Medical Data Analysis,"‘PRBC transfusion during surgery (units) = 1.0 ‘ini = 0.5 samples = 468 value = [234.0, 234.0) class = Yes gini = 0.499 Lv Ejection Fraction =60.5 samples = 74 gini= 05 value’? samples = 141 un = (97 572, 96-204) value = (71.189, 68.336] Fig. 3 Simple decision tree model illustrating the classification of patients with (class = yes) and without (class = no) acute kidney injury. Each box has the following components: selected variables for classification, Gini index, number of samples classified to the box according to the previous variable, the average number of patients for each classification with S-cr0ss validation, and the majority of classes at the split node. Blue land orange represent the yes class and the no class, respectively, and the color densities increase when the Gini indexes decrease. Abbreviations: PREC, packed red blood call; BMI, body mass index; CCS, Canadian Cardiovascular Society; LV, left ventricular; HGB, hemoglobin",Medical Data Analysis
234,A survey of sentiment analysis in social media,"Sentiment analysis, Social media, Data mining, Survey.","Sentiments or opinions from social media provide the most up-to-date and inclusive information, due to the proliferation of social media and the low barrier for posting the message. Despite the growing importance of sentiment analysis, this area lacks a concise and systematic arrangement of prior efforts. It is essential to: (1) analyze its progress over the years, (2) provide an overview of the main advances achieved so far, and (3) outline remaining limitations. Several essential aspects, therefore, are addressed within the scope of this survey. On the one hand, this paper focuses on presenting typical methods from three different perspectives (task-oriented, granularity-oriented, methodology-oriented) in the area of sentiment analysis. Specifically, a large quantity of techniques and methods are categorized and compared. On the other hand, different types of data and advanced tools for research are introduced, as well as their limitations. On the basis of these materials, the essential prospects lying ahead for sentiment analysis are identified and discussed.","Sentiment analysis became a very popular research domain and a lot of excellent researches have been accomplished toward to this area. In this survey, a series of the state-of-the-art literatures have been reviewed. In particular, this survey categorized and classified sentiment analysis researches from multiple perspectives, i.e., task-oriented, granularity-oriented, and methodology-oriented. In addition, we explored different types of data and tools that can be used in sentiment analysis research and suggested their strength and limitations. Finally, we emphasized the prospects for future development, suggestions for possible extensions, and specially presented an overview of multimodal sentiment analysis (MSA). However, since MSA methods are, in general, not being used widely in sentiment analysis and related NLP research area, there are significant and timely opportunities for future research in the multi-disciplinary field of multimodal fusion. This survey established a common terminology across various researches, enabling people from different background knowledge to easily understand, and laid a foundation for advanced research in sentiment analysis. Current studies are intended to pave the way for further researches and development activities by identifying weaknesses and deriving guidelines toward a holistic approach","A survey of sentiment analysis in social mediaSentiment analysis, Social media, Data mining, Survey.Sentiments or opinions from social media provide the most up-to-date and inclusive information, due to the proliferation of social media and the low barrier for posting the message. Despite the growing importance of sentiment analysis, this area lacks a concise and systematic arrangement of prior efforts. It is essential to: (1) analyze its progress over the years, (2) provide an overview of the main advances achieved so far, and (3) outline remaining limitations. Several essential aspects, therefore, are addressed within the scope of this survey. On the one hand, this paper focuses on presenting typical methods from three different perspectives (task-oriented, granularity-oriented, methodology-oriented) in the area of sentiment analysis. Specifically, a large quantity of techniques and methods are categorized and compared. On the other hand, different types of data and advanced tools for research are introduced, as well as their limitations. On the basis of these materials, the essential prospects lying ahead for sentiment analysis are identified and discussed.Sentiment analysis became a very popular research domain and a lot of excellent researches have been accomplished toward to this area. In this survey, a series of the state-of-the-art literatures have been reviewed. In particular, this survey categorized and classified sentiment analysis researches from multiple perspectives, i.e., task-oriented, granularity-oriented, and methodology-oriented. In addition, we explored different types of data and tools that can be used in sentiment analysis research and suggested their strength and limitations. Finally, we emphasized the prospects for future development, suggestions for possible extensions, and specially presented an overview of multimodal sentiment analysis (MSA). However, since MSA methods are, in general, not being used widely in sentiment analysis and related NLP research area, there are significant and timely opportunities for future research in the multi-disciplinary field of multimodal fusion. This survey established a common terminology across various researches, enabling people from different background knowledge to easily understand, and laid a foundation for advanced research in sentiment analysis. Current studies are intended to pave the way for further researches and development activities by identifying weaknesses and deriving guidelines toward a holistic approachEdge ratio: 0.2 Dense graph ‘Sparse graphs with different sparsity levels Fig. 6. Results of applying proposed graph sparsification method on a 2D dense synthetic data. Edge ratio is the ratio of edges after applying the method to the original graph. : FP removal Image 2 sayaons - (or0b. for two classes) 7 Image size per 20x20x6- ct, cy Input (@D volume of size 40x40x6) 64 fiters @r2vfiters ——@t2 ters «= @SA ters in each layer ineachlayer _ineachlayer_—_ineach layer Ce Encoder Decoder ‘Segmentation (binary map) a8 Con ~ Batch B ‘Down sampling [fJ:Up sampling [sigmoid Fuly ConnectedNodes ° Edges Less important edges Important edges Gaze data ee -°@ —> + Rone Graph representation Non-Parametric clustering Graph sparsification (Step 1) (Step 2) (Step 3) Nodule vs. Non-Nodule <Gamm 2 3D segmented Fe ROL extraction MultTask CNN lesion (Step 5) (Step 4)",sentiment analysis,"The text discusses the importance of social media sentiments for providing up-to-date and inclusive information, and the lack of a systematic arrangement of prior efforts in sentiment analysis. The paper presents typical methods from three different perspectives, categorizing and comparing a large quantity of techniques and methods, introducing different types of data and advanced tools, and identifying the essential prospects for sentiment analysis. The survey establishes a common terminology and lays a foundation for advanced research in sentiment analysis. The paper emphasizes the prospects for future development, including multimodal sentiment analysis, which offers significant opportunities for research in the multi-disciplinary field of multimodal fusion.",Object and Sentiment Recognition,"Edge ratio: 0.2 Dense graph ‘Sparse graphs with different sparsity levels Fig. 6. Results of applying proposed graph sparsification method on a 2D dense synthetic data. Edge ratio is the ratio of edges after applying the method to the original graph. : FP removal Image 2 sayaons - (or0b. for two classes) 7 Image size per 20x20x6- ct, cy Input (@D volume of size 40x40x6) 64 fiters @r2vfiters ——@t2 ters «= @SA ters in each layer ineachlayer _ineachlayer_—_ineach layer Ce Encoder Decoder ‘Segmentation (binary map) a8 Con ~ Batch B ‘Down sampling [fJ:Up sampling [sigmoid Fuly ConnectedNodes ° Edges Less important edges Important edges Gaze data ee -°@ —> + Rone Graph representation Non-Parametric clustering Graph sparsification (Step 1) (Step 2) (Step 3) Nodule vs. Non-Nodule <Gamm 2 3D segmented Fe ROL extraction MultTask CNN lesion (Step 5) (Step 4)",Medical Data Analysis
235,"Semantic Indexing of Multimedia Content Using Visual, Audio, and Text Cues","sentiment indexing, query by keywords, multimodal information fusion, statistical modeling of multimedia, video indexing and retrieval, SVM, GMM, HMM, spoken document retrieval, video event detection, video TREC","We present a learning-based approach to the semantic indexing of multimedia content using cues derived from audio, visual, and text features. We approach the problem by developing a set of statistical models for a predefined lexicon. Novel concepts are then mapped in terms of the concepts in the lexicon. To achieve robust detection of concepts, we exploit features from multiple modalities, namely, audio, video, and text. Concept representations are modeled using Gaussian mixture models (GMM), hidden Markov models (HMM), and support vector machines (SVM). Models such as Bayesian networks and SVMs are used in a latefusion approach to model concepts that are not explicitly modeled in terms of features. Our experiments indicate promise in the proposed classification and fusion methodologies: our proposed fusion scheme achieves more than 10% relative improvement over the best unimodal concept detector.","The paper presented an overview of a trainable QBK system for labeling semantic-concepts within unrestricted video. Feasibility of the framework was demonstrated for the semantic-concept rocket launch, first for concept classification using information in single modalities and then for concept classification using information from multiple modalities. These experimental results, whilst preliminary, suffice to show that information from multiple modalities (visual, audio, speech, and potentially video text) can be successfully integrated to improve semantic labeling performance over that achieved by any single modality. There is considerable potential for improving the schemes described for atomic and high-level concept classification. Future research directions include the utility of multimodal fusion in atomic concept models (using, e.g., coupled HMMs or other dynamic Bayesian networks), and the appropriateness of shot-level rather than scene-level (or other) labeling schemes. Schemes must also be identified for automatically determining the low-level features (from a predefined set of possibilities) which are most appropriate for labeling atomic concepts and for determining atomic concepts (amongst the predefined set of possibilities) which are related to higher-level semantic-concepts. In addition, the scalability","Semantic Indexing of Multimedia Content Using Visual, Audio, and Text Cuessentiment indexing, query by keywords, multimodal information fusion, statistical modeling of multimedia, video indexing and retrieval, SVM, GMM, HMM, spoken document retrieval, video event detection, video TRECWe present a learning-based approach to the semantic indexing of multimedia content using cues derived from audio, visual, and text features. We approach the problem by developing a set of statistical models for a predefined lexicon. Novel concepts are then mapped in terms of the concepts in the lexicon. To achieve robust detection of concepts, we exploit features from multiple modalities, namely, audio, video, and text. Concept representations are modeled using Gaussian mixture models (GMM), hidden Markov models (HMM), and support vector machines (SVM). Models such as Bayesian networks and SVMs are used in a latefusion approach to model concepts that are not explicitly modeled in terms of features. Our experiments indicate promise in the proposed classification and fusion methodologies: our proposed fusion scheme achieves more than 10% relative improvement over the best unimodal concept detector.The paper presented an overview of a trainable QBK system for labeling semantic-concepts within unrestricted video. Feasibility of the framework was demonstrated for the semantic-concept rocket launch, first for concept classification using information in single modalities and then for concept classification using information from multiple modalities. These experimental results, whilst preliminary, suffice to show that information from multiple modalities (visual, audio, speech, and potentially video text) can be successfully integrated to improve semantic labeling performance over that achieved by any single modality. There is considerable potential for improving the schemes described for atomic and high-level concept classification. Future research directions include the utility of multimodal fusion in atomic concept models (using, e.g., coupled HMMs or other dynamic Bayesian networks), and the appropriateness of shot-level rather than scene-level (or other) labeling schemes. Schemes must also be identified for automatically determining the low-level features (from a predefined set of possibilities) which are most appropriate for labeling atomic concepts and for determining atomic concepts (amongst the predefined set of possibilities) which are related to higher-level semantic-concepts. In addition, the scalability",semantic indexing,"The article discusses a learning-based approach for semantic indexing of multimedia content using cues from audio, visual, and text features. Statistical models are developed for a predefined lexicon, and novel concepts are mapped to the concepts in the lexicon. Multiple modalities are used for robust concept detection, and models such as Gaussian mixture models, hidden Markov models, and support vector machines are employed. The article demonstrates that integrating information from multiple modalities can improve semantic labeling performance, and future research directions include improving atomic and high-level concept classification, determining appropriate low-level features, and identifying schemes for automatic determination of atomic concepts.",Object and Sentiment Recognition,,Sentiment Analysis
236,Feature selection and ensemble construction: A two-step method for aspect based sentiment analysis,Sentiment analysis Aspect term extraction Feature selection Ensemble Conditional random field Support vector machine Maximum entropy Particle swarm optimization,"In this paper we present a cascaded framework of feature selection and classifier ensemble using particle swarm optimization (PSO) for aspect based sentiment analysis. Aspect based sentiment analysis is performed in two steps, viz. aspect term extraction and sentiment classification. The pruned, compact set of features performs better compared to the baseline model that makes use of the complete set of features for aspect term extraction and sentiment classification. We further construct an ensemble based on PSO, and put it in cascade after the feature selection module. We use the features that are identified based on the properties of different classifiers and domains. As base learning algorithms we use three classifiers, namely Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM). Experiments for aspect term extraction and sentiment analysis on two different kinds of domains show the effectiveness of our proposed approach.","In this paper we have presented an efficient method for feature selection and ensemble learning for aspect based sentiment analysis. The algorithm is based on single objective PSO. As base learning algorithms we use CRF, SVM and ME. In the first step we determine the best feature sets for aspect term extraction and sentiment classification. This yields a set of solutions, each of which represents a particular feature combination. Based on certain criteria we choose the most promising solutions from the final population of PSO. The models developed with these feature combinations are combined together using a PSO based ensemble technique. The ensemble learner finds out the most eligible models, that when combined together, maximizes some classification quality measures like F-measure (for aspect term extraction) or accuracy (for sentiment classification). As the base learning algorithms we use three classifiers, namely ME, CRF and SVM. We have identified and implemented various lexical, syntactic or semantic level features for solving the problems. Experiments on the benchmark datasets of SemEval-2014 show our proposed techniques attain state-of-the-art performance for both aspect term extraction and sentiment classification. We compare the performance with the best performing systems that were developed using the same setups, several baseline models and the existing systems. In all the settings our proposed methods showed the effectiveness with reasonable performance increments. The key contributions of the current work can be summarized as below: (i). proposal of a two-step process for feature selection and ensemble learning using PSO; (ii). developing a PSO based feature selection and ensemble learning technique for the application like sentiment analysis; (iii). building domain-independent models for aspect based sentiment analysis that achieves state-of-the-art performance; (iv). finding how efficiently we can improve the classifiers’ performance if it is trained with the most relevant set of features (particularly for sentiment analysis). The current work focuses on single objective optimization technique, where we deal with only one objective function at a time. In future we would like to explore how multiobjective optimization that deals with the optimization of more than one objective function be effective for solving the problems. Future work also includes the studies of how the proposed system works for sentiment analysis in other domains and languages.","Feature selection and ensemble construction: A two-step method for aspect based sentiment analysisSentiment analysis Aspect term extraction Feature selection Ensemble Conditional random field Support vector machine Maximum entropy Particle swarm optimizationIn this paper we present a cascaded framework of feature selection and classifier ensemble using particle swarm optimization (PSO) for aspect based sentiment analysis. Aspect based sentiment analysis is performed in two steps, viz. aspect term extraction and sentiment classification. The pruned, compact set of features performs better compared to the baseline model that makes use of the complete set of features for aspect term extraction and sentiment classification. We further construct an ensemble based on PSO, and put it in cascade after the feature selection module. We use the features that are identified based on the properties of different classifiers and domains. As base learning algorithms we use three classifiers, namely Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM). Experiments for aspect term extraction and sentiment analysis on two different kinds of domains show the effectiveness of our proposed approach.In this paper we have presented an efficient method for feature selection and ensemble learning for aspect based sentiment analysis. The algorithm is based on single objective PSO. As base learning algorithms we use CRF, SVM and ME. In the first step we determine the best feature sets for aspect term extraction and sentiment classification. This yields a set of solutions, each of which represents a particular feature combination. Based on certain criteria we choose the most promising solutions from the final population of PSO. The models developed with these feature combinations are combined together using a PSO based ensemble technique. The ensemble learner finds out the most eligible models, that when combined together, maximizes some classification quality measures like F-measure (for aspect term extraction) or accuracy (for sentiment classification). As the base learning algorithms we use three classifiers, namely ME, CRF and SVM. We have identified and implemented various lexical, syntactic or semantic level features for solving the problems. Experiments on the benchmark datasets of SemEval-2014 show our proposed techniques attain state-of-the-art performance for both aspect term extraction and sentiment classification. We compare the performance with the best performing systems that were developed using the same setups, several baseline models and the existing systems. In all the settings our proposed methods showed the effectiveness with reasonable performance increments. The key contributions of the current work can be summarized as below: (i). proposal of a two-step process for feature selection and ensemble learning using PSO; (ii). developing a PSO based feature selection and ensemble learning technique for the application like sentiment analysis; (iii). building domain-independent models for aspect based sentiment analysis that achieves state-of-the-art performance; (iv). finding how efficiently we can improve the classifiers’ performance if it is trained with the most relevant set of features (particularly for sentiment analysis). The current work focuses on single objective optimization technique, where we deal with only one objective function at a time. In future we would like to explore how multiobjective optimization that deals with the optimization of more than one objective function be effective for solving the problems. Future work also includes the studies of how the proposed system works for sentiment analysis in other domains and languages.",feature selection,"The paper proposes a method for aspect-based sentiment analysis using a cascaded framework of feature selection and classifier ensemble using particle swarm optimization (PSO). The method involves a two-step process for feature selection and ensemble learning using PSO, and builds domain-independent models for aspect-based sentiment analysis that achieve state-of-the-art performance. The proposed approach uses three base learning algorithms, Maximum Entropy, Conditional Random Field, and Support Vector Machine. The results of experiments on benchmark datasets show the effectiveness of the proposed technique with reasonable performance increments. Future work includes exploring multi-objective optimization and testing the proposed system in other domains and languages.",Object and Sentiment Recognition,,Sentiment Analysis
237,A hybrid approach to the sentiment analysis problem at the sentence level,Sentiment analysis Semantic rules Fuzzy sets Unsupervised machine learning SentiWordNet Naïve Bayes Maximum entropy Computing with sentiments,"The objective of this article is to present a hybrid approach to the Sentiment Analysis problem at the sentence level. This new method uses natural language processing (NLP) essential techniques, a sentiment lexicon enhanced with the assistance of SentiWordNet, and fuzzy sets to estimate the semantic orientation polarity and its intensity for sentences, which provides a foundation for computing with sentiments. The proposed hybrid method is applied to three different data-sets and the results achieved are compared to those obtained using Naïve Bayes and Maximum Entropy techniques. It is demonstrated that the presented hybrid approach is more accurate and precise than both Naïve Bayes and Maximum Entropy techniques, when the latter are utilised in isolation. In addition, it is shown that when applied to datasets containing snippets, the proposed method performs similarly to state of the art techniques","In general, our proposed hybrid system works very well at the sentence level with a high level of accuracy (88.02%) and precision (84.24%) when the method is applied against twitter-like datasets. The fact that our hybrid system significantly improved the results obtained using Naïve Bayes (NB) and Maximum Entropy (ME), satisfies our initial hypothesis that a hybrid method using sentiment lexicons, NLP essential techniques and fuzzy sets, should be able to perform well. Another benefit of our proposed system is that we have managed to identify different strengths in the polarity degree of the input sentences with regard to the specific base-case (negative or positive). There is an interesting and intended effect of the introduction of the fuzzy sets component of our method. Those sentences classified in the ‘poor’ side of the polarity intensity spectrum are prime candidates to be considered rather neutral or objective sentences, instead of subjective (this functionality could be built into a subjectivity determination schema). Our expectation is that the quality of the content of SentiWordNet, or more recent tools like SenticNet [9], should continue to improve with time. Those enhancements will contribute to the betterment of our proposed hybrid solution as it will reflect positively in the quality of our opinion lexicon. In theory, as time passes, both SentiWordNet and our proposed opinion lexicon should become better and more complete. The ability to incorporate new terms into the current opinion lexicon in an expedite way is another benefit provided by our proposed solution. In essence, hybrid techniques can play an important role in the advancement of the Sentiment Analysis discipline by combining together a number of elements that tend to produce better results. Similar results, in terms of combining techniques effectively, have been reported by other researches [52]. By carefully analysing the sentences processed using the aforementioned classification methods, we are capable of extracting the main characteristics in those sentences that posed a problem for our classification method. If we group together the cases that our system have considered challenging to classify properly, we find the following traits: • The use of jargon, argot, idiom and/or lingo is hard to deal with, and sometimes it misguides the system in classifying opinions properly. • Imagery, metaphors, similes, sarcasm, humour and other language figures that rely on previous knowledge and/or context represent a challenge for our system. For future research, a starting point would be the work of Justo et al. [32]. • Double negation offers difficulties that we must continue to study and improve. • In the presence of very complex paragraphs the precision of our proposed hybrid method is negatively impacted In terms of further work, we believe there are a number of avenues that should be pursued in the short-term: • Create an automatic real-time interface, via API, with SentiWordNet or functionally equivalent tool (see next item) to search dynamically for polarity and PoS tagging updates for all terms of interest. • Investigate the possibility of using SenticNet [9] as a source of a more mature and comprehensive set of semantic attributes to enrich our own lexicon -or replace it. The concept-level sentiment analysis approach introduced by Poria et al. [52], sentic patterns, and its dependency-based rules for concept-level sentiment analysis could provide a broader semantic coverage than the one we currently enjoy with SentiWordNet. • Work on an algorithm to incorporate context in the tagging/parsing process and in the sentiment-determination modules in order to improve the ability of the system to deal with humour, metaphors, similes, sarcasm and irony (an initial approach could be the utilisation of context-sensitive grammars during the parsing process). • Port the proof-of-concept prototype code from Scheme to C, C++, C# or Python, in order to increase efficiency and integration capabilities. • Continue developing a computing with sentiments approach, using as a foundation the method presented in this article. • Introduce the use of Induced OWA operators [71] as an aggregation mechanism in the calculation of the semantic orientation of sentences. This type of operator could be used to ensure that","A hybrid approach to the sentiment analysis problem at the sentence levelSentiment analysis Semantic rules Fuzzy sets Unsupervised machine learning SentiWordNet Naïve Bayes Maximum entropy Computing with sentimentsThe objective of this article is to present a hybrid approach to the Sentiment Analysis problem at the sentence level. This new method uses natural language processing (NLP) essential techniques, a sentiment lexicon enhanced with the assistance of SentiWordNet, and fuzzy sets to estimate the semantic orientation polarity and its intensity for sentences, which provides a foundation for computing with sentiments. The proposed hybrid method is applied to three different data-sets and the results achieved are compared to those obtained using Naïve Bayes and Maximum Entropy techniques. It is demonstrated that the presented hybrid approach is more accurate and precise than both Naïve Bayes and Maximum Entropy techniques, when the latter are utilised in isolation. In addition, it is shown that when applied to datasets containing snippets, the proposed method performs similarly to state of the art techniquesIn general, our proposed hybrid system works very well at the sentence level with a high level of accuracy (88.02%) and precision (84.24%) when the method is applied against twitter-like datasets. The fact that our hybrid system significantly improved the results obtained using Naïve Bayes (NB) and Maximum Entropy (ME), satisfies our initial hypothesis that a hybrid method using sentiment lexicons, NLP essential techniques and fuzzy sets, should be able to perform well. Another benefit of our proposed system is that we have managed to identify different strengths in the polarity degree of the input sentences with regard to the specific base-case (negative or positive). There is an interesting and intended effect of the introduction of the fuzzy sets component of our method. Those sentences classified in the ‘poor’ side of the polarity intensity spectrum are prime candidates to be considered rather neutral or objective sentences, instead of subjective (this functionality could be built into a subjectivity determination schema). Our expectation is that the quality of the content of SentiWordNet, or more recent tools like SenticNet [9], should continue to improve with time. Those enhancements will contribute to the betterment of our proposed hybrid solution as it will reflect positively in the quality of our opinion lexicon. In theory, as time passes, both SentiWordNet and our proposed opinion lexicon should become better and more complete. The ability to incorporate new terms into the current opinion lexicon in an expedite way is another benefit provided by our proposed solution. In essence, hybrid techniques can play an important role in the advancement of the Sentiment Analysis discipline by combining together a number of elements that tend to produce better results. Similar results, in terms of combining techniques effectively, have been reported by other researches [52]. By carefully analysing the sentences processed using the aforementioned classification methods, we are capable of extracting the main characteristics in those sentences that posed a problem for our classification method. If we group together the cases that our system have considered challenging to classify properly, we find the following traits: • The use of jargon, argot, idiom and/or lingo is hard to deal with, and sometimes it misguides the system in classifying opinions properly. • Imagery, metaphors, similes, sarcasm, humour and other language figures that rely on previous knowledge and/or context represent a challenge for our system. For future research, a starting point would be the work of Justo et al. [32]. • Double negation offers difficulties that we must continue to study and improve. • In the presence of very complex paragraphs the precision of our proposed hybrid method is negatively impacted In terms of further work, we believe there are a number of avenues that should be pursued in the short-term: • Create an automatic real-time interface, via API, with SentiWordNet or functionally equivalent tool (see next item) to search dynamically for polarity and PoS tagging updates for all terms of interest. • Investigate the possibility of using SenticNet [9] as a source of a more mature and comprehensive set of semantic attributes to enrich our own lexicon -or replace it. The concept-level sentiment analysis approach introduced by Poria et al. [52], sentic patterns, and its dependency-based rules for concept-level sentiment analysis could provide a broader semantic coverage than the one we currently enjoy with SentiWordNet. • Work on an algorithm to incorporate context in the tagging/parsing process and in the sentiment-determination modules in order to improve the ability of the system to deal with humour, metaphors, similes, sarcasm and irony (an initial approach could be the utilisation of context-sensitive grammars during the parsing process). • Port the proof-of-concept prototype code from Scheme to C, C++, C# or Python, in order to increase efficiency and integration capabilities. • Continue developing a computing with sentiments approach, using as a foundation the method presented in this article. • Introduce the use of Induced OWA operators [71] as an aggregation mechanism in the calculation of the semantic orientation of sentences. This type of operator could be used to ensure that",sentiment analysis,"The article presents a hybrid approach to Sentiment Analysis using NLP techniques, a sentiment lexicon enhanced with SentiWordNet, and fuzzy sets to estimate polarity and intensity. The method is applied to three datasets and compared to Naïve Bayes and Maximum Entropy techniques, demonstrating higher accuracy and precision. The system is effective in identifying strengths in polarity degree and classifying neutral or objective sentences. However, the system faces challenges in dealing with jargon, metaphors, and sarcasm. Future work includes creating an automatic real-time interface, investigating the use of SenticNet, incorporating context, and developing a computing with sentiments approach.",Object and Sentiment Recognition,,Sentiment Analysis
238,Enhancing deep learning sentiment analysis with ensemble techniques in social applications,"Ensemble, Sentiment analysis.","Deep learning techniques for Sentiment Analysis have become very popular. They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.e., surface methods). Traditional surface approaches are based on complex manually extracted features, and this extraction process is a fundamental question in feature driven methods. These long-established approaches can yield strong baselines, and their predictive capabilities can be used in conjunction with the arising deep learning methods. In this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features. The contributions of this paper are sixfold. First, we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm. This classifier serves as a baseline to compare to subsequent results. Second, we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in Sentiment Analysis. Third, we also propose two models for combining both surface and deep features to merge information from several sources. Fourth, we introduce a taxonomy for classifying the different models found in the literature, as well as the ones we propose. Fifth, we conduct several experiments to compare the performance of these models with the deep learning baseline. For this, we use seven public datasets that were extracted from the microblogging and movie reviews domain. Finally, as a result, a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on F1-Score","This paper proposes several models where classic hand-crafted features are combined with automatically extracted embedding features, as well as the ensemble of analyzers that learn from these varied features. In order to classify these different approaches, we propose a taxonomy of ensembles of classifiers and features that is based on two dimensions. Furthermore, with the aim of evaluating the proposed models, a deep learning baseline is defined, and the classification performances of several ensembles are compared to the performance of the baseline. With the intention of conducting a comparative experimental study, six public datasets are used for the evaluation of all the models, as well as six public sentiment classifiers. Finally, we conduct an statistical analysis in order to empirically verify that combining information from varied fea- O. Araque et al. / Expert Systems With Applications 77 (2017) 236–246 245 tures and/or analyzers is an adequate way to surpass the sentiment classification performance. There were three main research questions that drove this work. The first question was whether there is a framework to characterize existing approaches in relation to the ensemble of traditional and deep techniques in sentiment analysis. To the best of our knowledge, our proposal of a taxonomy and the resulting implementations is the first work to tackle this problem for sentiment analysis. The second question was whether the sentiment analysis performance of a deep classifier can be leveraged when using the proposed ensemble of classifiers and features models. Observing the scores table and the Friedman ranks (Table 5), we see that the proposed models generally improve the performance of the baseline. This indicates that the combination of information from diverse sources such as surface features, generic and affect word vectors effectively improves the classifier’s results in sentiment analysis tasks. Lastly, we raised the concern of which of the proposed models stand out in the improvement of the deep sentiment analysis performance. In this regard, the statistical results point out the CEMMeL SGA and MSG+bigrams models as the best performing alternatives. As expected, these models effectively combine different sources of sentiment information, resulting in a significant improvement with respect to the baseline. We remark the MSG+bigrams model, as it does not involve an ensemble of many classifiers, but only a classifier that is trained with an ensemble of deep and surface features. To summarize, this work takes advantage of the ensemble of existing traditional sentiment classifiers, as well as the combination of generic, sentiment-trained word embeddings and manually crafted features. Nevertheless, Considering the results of this work, we believe that a possible line of work would be applying these models to the task of aspect based sentiment analysis, with the hope of improving the classification performance. Furthermore, we intend to extend the domain of the proposed models to other languages and even paradigms, like Emotion analysis.","Enhancing deep learning sentiment analysis with ensemble techniques in social applicationsEnsemble, Sentiment analysis.Deep learning techniques for Sentiment Analysis have become very popular. They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.e., surface methods). Traditional surface approaches are based on complex manually extracted features, and this extraction process is a fundamental question in feature driven methods. These long-established approaches can yield strong baselines, and their predictive capabilities can be used in conjunction with the arising deep learning methods. In this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features. The contributions of this paper are sixfold. First, we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm. This classifier serves as a baseline to compare to subsequent results. Second, we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in Sentiment Analysis. Third, we also propose two models for combining both surface and deep features to merge information from several sources. Fourth, we introduce a taxonomy for classifying the different models found in the literature, as well as the ones we propose. Fifth, we conduct several experiments to compare the performance of these models with the deep learning baseline. For this, we use seven public datasets that were extracted from the microblogging and movie reviews domain. Finally, as a result, a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on F1-ScoreThis paper proposes several models where classic hand-crafted features are combined with automatically extracted embedding features, as well as the ensemble of analyzers that learn from these varied features. In order to classify these different approaches, we propose a taxonomy of ensembles of classifiers and features that is based on two dimensions. Furthermore, with the aim of evaluating the proposed models, a deep learning baseline is defined, and the classification performances of several ensembles are compared to the performance of the baseline. With the intention of conducting a comparative experimental study, six public datasets are used for the evaluation of all the models, as well as six public sentiment classifiers. Finally, we conduct an statistical analysis in order to empirically verify that combining information from varied fea- O. Araque et al. / Expert Systems With Applications 77 (2017) 236–246 245 tures and/or analyzers is an adequate way to surpass the sentiment classification performance. There were three main research questions that drove this work. The first question was whether there is a framework to characterize existing approaches in relation to the ensemble of traditional and deep techniques in sentiment analysis. To the best of our knowledge, our proposal of a taxonomy and the resulting implementations is the first work to tackle this problem for sentiment analysis. The second question was whether the sentiment analysis performance of a deep classifier can be leveraged when using the proposed ensemble of classifiers and features models. Observing the scores table and the Friedman ranks (Table 5), we see that the proposed models generally improve the performance of the baseline. This indicates that the combination of information from diverse sources such as surface features, generic and affect word vectors effectively improves the classifier’s results in sentiment analysis tasks. Lastly, we raised the concern of which of the proposed models stand out in the improvement of the deep sentiment analysis performance. In this regard, the statistical results point out the CEMMeL SGA and MSG+bigrams models as the best performing alternatives. As expected, these models effectively combine different sources of sentiment information, resulting in a significant improvement with respect to the baseline. We remark the MSG+bigrams model, as it does not involve an ensemble of many classifiers, but only a classifier that is trained with an ensemble of deep and surface features. To summarize, this work takes advantage of the ensemble of existing traditional sentiment classifiers, as well as the combination of generic, sentiment-trained word embeddings and manually crafted features. Nevertheless, Considering the results of this work, we believe that a possible line of work would be applying these models to the task of aspect based sentiment analysis, with the hope of improving the classification performance. Furthermore, we intend to extend the domain of the proposed models to other languages and even paradigms, like Emotion analysis.",Deep Learning and Machine Learning,"The text discusses how deep learning techniques have become popular for sentiment analysis, providing better performance than traditional feature-based techniques. The paper proposes models that combine classic hand-crafted features with automatically extracted embedding features, as well as the ensemble of analyzers that learn from these varied features. The authors introduce a taxonomy for classifying the different models found in the literature, conduct several experiments to compare the performance of these models, and confirm that the proposed models surpass the deep learning baseline. The paper also raises possible lines of work for applying these models to aspect-based sentiment analysis and extending them to other languages and paradigms like emotion analysis.",Deep Learning and Machine Learning,,Sentiment Analysis
239,"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification","sentiment classification, structural correspodence learning(SCL).","Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.","Sentiment classification has seen a great deal of attention. Its application to many different domains of discourse makes it an ideal candidate for domain adaptation. This work addressed two important questions of domain adaptation. First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. (2006). We chose pivot features using not only common frequency among domains but also mutual information with the source labels. We also showed how to correct structural correspondence misalignments by using a small amount of labeled target domain data. Second, we provided a method for selecting those source domains most likely to adapt well to given target domains. The unsupervised A-distance measure of divergence between domains correlates well with loss due to adaptation. Thus we can use the Adistance to select source domains to label which will give low target domain error. In the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking. We are also actively searching for a larger and more varied set of domains on which to test our techniques.","Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classificationsentiment classification, structural correspodence learning(SCL).Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.Sentiment classification has seen a great deal of attention. Its application to many different domains of discourse makes it an ideal candidate for domain adaptation. This work addressed two important questions of domain adaptation. First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. (2006). We chose pivot features using not only common frequency among domains but also mutual information with the source labels. We also showed how to correct structural correspondence misalignments by using a small amount of labeled target domain data. Second, we provided a method for selecting those source domains most likely to adapt well to given target domains. The unsupervised A-distance measure of divergence between domains correlates well with loss due to adaptation. Thus we can use the Adistance to select source domains to label which will give low target domain error. In the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking. We are also actively searching for a larger and more varied set of domains on which to test our techniques.",sentiment classification,"The text discusses the challenge of automatic sentiment classification in different domains and the impracticality of annotating corpora for every possible domain. The article presents a study on domain adaptation for sentiment classifiers, particularly for online reviews of various products. The study includes the extension of the structural correspondence learning (SCL) algorithm for sentiment classification, which improves the adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. The article also identifies a measure of domain similarity that correlates with the potential for adaptation of a classifier from one domain to another, which can be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. The authors aim to include recent advances in sentiment classification and address the more realistic problem of ranking, as well as testing their techniques on a larger and more varied set of domains.",Object and Sentiment Recognition,,Sentiment Analysis
240,Effectiveness of a short audiovisual emotion recognition training program in adults,"emotion recognition ability, audiovisual training, non-clinical adults, lifespan sample, multiple sensory channels, transfer effects, older adults.","The ability to recognize emotions from others’ nonverbal behavior (emotion recognition ability, ERA) is crucial to successful social functioning. However, currently no self-administered ERA training for non-clinical adults covering multiple sensory channels exists. We conducted four studies in a lifespan sample of participants in the laboratory and online (total N = 531) to examine the effectiveness of a short computer-based training for 14 different emotions using audiovisual clips of emotional expressions. Results showed that overall, young and middle-aged participants that had received the training scored significantly higher on facial, vocal, and audiovisual emotion recognition than the control groups. The training effect for audiovisual ERA persisted over 4 weeks. In older adults (59–90 years), however, the training had no effect. The new, brief training could be useful in applied settings such as professional training, at least for younger and middle-aged adults. In older adults, improving ERA might require a longer and more interactive intervention.","The present research is the first to validate a short, self-administered training program intended to improve emotion recognition ability (ERA) in the face, voice, and body across a wide range of positive and negative emotions. The training was found to increase ERA in younger and middle-aged adults when completed in the laboratory and online, and the effects persisted over at least four weeks. The training was similarly effective in both men and women and demonstrated transfer effects on different tests. However, the training did not improve ERA in older adults. The study also suggested that interventions focusing on many emotions and on multiple sensory modalities simultaneously might be more ecologically valid than more specialized interventions. The present studies have several limitations that need to be addressed in future research, such as investigating the effects of the training on real-life behaviors and outcomes, determining individual baseline ERA, and developing more effective trainings for improving ERA in older adults.","Effectiveness of a short audiovisual emotion recognition training program in adultsemotion recognition ability, audiovisual training, non-clinical adults, lifespan sample, multiple sensory channels, transfer effects, older adults.The ability to recognize emotions from others’ nonverbal behavior (emotion recognition ability, ERA) is crucial to successful social functioning. However, currently no self-administered ERA training for non-clinical adults covering multiple sensory channels exists. We conducted four studies in a lifespan sample of participants in the laboratory and online (total N = 531) to examine the effectiveness of a short computer-based training for 14 different emotions using audiovisual clips of emotional expressions. Results showed that overall, young and middle-aged participants that had received the training scored significantly higher on facial, vocal, and audiovisual emotion recognition than the control groups. The training effect for audiovisual ERA persisted over 4 weeks. In older adults (59–90 years), however, the training had no effect. The new, brief training could be useful in applied settings such as professional training, at least for younger and middle-aged adults. In older adults, improving ERA might require a longer and more interactive intervention.The present research is the first to validate a short, self-administered training program intended to improve emotion recognition ability (ERA) in the face, voice, and body across a wide range of positive and negative emotions. The training was found to increase ERA in younger and middle-aged adults when completed in the laboratory and online, and the effects persisted over at least four weeks. The training was similarly effective in both men and women and demonstrated transfer effects on different tests. However, the training did not improve ERA in older adults. The study also suggested that interventions focusing on many emotions and on multiple sensory modalities simultaneously might be more ecologically valid than more specialized interventions. The present studies have several limitations that need to be addressed in future research, such as investigating the effects of the training on real-life behaviors and outcomes, determining individual baseline ERA, and developing more effective trainings for improving ERA in older adults.",Person recognition,"The study conducted four experiments to examine the effectiveness of a self-administered computer-based training for emotion recognition ability (ERA) in non-clinical adults. The training covered 14 different emotions and multiple sensory channels. The study found that the training improved ERA in younger and middle-aged adults and that the effects persisted for at least four weeks. However, the training had no effect on older adults. The study suggests that interventions focusing on many emotions and multiple sensory modalities might be more ecologically valid. The study has limitations that need to be addressed in future research.",Object and Sentiment Recognition,,Sentiment Analysis
241,Facial Emotion Recognition in Real Time,"real-time, facial expression, emotion recognition, transfer learning, dataset, accuracy, running average, Python","We have developed a convolutional neural network for
classifying human emotions from dynamic facial expressions in real time. We use transfer learning on the fullyconnected layers of an existing convolutional neural network which was pretrained for human emotion classification. A variety of datasets, as well as our own unique
image dataset, is used to train the model. An overall
training accuracy of 90.7% and test accuracy of 57.1%
is achieved. Finally, a live video stream connected to a
face detector feeds images to the neural network. The network subsequently classifies an arbitrary number of faces
per image simultaneously in real time, wherein appropriate emojis are superimposed over the subjects’ faces
(https://youtu.be/MDHtzOdnSgA). The results
demonstrate the feasibility of implementing neural networks
in real time to detect human emotion","The goal of this project was to implement real-time facial
emotion recognition. Using our custom trained VGG S network with a face-detector provided by OpenCV, we successfully implemented an application wherein an emoji indicating one of six expressions (anger, fear, neutral, happy, sad,
surpirse) is superimposed over a user’s face in real time (see
https://youtu.be/MDHtzOdnSgA for a demonstration). Some of the results are shown in Figure 14.
While we achieved a successful implementation, significant improvements can be made by addressing several key issues. First, a much larger dataset should be designed to
improve the model’s generality. While we achieved > 90%
accuracy in laboratory conditions (perfect lighting, camera
at eye level, subject facing camera with an exaggerated expression), any deviation from that caused the accuracy to
fall significantly. In particular, any shadow on a subject’s
face would cause an incorrect classification of ‘angry’. In
addition, the webcam needs to be level to the subjects’ faces
for accurate classification. Augmenting our home-brewed
dataset to include off-center faces could have addressed this
problem. Heavier pre-processing of the data would have
certainly improved test-time accuracy. For example, adjusting the brightness to the same level on all the images
might have removed the requirement for providing jittered
input images. Also, fully training a network other than
VGG S might yield substantial improvements in computation speed, since VGG S is relatively slow.
As mentioned previously, a particularly difficult aspect
of real-time recognition is deciding how to classify transition frames from neutral to fully formed expressions of
emotion. One viable solution is to use a running average of
the top classes reported by each frame which would ameliorate the problem of noise/errors caused by dynamic expressions [15]. Unfortunately, the relatively slow frame-rate of
our demonstration made this solution untenable. Regardless, our implementation appeared to classify a subject’s
emotion reliably. Future implementations that run at higher
frame-rates would require a running average.
Our project was implemented under Python 2.7 and the
source code can be downloaded from a github repository","Facial Emotion Recognition in Real Timereal-time, facial expression, emotion recognition, transfer learning, dataset, accuracy, running average, PythonWe have developed a convolutional neural network for
classifying human emotions from dynamic facial expressions in real time. We use transfer learning on the fullyconnected layers of an existing convolutional neural network which was pretrained for human emotion classification. A variety of datasets, as well as our own unique
image dataset, is used to train the model. An overall
training accuracy of 90.7% and test accuracy of 57.1%
is achieved. Finally, a live video stream connected to a
face detector feeds images to the neural network. The network subsequently classifies an arbitrary number of faces
per image simultaneously in real time, wherein appropriate emojis are superimposed over the subjects’ faces
(https://youtu.be/MDHtzOdnSgA). The results
demonstrate the feasibility of implementing neural networks
in real time to detect human emotionThe goal of this project was to implement real-time facial
emotion recognition. Using our custom trained VGG S network with a face-detector provided by OpenCV, we successfully implemented an application wherein an emoji indicating one of six expressions (anger, fear, neutral, happy, sad,
surpirse) is superimposed over a user’s face in real time (see
https://youtu.be/MDHtzOdnSgA for a demonstration). Some of the results are shown in Figure 14.
While we achieved a successful implementation, significant improvements can be made by addressing several key issues. First, a much larger dataset should be designed to
improve the model’s generality. While we achieved > 90%
accuracy in laboratory conditions (perfect lighting, camera
at eye level, subject facing camera with an exaggerated expression), any deviation from that caused the accuracy to
fall significantly. In particular, any shadow on a subject’s
face would cause an incorrect classification of ‘angry’. In
addition, the webcam needs to be level to the subjects’ faces
for accurate classification. Augmenting our home-brewed
dataset to include off-center faces could have addressed this
problem. Heavier pre-processing of the data would have
certainly improved test-time accuracy. For example, adjusting the brightness to the same level on all the images
might have removed the requirement for providing jittered
input images. Also, fully training a network other than
VGG S might yield substantial improvements in computation speed, since VGG S is relatively slow.
As mentioned previously, a particularly difficult aspect
of real-time recognition is deciding how to classify transition frames from neutral to fully formed expressions of
emotion. One viable solution is to use a running average of
the top classes reported by each frame which would ameliorate the problem of noise/errors caused by dynamic expressions [15]. Unfortunately, the relatively slow frame-rate of
our demonstration made this solution untenable. Regardless, our implementation appeared to classify a subject’s
emotion reliably. Future implementations that run at higher
frame-rates would require a running average.
Our project was implemented under Python 2.7 and the
source code can be downloaded from a github repository",Emotion detection using normal face,"The article discusses the development of a convolutional neural network (CNN) for real-time facial emotion recognition, which was achieved using transfer learning on a pre-trained CNN for human emotion classification. The model achieved an overall training accuracy of 90.7% and test accuracy of 57.1% and was able to classify an arbitrary number of faces simultaneously in real time. The study demonstrated the feasibility of using neural networks for emotion detection in real time, but identified several key issues that need to be addressed to improve accuracy, including the need for a larger and more diverse dataset, heavier pre-processing of the data, and developing a running average solution for classifying transition frames. The source code for the project is available on Github.



",Object and Sentiment Recognition,,Sentiment Analysis
242,Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability,"Virtual reality, facial emotion recognition, dynamic virtual faces, schizophrenia, social skills, psychotherapy, embodied conversational agents, cyberinterventions, emotional and social training.","The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used facial stimuli are photographs. Although their use has been shown to be valid, photographs are unable to capture the dynamic aspects of human expressions. This limitation can be overcome by creating virtual agents with feasible expressed emotions. The main objective of the present study was to create a new set of dynamic virtual faces with high realism that could be integrated into a virtual reality (VR) cyberintervention to train people with schizophrenia in the full repertoire of social skills. A set of highly realistic virtual faces was created based on the Facial Action Coding System. Facial movement animation was also included so as to mimic the dynamism of human facial expressions. Consecutive healthy participants (n = 98) completed a facial emotion recognition task using both natural faces (photographs) and virtual agents expressing five basic emotions plus a neutral one. Repeated-measures ANOVA revealed no significant difference in participants’ accuracy of recognition between the two presentation conditions. However, anger was better recognized in the VR images, and disgust was better recognized in photographs. Age, the participant’s gender and reaction times were also explored. Implications of the use of virtual agents with realistic human expressions in cyberinterventions are discussed.","In sum, the results show that VR faces are as valid as standardized natural faces for accurately recreating human-like facial expressions of emotions.

The present findings demonstrate that VR environments enable the introduction of virtual agents that can be created using feasible faces and other interpersonal communication cues (sounds, laughs, affect prosody), and this has obvious applications in relation to emotional and social training. As the virtual character’s morphology, external appearance and movements in time and space can be changed and controlled externally, virtual agents and environments have become a powerful tool for experimental psychology (Loomis et al. 1999). The findings also have several clinical implications, since the advances being made through virtual reality technology can help to overcome some of the limitations associated with the use of static faces. In addition, this technology is able to simulate social encounters: for example, using virtual reality with a correct pattern of emotional characters could generate alternative social environments. Virtual reality also allows therapists to control and manipulate the avatar’s behaviour in order to provide immediate feedback to people suffering from emotional and behavioural disorders, such as schizophrenia or social anxiety disorder. Our research group is currently using the task described here with psychotic patients, and the preliminary results show that virtual faces are a useful tool not only for assessing the ability of patients with schizophrenia to recognize emotions, but also for studying the clinical, cognitive and personality characteristics which underlie this impairment (Gutiérrez-Maldonado et al. 2012).

Several limitations of the present study need to be acknowledged. Firstly, it has to be stressed in this context that several studies have shown that dynamic and static facial expressions are processed differently neurologically (Collignon et al. 2008). This has to be considered an important limitation when comparing both facial stimuli. However, to the best of our knowledge, no well-validated set of dynamic video sequences is available to the research community that permits a reliable comparison with the virtual faces. Furthermore, the purpose of the present study was to examine the feasibility of the created virtual faces as well as their evaluation as perceived social agents, rather than the exploration of the neural mechanism involved in the facial emotion capability. Secondly, the predominance of women in the sample may constitute a weakness, even though no significant effect of participants’ gender was found. Thirdly, although culture is known to be an important variable when studying emotion (Russell 1994), this aspect was not included in the analysis of the present data. However, some authors in this field consider that social cognition needs to be carefully differentiated in relation to cultural influences and should be seen as a “universal” category enabling humans to develop language, culture or technology (Tomasello et al. 2005). Finally, only six expressions of emotions were included in the study, and some authors consider that these correspond to only a small number of basic emotions with prototypical expressions produced by neuromotor programs (Scherer and Ellgring 2007).

Despite these limitations, the present study adds to the existing literature on the application of virtual reality in the context of research, assessment and psychotherapy for psychiatric and psychological disorders. As mentioned before, the next stage of our research will be to include the virtual agents as social virtual entities in our VR social skills training programme: the Soskitrain (for a further description of the programme, see Rus-Calafell et al. 2012). Further studies are now needed to establish more fully the potential of embodied conversational agents in the design of new treatment programmes and cyberinterventions for disorders of this kind.","Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition abilityVirtual reality, facial emotion recognition, dynamic virtual faces, schizophrenia, social skills, psychotherapy, embodied conversational agents, cyberinterventions, emotional and social training.The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used facial stimuli are photographs. Although their use has been shown to be valid, photographs are unable to capture the dynamic aspects of human expressions. This limitation can be overcome by creating virtual agents with feasible expressed emotions. The main objective of the present study was to create a new set of dynamic virtual faces with high realism that could be integrated into a virtual reality (VR) cyberintervention to train people with schizophrenia in the full repertoire of social skills. A set of highly realistic virtual faces was created based on the Facial Action Coding System. Facial movement animation was also included so as to mimic the dynamism of human facial expressions. Consecutive healthy participants (n = 98) completed a facial emotion recognition task using both natural faces (photographs) and virtual agents expressing five basic emotions plus a neutral one. Repeated-measures ANOVA revealed no significant difference in participants’ accuracy of recognition between the two presentation conditions. However, anger was better recognized in the VR images, and disgust was better recognized in photographs. Age, the participant’s gender and reaction times were also explored. Implications of the use of virtual agents with realistic human expressions in cyberinterventions are discussed.In sum, the results show that VR faces are as valid as standardized natural faces for accurately recreating human-like facial expressions of emotions.

The present findings demonstrate that VR environments enable the introduction of virtual agents that can be created using feasible faces and other interpersonal communication cues (sounds, laughs, affect prosody), and this has obvious applications in relation to emotional and social training. As the virtual character’s morphology, external appearance and movements in time and space can be changed and controlled externally, virtual agents and environments have become a powerful tool for experimental psychology (Loomis et al. 1999). The findings also have several clinical implications, since the advances being made through virtual reality technology can help to overcome some of the limitations associated with the use of static faces. In addition, this technology is able to simulate social encounters: for example, using virtual reality with a correct pattern of emotional characters could generate alternative social environments. Virtual reality also allows therapists to control and manipulate the avatar’s behaviour in order to provide immediate feedback to people suffering from emotional and behavioural disorders, such as schizophrenia or social anxiety disorder. Our research group is currently using the task described here with psychotic patients, and the preliminary results show that virtual faces are a useful tool not only for assessing the ability of patients with schizophrenia to recognize emotions, but also for studying the clinical, cognitive and personality characteristics which underlie this impairment (Gutiérrez-Maldonado et al. 2012).

Several limitations of the present study need to be acknowledged. Firstly, it has to be stressed in this context that several studies have shown that dynamic and static facial expressions are processed differently neurologically (Collignon et al. 2008). This has to be considered an important limitation when comparing both facial stimuli. However, to the best of our knowledge, no well-validated set of dynamic video sequences is available to the research community that permits a reliable comparison with the virtual faces. Furthermore, the purpose of the present study was to examine the feasibility of the created virtual faces as well as their evaluation as perceived social agents, rather than the exploration of the neural mechanism involved in the facial emotion capability. Secondly, the predominance of women in the sample may constitute a weakness, even though no significant effect of participants’ gender was found. Thirdly, although culture is known to be an important variable when studying emotion (Russell 1994), this aspect was not included in the analysis of the present data. However, some authors in this field consider that social cognition needs to be carefully differentiated in relation to cultural influences and should be seen as a “universal” category enabling humans to develop language, culture or technology (Tomasello et al. 2005). Finally, only six expressions of emotions were included in the study, and some authors consider that these correspond to only a small number of basic emotions with prototypical expressions produced by neuromotor programs (Scherer and Ellgring 2007).

Despite these limitations, the present study adds to the existing literature on the application of virtual reality in the context of research, assessment and psychotherapy for psychiatric and psychological disorders. As mentioned before, the next stage of our research will be to include the virtual agents as social virtual entities in our VR social skills training programme: the Soskitrain (for a further description of the programme, see Rus-Calafell et al. 2012). Further studies are now needed to establish more fully the potential of embodied conversational agents in the design of new treatment programmes and cyberinterventions for disorders of this kind.",Emotion detection using normal face,"The article discusses a study that aimed to create a set of highly realistic virtual faces for use in virtual reality (VR) cyberinterventions to train people with schizophrenia in social skills. The study found that the VR faces were as valid as standardized natural faces for accurately recreating human-like facial expressions of emotions. The use of virtual agents and environments has several clinical implications, and advances in virtual reality technology can help to overcome some of the limitations associated with the use of static faces. However, there are limitations to the study, and further research is needed to establish the potential of virtual agents in the design of new treatment programs and cyberinterventions for psychiatric and psychological disorders.",Object and Sentiment Recognition,,Sentiment Analysis
243,Effects of facial emotion recognition remediation on visual scanning of novel face stimuli,"emotion recognition, schizophrenia, visual scanning, facial expressions, Ekman's Micro-Expression Training Tool, gender, foveal attention, cognitive functioning, interpersonal functioning","Previous research shows that emotion recognition in schizophrenia can be improved with targeted remediation that draws attention to important facial features (eyes, nose, mouth). Moreover, the effects of training have been shown to last for up to one month after training. The aim of this study was to investigate whether improved emotion recognition of novel faces is associated with concomitant changes in visual scanning of these same novel facial expressions. Thirty-nine participants with schizophrenia received emotion recognition training using Ekman's Micro-Expression Training Tool (METT), with emotion recognition and visual scanpath (VSP) recordings to face stimuli collected simultaneously. Baseline ratings of interpersonal and cognitive functioning were also collected from all participants. Post-METT training, participants showed changes in foveal attention to the features of facial expressions of emotion not used in METT training, which were generally consistent with the information about important features from the METT. In particular, there were changes in how participants looked at the features of facial expressions of emotion surprise, disgust, fear, happiness, and neutral, demonstrating that improved emotion recognition is paralleled by changes in the way participants with schizophrenia viewed novel facial expressions of emotion. However, there were overall decreases in foveal attention to sad and neutral faces that indicate more intensive instruction might be needed for these faces during training. Most importantly, the evidence shows that participant gender may affect training outcomes.","The aim of this study was to determine whether the improved emotion recognition of novel face stimuli we found in our previous ERT study (Marsh et al., 2010), would be associated with concomitant changes in simultaneously recorded VSPs in schizophrenia. Specifically, we examined whether ERT with the METT produced changes in fixations on important facial features (eyes, nose, mouth and salient features) of novel faces, and whether these changes would be associated with improved emotion","Effects of facial emotion recognition remediation on visual scanning of novel face stimuliemotion recognition, schizophrenia, visual scanning, facial expressions, Ekman's Micro-Expression Training Tool, gender, foveal attention, cognitive functioning, interpersonal functioningPrevious research shows that emotion recognition in schizophrenia can be improved with targeted remediation that draws attention to important facial features (eyes, nose, mouth). Moreover, the effects of training have been shown to last for up to one month after training. The aim of this study was to investigate whether improved emotion recognition of novel faces is associated with concomitant changes in visual scanning of these same novel facial expressions. Thirty-nine participants with schizophrenia received emotion recognition training using Ekman's Micro-Expression Training Tool (METT), with emotion recognition and visual scanpath (VSP) recordings to face stimuli collected simultaneously. Baseline ratings of interpersonal and cognitive functioning were also collected from all participants. Post-METT training, participants showed changes in foveal attention to the features of facial expressions of emotion not used in METT training, which were generally consistent with the information about important features from the METT. In particular, there were changes in how participants looked at the features of facial expressions of emotion surprise, disgust, fear, happiness, and neutral, demonstrating that improved emotion recognition is paralleled by changes in the way participants with schizophrenia viewed novel facial expressions of emotion. However, there were overall decreases in foveal attention to sad and neutral faces that indicate more intensive instruction might be needed for these faces during training. Most importantly, the evidence shows that participant gender may affect training outcomes.The aim of this study was to determine whether the improved emotion recognition of novel face stimuli we found in our previous ERT study (Marsh et al., 2010), would be associated with concomitant changes in simultaneously recorded VSPs in schizophrenia. Specifically, we examined whether ERT with the METT produced changes in fixations on important facial features (eyes, nose, mouth and salient features) of novel faces, and whether these changes would be associated with improved emotion",Emotion detection using normal face,"The study aimed to investigate whether improved emotion recognition of novel faces is associated with concomitant changes in visual scanning of these same novel facial expressions in participants with schizophrenia who received emotion recognition training using Ekman's Micro-Expression Training Tool (METT). The results showed that participants had changes in foveal attention to the features of facial expressions of emotion not used in METT training, and improved emotion recognition was paralleled by changes in the way participants viewed novel facial expressions of emotion. However, there were overall decreases in foveal attention to sad and neutral faces that indicate more intensive instruction might be needed for these faces during training. Participant gender may also affect training outcomes.",Object and Sentiment Recognition,,Object Recognition
244,Facial emotion recognition,"Emotional computing, facial emotion recognition, learning methods, SVM, DBM, feature-level fusion, model-level fusion, decision-level fusion.","The interest on emotional computing has been increasing as many applications were in demand by multiple markets. This paper mainly focuses on different learning methods, and has implemented several methods: Support Vector Machine (SVM) and Deep Boltzmann Machine (DBM) for facial emotion recognition. The training and testing data sets of facial emotion prediction are from FERA 2015, and geometric features and appearance features are combined together. Different prediction systems are developed and the prediction results are compared. This paper aims to design an suitable system for facial emotion recognition.","Comparing these three different fusion strategies, simple accumulating different feature sets as feature-level fusion ignores the different information in time and space for different features. At the same time, finding an appropriate joint feature vectors is still an unsolved problem. For model-level fusion, achieving multi-time-scale labeling multimodal fusion is still an unsolved problem.

In conclusion, despite the limitation of decision-level fusion in correlation between different feature sets, decision-level fusion is chosen by most majority of researchers currently.","Facial emotion recognitionEmotional computing, facial emotion recognition, learning methods, SVM, DBM, feature-level fusion, model-level fusion, decision-level fusion.The interest on emotional computing has been increasing as many applications were in demand by multiple markets. This paper mainly focuses on different learning methods, and has implemented several methods: Support Vector Machine (SVM) and Deep Boltzmann Machine (DBM) for facial emotion recognition. The training and testing data sets of facial emotion prediction are from FERA 2015, and geometric features and appearance features are combined together. Different prediction systems are developed and the prediction results are compared. This paper aims to design an suitable system for facial emotion recognition.Comparing these three different fusion strategies, simple accumulating different feature sets as feature-level fusion ignores the different information in time and space for different features. At the same time, finding an appropriate joint feature vectors is still an unsolved problem. For model-level fusion, achieving multi-time-scale labeling multimodal fusion is still an unsolved problem.

In conclusion, despite the limitation of decision-level fusion in correlation between different feature sets, decision-level fusion is chosen by most majority of researchers currently.",Facial Emotion Recognition,"This paper discusses the increasing interest in emotional computing and focuses on different learning methods for facial emotion recognition, including Support Vector Machines and Deep Boltzmann Machines. The authors use data sets from FERA 2015 and combine geometric and appearance features for prediction. They compare different prediction systems and conclude that decision-level fusion is the most commonly used method, despite limitations in correlation between different feature sets. The authors also note that finding an appropriate joint feature vector remains an unsolved problem.",Object and Sentiment Recognition,,Object Recognition
245,Facial Emotion Recognition Using Transfer Learning in the Deep CNN,convolutional neural network (CNN); facial emotion recognition; transfer learning,"Human facial emotion recognition (FER) has attracted the attention of the research community for its promising applications. Mapping different facial expressions to the respective emotional states are the main task in FER. The classical FER consists of two major steps: feature extraction and emotion recognition. Currently, the Deep Neural Networks, especially the Convolutional Neural Network (CNN), is widely used in FER by virtue of its inherent feature extraction mechanism from images. Several works have been reported on CNN with only a few layers to resolve FER problems. However, standard shallow CNNs with straightforward learning schemes have limited feature extraction capability to capture emotion information from high-resolution images. A notable drawback of the most existing methods is that they consider only the frontal images (i.e., ignore profile views for convenience), although the profile views taken from different angles are important for a practical FER system. For developing a highly accurate FER system, this study proposes a very Deep CNN (DCNN) modeling through Transfer Learning (TL) technique where a pre-trained DCNN model is adopted by replacing its dense upper layer(s) compatible with FER, and the model is fine-tuned with facial emotion data. A novel pipeline strategy is introduced, where the training of the dense layer(s) is followed by tuning each of the pre-trained DCNN blocks successively that has led to gradual improvement of the accuracy of FER to a higher level. The proposed FER system is verified on eight different pre-trained DCNN models (VGG-16, VGG-19, ResNet-18, ResNet-34, ResNet-50, ResNet-152, Inception-v3 and DenseNet-161) and well-known KDEF and JAFFE facial image datasets. FER is very challenging even for frontal views alone. FER on the KDEF dataset poses further challenges due to the diversity of images with different profile views together with frontal views. The proposed method achieved remarkable accuracy on both datasets with pre-trained models. On a 10-fold cross-validation way, the best achieved FER accuracies with DenseNet-161 on test sets of KDEF and JAFFE are 96.51% and 99.52%, respectively. The evaluation results reveal the superiority of the proposed FER system over the existing ones regarding emotion detection accuracy. Moreover, the achieved performance on the KDEF dataset with profile views is promising as it clearly demonstrates the required proficiency for real-life applications.","In this study, an efficient DCNN using TL with pipeline tuning strategy has been proposed for emotion recognition from facial images. According to the experimental results, using eight different pre-trained DCNN models on well-known KDEF and JAFFE emotion datasets with different profile views, the proposed method shows very high recognition accuracy. In the present study, experiments conducted with general settings regardless of the pre-trained DCNN model for simplicity and a few confusing facial images, mostly profile views, are misclassified. Further fine-tuning hyperparameters of individual pre-trained models and extending special attention to profile views might enhance the classification accuracy. The current research, especially the performance with profile views, will be compatible with broader real-life industry applications, such as monitoring patients in the hospital or surveillance security. Moreover, the idea of facial emotion recognition may be extended to emotion recognition from speech or body movements to cover emerging industrial applications.","Facial Emotion Recognition Using Transfer Learning in the Deep CNNconvolutional neural network (CNN); facial emotion recognition; transfer learningHuman facial emotion recognition (FER) has attracted the attention of the research community for its promising applications. Mapping different facial expressions to the respective emotional states are the main task in FER. The classical FER consists of two major steps: feature extraction and emotion recognition. Currently, the Deep Neural Networks, especially the Convolutional Neural Network (CNN), is widely used in FER by virtue of its inherent feature extraction mechanism from images. Several works have been reported on CNN with only a few layers to resolve FER problems. However, standard shallow CNNs with straightforward learning schemes have limited feature extraction capability to capture emotion information from high-resolution images. A notable drawback of the most existing methods is that they consider only the frontal images (i.e., ignore profile views for convenience), although the profile views taken from different angles are important for a practical FER system. For developing a highly accurate FER system, this study proposes a very Deep CNN (DCNN) modeling through Transfer Learning (TL) technique where a pre-trained DCNN model is adopted by replacing its dense upper layer(s) compatible with FER, and the model is fine-tuned with facial emotion data. A novel pipeline strategy is introduced, where the training of the dense layer(s) is followed by tuning each of the pre-trained DCNN blocks successively that has led to gradual improvement of the accuracy of FER to a higher level. The proposed FER system is verified on eight different pre-trained DCNN models (VGG-16, VGG-19, ResNet-18, ResNet-34, ResNet-50, ResNet-152, Inception-v3 and DenseNet-161) and well-known KDEF and JAFFE facial image datasets. FER is very challenging even for frontal views alone. FER on the KDEF dataset poses further challenges due to the diversity of images with different profile views together with frontal views. The proposed method achieved remarkable accuracy on both datasets with pre-trained models. On a 10-fold cross-validation way, the best achieved FER accuracies with DenseNet-161 on test sets of KDEF and JAFFE are 96.51% and 99.52%, respectively. The evaluation results reveal the superiority of the proposed FER system over the existing ones regarding emotion detection accuracy. Moreover, the achieved performance on the KDEF dataset with profile views is promising as it clearly demonstrates the required proficiency for real-life applications.In this study, an efficient DCNN using TL with pipeline tuning strategy has been proposed for emotion recognition from facial images. According to the experimental results, using eight different pre-trained DCNN models on well-known KDEF and JAFFE emotion datasets with different profile views, the proposed method shows very high recognition accuracy. In the present study, experiments conducted with general settings regardless of the pre-trained DCNN model for simplicity and a few confusing facial images, mostly profile views, are misclassified. Further fine-tuning hyperparameters of individual pre-trained models and extending special attention to profile views might enhance the classification accuracy. The current research, especially the performance with profile views, will be compatible with broader real-life industry applications, such as monitoring patients in the hospital or surveillance security. Moreover, the idea of facial emotion recognition may be extended to emotion recognition from speech or body movements to cover emerging industrial applications.CK+ Dataset (EitW Network) Confusion MatrixJAFFE Dataset (EitW Network) Confusion MatrixInput Image Output Image",Facial Emotion Recognition,"This paper discusses the challenges of human facial emotion recognition (FER) and proposes a very Deep CNN (DCNN) modeling approach through Transfer Learning (TL) technique to improve FER accuracy. The proposed FER system is verified on eight different pre-trained DCNN models and well-known facial image datasets. The proposed method achieved remarkable accuracy on both datasets with pre-trained models. The evaluation results reveal the superiority of the proposed FER system over the existing ones regarding emotion detection accuracy. Moreover, the achieved performance on the KDEF dataset with profile views is promising as it clearly demonstrates the required proficiency for real-life applications. The current research can be compatible with broader real-life industry applications, such as monitoring patients in the hospital or surveillance security.",Object and Sentiment Recognition,CK+ Dataset (EitW Network) Confusion MatrixJAFFE Dataset (EitW Network) Confusion MatrixInput Image Output Image,Object Recognition
246,Modified Convolutional Neural Network Architecture Analysis for Facial Emotion Recognition,"Training, Convolutional neural networks, Testing, Convolution, Feature extraction, Emotion recognition.","Facial expressions are one of the key features of a human being and it can be used to speculate the emotional state at a particular moment. This paper employs the Convolutional Neural Network and Deep Neural Network to develop a facial emotion recognition model that categorizes a facial expression into seven different emotions categorized as Afraid, Angry, Disgusted, Happy, Neutral, Sad and Surprised. This paper compares the performance of two existing deep neural network architectures with our proposed architecture, namely the Venturi Architecture in terms of training accuracy, training loss, testing accuracy and testing loss. This paper uses the Karolinska Directed Emotional Faces dataset which is a set of 4900 pictures of human facial expressions. Two layers of feature maps were used to convolute the features from the images, and then it was passed on to the deep neural network with up to 6 hidden layers. The proposed Venturi architecture shows significant accuracy improvement compared to the modified triangular architecture and the rectangular architecture.","In this paper, three different architecture for the deep neural network of the convolutional neural networks i.e. The Rectangular Architecture, The Modified Triangular Architecture [9] and the newly proposed Venturi Architecture were analyzed for their training accuracy, testing or validation accuracy, training loss, testing or validation loss. It was found out that in terms of training accuracy The Venturi Architecture showed the highest training accuracy whereas The Modified Triangular Architecture showed the worst training accuracy at 98.29%. The Venturi Architecture also shows the best testing or validity accuracy as compared to other 2 Architecture at 86.78% whereas The Rectangular Architecture was with the worst validity accuracy at 79.61%. The proposed venturi architecture shows a 4.08% accuracy improvement than the modified triangular architecture and 7.17% accuracy improvement than the rectangular architecture. In future, plan to evaluate the performance of the Venturi architecture on other facial emotion database and to use this architecture to evaluate a multi modal deep neural network with both the facial emotion images and emotion audio samples for better efficacy.","Modified Convolutional Neural Network Architecture Analysis for Facial Emotion RecognitionTraining, Convolutional neural networks, Testing, Convolution, Feature extraction, Emotion recognition.Facial expressions are one of the key features of a human being and it can be used to speculate the emotional state at a particular moment. This paper employs the Convolutional Neural Network and Deep Neural Network to develop a facial emotion recognition model that categorizes a facial expression into seven different emotions categorized as Afraid, Angry, Disgusted, Happy, Neutral, Sad and Surprised. This paper compares the performance of two existing deep neural network architectures with our proposed architecture, namely the Venturi Architecture in terms of training accuracy, training loss, testing accuracy and testing loss. This paper uses the Karolinska Directed Emotional Faces dataset which is a set of 4900 pictures of human facial expressions. Two layers of feature maps were used to convolute the features from the images, and then it was passed on to the deep neural network with up to 6 hidden layers. The proposed Venturi architecture shows significant accuracy improvement compared to the modified triangular architecture and the rectangular architecture.In this paper, three different architecture for the deep neural network of the convolutional neural networks i.e. The Rectangular Architecture, The Modified Triangular Architecture [9] and the newly proposed Venturi Architecture were analyzed for their training accuracy, testing or validation accuracy, training loss, testing or validation loss. It was found out that in terms of training accuracy The Venturi Architecture showed the highest training accuracy whereas The Modified Triangular Architecture showed the worst training accuracy at 98.29%. The Venturi Architecture also shows the best testing or validity accuracy as compared to other 2 Architecture at 86.78% whereas The Rectangular Architecture was with the worst validity accuracy at 79.61%. The proposed venturi architecture shows a 4.08% accuracy improvement than the modified triangular architecture and 7.17% accuracy improvement than the rectangular architecture. In future, plan to evaluate the performance of the Venturi architecture on other facial emotion database and to use this architecture to evaluate a multi modal deep neural network with both the facial emotion images and emotion audio samples for better efficacy.Correct Rate 90 80 70 60 50 30 20 Emotions MPhotographs mVRNEUTRAL ALEGRIA TRISTEZA ASCO Ww NEUTRAL ALEGRIA TRISTEZA ASCONEUTRAL ALEGRIA TRISTEZA ASCO Ww NEUTRAL ALEGRIA TRISTEZA ASCO",Facial Emotion Recognition,"This paper describes a facial emotion recognition model that uses Convolutional Neural Networks (CNN) and Deep Neural Networks (DNN) to classify facial expressions into seven emotions. The proposed Venturi architecture is compared to two existing architectures, and the Karolinska Directed Emotional Faces dataset is used for testing. The Venturi architecture shows significant improvement in accuracy compared to the other architectures. In future work, the authors plan to evaluate the performance of the Venturi architecture on other databases and to use it in a multi-modal deep neural network for improved efficacy.",Object and Sentiment Recognition,Correct Rate 90 80 70 60 50 30 20 Emotions MPhotographs mVRNEUTRAL ALEGRIA TRISTEZA ASCO Ww NEUTRAL ALEGRIA TRISTEZA ASCONEUTRAL ALEGRIA TRISTEZA ASCO Ww NEUTRAL ALEGRIA TRISTEZA ASCO,Object Recognition
247,Facial emotion recognition in real-time and static images,"Face, Feature extraction, Support vector machines, Training, Real-time systems, Webcams, Videos, Facial emotion recognition","Facial expressions are a form of nonverbal communication. Various studies have been done for the classification of these facial expressions. There is strong evidence for the universal facial expressions of eight emotions which include: neutral happy, sadness, anger, contempt, disgust, fear, and surprise. So it is very important to detect these emotions on the face as it has wide applications in the field of Computer Vision and Artificial Intelligence. These fields are researching on the facial emotions to get the sentiments of the humans automatically. In Robotics, emotions classification can be used to enhance human-robot interactions since the robot is capable of interpreting a human reaction [13]. In this paper, the emotion detection has been done in both real-time and static images. In this project, we have used the Cohn-Kanade Database (CK) and the Extended Cohn-Kanade (CK+) database, which comprises many static images 640 × 400 pixels and for the real-time using the webcam. The target expression for each sequence in the datasets are fully FACS (Facial action coding system) coded and emotion labels have been revised and validated [3]. So for emotion recognition initially we need to detect the faces by using HAAR filter from OpenCV in the static images or in the real-time videos. Once the face is detected it can be cropped and processed for further detection of facial landmarks. Then using facial landmarks the datasets are trained using the machine learning algorithm (Support Vector Machine) and then classified according to the eight emotions [1]. Using SVM we were getting the accuracy of around 93.7%. These facial landmarks can be modified for getting better accuracy.","In this paper, we presented the fully automatic recognition of facial emotions using the computer vision and machine learning algorithms which classify these eight different emotions. We tried many algorithms for the classification but the best which came out of the results was the support vectors machines with the accuracy of around 94.1%. Our results imply that user independent, fully automatic real-time coding of facial expressions in the continuous video stream is an achievable goal with present power of the computer, at least for applications in which frontal views can be assumed using the webcam. This machine learning based system for the emotion recognition can be extended to the deep learning system using the Convolutional Neural networks which will have many layers and the chances of getting much higher accuracy is there around 99.5% [2]. This project can be extended in which it will detect as many emotions of different people in one frame in the real-time videos [8]. Emotion recognition is going to be very useful in the near future in the research field of robotics and artificial Intelligence for example if a robot can sense the sentiment of any human and that robot can act accordingly without any intervention of any other humans. This automatic machine learning system for emotion recognition can also be extended with the detection of mixed emotions other than these eight universal emotions.","Facial emotion recognition in real-time and static imagesFace, Feature extraction, Support vector machines, Training, Real-time systems, Webcams, Videos, Facial emotion recognitionFacial expressions are a form of nonverbal communication. Various studies have been done for the classification of these facial expressions. There is strong evidence for the universal facial expressions of eight emotions which include: neutral happy, sadness, anger, contempt, disgust, fear, and surprise. So it is very important to detect these emotions on the face as it has wide applications in the field of Computer Vision and Artificial Intelligence. These fields are researching on the facial emotions to get the sentiments of the humans automatically. In Robotics, emotions classification can be used to enhance human-robot interactions since the robot is capable of interpreting a human reaction [13]. In this paper, the emotion detection has been done in both real-time and static images. In this project, we have used the Cohn-Kanade Database (CK) and the Extended Cohn-Kanade (CK+) database, which comprises many static images 640 × 400 pixels and for the real-time using the webcam. The target expression for each sequence in the datasets are fully FACS (Facial action coding system) coded and emotion labels have been revised and validated [3]. So for emotion recognition initially we need to detect the faces by using HAAR filter from OpenCV in the static images or in the real-time videos. Once the face is detected it can be cropped and processed for further detection of facial landmarks. Then using facial landmarks the datasets are trained using the machine learning algorithm (Support Vector Machine) and then classified according to the eight emotions [1]. Using SVM we were getting the accuracy of around 93.7%. These facial landmarks can be modified for getting better accuracy.In this paper, we presented the fully automatic recognition of facial emotions using the computer vision and machine learning algorithms which classify these eight different emotions. We tried many algorithms for the classification but the best which came out of the results was the support vectors machines with the accuracy of around 94.1%. Our results imply that user independent, fully automatic real-time coding of facial expressions in the continuous video stream is an achievable goal with present power of the computer, at least for applications in which frontal views can be assumed using the webcam. This machine learning based system for the emotion recognition can be extended to the deep learning system using the Convolutional Neural networks which will have many layers and the chances of getting much higher accuracy is there around 99.5% [2]. This project can be extended in which it will detect as many emotions of different people in one frame in the real-time videos [8]. Emotion recognition is going to be very useful in the near future in the research field of robotics and artificial Intelligence for example if a robot can sense the sentiment of any human and that robot can act accordingly without any intervention of any other humans. This automatic machine learning system for emotion recognition can also be extended with the detection of mixed emotions other than these eight universal emotions.",Facial Emotion Recognition,The paper discusses the importance of facial expression recognition in computer vision and artificial intelligence. The paper uses the Cohn-Kanade Database (CK) and the Extended Cohn-Kanade (CK+) database to detect facial landmarks and classify emotions using machine learning algorithms such as Support Vector Machines (SVM). The results showed an accuracy of 94.1%. The system can be extended to deep learning using Convolutional Neural Networks (CNN) to achieve higher accuracy. The study can be expanded to detect multiple emotions of different people in real-time videos. Emotion recognition is important for human-robot interactions in the field of robotics and artificial intelligence.,Object and Sentiment Recognition,,Object Recognition
248,Deep spatial-temporal feature fusion for facial expression recognition in static images,Facial expression recognitionDeep neural networkOptical flowSpatial-temporal feature fusionTransfer learning,"Traditional methods of performing facial expression recognition commonly use hand-crafted spatial features. This paper proposes a multi-channel deep neural network that learns and fuses the spatial-temporal features for recognizing facial expressions in static images. The essential idea of this method is to extract optical flow from the changes between the peak expression face image (emotional-face) and the neutral face image (neutral-face) as the temporal information of a certain facial expression, and use the gray-level image of emotional-face as the spatial information. A Multi-channel Deep Spatial-Temporal feature Fusion neural Network (MDSTFN) is presented to perform the deep spatial-temporal feature extraction and fusion from static images. Each channel of the proposed method is fine-tuned from a pre-trained deep convolutional neural networks (CNN) instead of training a new CNN from scratch. In addition, average-face is used as a substitute for neutral-face in real-world applications. Extensive experiments are conducted to evaluate the proposed method on benchmarks databases including CK+, MMI, and RaFD. The results show that the optical flow information from emotional-face and neutral-face is a useful complement to spatial feature and can effectively improve the performance of facial expression recognition from static images. Compared with state-of-the-art methods, the proposed method can achieve better recognition accuracy, with rates of 98.38% on the CK+ database, 99.17% on the RaFD database, and 99.59% on the MMI database, respectively.","This paper presented a deep neural network architecture with multi-channels to extract and fuse the spatial-temporal features of static image for FER. The optical flow computed from the changes between emotional-face and neutral-face is used to represent the temporal changes of expression, while the gray-level image of emotional-face is used to provide the spatial information of expression. The feature extraction channels of the MDSTFN (Multi-channel Deep Spatial-Temporal feature Fusion neural Network) are fine-tuned from a pre-trained CNN model. This transfer learning scheme can not only obtain a better feature extractor for FER on a small number of image samples but can also reduce the risk of overfitting. Three kinds of strategies were investigated to fuse the temporal and spatial features obtained by multiple feature channels. On three benchmark databases (CK+, RaFD, and MMI), extensive experiments were conducted to evaluate the proposed method under various parameters such as channel combination, fusion strategy, cross-database, and pre-trained CNN models. The results show that the MDSTFN-based method is a feasible deep spatial-temporal feature extraction architecture for facial expression recognition. Replacing neutral-face with average-face improves the practicality of the proposed method, while the results of a comparison show that the MDSTFN-based method can achieve better accuracy than state-of-the-art methods.","Deep spatial-temporal feature fusion for facial expression recognition in static imagesFacial expression recognitionDeep neural networkOptical flowSpatial-temporal feature fusionTransfer learningTraditional methods of performing facial expression recognition commonly use hand-crafted spatial features. This paper proposes a multi-channel deep neural network that learns and fuses the spatial-temporal features for recognizing facial expressions in static images. The essential idea of this method is to extract optical flow from the changes between the peak expression face image (emotional-face) and the neutral face image (neutral-face) as the temporal information of a certain facial expression, and use the gray-level image of emotional-face as the spatial information. A Multi-channel Deep Spatial-Temporal feature Fusion neural Network (MDSTFN) is presented to perform the deep spatial-temporal feature extraction and fusion from static images. Each channel of the proposed method is fine-tuned from a pre-trained deep convolutional neural networks (CNN) instead of training a new CNN from scratch. In addition, average-face is used as a substitute for neutral-face in real-world applications. Extensive experiments are conducted to evaluate the proposed method on benchmarks databases including CK+, MMI, and RaFD. The results show that the optical flow information from emotional-face and neutral-face is a useful complement to spatial feature and can effectively improve the performance of facial expression recognition from static images. Compared with state-of-the-art methods, the proposed method can achieve better recognition accuracy, with rates of 98.38% on the CK+ database, 99.17% on the RaFD database, and 99.59% on the MMI database, respectively.This paper presented a deep neural network architecture with multi-channels to extract and fuse the spatial-temporal features of static image for FER. The optical flow computed from the changes between emotional-face and neutral-face is used to represent the temporal changes of expression, while the gray-level image of emotional-face is used to provide the spatial information of expression. The feature extraction channels of the MDSTFN (Multi-channel Deep Spatial-Temporal feature Fusion neural Network) are fine-tuned from a pre-trained CNN model. This transfer learning scheme can not only obtain a better feature extractor for FER on a small number of image samples but can also reduce the risk of overfitting. Three kinds of strategies were investigated to fuse the temporal and spatial features obtained by multiple feature channels. On three benchmark databases (CK+, RaFD, and MMI), extensive experiments were conducted to evaluate the proposed method under various parameters such as channel combination, fusion strategy, cross-database, and pre-trained CNN models. The results show that the MDSTFN-based method is a feasible deep spatial-temporal feature extraction architecture for facial expression recognition. Replacing neutral-face with average-face improves the practicality of the proposed method, while the results of a comparison show that the MDSTFN-based method can achieve better accuracy than state-of-the-art methods.",Facial Emotion Recognition,"This paper proposes a multi-channel deep neural network called MDSTFN for recognizing facial expressions in static images by extracting and fusing spatial-temporal features. The method uses optical flow from the changes between emotional-face and neutral-face as the temporal information and gray-level image of emotional-face as spatial information. The feature extraction channels of MDSTFN are fine-tuned from a pre-trained CNN model. The proposed method achieves better recognition accuracy compared to state-of-the-art methods, with rates of 98.38% on CK+, 99.17% on RaFD, and 99.59% on MMI databases, respectively. Replacing neutral-face with average-face improves the practicality of the proposed method.",Object and Sentiment Recognition,,Object Recognition
249,Human Facial Expression Recognition from static images using shape and appearance feature,"Feature extraction
,
Support vector machines
,
Face
,
Shape
,
Face recognition
,
Training
,
Emotion recognition","Automatic recognition and interpretation of human emotions are becoming an integral part of intelligent products and services, which can lead to a breakthrough in domains such as healthcare, marketing, security, education and environment. This leads us towards Facial Expression Recognition (FER) systems, whose main objective is to detect an expressed emotion and recognize the same. The proposed work introduces an FER system, which models the relationship between human facial expression and corresponding induced emotion for static images by extracting shape and appearance features. Automatic interpretation of facial expressions from static images is very challenging, as information available in a static image is less when compared with image sequences. Proposed method incites efficient extraction of shape and appearance features from still images for varying facial expressions using Histogram of Oriented Gradient (HOG) features, and Support Vector Machine (SVM) is employed for classification. The proposed work is implemented on Cohn-kanade data set for six basic expressions (happy, sad, surprise, anger, fear and disgust). Results show that identification of human expressions from static images has a better detection rate when shape and appearance features are used rather than texture or geometric features. Use of HOG for feature extraction makes the FER system robust, and real time implementation is made much easier.","The proposed work shows how HOG features can be exploited for facial expression recognition. Use of HOG features make the performance of FER system to be subject independent. The accuracy of the work is found be 92.56% when implemented using Cohn-kanade dataset for six basic expressions. Results indicate that shape features on face carry more information for emotion modelling when compared with texture and geometric features. Shape features are better when compared with geometric features due to the fact that a small pose variation degrades the performance of FER system which relies on geometric features where as a small pose variation doesn't reflect any changes on a FER system which relies on HOG features. Detection rates for disgust, fear and sad is less in the proposed work. Detection rates can be further improved by combining shape, texture and geometric features. Optimized cell sizes may be considered for real time implementation so as to address both detection rate and processing speed. The influence of non-frontal face on the performance of FER system could be addressed in the future work.","Human Facial Expression Recognition from static images using shape and appearance featureFeature extraction
,
Support vector machines
,
Face
,
Shape
,
Face recognition
,
Training
,
Emotion recognitionAutomatic recognition and interpretation of human emotions are becoming an integral part of intelligent products and services, which can lead to a breakthrough in domains such as healthcare, marketing, security, education and environment. This leads us towards Facial Expression Recognition (FER) systems, whose main objective is to detect an expressed emotion and recognize the same. The proposed work introduces an FER system, which models the relationship between human facial expression and corresponding induced emotion for static images by extracting shape and appearance features. Automatic interpretation of facial expressions from static images is very challenging, as information available in a static image is less when compared with image sequences. Proposed method incites efficient extraction of shape and appearance features from still images for varying facial expressions using Histogram of Oriented Gradient (HOG) features, and Support Vector Machine (SVM) is employed for classification. The proposed work is implemented on Cohn-kanade data set for six basic expressions (happy, sad, surprise, anger, fear and disgust). Results show that identification of human expressions from static images has a better detection rate when shape and appearance features are used rather than texture or geometric features. Use of HOG for feature extraction makes the FER system robust, and real time implementation is made much easier.The proposed work shows how HOG features can be exploited for facial expression recognition. Use of HOG features make the performance of FER system to be subject independent. The accuracy of the work is found be 92.56% when implemented using Cohn-kanade dataset for six basic expressions. Results indicate that shape features on face carry more information for emotion modelling when compared with texture and geometric features. Shape features are better when compared with geometric features due to the fact that a small pose variation degrades the performance of FER system which relies on geometric features where as a small pose variation doesn't reflect any changes on a FER system which relies on HOG features. Detection rates for disgust, fear and sad is less in the proposed work. Detection rates can be further improved by combining shape, texture and geometric features. Optimized cell sizes may be considered for real time implementation so as to address both detection rate and processing speed. The influence of non-frontal face on the performance of FER system could be addressed in the future work.",Facial Emotion Recognition,"The paper discusses the importance of facial expression recognition (FER) in various domains and introduces an FER system that uses Histogram of Oriented Gradient (HOG) features and Support Vector Machine (SVM) for classification. The proposed method extracts shape and appearance features from still images for varying facial expressions, making it subject independent and robust. The accuracy of the proposed system is found to be 92.56% on the Cohn-kanade dataset for six basic expressions. Results show that shape features are better than texture or geometric features for emotion modeling, and HOG features make the FER system robust and suitable for real-time implementation. However, the detection rates for disgust, fear, and sadness are lower than other emotions, which can be improved by combining shape, texture, and geometric features. Future work can focus on addressing the influence of non-frontal faces on the performance of the FER system and optimizing cell sizes for real-time implementation.",Object and Sentiment Recognition,,Object Recognition
250,Deep Facial Expression Recognition: A Survey,"Databases
,
Face recognition
,
Three-dimensional displays
,
Lighting
,
Training data","With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and the recent success of deep learning techniques in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER. Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose, and identity bias. In this survey, we provide a comprehensive review of deep FER, including datasets and algorithms that provide insights into these intrinsic problems. First, we introduce the available datasets that are widely used in the literature and provide accepted data selection and evaluation principles for these datasets. We then describe the standard pipeline of a deep FER system with the related background knowledge and suggestions for applicable implementations for each stage. For the state-of-the-art in deep FER, we introduce existing novel deep neural networks and related training strategies that are designed for FER based on both static images and dynamic image sequences and discuss their advantages and limitations. Competitive performances and experimental comparisons on widely used benchmarks are also summarized. We then extend our survey to additional related issues and application scenarios. Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust deep FER systems.","As the FER literature shifts its main focus to the challenging in-the-wild environmental conditions, many researchers have committed to employing deep learning technologies to handle difficulties, such as illumination variation, occlusions, nonfrontal head poses, identity bias and the recognition of low-intensity expressions. Given that FER is a data-driven task and that training a sufficiently deep network to capture subtle expression-related deformations requires a large amount of training data, the major challenge that deep FER systems face is the lack of training data in terms of both quantity and quality.

Because people of different age ranges, cultures and genders display and interpret facial expression in different ways, an ideal facial expression dataset is expected to include abundant sample images with precise face attribute labels, not just expression but other attributes such as age, gender and ethnicity, which would facilitate related research on cross-age range, cross-gender and cross-cultural FER using deep learning techniques, such as multitask deep networks and transfer learning. In addition, although occlusion and multipose problems have received relatively wide interest in the field of deep face recognition, occlusion-robust and pose-invariant issues have received less attention in deep FER. One of the main reasons is the lack of a sufficient facial expression dataset with occlusion type and head-pose annotations.

On the other hand, accurately annotating a large volume of image data with the large variation and complexity of natural scenarios is an obvious impediment to the construction of expression datasets. A reasonable approach is to employ crowd-sourcing models [34], [45], [208] under the guidance of expert annotators. Additionally, a fully automatic labeling tool [43] refined by experts is an alternative to provide approximate but efficient annotations. In both cases, a subsequent reliable estimation or labeling learning process is necessary to filter out noisy annotations. In particular, few comparatively large-scale datasets that consider real-world scenarios and contain a wide range of facial expressions have recently become publicly available, i.e., EmotioNet [43], RAF-DB [34], [44] and AffectNet [45], and we anticipate that with advances in technology and the widespread of the Internet, more complementary facial expression datasets will be constructed to promote the development of deep FER.","Deep Facial Expression Recognition: A SurveyDatabases
,
Face recognition
,
Three-dimensional displays
,
Lighting
,
Training dataWith the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and the recent success of deep learning techniques in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER. Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose, and identity bias. In this survey, we provide a comprehensive review of deep FER, including datasets and algorithms that provide insights into these intrinsic problems. First, we introduce the available datasets that are widely used in the literature and provide accepted data selection and evaluation principles for these datasets. We then describe the standard pipeline of a deep FER system with the related background knowledge and suggestions for applicable implementations for each stage. For the state-of-the-art in deep FER, we introduce existing novel deep neural networks and related training strategies that are designed for FER based on both static images and dynamic image sequences and discuss their advantages and limitations. Competitive performances and experimental comparisons on widely used benchmarks are also summarized. We then extend our survey to additional related issues and application scenarios. Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust deep FER systems.As the FER literature shifts its main focus to the challenging in-the-wild environmental conditions, many researchers have committed to employing deep learning technologies to handle difficulties, such as illumination variation, occlusions, nonfrontal head poses, identity bias and the recognition of low-intensity expressions. Given that FER is a data-driven task and that training a sufficiently deep network to capture subtle expression-related deformations requires a large amount of training data, the major challenge that deep FER systems face is the lack of training data in terms of both quantity and quality.

Because people of different age ranges, cultures and genders display and interpret facial expression in different ways, an ideal facial expression dataset is expected to include abundant sample images with precise face attribute labels, not just expression but other attributes such as age, gender and ethnicity, which would facilitate related research on cross-age range, cross-gender and cross-cultural FER using deep learning techniques, such as multitask deep networks and transfer learning. In addition, although occlusion and multipose problems have received relatively wide interest in the field of deep face recognition, occlusion-robust and pose-invariant issues have received less attention in deep FER. One of the main reasons is the lack of a sufficient facial expression dataset with occlusion type and head-pose annotations.

On the other hand, accurately annotating a large volume of image data with the large variation and complexity of natural scenarios is an obvious impediment to the construction of expression datasets. A reasonable approach is to employ crowd-sourcing models [34], [45], [208] under the guidance of expert annotators. Additionally, a fully automatic labeling tool [43] refined by experts is an alternative to provide approximate but efficient annotations. In both cases, a subsequent reliable estimation or labeling learning process is necessary to filter out noisy annotations. In particular, few comparatively large-scale datasets that consider real-world scenarios and contain a wide range of facial expressions have recently become publicly available, i.e., EmotioNet [43], RAF-DB [34], [44] and AffectNet [45], and we anticipate that with advances in technology and the widespread of the Internet, more complementary facial expression datasets will be constructed to promote the development of deep FER.Facil Expression Datel [Taina See att a [eas fee Trained on tmageNet === Test Image Cropped FaceCome Black t | Con Black? Cone Blok om Back ane Blok Added (Promatiscne) | (romatincnne) | (Boze tisctune) | froma Finest) ‘Gisetine) | Fully Comoced1s Conyolutiqnal ~ Pooling 2""! Convolutjonal— Pooling Dense ——— ee Layer | jf = Input Image TCR estas acrMs CMF: Convolved Feature Map SFM: Subsampled Feature Map.Convolutional Base (Feature Extraction) = Classifier (Feature Classification) => Emotion",Facial Emotion Recognition,"This article discusses the use of deep neural networks for facial expression recognition (FER) under challenging in-the-wild conditions. Deep FER systems face challenges related to overfitting and expression-unrelated variations, such as illumination, head pose, and identity bias. The article provides a comprehensive review of deep FER, including datasets and algorithms, and discusses the remaining challenges and future directions for the design of robust deep FER systems. The lack of training data, occlusion-robust and pose-invariant issues, and accurately annotating large volumes of image data are identified as challenges in the construction of expression datasets. The article suggests using crowd-sourcing models and automatic labeling tools under the guidance of expert annotators to address these challenges. Several publicly available facial expression datasets, including EmotioNet, RAF-DB, and AffectNet, are discussed, and the article anticipates the construction of more complementary facial expression datasets in the future.",Object and Sentiment Recognition,"Facil Expression Datel [Taina See att a [eas fee Trained on tmageNet === Test Image Cropped FaceCome Black t | Con Black? Cone Blok om Back ane Blok Added (Promatiscne) | (romatincnne) | (Boze tisctune) | froma Finest) ‘Gisetine) | Fully Comoced1s Conyolutiqnal ~ Pooling 2""! Convolutjonal— Pooling Dense ——— ee Layer | jf = Input Image TCR estas acrMs CMF: Convolved Feature Map SFM: Subsampled Feature Map.Convolutional Base (Feature Extraction) = Classifier (Feature Classification) => Emotion",Object Recognition
251,Sentiment Analysis on Social Media,"sentiment analysis, social media.","The Web is a huge virtual space where to express and share individual opinions, influencing any aspect of life, with implications for marketing and communication alike. Social Media are influencing consumers’ preferences by shaping their attitudes and behaviors. Monitoring the Social Media activities is a good way to measure customers’ loyalty, keeping a track on their sentiment towards brands or products. Social Media are the next logical marketing arena. Currently, Facebook dominates the digital marketing space, followed closely by Twitter. This paper describes a Sentiment Analysis study performed on over than 1000 Facebook posts about newscasts, comparing the sentiment for Rai - the Italian public broadcasting service - towards the emerging and more dynamic private company La7. This study maps study results with observations made by the Osservatorio di Pavia, which is an Italian institute of research specialized in media analysis at theoretical and empirical level, engaged in the analysis of political communication in the mass media. This study takes also in account the data provided by Auditel regarding newscast audience, correlating the analysis of Social Media, of Facebook in particular, with measurable data, available to public domain.","This paper describes a Sentiment Analysis study performed on over than 1000 Facebook posts about newscasts, comparing the sentiment for Rai - the Italian public broadcasting service - towards the emerging and more dynamic private company La7. It maps Sentiment Analysis on Social Media with observations and measurable data. Its results accurately reflect the reality as described by the Osservatorio di Pavia and Auditel, highlightening the importance of Facebook as a platform for online marketing. Monitoring the social media activities is a good way to measure customers’ loyalty and interests, keeping track of their sentiment towards brands or products. This study has been performed by a Knowledge Mining system used by some security sector-related government institutions and agencies in Italy to limit information overload in OSINT and Web Mining. The linguistic and semantic approaches implemented in this system enable the research, the analysis, the classification of great volumes of heterogeneous documents, helping documental analysts to cut through the information labyrinth, analysts to take account of complexity of public views, assigning automatically a sentiment polarity, rapidly accessing all the potential texts of interest.","Sentiment Analysis on Social Mediasentiment analysis, social media.The Web is a huge virtual space where to express and share individual opinions, influencing any aspect of life, with implications for marketing and communication alike. Social Media are influencing consumers’ preferences by shaping their attitudes and behaviors. Monitoring the Social Media activities is a good way to measure customers’ loyalty, keeping a track on their sentiment towards brands or products. Social Media are the next logical marketing arena. Currently, Facebook dominates the digital marketing space, followed closely by Twitter. This paper describes a Sentiment Analysis study performed on over than 1000 Facebook posts about newscasts, comparing the sentiment for Rai - the Italian public broadcasting service - towards the emerging and more dynamic private company La7. This study maps study results with observations made by the Osservatorio di Pavia, which is an Italian institute of research specialized in media analysis at theoretical and empirical level, engaged in the analysis of political communication in the mass media. This study takes also in account the data provided by Auditel regarding newscast audience, correlating the analysis of Social Media, of Facebook in particular, with measurable data, available to public domain.This paper describes a Sentiment Analysis study performed on over than 1000 Facebook posts about newscasts, comparing the sentiment for Rai - the Italian public broadcasting service - towards the emerging and more dynamic private company La7. It maps Sentiment Analysis on Social Media with observations and measurable data. Its results accurately reflect the reality as described by the Osservatorio di Pavia and Auditel, highlightening the importance of Facebook as a platform for online marketing. Monitoring the social media activities is a good way to measure customers’ loyalty and interests, keeping track of their sentiment towards brands or products. This study has been performed by a Knowledge Mining system used by some security sector-related government institutions and agencies in Italy to limit information overload in OSINT and Web Mining. The linguistic and semantic approaches implemented in this system enable the research, the analysis, the classification of great volumes of heterogeneous documents, helping documental analysts to cut through the information labyrinth, analysts to take account of complexity of public views, assigning automatically a sentiment polarity, rapidly accessing all the potential texts of interest.NODES OUTPUTLAYER,258 256 256 256 256 258 NODES MODES «NODES «MODES. © NODES NODES ovrPur LAYER HIDDEN LAYER INPUT LAYERConvolution INPUTIMAGE ——=!> + — > ~~ Pooling ReLu Convolution Flatten 4 ea = aie ReLu Y Fully Connected Paper — OUTPUT",sentiment analysis,"The article discusses the influence of social media on consumers' attitudes and behaviors, and how monitoring social media activities can help measure customer loyalty and sentiment towards brands or products. The article describes a sentiment analysis study conducted on over 1000 Facebook posts about newscasts, comparing the sentiment for Rai, the Italian public broadcasting service, with the emerging private company La7. The study maps sentiment analysis on social media with observations and measurable data, highlighting the importance of Facebook as a platform for online marketing. The study was performed using a Knowledge Mining system, which enables research, analysis, and classification of large volumes of heterogeneous documents to help analysts cut through information overload.",Object and Sentiment Recognition,"NODES OUTPUTLAYER,258 256 256 256 256 258 NODES MODES «NODES «MODES. © NODES NODES ovrPur LAYER HIDDEN LAYER INPUT LAYERConvolution INPUTIMAGE ——=!> + — > ~~ Pooling ReLu Convolution Flatten 4 ea = aie ReLu Y Fully Connected Paper — OUTPUT",Object Recognition
252,An Overview of Sentiment Analysis in Social Media and its Applications in Disaster Relief,"Sentiment Analysis, Disaster Relief, Visualization, Social Media","Sentiment analysis refers to the class of computational and natural language processing based techniques used to identify, extract or characterize subjective information, such as opinions, expressed in a given piece of text. The main purpose of sentiment analysis is to classify a writer’s attitude towards various topics into positive, negative or neutral categories. Sentiment analysis has many applications in different domains including, but not limited to, business intelligence, politics, sociology, etc. Recent years, on the other hand, have witnessed the advent of social networking websites, microblogs, wikis and Web applications and consequently, an unprecedented growth in user-generated data is poised for sentiment mining. Data such as web-postings, Tweets, videos, etc., all express opinions on various topics and events, offer immense opportunities to study and analyze human opinions and sentiment. In this chapter, we study the information published by individuals in social media in cases of natural disasters and emergencies and investigate if such information could be used by first responders to improve situational awareness and crisis management. In particular, we explore applications of sentiment analysis and demonstrate how sentiment mining in social media can be exploited to determine how local crowds react during a disaster, and how such information can be used to improve disaster management. Such information can also be used to help assess the extent of the devastation and find people who are in specific need during an emergency situation. We first provide the formal definition of sentiment analysis in social media and cover traditional and the state-of-the-art approaches while highlighting contributions, shortcomings, and pitfalls due to the composition of online media streams. Next we discuss the relationship among social media, disaster relief and situational awareness and explain how social media is used in these contexts with the focus on sentiment analysis. In order to enable quick analysis of real-time geo-distributed data, we will detail applications of visual analytics with an emphasis on sentiment visualization. Finally, we conclude the chapter with a discussion of research challenges in sentiment analysis and its application in disaster relief.","ness, while we also detailed applications of visual analytics with an emphasis on sentiment analysis. In this section we discuss some of the challenges facing the studies in sentiment analysis and its application in disaster relief, as well as visual analytics, as potential research directions for further considerations. Despite few works that have exploited network information such as followee-follower networks and @-networks for more accurate sentiment analysis, majority of the works have not incorporated such useful information; they have considered the problem of sentiment analysis with solely taking into account the lexicon or linguistic based features. One potential avenue of future work is to deploy extra information such as emotional correlation information including spatial-temporal patterns and homophily effect as a measurement of sentiment of social media posts. For example, during winters, people in Florida are expected to be happier than people in Wisconsin. Also, homophily effect suggest that similar people might behave in a more similar way than other people regarding a specific event. Moreover, as the task of collecting sentiment labels in social media is extremely time consuming and tedious, unsupervised or semi-supervised approaches are required to reduce the cost of sentiment classification. Although some studies have addressed this issue, the literature is premature in this realm and still lacks strong contributions. For example, future studies could explore the contributions of other available emotion indications in social media such as product ratings and reviews. Most studies in disaster relief have used plain machine learning techniques with simple lexicon features. Therefore, more complex machine learning based approaches along with stronger features are required. Furthermore, leveraging the findings of psychological and sociological studies on individuals’ behaviors (e.g. hope, fear) during disasters, could be another interesting research direction. This additional information could help better understand people’s behaviors and feelings during disasters and also help decision makers know how they can handle this situation. Investigating how this information could be preprocessed to be immediately usable by corresponding authorities, is another interesting future research direction. Furthermore, visualization techniques need to be improved to allow for real time visual analytics of disasters related posts in order to help first responders easily track the changes during disasters and make proper and quick decisions.","An Overview of Sentiment Analysis in Social Media and its Applications in Disaster ReliefSentiment Analysis, Disaster Relief, Visualization, Social MediaSentiment analysis refers to the class of computational and natural language processing based techniques used to identify, extract or characterize subjective information, such as opinions, expressed in a given piece of text. The main purpose of sentiment analysis is to classify a writer’s attitude towards various topics into positive, negative or neutral categories. Sentiment analysis has many applications in different domains including, but not limited to, business intelligence, politics, sociology, etc. Recent years, on the other hand, have witnessed the advent of social networking websites, microblogs, wikis and Web applications and consequently, an unprecedented growth in user-generated data is poised for sentiment mining. Data such as web-postings, Tweets, videos, etc., all express opinions on various topics and events, offer immense opportunities to study and analyze human opinions and sentiment. In this chapter, we study the information published by individuals in social media in cases of natural disasters and emergencies and investigate if such information could be used by first responders to improve situational awareness and crisis management. In particular, we explore applications of sentiment analysis and demonstrate how sentiment mining in social media can be exploited to determine how local crowds react during a disaster, and how such information can be used to improve disaster management. Such information can also be used to help assess the extent of the devastation and find people who are in specific need during an emergency situation. We first provide the formal definition of sentiment analysis in social media and cover traditional and the state-of-the-art approaches while highlighting contributions, shortcomings, and pitfalls due to the composition of online media streams. Next we discuss the relationship among social media, disaster relief and situational awareness and explain how social media is used in these contexts with the focus on sentiment analysis. In order to enable quick analysis of real-time geo-distributed data, we will detail applications of visual analytics with an emphasis on sentiment visualization. Finally, we conclude the chapter with a discussion of research challenges in sentiment analysis and its application in disaster relief.ness, while we also detailed applications of visual analytics with an emphasis on sentiment analysis. In this section we discuss some of the challenges facing the studies in sentiment analysis and its application in disaster relief, as well as visual analytics, as potential research directions for further considerations. Despite few works that have exploited network information such as followee-follower networks and @-networks for more accurate sentiment analysis, majority of the works have not incorporated such useful information; they have considered the problem of sentiment analysis with solely taking into account the lexicon or linguistic based features. One potential avenue of future work is to deploy extra information such as emotional correlation information including spatial-temporal patterns and homophily effect as a measurement of sentiment of social media posts. For example, during winters, people in Florida are expected to be happier than people in Wisconsin. Also, homophily effect suggest that similar people might behave in a more similar way than other people regarding a specific event. Moreover, as the task of collecting sentiment labels in social media is extremely time consuming and tedious, unsupervised or semi-supervised approaches are required to reduce the cost of sentiment classification. Although some studies have addressed this issue, the literature is premature in this realm and still lacks strong contributions. For example, future studies could explore the contributions of other available emotion indications in social media such as product ratings and reviews. Most studies in disaster relief have used plain machine learning techniques with simple lexicon features. Therefore, more complex machine learning based approaches along with stronger features are required. Furthermore, leveraging the findings of psychological and sociological studies on individuals’ behaviors (e.g. hope, fear) during disasters, could be another interesting research direction. This additional information could help better understand people’s behaviors and feelings during disasters and also help decision makers know how they can handle this situation. Investigating how this information could be preprocessed to be immediately usable by corresponding authorities, is another interesting future research direction. Furthermore, visualization techniques need to be improved to allow for real time visual analytics of disasters related posts in order to help first responders easily track the changes during disasters and make proper and quick decisions.Input space Feature space"" | Small Margin Frame Margin Support VectorsIIe BAe Sele BER SKI Coe nee SBE)",sentiment analysis,"Sentiment analysis is a technique used to identify and extract subjective information such as opinions expressed in text. It is used to classify attitudes into positive, negative, or neutral categories and has many applications in various domains such as business, politics, and sociology. With the advent of social media and the growth of user-generated data, sentiment analysis has become important in disaster management. Social media can be used to determine how local crowds react during a disaster, assess the extent of devastation, and find people who are in specific need during an emergency situation. However, there are challenges in sentiment analysis such as the lack of emotional correlation information and the need for unsupervised or semi-supervised approaches. In disaster relief, more complex machine learning based approaches with stronger features are required, and leveraging psychological and sociological studies on individual behavior during disasters could be an interesting research direction. Visualization techniques also need to be improved to allow for real-time visual analytics of disaster-related posts to help first responders track changes and make quick decisions.",Object and Sentiment Recognition,"Input space Feature space"" | Small Margin Frame Margin Support VectorsIIe BAe Sele BER SKI Coe nee SBE)",Object Recognition
253,Social media sentiment analysis based on COVID-19,natural language processing; recurrent neural network; sentiment analysis; social media; visualization,"In today’s world, the social media is everywhere, and everybody come in contact with it every day. With social media datas, we are able to do a lot of analysis and statistics nowdays. Within this scope of article, we conclude and analyse the sentiments and manifestations (comments, hastags, posts, tweets) of the users of the Twitter social media platform, based on the main trends (by keyword, which is mostly the ‘covid’ and coronavirus theme in this article) with Natural Language Processing and with Sentiment Classification using Recurrent Neural Network. Where we analyse, compile, visualize statistics, and summarize for further processing. The trained model works much more accurately, with a smaller margin of error, in determining emotional polarity in today’s ‘modern’ often with ambiguous tweets. Especially with RNN. We use this fresh scraped data collections (by the keyword’s theme) with our RNN model what we have created and trained to determine what emotional manifestations occurred on a given topic in a given time interval.","In this work, we use a Recurrent Neural Network (RNN) to classify emotions on tweets. We developed a model to analyse the emotional nature of various tweets, using the recurrent neural network for emotional prediction, searching for connections between words, and marking them with positive or negative emotions. Where instead of simple positive and negative extremes, we have classified the various texts into a much more articulated class of emotional strength (weakly positive/negative, strongly positive/negative). This Figure 9. Analysis of sample of 500 tweets by TextBlob and RNN, using ‘covid’ keyword. The time period stands between 13 May 2020 and 14 May 2020. (a) TextBlob result, (b) RNN result. Figure 10. 13 May 2020 Part of the dataSet of the ‘covid’ keyword. JOURNAL OF INFORMATION AND TELECOMMUNICATION 13 has been combined with a keyword-based special data scraper, so we can apply our taught RNN model with these specific freshly scraped datasets. As a result, we get an emotional classification related to specific topics. What kind of tweets they were and what emotional class they belong to, what is the distribution on that topic at the emotional level within the given start interval. In the article, we focused most on the coronavirus and related emotional changes and fluctuations, and it was shown that the overall positive manifestation and presence on the social platform remained on social media surfaces during this pandemic. Of course, in addition to negative and other manifestations. Over time, positivity has strengthened, but there is also a stronger negative array that is natural. According to our expectations this topic remain positive manifestations, sometimes with a higher and sometimes with a smaller percentage. It can be seen that the recurrent neural network provides good performance and prediction in text classification. Where the RNN model brought a smaller amount of data in neutral result or completely reduced to zero that. Which proves that our model is ‘able to make’ a decision and categorize in some direction even on the basis of small details. Our comparisons were made mainly against TextBlob, which also worked very well and delivered stable results, but there were many times when the neutral results were above 30% compared to our RNN model, which we cannot use as usefully for further evaluations as for our RNN model. The classification of emotions for both models (TextBlob, RNN) was properly segmented. For future work and further development, it may be advisable to create an interface that better visualizes and interacts with users, which can be supplemented with sophisticated database management for archiving, tracking, and exploring datas to other areas. We can further expand the analysis by introducing various classifications and clusters as well as other data analyses. Allowing examinations and comparisons from a new perspective, in addition to emotional analyses, may even provide an opportunity to further support current results and compare the conclusions. In addition, implementing or refactoring future potential tensorflow features and keeping it up to date.","Social media sentiment analysis based on COVID-19natural language processing; recurrent neural network; sentiment analysis; social media; visualizationIn today’s world, the social media is everywhere, and everybody come in contact with it every day. With social media datas, we are able to do a lot of analysis and statistics nowdays. Within this scope of article, we conclude and analyse the sentiments and manifestations (comments, hastags, posts, tweets) of the users of the Twitter social media platform, based on the main trends (by keyword, which is mostly the ‘covid’ and coronavirus theme in this article) with Natural Language Processing and with Sentiment Classification using Recurrent Neural Network. Where we analyse, compile, visualize statistics, and summarize for further processing. The trained model works much more accurately, with a smaller margin of error, in determining emotional polarity in today’s ‘modern’ often with ambiguous tweets. Especially with RNN. We use this fresh scraped data collections (by the keyword’s theme) with our RNN model what we have created and trained to determine what emotional manifestations occurred on a given topic in a given time interval.In this work, we use a Recurrent Neural Network (RNN) to classify emotions on tweets. We developed a model to analyse the emotional nature of various tweets, using the recurrent neural network for emotional prediction, searching for connections between words, and marking them with positive or negative emotions. Where instead of simple positive and negative extremes, we have classified the various texts into a much more articulated class of emotional strength (weakly positive/negative, strongly positive/negative). This Figure 9. Analysis of sample of 500 tweets by TextBlob and RNN, using ‘covid’ keyword. The time period stands between 13 May 2020 and 14 May 2020. (a) TextBlob result, (b) RNN result. Figure 10. 13 May 2020 Part of the dataSet of the ‘covid’ keyword. JOURNAL OF INFORMATION AND TELECOMMUNICATION 13 has been combined with a keyword-based special data scraper, so we can apply our taught RNN model with these specific freshly scraped datasets. As a result, we get an emotional classification related to specific topics. What kind of tweets they were and what emotional class they belong to, what is the distribution on that topic at the emotional level within the given start interval. In the article, we focused most on the coronavirus and related emotional changes and fluctuations, and it was shown that the overall positive manifestation and presence on the social platform remained on social media surfaces during this pandemic. Of course, in addition to negative and other manifestations. Over time, positivity has strengthened, but there is also a stronger negative array that is natural. According to our expectations this topic remain positive manifestations, sometimes with a higher and sometimes with a smaller percentage. It can be seen that the recurrent neural network provides good performance and prediction in text classification. Where the RNN model brought a smaller amount of data in neutral result or completely reduced to zero that. Which proves that our model is ‘able to make’ a decision and categorize in some direction even on the basis of small details. Our comparisons were made mainly against TextBlob, which also worked very well and delivered stable results, but there were many times when the neutral results were above 30% compared to our RNN model, which we cannot use as usefully for further evaluations as for our RNN model. The classification of emotions for both models (TextBlob, RNN) was properly segmented. For future work and further development, it may be advisable to create an interface that better visualizes and interacts with users, which can be supplemented with sophisticated database management for archiving, tracking, and exploring datas to other areas. We can further expand the analysis by introducing various classifications and clusters as well as other data analyses. Allowing examinations and comparisons from a new perspective, in addition to emotional analyses, may even provide an opportunity to further support current results and compare the conclusions. In addition, implementing or refactoring future potential tensorflow features and keeping it up to date.ro prem!Average svM —— | coneatenation | te ‘Pieeral stocks beced tadCNN fine-tune trom pr-trained model. \ (Cw ine tuned from pre-trained model Cun fine-tuned from pretrained model ENN Fine-tune from pretained modelGenie Lemma 7 I I Image pre-processing an | Feature fusion I ‘ I teinie eaiciniines",sentiment analysis,"The article discusses using Natural Language Processing and Sentiment Classification with a Recurrent Neural Network to analyze the sentiments and manifestations of Twitter users regarding the COVID-19 pandemic. The article shows that the trained model works accurately and can determine emotional polarity in tweets. The emotional classifications were properly segmented and showed that the overall positive manifestation remained on social media surfaces during the pandemic, with occasional negative and other manifestations. The article also suggests future work in creating an interface that better visualizes and interacts with users and implementing or refactoring potential tensorflow features.",Object and Sentiment Recognition,ro prem!Average svM —— | coneatenation | te ‘Pieeral stocks beced tadCNN fine-tune trom pr-trained model. \ (Cw ine tuned from pre-trained model Cun fine-tuned from pretrained model ENN Fine-tune from pretained modelGenie Lemma 7 I I Image pre-processing an | Feature fusion I ‘ I teinie eaiciniines,Object Recognition
254,Social Media Analytics: Analysis and Visualisation of News Diffusion using NodeXL,"Social Media, Social Network Analysis, Twitter, Information Diffusion, Sentiment Analysis.","Purpose: The purpose of this paper is to provide an overview of NodeXL in the context of news diffusion. Journalists often include a social media dimension in their stories but lack the tools to get digital photos of the virtual crowds about which they write. NodeXL is an easy to use tool for collecting, analysing, visualizing, and reporting on the patterns found in collections of connections in streams of social media. With a network map patterns emerge that highlight key people, groups, divisions and bridges, themes and related resources. Design/methodology/approach: This study conducts a literature review of previous empirical work which has utilised NodeXL and highlights the potential of NodeXL to provide network insights of virtual crowds during emerging news events. It then develops a number of guidelines which can be utlised by news media teams to measure and map information diffusion during emerging news events. Findings: One emergent software application known as NodeXL has allowed journalists to take ‘group photos’ of the connections among a group of users on social media. It was found that a diverse range of disciplines utilise NodeXL in academic research. Furthermore based on the features of NodeXL a number of guidelines were developed which provide insight into how to measure and map emerging news events on Twitter. Social implications: With a set of social media network images a journalist can cover a set of social media content streams and quickly grasp ""situational awareness"" of the shape of the crowd. Since social media popular support is often cited but not documented, NodeXL social media network maps can help journalists quickly document the social landscape utilising an innovative approach. Originality/value: This is the first empirical study to review literature on NodeXL, and to provide insight into the value of network visualisations and analytics for the news media domain. Moreover, it is the first empirical study to develop guidelines that will act as a valuable resource for newsrooms looking to acquire insight into emerging news events from the stream of social media posts. In the era of fake news and automated accounts i.e., bots the ability to highlight opinion leaders and ascertain their allegiances will be of importance in today’s news climate.","This paper has provided an overview of some of the diverse uses of NodeXL from across a number of academic disciplines. The abstract provides an interesting starting point in reviewing the literature utilising NodeXL for research purposes. Now, as social media platforms become more and more popular it is critically important to study them. An interesting feature of NodeXL is that it requires no technical and/or programming knowledge which potentially allows it to be utlised across a wide range of disciplines from science and engineering, the social sciences, and the humanities.","Social Media Analytics: Analysis and Visualisation of News Diffusion using NodeXLSocial Media, Social Network Analysis, Twitter, Information Diffusion, Sentiment Analysis.Purpose: The purpose of this paper is to provide an overview of NodeXL in the context of news diffusion. Journalists often include a social media dimension in their stories but lack the tools to get digital photos of the virtual crowds about which they write. NodeXL is an easy to use tool for collecting, analysing, visualizing, and reporting on the patterns found in collections of connections in streams of social media. With a network map patterns emerge that highlight key people, groups, divisions and bridges, themes and related resources. Design/methodology/approach: This study conducts a literature review of previous empirical work which has utilised NodeXL and highlights the potential of NodeXL to provide network insights of virtual crowds during emerging news events. It then develops a number of guidelines which can be utlised by news media teams to measure and map information diffusion during emerging news events. Findings: One emergent software application known as NodeXL has allowed journalists to take ‘group photos’ of the connections among a group of users on social media. It was found that a diverse range of disciplines utilise NodeXL in academic research. Furthermore based on the features of NodeXL a number of guidelines were developed which provide insight into how to measure and map emerging news events on Twitter. Social implications: With a set of social media network images a journalist can cover a set of social media content streams and quickly grasp ""situational awareness"" of the shape of the crowd. Since social media popular support is often cited but not documented, NodeXL social media network maps can help journalists quickly document the social landscape utilising an innovative approach. Originality/value: This is the first empirical study to review literature on NodeXL, and to provide insight into the value of network visualisations and analytics for the news media domain. Moreover, it is the first empirical study to develop guidelines that will act as a valuable resource for newsrooms looking to acquire insight into emerging news events from the stream of social media posts. In the era of fake news and automated accounts i.e., bots the ability to highlight opinion leaders and ascertain their allegiances will be of importance in today’s news climate.This paper has provided an overview of some of the diverse uses of NodeXL from across a number of academic disciplines. The abstract provides an interesting starting point in reviewing the literature utilising NodeXL for research purposes. Now, as social media platforms become more and more popular it is critically important to study them. An interesting feature of NodeXL is that it requires no technical and/or programming knowledge which potentially allows it to be utlised across a wide range of disciplines from science and engineering, the social sciences, and the humanities.ay| ex Vick ic(d, 9) Pyrn(c | d) = Z(d) p (>:ie Aa 4 ey i) or ae 20",sentiment analysis,"This paper provides an overview of NodeXL, a tool for collecting, analyzing, visualizing, and reporting on social media connections. It highlights the potential of NodeXL to provide network insights during emerging news events and develops guidelines for news media teams to measure and map information diffusion on Twitter. The study finds that NodeXL is used across a diverse range of disciplines and can help journalists quickly document the social landscape of a crowd. This is the first empirical study to review literature on NodeXL and provide insight into the value of network visualizations and analytics for the news media domain. The tool requires no technical or programming knowledge and can be utilized across a wide range of disciplines.",Object and Sentiment Recognition,"ay| ex Vick ic(d, 9) Pyrn(c | d) = Z(d) p (>:ie Aa 4 ey i) or ae 20",Sentiment Analysis
255,Machine Learning and Semantic Sentiment Analysis based Algorithms for Suicide Sentiment Prediction in Social Networks,: Sentiment Analysis; Machine Learning; Suicide; Social Networks; Tweets; Semantic Sentiment Analysis.,"Sentiment analysis is one of the new challenges appeared in automatic language processing with the advent of social networks. Taking advantage of the amount of information is now available, research and industry have sought ways to automatically analyze sentiments and user opinions expressed in social networks. In this paper, we place ourselves in a difficult context, on the sentiments that could thinking of suicide. In particular, we propose to address the lack of terminological resources related to suicide by a method of constructing a vocabulary associated with suicide. We then propose, for a better analysis, to investigate Weka as a tool of data mining based on machine learning algorithms that can extract useful information from Twitter data collected by Twitter4J. Therefore, an algorithm of computing semantic analysis between tweets in training set and tweets in data set based on WordNet is proposed. Experimental results demonstrate that our method based on machine learning algorithms and semantic sentiment analysis can extract predictions of suicidal ideation using Twitter Data. In addition, this work verify the effectiveness of performance in term of accuracy and precision on semantic sentiment analysis that could thinking of suicide.","As part of this work, we present our potentially method based on different machine learning for using the social network Twitter as a preventive force in the fight against suicide. In addition, our work can analyze semantically the Twitter data based on WordNet. In our future work, we plan to further improve and refine our techniques in order to enhance the accuracy of our method. Thereafter, we involved to test multilingual WordNet for tweets and to orient this work in a big data environment.","Machine Learning and Semantic Sentiment Analysis based Algorithms for Suicide Sentiment Prediction in Social Networks: Sentiment Analysis; Machine Learning; Suicide; Social Networks; Tweets; Semantic Sentiment Analysis.Sentiment analysis is one of the new challenges appeared in automatic language processing with the advent of social networks. Taking advantage of the amount of information is now available, research and industry have sought ways to automatically analyze sentiments and user opinions expressed in social networks. In this paper, we place ourselves in a difficult context, on the sentiments that could thinking of suicide. In particular, we propose to address the lack of terminological resources related to suicide by a method of constructing a vocabulary associated with suicide. We then propose, for a better analysis, to investigate Weka as a tool of data mining based on machine learning algorithms that can extract useful information from Twitter data collected by Twitter4J. Therefore, an algorithm of computing semantic analysis between tweets in training set and tweets in data set based on WordNet is proposed. Experimental results demonstrate that our method based on machine learning algorithms and semantic sentiment analysis can extract predictions of suicidal ideation using Twitter Data. In addition, this work verify the effectiveness of performance in term of accuracy and precision on semantic sentiment analysis that could thinking of suicide.As part of this work, we present our potentially method based on different machine learning for using the social network Twitter as a preventive force in the fight against suicide. In addition, our work can analyze semantically the Twitter data based on WordNet. In our future work, we plan to further improve and refine our techniques in order to enhance the accuracy of our method. Thereafter, we involved to test multilingual WordNet for tweets and to orient this work in a big data environment.ay| ex Vick ic(d, 9) Pyrn(c | d) = Z(d) p (>:ie Aa 4 ey i) or ae 20",sentiment analysis,"The text discusses sentiment analysis in the context of social media and proposes a method for constructing a vocabulary related to suicide to better analyze tweets related to suicidal ideation. The authors use Weka, a tool for data mining based on machine learning algorithms, to extract useful information from Twitter data and propose an algorithm for computing semantic analysis based on WordNet. The experimental results demonstrate the effectiveness of the method in predicting suicidal ideation with high accuracy and precision. The authors suggest using social media as a preventive force in the fight against suicide and plan to further improve and refine their techniques, including testing multilingual WordNet and working in a big data environment.",Object and Sentiment Recognition,"ay| ex Vick ic(d, 9) Pyrn(c | d) = Z(d) p (>:ie Aa 4 ey i) or ae 20",Sentiment Analysis
256,Social media sentiment analysis: lexicon versus machine learning,"Sentiment analysis, Social media, Consumer-generated content","Purpose – With the soaring volumes of brand-related social media conversations, digital marketers have extensive opportunities to track and analyse consumers’ feelings and opinions about brands, products or services embedded within consumer-generated content (CGC). These “Big Data” opportunities render manual approaches to sentiment analysis impractical and raise the need to develop automated tools to analyse consumer sentiment expressed in text format. This paper aims to evaluate and compare the performance of two prominent approaches to automated sentiment analysis applied to CGC on social media and explores the benefits of combining them. Design/methodology/approach – A sample of 850 consumer comments from 83 Facebook brand pages are used to test and compare lexicon-based and machine learning approaches to sentiment analysis, as well as their combination, using the LIWC2015 lexicon and RTextTools machine learning package. Findings – Results show the two approaches are similar in accuracy, both achieving higher accuracy when classifying positive sentiment than negative sentiment. However, they differ substantially in their classification ensembles. The combined approach demonstrates significantly improved performance in classifying positive sentiment. Research limitations/implications – Further research is required to improve the accuracy of negative sentiment classification. The combined approach needs to be applied to other kinds of CGCs on social media such as tweets. Practical implications – The findings inform decision-making around which sentiment analysis approaches (or a combination thereof) is best to analyse CGC on social media. Originality/value – This study combines two sentiment analysis approaches and demonstrates significantly improved performance","when applied on social media conversations, the two automated approaches have similar performance. Second, we demonstrate that combining the different approaches significantly improves classification performance in terms of precision and recall for positive sentiment. This finding suggests the great potential of a combined approach to gain deeper insights into positive social media conversations. Given the soaring volumes of brand-related social media conversations and the lack of guidance as to what tools are adequate to analyse such “Big Data”, our study fills a gap in the literature and adds to industry best practices. Our findings form the basis of decision-making around which approach is best for marketers to analyse consumers’ social media conversations and how to best combine approaches to achieve better outcome. Sentiment analysis is only one way to explore online conversations with other analytic approaches available for knowledge discovery. For these reasons, further research is required to guide marketers on how to select and match the various text analysis approaches with the different social","Social media sentiment analysis: lexicon versus machine learningSentiment analysis, Social media, Consumer-generated contentPurpose – With the soaring volumes of brand-related social media conversations, digital marketers have extensive opportunities to track and analyse consumers’ feelings and opinions about brands, products or services embedded within consumer-generated content (CGC). These “Big Data” opportunities render manual approaches to sentiment analysis impractical and raise the need to develop automated tools to analyse consumer sentiment expressed in text format. This paper aims to evaluate and compare the performance of two prominent approaches to automated sentiment analysis applied to CGC on social media and explores the benefits of combining them. Design/methodology/approach – A sample of 850 consumer comments from 83 Facebook brand pages are used to test and compare lexicon-based and machine learning approaches to sentiment analysis, as well as their combination, using the LIWC2015 lexicon and RTextTools machine learning package. Findings – Results show the two approaches are similar in accuracy, both achieving higher accuracy when classifying positive sentiment than negative sentiment. However, they differ substantially in their classification ensembles. The combined approach demonstrates significantly improved performance in classifying positive sentiment. Research limitations/implications – Further research is required to improve the accuracy of negative sentiment classification. The combined approach needs to be applied to other kinds of CGCs on social media such as tweets. Practical implications – The findings inform decision-making around which sentiment analysis approaches (or a combination thereof) is best to analyse CGC on social media. Originality/value – This study combines two sentiment analysis approaches and demonstrates significantly improved performancewhen applied on social media conversations, the two automated approaches have similar performance. Second, we demonstrate that combining the different approaches significantly improves classification performance in terms of precision and recall for positive sentiment. This finding suggests the great potential of a combined approach to gain deeper insights into positive social media conversations. Given the soaring volumes of brand-related social media conversations and the lack of guidance as to what tools are adequate to analyse such “Big Data”, our study fills a gap in the literature and adds to industry best practices. Our findings form the basis of decision-making around which approach is best for marketers to analyse consumers’ social media conversations and how to best combine approaches to achieve better outcome. Sentiment analysis is only one way to explore online conversations with other analytic approaches available for knowledge discovery. For these reasons, further research is required to guide marketers on how to select and match the various text analysis approaches with the different social‘Gi: macronleaksiles macron des qui pour’ et par sur (G3: macronleaks wiklleaks russian _|GE: macronleaks macron G7: macronleaks macron | GB: macronleaks les. = macron files update 2 assessment hacked pen sahouraxo msmfrench damg french macron qui france = several french french emails leaks france's jeshdcaplan enviBfehe macrongate-est hackers = 2 SS = staff people gréuoite "" prssidentelie2017- ‘macronleaks military gru russian fancy bear traced malcolmnaho= ‘confirmation intelligence = me == masked Tt 3 ‘ = (G10: macronleaks une! aa MAL Sse VY les pourgurtoutet G16: ‘G2: macionleaks macron fle ef pour & jackposobiec Cara. nussie ious dite 2. des french G5: macronleal n french medi ' macrongate meee(Dividea] Polarized Crowds [Fragmented] nd Clusters {In Hub & Spoke] Broadcast Network [Dividea} finttab Broadcast Netw [clustered] Community Cluster [our Hub & spoke} ‘Support Network Tight [clusteredGgedauaaccagd seeaatis[Divided] [Unified] Polarized Crowds Tight Crowd [Clustered] Community Clusters [In-Hub & Spoke] [Out-Hub & Spoke] Broadcast Network Support Network [Fragmented] Brand Clusters[Divided] [Unified] Polarized Crowds Tight Crowd [Fragmented] [Clustered] Brand Clusters Community Clusters [In-Hub & Spoke] @ : = es se == e [Out-Hub & Spoke] Broadcast Network = , Support Network",sentiment analysis,"This paper evaluates and compares two approaches to automated sentiment analysis, lexicon-based and machine learning, and explores the benefits of combining them for analyzing consumer-generated content on social media. The study finds that both approaches have similar accuracy, but differ in their classification ensembles. The combined approach significantly improves classification performance for positive sentiment. Further research is needed to improve accuracy for negative sentiment classification and to apply the combined approach to other types of CGCs on social media. The findings inform decision-making for marketers analyzing social media conversations and add to industry best practices.",Object and Sentiment Recognition,"‘Gi: macronleaksiles macron des qui pour’ et par sur (G3: macronleaks wiklleaks russian _|GE: macronleaks macron G7: macronleaks macron | GB: macronleaks les. = macron files update 2 assessment hacked pen sahouraxo msmfrench damg french macron qui france = several french french emails leaks france's jeshdcaplan enviBfehe macrongate-est hackers = 2 SS = staff people gréuoite "" prssidentelie2017- ‘macronleaks military gru russian fancy bear traced malcolmnaho= ‘confirmation intelligence = me == masked Tt 3 ‘ = (G10: macronleaks une! aa MAL Sse VY les pourgurtoutet G16: ‘G2: macionleaks macron fle ef pour & jackposobiec Cara. nussie ious dite 2. des french G5: macronleal n french medi ' macrongate meee(Dividea] Polarized Crowds [Fragmented] nd Clusters {In Hub & Spoke] Broadcast Network [Dividea} finttab Broadcast Netw [clustered] Community Cluster [our Hub & spoke} ‘Support Network Tight [clusteredGgedauaaccagd seeaatis[Divided] [Unified] Polarized Crowds Tight Crowd [Clustered] Community Clusters [In-Hub & Spoke] [Out-Hub & Spoke] Broadcast Network Support Network [Fragmented] Brand Clusters[Divided] [Unified] Polarized Crowds Tight Crowd [Fragmented] [Clustered] Brand Clusters Community Clusters [In-Hub & Spoke] @ : = es se == e [Out-Hub & Spoke] Broadcast Network = , Support Network",Sentiment Analysis
257,Sentiment Analysis in Czech Social Media Using Supervised Machine Learning,"sentiment analysis, czech social media","This article provides an in-depth research of machine learning methods for sentiment analysis of Czech social media. Whereas in English, Chinese, or Spanish this field has a long history and evaluation datasets for various domains are widely available, in case of Czech language there has not yet been any systematical research conducted. We tackle this issue and establish a common ground for further research by providing a large humanannotated Czech social media corpus. Furthermore, we evaluate state-of-the-art supervised machine learning methods for sentiment analysis. We explore different pre-processing techniques and employ various features and classifiers. Moreover, in addition to our newly created social media dataset, we also report results on other widely popular domains, such as movie and product reviews. We believe that this article will not only extend the current sentiment analysis research to another family of languages, but will also encourage competition which potentially leads to the production of high-end commercial solutions.","This article presented an in-depth research of supervised machine learning methods for sentiment analysis of Czech social media. We created a large Facebook dataset containing 10,000 posts, accompanied by human annotation with substantial agreement (Cohen’s κ 0.66). The dataset is freely available for non-commercial purposes.19 We thoroughly evaluated various state-of-the-art features and classifiers as well as different language-specific preprocessing techniques. We significantly outperformed the baseline (unigram feature without preprocessing) in three-class classification and achieved Fmeasure 0.69 using a combination of features (unigrams, bigrams, POS features, emoticons, character n-grams) and preprocessing techniques (unsupervised stemming and phonetic transcription). In addition, we reported results in two other domains (movie and product reviews) with a significant improvement over the baseline. To the best of our knowledge, this article is the only of its kind that deals with sentiment analysis in Czech social media in such a thorough manner. Not only it uses a dataset that is magnitudes larger than any from the related work, but also incorporates state-of-the-art features and classifiers. We believe that the outcomes of this article will not only help to set the common ground for sentiment analysis for the Czech language but also help to extend the research outside the mainstream languages in this research field.","Sentiment Analysis in Czech Social Media Using Supervised Machine Learningsentiment analysis, czech social mediaThis article provides an in-depth research of machine learning methods for sentiment analysis of Czech social media. Whereas in English, Chinese, or Spanish this field has a long history and evaluation datasets for various domains are widely available, in case of Czech language there has not yet been any systematical research conducted. We tackle this issue and establish a common ground for further research by providing a large humanannotated Czech social media corpus. Furthermore, we evaluate state-of-the-art supervised machine learning methods for sentiment analysis. We explore different pre-processing techniques and employ various features and classifiers. Moreover, in addition to our newly created social media dataset, we also report results on other widely popular domains, such as movie and product reviews. We believe that this article will not only extend the current sentiment analysis research to another family of languages, but will also encourage competition which potentially leads to the production of high-end commercial solutions.This article presented an in-depth research of supervised machine learning methods for sentiment analysis of Czech social media. We created a large Facebook dataset containing 10,000 posts, accompanied by human annotation with substantial agreement (Cohen’s κ 0.66). The dataset is freely available for non-commercial purposes.19 We thoroughly evaluated various state-of-the-art features and classifiers as well as different language-specific preprocessing techniques. We significantly outperformed the baseline (unigram feature without preprocessing) in three-class classification and achieved Fmeasure 0.69 using a combination of features (unigrams, bigrams, POS features, emoticons, character n-grams) and preprocessing techniques (unsupervised stemming and phonetic transcription). In addition, we reported results in two other domains (movie and product reviews) with a significant improvement over the baseline. To the best of our knowledge, this article is the only of its kind that deals with sentiment analysis in Czech social media in such a thorough manner. Not only it uses a dataset that is magnitudes larger than any from the related work, but also incorporates state-of-the-art features and classifiers. We believe that the outcomes of this article will not only help to set the common ground for sentiment analysis for the Czech language but also help to extend the research outside the mainstream languages in this research field.‘Gi: macronleaksiles macron des qui pour’ et par sur (G3: macronleaks wiklleaks russian _|GE: macronleaks macron G7: macronleaks macron | GB: macronleaks les. = macron files update 2 assessment hacked pen sahouraxo msmfrench damg french macron qui france = several french french emails leaks france's jeshdcaplan enviBfehe macrongate-est hackers = 2 SS = staff people gréuoite "" prssidentelie2017- ‘macronleaks military gru russian fancy bear traced malcolmnaho= ‘confirmation intelligence = me == masked Tt 3 ‘ = (G10: macronleaks une! aa MAL Sse VY les pourgurtoutet G16: ‘G2: macionleaks macron fle ef pour & jackposobiec Cara. nussie ious dite 2. des french G5: macronleal n french medi ' macrongate meee(Dividea] Polarized Crowds [Fragmented] nd Clusters {In Hub & Spoke] Broadcast Network [Dividea} finttab Broadcast Netw [clustered] Community Cluster [our Hub & spoke} ‘Support Network Tight [clusteredGgedauaaccagd seeaatis[Divided] [Unified] Polarized Crowds Tight Crowd [Clustered] Community Clusters [In-Hub & Spoke] [Out-Hub & Spoke] Broadcast Network Support Network [Fragmented] Brand Clusters[Divided] [Unified] Polarized Crowds Tight Crowd [Fragmented] [Clustered] Brand Clusters Community Clusters [In-Hub & Spoke] @ : = es se == e [Out-Hub & Spoke] Broadcast Network = , Support Network",sentiment analysis,"The article is about conducting research on machine learning methods for sentiment analysis of Czech social media. The authors created a large dataset of 10,000 posts with human annotation and evaluated various features and classifiers for sentiment analysis. They outperformed the baseline and reported results on other domains. The article aims to extend sentiment analysis research to another family of languages and encourage competition for high-end commercial solutions. The outcomes of this article will help to set the common ground for sentiment analysis for the Czech language and extend research outside the mainstream languages in this field.",Object and Sentiment Recognition,"‘Gi: macronleaksiles macron des qui pour’ et par sur (G3: macronleaks wiklleaks russian _|GE: macronleaks macron G7: macronleaks macron | GB: macronleaks les. = macron files update 2 assessment hacked pen sahouraxo msmfrench damg french macron qui france = several french french emails leaks france's jeshdcaplan enviBfehe macrongate-est hackers = 2 SS = staff people gréuoite "" prssidentelie2017- ‘macronleaks military gru russian fancy bear traced malcolmnaho= ‘confirmation intelligence = me == masked Tt 3 ‘ = (G10: macronleaks une! aa MAL Sse VY les pourgurtoutet G16: ‘G2: macionleaks macron fle ef pour & jackposobiec Cara. nussie ious dite 2. des french G5: macronleal n french medi ' macrongate meee(Dividea] Polarized Crowds [Fragmented] nd Clusters {In Hub & Spoke] Broadcast Network [Dividea} finttab Broadcast Netw [clustered] Community Cluster [our Hub & spoke} ‘Support Network Tight [clusteredGgedauaaccagd seeaatis[Divided] [Unified] Polarized Crowds Tight Crowd [Clustered] Community Clusters [In-Hub & Spoke] [Out-Hub & Spoke] Broadcast Network Support Network [Fragmented] Brand Clusters[Divided] [Unified] Polarized Crowds Tight Crowd [Fragmented] [Clustered] Brand Clusters Community Clusters [In-Hub & Spoke] @ : = es se == e [Out-Hub & Spoke] Broadcast Network = , Support Network",Sentiment Analysis
258,Deep learning and multilingual sentiment analysis on social media data: An overview,"Sentiment analysis, Multilingual, Cross-lingual, Code-switching, Social media.","Twenty-four studies on twenty-three distinct languages and eleven social media illustrate the steady interest in deep learning approaches for multilingual sentiment analysis of social media. We improve over previous reviews with wider coverage from 2017 to 2020 as well as a study focused on the underlying ideas and commonalities behind the different solutions to achieve multilingual sentiment analysis. Interesting findings of our research are (i) the shift of research interest to cross-lingual and code-switching approaches, (ii) the apparent stagnation of the less complex architectures derived from a backbone featuring an embedding layer, a feature extractor based on a single CNN or LSTM and a classifier, (iii) the lack of approaches tackling multilingual aspect-based sentiment analysis through deep learning, and, surprisingly, (iv) the lack of more complex architectures such as the transformers-based, despite results suggest the more difficult tasks requires more elaborated architectures","In this work, we reviewed 24 that studied 23 and 11 different languages and sources. The observed trend evidences the steady interest in this domain, so we expect to see this direction continue. As regards the different MSA setups, the multilingual approach seems to be decreasing in interest. However, aspect-based sentiment analysis is still an understudied domain and an open research field with a lot of scope for future works. We highlighted the main ideas authors proposed to tackle the challenge that represents the lack of annotated data or to achieve language independent models. Despite state-of-the-art results in some cases, the simpler backbone comprising embeddings, a feature extractor, and a classifier seems to be unappropriated for more complex scenarios. Also, there are unsolved questions such as which type of embedding captures better the particulars of MSA. We hint about future research directions, for example, if ideas such as contextualized embeddings, which have proven very useful in other tasks, can further improve MSA. Finally, although studies have covered very different languages such as Arabic, Chinese, or Hindi, the world is extraordinarily rich in languages, cultures, and ways of expressing feelings. Thus, better approaches need to be assessed or developed for new scenarios.","Deep learning and multilingual sentiment analysis on social media data: An overviewSentiment analysis, Multilingual, Cross-lingual, Code-switching, Social media.Twenty-four studies on twenty-three distinct languages and eleven social media illustrate the steady interest in deep learning approaches for multilingual sentiment analysis of social media. We improve over previous reviews with wider coverage from 2017 to 2020 as well as a study focused on the underlying ideas and commonalities behind the different solutions to achieve multilingual sentiment analysis. Interesting findings of our research are (i) the shift of research interest to cross-lingual and code-switching approaches, (ii) the apparent stagnation of the less complex architectures derived from a backbone featuring an embedding layer, a feature extractor based on a single CNN or LSTM and a classifier, (iii) the lack of approaches tackling multilingual aspect-based sentiment analysis through deep learning, and, surprisingly, (iv) the lack of more complex architectures such as the transformers-based, despite results suggest the more difficult tasks requires more elaborated architecturesIn this work, we reviewed 24 that studied 23 and 11 different languages and sources. The observed trend evidences the steady interest in this domain, so we expect to see this direction continue. As regards the different MSA setups, the multilingual approach seems to be decreasing in interest. However, aspect-based sentiment analysis is still an understudied domain and an open research field with a lot of scope for future works. We highlighted the main ideas authors proposed to tackle the challenge that represents the lack of annotated data or to achieve language independent models. Despite state-of-the-art results in some cases, the simpler backbone comprising embeddings, a feature extractor, and a classifier seems to be unappropriated for more complex scenarios. Also, there are unsolved questions such as which type of embedding captures better the particulars of MSA. We hint about future research directions, for example, if ideas such as contextualized embeddings, which have proven very useful in other tasks, can further improve MSA. Finally, although studies have covered very different languages such as Arabic, Chinese, or Hindi, the world is extraordinarily rich in languages, cultures, and ways of expressing feelings. Thus, better approaches need to be assessed or developed for new scenarios.1..1,MPQA 2 10,624 0.31/0.69 |W_ i Av g)12OJ 1 OE (a, t; 0) 00.—=OUWN » 00 + AO.GUENMNEED 66sse6e HEHE H fo 200000}aballaI (©G OOOO) COOOOOO) (CO8C8 (@O0O0O0O0) (0000008)ler — ej |! + Hey — eg! (6) 7 ny + neaK Ecn(p,t;0) = — > trlogdg(p;9). (8) bel1 Brecl[erse2]) = 5 ||lersea] ~ [4:45]|?. @) This model of a standard autoencoder is boxed inci:ch] = = we) p+o2)a crycom a od Creer on * ea Eo bl wf 6 fb! WwetionsPredicted Sorry,Hugs YouRock Teehee 1|Understand Wow, Just Wow Sentiment Distribution Semantic Representations Indicesa wa wei wee al eetaeda.tials",sentiment analysis,"The text discusses a review of 24 studies on multilingual sentiment analysis of social media in 23 different languages from 2017 to 2020. The review shows a trend towards cross-lingual and code-switching approaches, and a lack of complex architectures such as transformers-based for more difficult tasks. Aspect-based sentiment analysis is still an understudied domain with potential for future works. The simpler backbone architecture is insufficient for more complex scenarios, and there are still unsolved questions regarding which type of embedding works best. The text concludes that more research is needed to better address the diversity of languages, cultures, and expressions of feelings.",Object and Sentiment Recognition,"1..1,MPQA 2 10,624 0.31/0.69 |W_ i Av g)12OJ 1 OE (a, t; 0) 00.—=OUWN » 00 + AO.GUENMNEED 66sse6e HEHE H fo 200000}aballaI (©G OOOO) COOOOOO) (CO8C8 (@O0O0O0O0) (0000008)ler — ej |! + Hey — eg! (6) 7 ny + neaK Ecn(p,t;0) = — > trlogdg(p;9). (8) bel1 Brecl[erse2]) = 5 ||lersea] ~ [4:45]|?. @) This model of a standard autoencoder is boxed inci:ch] = = we) p+o2)a crycom a od Creer on * ea Eo bl wf 6 fb! WwetionsPredicted Sorry,Hugs YouRock Teehee 1|Understand Wow, Just Wow Sentiment Distribution Semantic Representations Indicesa wa wei wee al eetaeda.tials",Sentiment Analysis
259,A Probabilistic Generative Model for Mining Cybercriminal Network from Online Social Media: A Review,"Latent Dirichlet Allocation(LDA), Laplacian Semantic Ranking, Inferential Language Model, Text Mining","Social media has been increasingly utilized as an area of sharing and gathering of information. Data mining is the process of analyzing data from different context and summarizes them into useful information. It allows the users to analyze the data, categorize them and identifies the relationship inferred in them. Text mining often referred to as text data mining can be used to derive information from text. Text analysis can be used in information retrieval, information extraction, pattern recognition, frequency distribution and data mining techniques. An application of this is to scan a set of documents in natural language for predictive classification purposes. Recent researches shows that the number of crimes are increasing through social media that may cause tremendous loss to organizations. Existing security methods are weak in cyber crime forensics and predictions. The contribution of this paper is to mine cybercriminal network which can reveal both implicit and explicit meanings among cybercriminal based on their conversation messages","This paper analysis various generative models and network mining techniques that can be used to uncover the cybercriminal network. The ability to mine social media to extract relevant information is a crucial task. Since a great proportion of information contained in social media are in unstructured form, there is a need state-ofthe art tool to collect and apply intelligence methods .Existing cyber technologies are not effective and they are weak in cybercrime forensics. Here a novel context sensitive text mining method is recommended by which latent concepts are extracted and these latent concepts are subjected to extract the semantics which describes the cybercriminal relationships. This system can be enhanced by genetic algorithm. Genetic Algorithm is a robust search method requiring little information to search effectively in a large or poorlyunderstood search space.The working of genetic algorithm is as follow: First a population is created from a group of individuals and 3 International Journal of Computer Applications (0975 - 8887) Volume 134 - No.14, January 2016 then these individuals are evaluated. The evaluation is performed and each individual are given a fitness score based on which they are evaluated .Two individuals are selected based on their fitness score, higher the score greater the chance to be selected. This process continues until a best solution is obtained from a set of candidate solutions. Genetic algorithm takes the advantage of giving greater weight to individuals with best fitness score and concentrate the search in regions which leads to select the best topics. Genetic algorithm provides a heuristic search to solve optimization problems. Here Genetic algorithm provides a better solution in which more concepts can be extracted and time efficiency can be improved. By mining the network security intelligence in social media not only facilitates the cyber attack but also has an intelligence to predict the cyber attack before they can be launched.","A Probabilistic Generative Model for Mining Cybercriminal Network from Online Social Media: A ReviewLatent Dirichlet Allocation(LDA), Laplacian Semantic Ranking, Inferential Language Model, Text MiningSocial media has been increasingly utilized as an area of sharing and gathering of information. Data mining is the process of analyzing data from different context and summarizes them into useful information. It allows the users to analyze the data, categorize them and identifies the relationship inferred in them. Text mining often referred to as text data mining can be used to derive information from text. Text analysis can be used in information retrieval, information extraction, pattern recognition, frequency distribution and data mining techniques. An application of this is to scan a set of documents in natural language for predictive classification purposes. Recent researches shows that the number of crimes are increasing through social media that may cause tremendous loss to organizations. Existing security methods are weak in cyber crime forensics and predictions. The contribution of this paper is to mine cybercriminal network which can reveal both implicit and explicit meanings among cybercriminal based on their conversation messagesThis paper analysis various generative models and network mining techniques that can be used to uncover the cybercriminal network. The ability to mine social media to extract relevant information is a crucial task. Since a great proportion of information contained in social media are in unstructured form, there is a need state-ofthe art tool to collect and apply intelligence methods .Existing cyber technologies are not effective and they are weak in cybercrime forensics. Here a novel context sensitive text mining method is recommended by which latent concepts are extracted and these latent concepts are subjected to extract the semantics which describes the cybercriminal relationships. This system can be enhanced by genetic algorithm. Genetic Algorithm is a robust search method requiring little information to search effectively in a large or poorlyunderstood search space.The working of genetic algorithm is as follow: First a population is created from a group of individuals and 3 International Journal of Computer Applications (0975 - 8887) Volume 134 - No.14, January 2016 then these individuals are evaluated. The evaluation is performed and each individual are given a fitness score based on which they are evaluated .Two individuals are selected based on their fitness score, higher the score greater the chance to be selected. This process continues until a best solution is obtained from a set of candidate solutions. Genetic algorithm takes the advantage of giving greater weight to individuals with best fitness score and concentrate the search in regions which leads to select the best topics. Genetic algorithm provides a heuristic search to solve optimization problems. Here Genetic algorithm provides a better solution in which more concepts can be extracted and time efficiency can be improved. By mining the network security intelligence in social media not only facilitates the cyber attack but also has an intelligence to predict the cyber attack before they can be launched.he @emeraldinsight rae Hea oefm emeraldinsightImersection Size (ML or LB) aM ""Legend { aE |UB=Lotcon Based n 128 | ML= Machine learning! 68 when lexicon-based and machine ASN eal a) leasning approaches are combined so I — my? Ae Ge we ow ww M ™ = = ML&LB&M (ML&LB) (LB&M) = (ML&M) = LBonly ML only M only tutnetas «60 taneea 0 bua‘Intersection Size (OA. er LB) au . £239 when lexicon-based and machine M=Manual t ‘ J iad | ge . I . , ! 5 oo ee Ce wa v a 6 M ™ ” M “ “ wy MUALBEM (MLALB) (BEM) © (MLM) —LBonly ML only only Gece: Sue eaeIntersection Size (ML or LB) &M ML_ LB M 68 when lexicon-based and machine learning approaches are combined 128 ' Legend | i | LB = Lexicon-Based | ML = Machine learning i | M= Manual 100 50 37 24 o of |! _ 7 LB l e ML ; e M e MLULB) ML@LB ML a ""4 iB M M M ML&LB&M (ML&LB) (LB&M) = (ML&M) butnotM butnotML butnotLB LB only ML only MonlyIntersection Size i ML Le, | LB=Lexicon-based 239 when lexicon-based and machine M = Manual learning approaches are combined ime 152 180 100 50] LB ML. I e e MLL LB MALB) ML) LB ML LB) (ML 1B ML LB ’ s2° “RE i we ML&LB&M (ML&LB) (LB&M) —(ML&M) LB only ML only Monly tutioetl 60 oben. bites | ML= Machine learning!",text  mining,"The text discusses the increasing use of social media for sharing and gathering information, and the process of data mining to analyze and categorize this data. Text mining is used to derive information from text, which can be applied to various techniques including information retrieval, extraction, pattern recognition, and data mining. The paper highlights the importance of mining cybercriminal networks in social media to reveal implicit and explicit relationships among cybercriminals based on their conversations. The use of genetic algorithms in text mining is recommended for better time efficiency and more efficient concept extraction. This can help in predicting and preventing cyber attacks, which is crucial given the weaknesses of existing cyber security methods.",Natural Language Processing,"he @emeraldinsight rae Hea oefm emeraldinsightImersection Size (ML or LB) aM ""Legend { aE |UB=Lotcon Based n 128 | ML= Machine learning! 68 when lexicon-based and machine ASN eal a) leasning approaches are combined so I — my? Ae Ge we ow ww M ™ = = ML&LB&M (ML&LB) (LB&M) = (ML&M) = LBonly ML only M only tutnetas «60 taneea 0 bua‘Intersection Size (OA. er LB) au . £239 when lexicon-based and machine M=Manual t ‘ J iad | ge . I . , ! 5 oo ee Ce wa v a 6 M ™ ” M “ “ wy MUALBEM (MLALB) (BEM) © (MLM) —LBonly ML only only Gece: Sue eaeIntersection Size (ML or LB) &M ML_ LB M 68 when lexicon-based and machine learning approaches are combined 128 ' Legend | i | LB = Lexicon-Based | ML = Machine learning i | M= Manual 100 50 37 24 o of |! _ 7 LB l e ML ; e M e MLULB) ML@LB ML a ""4 iB M M M ML&LB&M (ML&LB) (LB&M) = (ML&M) butnotM butnotML butnotLB LB only ML only MonlyIntersection Size i ML Le, | LB=Lexicon-based 239 when lexicon-based and machine M = Manual learning approaches are combined ime 152 180 100 50] LB ML. I e e MLL LB MALB) ML) LB ML LB) (ML 1B ML LB ’ s2° “RE i we ML&LB&M (ML&LB) (LB&M) —(ML&M) LB only ML only Monly tutioetl 60 oben. bites | ML= Machine learning!",Sentiment Analysis
260,Sentiment Data Flow Analysis by Means of Dynamic Linguistic Patterns,"computational intelligence, sentiment analysis.","Computational intelligence (CI) is an established research field that focuses on using brain- or natureinspired models to solve complex real-world problems that cannot otherwise be effectively solved by traditional models. One of the biggest challenges of CI is the emulation of human intelligence, which includes not only human-like reasoning but also human interaction, e.g., human language and human emotions. Emotions, sentiment, and judgments on the scale of good—bad, desirable—undesirable, approval—disapproval are essential for human-to-human communication. Understanding human emotions and deciphering the way humans reason about them or express them in their language is key to enhancing human-machine interaction","The paper shows how computational intelligence and linguistics can be blended in order to understand sentiment associated with the text. The presented approach combines the use of various linguistic patterns based on the syntactic structure of the sentences. Similarly to the function of electronic logic gates, the algorithm determines the polarity of each word and flows, or extends, it through the dependency arcs in order to determine the final polarity label of the sentence. Overall, the proposed Table 4 Precision achieved with different classification algorithms on different datasets. Algorithm Movie Review dataset Blitzerderived dataset Amazon dataset RNN (Socher et al. 2012 [38]) 80.00% – – RNTN (Socher et al. 2013 [7]) 85.40% 61.93% 68.21% Poria et al. 2014 [3] 86.21% 87.00% 79.33% Sentic Patterns 87.15% 86.46% 80.62% ELM Classifier 71.11% 74.49% 71.29% Ensemble Classification (proposed method) 88.12% 88.27% 82.75% Table 5 Results obtained using SentiWordNet. Dataset Using SenticNet Using SentiWordNet Movie Review 88.12% 87.63% Blitzer 88.27% 88.09% Amazon 82.75% 80.28% Table 6 Some examples where a CI classifier was used to obtain the polarity label. Sentence Polarity I had to return the phone after 2 days of use. Negative The phone runs recent operating system. Positive The phone has a big and capacitive touchscreen. Positive My iPhone battery lasts only few hours. Negative I remember that I slept at the movie hall. Negative Authorized licensed use limited to: Indian Instt of Engg Science & Tech- SHIBPUR. Downloaded on March 15,2023 at 09:58:17 UTC from IEEE Xplore. Restrictions apply. 36 IEEE Computational intelligence magazine | November 2015 approach, relying on the novel sentiment flow algorithm, has outperformed the majority of the main existing approaches on the benchmark datasets, showing outstanding effectiveness. The future work will aim to discover more linguistic patterns, generalizing the patterns and use of deep learning for the CI module. In the future, we plan to carry out additional experiments using diverse datasets to further evaluate the domain independence of the capabilities of linguistic patterns, and compare their performance with other benchmark stateof-the-art approaches (such as [12]). Further, whilst our presumption is that all linguistic patterns are equally important for calculating the polarity of natural language sentences, it would be interesting to carry out further detailed theoretical analysis to investigate relative contributions of sentic patterns across a range of datasets from different domains. As discussed in Section VI-A, sarcasm detection plays an important role in sentiment analysis: irony usually inverts the polarity of the sentence. We plan to develop a sarcasm detection module trained on available datasets3 [39]. Identifying implicit negation [19] when irony is detected, is also one of the key future tasks of our framework. Finally, we plan to develop other modules, e.g., microtext analysis and anaphora resolution, as part of our vision to develop a novel holistic approach for solving the multi-faceted dynamic sentiment analysis challenge [40].","Sentiment Data Flow Analysis by Means of Dynamic Linguistic Patternscomputational intelligence, sentiment analysis.Computational intelligence (CI) is an established research field that focuses on using brain- or natureinspired models to solve complex real-world problems that cannot otherwise be effectively solved by traditional models. One of the biggest challenges of CI is the emulation of human intelligence, which includes not only human-like reasoning but also human interaction, e.g., human language and human emotions. Emotions, sentiment, and judgments on the scale of good—bad, desirable—undesirable, approval—disapproval are essential for human-to-human communication. Understanding human emotions and deciphering the way humans reason about them or express them in their language is key to enhancing human-machine interactionThe paper shows how computational intelligence and linguistics can be blended in order to understand sentiment associated with the text. The presented approach combines the use of various linguistic patterns based on the syntactic structure of the sentences. Similarly to the function of electronic logic gates, the algorithm determines the polarity of each word and flows, or extends, it through the dependency arcs in order to determine the final polarity label of the sentence. Overall, the proposed Table 4 Precision achieved with different classification algorithms on different datasets. Algorithm Movie Review dataset Blitzerderived dataset Amazon dataset RNN (Socher et al. 2012 [38]) 80.00% – – RNTN (Socher et al. 2013 [7]) 85.40% 61.93% 68.21% Poria et al. 2014 [3] 86.21% 87.00% 79.33% Sentic Patterns 87.15% 86.46% 80.62% ELM Classifier 71.11% 74.49% 71.29% Ensemble Classification (proposed method) 88.12% 88.27% 82.75% Table 5 Results obtained using SentiWordNet. Dataset Using SenticNet Using SentiWordNet Movie Review 88.12% 87.63% Blitzer 88.27% 88.09% Amazon 82.75% 80.28% Table 6 Some examples where a CI classifier was used to obtain the polarity label. Sentence Polarity I had to return the phone after 2 days of use. Negative The phone runs recent operating system. Positive The phone has a big and capacitive touchscreen. Positive My iPhone battery lasts only few hours. Negative I remember that I slept at the movie hall. Negative Authorized licensed use limited to: Indian Instt of Engg Science & Tech- SHIBPUR. Downloaded on March 15,2023 at 09:58:17 UTC from IEEE Xplore. Restrictions apply. 36 IEEE Computational intelligence magazine | November 2015 approach, relying on the novel sentiment flow algorithm, has outperformed the majority of the main existing approaches on the benchmark datasets, showing outstanding effectiveness. The future work will aim to discover more linguistic patterns, generalizing the patterns and use of deep learning for the CI module. In the future, we plan to carry out additional experiments using diverse datasets to further evaluate the domain independence of the capabilities of linguistic patterns, and compare their performance with other benchmark stateof-the-art approaches (such as [12]). Further, whilst our presumption is that all linguistic patterns are equally important for calculating the polarity of natural language sentences, it would be interesting to carry out further detailed theoretical analysis to investigate relative contributions of sentic patterns across a range of datasets from different domains. As discussed in Section VI-A, sarcasm detection plays an important role in sentiment analysis: irony usually inverts the polarity of the sentence. We plan to develop a sarcasm detection module trained on available datasets3 [39]. Identifying implicit negation [19] when irony is detected, is also one of the key future tasks of our framework. Finally, we plan to develop other modules, e.g., microtext analysis and anaphora resolution, as part of our vision to develop a novel holistic approach for solving the multi-faceted dynamic sentiment analysis challenge [40].Erasmus University Rotterdam CotoErasmus University Rotterdam Cartonsae eeReference Sentiment Lexicon Reference Semantic Lexicon Semantic Lexicon Sentiment Lexicon -> Mapping —-—-— Antonym Relation Non-Antonym RelationEnglish Machine Translation Word Sense Disambiguator POS Tagger > Lemmatizer - Document Scorer Sentiment] Lexicon Semantic Lexicon, SPROP Semantic Lexicon, Sentiment Lexicon Word Sense Disambiguator POS Tagger > Lemmatizer > Document Scorer Dutch ——= > Sentiment Analysis Approach — _ Information Flow ——-—-—P UsedBy Relationship=o (Her Ca 4 Lden Ga Ca |®| IN|> formation Ftow = VaalBy Relations e) Sentiment Lexi Bmoticou-fbasd vent, AnalySegmentation Information Flow ~~ —= te UsulBy Relationship Emoticon Detoetion Peeprocessings eo Lesion Sentiment Lexicon Emoticon-Based Sentiment Analysis st Text-Based Sentiment, Aualysis Sentiment Aggregationmnaraeomata carne) ed = 4 ees Se ee patentee eet a perenne ed ees Stew ig Tab iWLDA.D TL fat HILDAD LE ag HILDA.D ALE HUDAP TY |e it MILDAP LT mA? oa aa UILDASTI Ep i MILDAS LI unpa'sitt POT es SPADESTI SPADESLI SPADES HL CDT HULDA.D TU [SRB HEDA.D Ltt EE | HILDA Wt Bat MILDA.P TH HILDA.P LI LDAP HAL [EET I HUDASTH HILDA'S Ltt HILDA.SH UL SPADES TH SPADES. SPADES 11 HILDA.D TX HILDA.DLX HILDAD HX MILDA.P TX, HILDA. LX HILDA P HX MLDASTX HILDA'S 1X HILDAS HX SPADES TX SPADE.SLX SPADE SIX HILDA.D TF HILDA.D LE HILDA‘ HF HILDA. TE MILDA.P LF HILDA. HF HILDAST HILDASI F HILDASH ¥ SPADES T SPADES Lf SPADESMaco Ga . Laen Ga = 0.5 + ( || |N|HLDAD TH f HILDAD LE AILDA.D UE HUDAP TH MULDAPLY LDAP HL HIEDASTI HILDASLI HILDA'S HL SPADESTI SPADESLI SPADES HL HUDAD Tu HEDA-D Li HILDAD HI MILDA.P TH HILDA LI HILDA.P HUT f HUDASTH HILDA'S Ltt HILDAS UU. SPADES TH SPADES. SPADES 11 HILDA.D TX HILDA.DLX HILDAD HX MILDA.P TX, HILDA. LX HILDA P HX MLDASTX HILDA'S 1X HILDAS HX SPADES TX SPADE.SLX SPADE SIX HILDA.D TF HILDA.D LE HILDA‘ HF HILDA. TE MILDA.P LF HILDA. HF HILDAST HILDASI.F HILDASH ¥ SPADES T SPADES Lf SPADESCy =cB5 Gag = Max G(1) Sentence-level ATTRIBUTION Nucleus Paragraph-level GE Document-level ATTRIBUTION Satellite BACKGROUND Nucleus BACKGROUND Satellite ContTRAST Nucleus CONTRAST Satellite ELABORATION Nucleus ELABORATION Satellite ENABLEMENT Nucleus ENABLEMENT Satellite aNucleus ATTRIBUTION Satellite BACKGROUND Satellite CONTRAST Satellite ELABORATION Satellite ENABLEMENT Satellite Paragraph-level Document-levelwa me.| ‘ ‘a= a g 5 3S 3 a ANN ooc oO so.IMj{evoy 0.5 oH oO 0.4 a ean ooo onszodoig 12 S 91095 aAT}esaNH# aaTysod# SPIO F# <VS +VS -VvS vs PAST JROT Joast-doy, Soq}TT[OeS TopoNN aI09G aatyesanH# aatysogd# splomM# <VS +VS -vS vs SqToApY soarjoolpy sqioA, suno Ny SUIeISIG sureIsTuy) Aouonbesq ATRUIG seuute'yT syosuAg LSU QUOUTITJUIS SPIOM,e+ruz o £ 2 D wn Annmenvwnarandrse soinyeoj Jo uolyzodoig 91005 aAT}eSa NH oAlsog# Sployy7# +VS +VS -vSs vs PPAoLJeoT peaoy-doy, SOqTTJO}LS TPN 9100S aaTyesonNH# oat} sog# SpIoy\ # VS +VS -vs VS sqIoApy soatjoolpy SqIOA, sunON, Aguonbesy Areulg, seue'T syesuAg ISu qUOUITWIGz o £ 2 D wn Aor oO ——) oO somyeoy jo uon1odoig 91095 aAT}eSaNH# aatysog# SPION\ F# +VS +VS -vSs vs [AOL JeOT peaot-doy, SO4TTJO}RS TopPNN 9109G aaTyeSanH oatysog# SPIOM# <VS +VS -vs VS Sq1oApV soatjoolpy Sq19A, SUNLON, surVIsig surerstuy) Aouonboty Areulg seume'T syosuAg LISa quouwltyueg7 ‘ | ‘ ‘ ‘ ‘ ‘a= a g 5 3S 3 a ANN ooc oO so.IMj{evoy 0.5 oH oO 0.4 a ean ooo onszodoig 12 S 91095 aAT}esaNH# aaTysod# SPIO F# <VS +VS -VvS vs PAST JROT Joast-doy, Soq}TT[OeS TopoNN aI09G aatyesanH# aatysogd# splomM# <VS +VS -vS vs SqToApY soarjoolpy sqioA, suno Ny SUIeISIG sureIsTuy) Aouonbesq ATRUIG seuute'yT syosuAg LSU QUOUTITJUIS SPIOM,rm",computational intelligence,"The text discusses computational intelligence (CI) and its focus on using brain- or nature-inspired models to solve complex real-world problems. Emulating human intelligence, including human language and emotions, is a big challenge for CI. The paper proposes an approach that combines CI and linguistics to understand sentiment associated with text. The approach uses various linguistic patterns based on the syntactic structure of sentences to determine the polarity label of the sentence. The proposed approach outperforms existing approaches on benchmark datasets. Future work includes discovering more linguistic patterns, sarcasm detection, and developing other modules to solve the multi-faceted dynamic sentiment analysis challenge.",Object and Sentiment Recognition,"Erasmus University Rotterdam CotoErasmus University Rotterdam Cartonsae eeReference Sentiment Lexicon Reference Semantic Lexicon Semantic Lexicon Sentiment Lexicon -> Mapping —-—-— Antonym Relation Non-Antonym RelationEnglish Machine Translation Word Sense Disambiguator POS Tagger > Lemmatizer - Document Scorer Sentiment] Lexicon Semantic Lexicon, SPROP Semantic Lexicon, Sentiment Lexicon Word Sense Disambiguator POS Tagger > Lemmatizer > Document Scorer Dutch ——= > Sentiment Analysis Approach — _ Information Flow ——-—-—P UsedBy Relationship=o (Her Ca 4 Lden Ga Ca |®| IN|> formation Ftow = VaalBy Relations e) Sentiment Lexi Bmoticou-fbasd vent, AnalySegmentation Information Flow ~~ —= te UsulBy Relationship Emoticon Detoetion Peeprocessings eo Lesion Sentiment Lexicon Emoticon-Based Sentiment Analysis st Text-Based Sentiment, Aualysis Sentiment Aggregationmnaraeomata carne) ed = 4 ees Se ee patentee eet a perenne ed ees Stew ig Tab iWLDA.D TL fat HILDAD LE ag HILDA.D ALE HUDAP TY |e it MILDAP LT mA? oa aa UILDASTI Ep i MILDAS LI unpa'sitt POT es SPADESTI SPADESLI SPADES HL CDT HULDA.D TU [SRB HEDA.D Ltt EE | HILDA Wt Bat MILDA.P TH HILDA.P LI LDAP HAL [EET I HUDASTH HILDA'S Ltt HILDA.SH UL SPADES TH SPADES. SPADES 11 HILDA.D TX HILDA.DLX HILDAD HX MILDA.P TX, HILDA. LX HILDA P HX MLDASTX HILDA'S 1X HILDAS HX SPADES TX SPADE.SLX SPADE SIX HILDA.D TF HILDA.D LE HILDA‘ HF HILDA. TE MILDA.P LF HILDA. HF HILDAST HILDASI F HILDASH ¥ SPADES T SPADES Lf SPADESMaco Ga . Laen Ga = 0.5 + ( || |N|HLDAD TH f HILDAD LE AILDA.D UE HUDAP TH MULDAPLY LDAP HL HIEDASTI HILDASLI HILDA'S HL SPADESTI SPADESLI SPADES HL HUDAD Tu HEDA-D Li HILDAD HI MILDA.P TH HILDA LI HILDA.P HUT f HUDASTH HILDA'S Ltt HILDAS UU. SPADES TH SPADES. SPADES 11 HILDA.D TX HILDA.DLX HILDAD HX MILDA.P TX, HILDA. LX HILDA P HX MLDASTX HILDA'S 1X HILDAS HX SPADES TX SPADE.SLX SPADE SIX HILDA.D TF HILDA.D LE HILDA‘ HF HILDA. TE MILDA.P LF HILDA. HF HILDAST HILDASI.F HILDASH ¥ SPADES T SPADES Lf SPADESCy =cB5 Gag = Max G(1) Sentence-level ATTRIBUTION Nucleus Paragraph-level GE Document-level ATTRIBUTION Satellite BACKGROUND Nucleus BACKGROUND Satellite ContTRAST Nucleus CONTRAST Satellite ELABORATION Nucleus ELABORATION Satellite ENABLEMENT Nucleus ENABLEMENT Satellite aNucleus ATTRIBUTION Satellite BACKGROUND Satellite CONTRAST Satellite ELABORATION Satellite ENABLEMENT Satellite Paragraph-level Document-levelwa me.| ‘ ‘a= a g 5 3S 3 a ANN ooc oO so.IMj{evoy 0.5 oH oO 0.4 a ean ooo onszodoig 12 S 91095 aAT}esaNH# aaTysod# SPIO F# <VS +VS -VvS vs PAST JROT Joast-doy, Soq}TT[OeS TopoNN aI09G aatyesanH# aatysogd# splomM# <VS +VS -vS vs SqToApY soarjoolpy sqioA, suno Ny SUIeISIG sureIsTuy) Aouonbesq ATRUIG seuute'yT syosuAg LSU QUOUTITJUIS SPIOM,e+ruz o £ 2 D wn Annmenvwnarandrse soinyeoj Jo uolyzodoig 91005 aAT}eSa NH oAlsog# Sployy7# +VS +VS -vSs vs PPAoLJeoT peaoy-doy, SOqTTJO}LS TPN 9100S aaTyesonNH# oat} sog# SpIoy\ # VS +VS -vs VS sqIoApy soatjoolpy SqIOA, sunON, Aguonbesy Areulg, seue'T syesuAg ISu qUOUITWIGz o £ 2 D wn Aor oO ——) oO somyeoy jo uon1odoig 91095 aAT}eSaNH# aatysog# SPION\ F# +VS +VS -vSs vs [AOL JeOT peaot-doy, SO4TTJO}RS TopPNN 9109G aaTyeSanH oatysog# SPIOM# <VS +VS -vs VS Sq1oApV soatjoolpy Sq19A, SUNLON, surVIsig surerstuy) Aouonboty Areulg seume'T syosuAg LISa quouwltyueg7 ‘ | ‘ ‘ ‘ ‘ ‘a= a g 5 3S 3 a ANN ooc oO so.IMj{evoy 0.5 oH oO 0.4 a ean ooo onszodoig 12 S 91095 aAT}esaNH# aaTysod# SPIO F# <VS +VS -VvS vs PAST JROT Joast-doy, Soq}TT[OeS TopoNN aI09G aatyesanH# aatysogd# splomM# <VS +VS -vS vs SqToApY soarjoolpy sqioA, suno Ny SUIeISIG sureIsTuy) Aouonbesq ATRUIG seuute'yT syosuAg LSU QUOUTITJUIS SPIOM,rm",Sentiment Analysis
261,Work-related fatal motor vehicle traffic crashes: Matching of 2010 data from the Census of Fatal Occupational Injuries and the Fatality Analysis Reporting System,"Work-related motor vehicle traffic crashes , Data matching , Census of Fatal Occupational Injuries , Fatality Analysis Reporting System.","Motor vehicle traffic crashes (MVTCs) remain the leading cause of work-related fatal injuries in the United States, with crashes on public roadways accounting for 25% of all work-related deaths in 2012. In the United States, the Bureau of Labor Statistics (BLS) Census of Fatal Occupational Injuries (CFOI) provides accurate counts of fatal work injuries based on confirmation of work relationship from multiple sources, while the National Highway Traffic Safety Administration (NHTSA) Fatality Analysis Reporting System (FARS) provides detailed data on fatal MVTCs based on police reports. Characterization of fatal work-related MVTCs is currently limited by data sources that lack either data on potential risk factors (CFOI) or work-relatedness confirmation and employment characteristics (FARS). BLS and the National Institute for Occupational Safety and Health (NIOSH) collaborated to analyze a merged data file created by BLS using CFOI and FARS data. A matching algorithm was created to link 2010 data from CFOI and FARS using date of incident and other case characteristics, allowing for flexibility in variables to address coding discrepancies. Using the matching algorithm, 953 of the 1044 CFOI “Highway” cases (91%) for 2010 were successfully linked to FARS. Further analysis revealed systematic differences between cases identified as work-related by both systems and by CFOI alone. Among cases identified as work-related by CFOI alone, the fatally-injured worker was considerably more likely to have been employed outside the transportation and warehousing industry or transportation-related occupations, and to have been the occupant of a vehicle other than a heavy truck. This study is the first step of a collaboration between BLS, NHTSA, and NIOSH to improve the completeness and quality of data on fatal work-related MVTCs. It has demonstrated the feasibility and value of matching data on fatal work-related traffic crashes from CFOI and FARS. The results will lead to improvements in CFOI and FARS case capture, while also providing researchers with a better description of fatal work-related MVTCs than would be available from the two data sources separately.","This study has demonstrated the feasibility and value of matching data on fatal work-related MVTCs from CFOI and FARS, verifying that there are systematic differences between cases captured in both systems and those captured by CFOI alone. Among cases captured by CFOI alone, the fatally-injured worker was considerably more likely to have been employed outside the transportation and warehousing industry, outside transportation and material moving occupations, and to have been the occupant of a vehicle other than a heavy truck. These findings emphasize the diversity of the members of the workforce involved in fatal work-related traffic crashes, and they suggest that crash prevention efforts should encompass a wide range of industries and occupations. The matching process reported in this study is the first step in a collaboration between BLS, NHTSA, and NIOSH to improve the completeness and quality of data on fatal work-related MVTCs. Matching of additional years of data is planned, and subsequent analyses will use the merged dataset to provide richer insights about risk factors for fatal work-related traffic crashes. Rather than simply providing crash counts and rates from CFOI by employment or demographic characteristics, it may now be possible to differentiate crash circumstances and behavioral factors by industry, occupation, and other employment characteristics. These analyses will be invaluable to employers, researchers, developers of crash-prevention technologies, and policy makers, enabling them to focus on developing and implementing interventions that address known risk factors in specific industries and operating environments.","Work-related fatal motor vehicle traffic crashes: Matching of 2010 data from the Census of Fatal Occupational Injuries and the Fatality Analysis Reporting SystemWork-related motor vehicle traffic crashes , Data matching , Census of Fatal Occupational Injuries , Fatality Analysis Reporting System.Motor vehicle traffic crashes (MVTCs) remain the leading cause of work-related fatal injuries in the United States, with crashes on public roadways accounting for 25% of all work-related deaths in 2012. In the United States, the Bureau of Labor Statistics (BLS) Census of Fatal Occupational Injuries (CFOI) provides accurate counts of fatal work injuries based on confirmation of work relationship from multiple sources, while the National Highway Traffic Safety Administration (NHTSA) Fatality Analysis Reporting System (FARS) provides detailed data on fatal MVTCs based on police reports. Characterization of fatal work-related MVTCs is currently limited by data sources that lack either data on potential risk factors (CFOI) or work-relatedness confirmation and employment characteristics (FARS). BLS and the National Institute for Occupational Safety and Health (NIOSH) collaborated to analyze a merged data file created by BLS using CFOI and FARS data. A matching algorithm was created to link 2010 data from CFOI and FARS using date of incident and other case characteristics, allowing for flexibility in variables to address coding discrepancies. Using the matching algorithm, 953 of the 1044 CFOI “Highway” cases (91%) for 2010 were successfully linked to FARS. Further analysis revealed systematic differences between cases identified as work-related by both systems and by CFOI alone. Among cases identified as work-related by CFOI alone, the fatally-injured worker was considerably more likely to have been employed outside the transportation and warehousing industry or transportation-related occupations, and to have been the occupant of a vehicle other than a heavy truck. This study is the first step of a collaboration between BLS, NHTSA, and NIOSH to improve the completeness and quality of data on fatal work-related MVTCs. It has demonstrated the feasibility and value of matching data on fatal work-related traffic crashes from CFOI and FARS. The results will lead to improvements in CFOI and FARS case capture, while also providing researchers with a better description of fatal work-related MVTCs than would be available from the two data sources separately.This study has demonstrated the feasibility and value of matching data on fatal work-related MVTCs from CFOI and FARS, verifying that there are systematic differences between cases captured in both systems and those captured by CFOI alone. Among cases captured by CFOI alone, the fatally-injured worker was considerably more likely to have been employed outside the transportation and warehousing industry, outside transportation and material moving occupations, and to have been the occupant of a vehicle other than a heavy truck. These findings emphasize the diversity of the members of the workforce involved in fatal work-related traffic crashes, and they suggest that crash prevention efforts should encompass a wide range of industries and occupations. The matching process reported in this study is the first step in a collaboration between BLS, NHTSA, and NIOSH to improve the completeness and quality of data on fatal work-related MVTCs. Matching of additional years of data is planned, and subsequent analyses will use the merged dataset to provide richer insights about risk factors for fatal work-related traffic crashes. Rather than simply providing crash counts and rates from CFOI by employment or demographic characteristics, it may now be possible to differentiate crash circumstances and behavioral factors by industry, occupation, and other employment characteristics. These analyses will be invaluable to employers, researchers, developers of crash-prevention technologies, and policy makers, enabling them to focus on developing and implementing interventions that address known risk factors in specific industries and operating environments.Y - - ;",Medical Data Analysis,"This article discusses the issue of motor vehicle traffic crashes (MVTCs) as the leading cause of work-related fatal injuries in the United States. The Bureau of Labor Statistics (BLS) and the National Highway Traffic Safety Administration (NHTSA) are two sources that provide data on fatal work-related MVTCs. However, the data sources lack either data on potential risk factors or work-relatedness confirmation and employment characteristics. Therefore, BLS and NIOSH collaborated to analyze a merged data file created by BLS using CFOI and FARS data. The matching algorithm was created to link 2010 data from CFOI and FARS using date of incident and other case characteristics, allowing for flexibility in variables to address coding discrepancies. The study has demonstrated the feasibility and value of matching data on fatal work-related MVTCs from CFOI and FARS, verifying that there are systematic differences between cases captured in both systems and those captured by CFOI alone. The findings emphasize the diversity of the members of the workforce involved in fatal work-related traffic crashes and suggest that crash prevention efforts should encompass a wide range of industries and occupations. The matching process reported in this study is the first step in a collaboration between BLS, NHTSA, and NIOSH to improve the completeness and quality of data on fatal work-related MVTCs. Matching of additional years of data is planned, and subsequent analyses will use the merged dataset to provide richer insights about risk factors for fatal work-related traffic crashes.",Medical Data Analysis,Y - - ;,Sentiment Analysis
262,Improved multiscale matched filter for retina vessel segmentation using PSO algorithm,Multiscale matched filter; Optimization; Retina vessels; PSO,The concept of matched filter is widely used in the area of retina vessel segmentation. Multiscale matched filters have superior performance over single scale filters. The proposed approach makes use of the improved noise suppression features of multiscale filters. A major performance issue here is the determination of the right parameter values of the filter. The approach employs particle swarm optimization for finding the optimal filter parameters of the multiscale Gaussian matched filter for achieving improved accuracy of retina vessel segmentation. The proposed approach is tested on DRIVE and STARE retina database and obtained better results when compared to other available retina vessel segmentation algorithms.,"Matched filter is well known in retina blood vessel segmentation. Since the widths of the vessels are varying across the image, matched filter with single scale does not provide acceptable performance figures. The proposed approach makes use of two matched filters. Particle swarm optimization based parameter selection is employed for selecting the right values of parameters of the multiscale matched filter. The approach is tested on DRIVE and STARE databases to demonstrate the performance advantages over existing approaches. A sensitivity of 71.32%, specificity of 98.66% and accuracy of 96.33% are obtained on DRIVE database, and a sensitivity of 71.72%, specificity of 96.87% and accuracy of 95% are obtained on STARE database. Results demonstrate that the multiscale matched filter works better than single scale matched filter for vessel segmentation. Future work proposes to introduce vessel morphology to improve the vessel segmentation results for achieving improved performances.","Improved multiscale matched filter for retina vessel segmentation using PSO algorithmMultiscale matched filter; Optimization; Retina vessels; PSOThe concept of matched filter is widely used in the area of retina vessel segmentation. Multiscale matched filters have superior performance over single scale filters. The proposed approach makes use of the improved noise suppression features of multiscale filters. A major performance issue here is the determination of the right parameter values of the filter. The approach employs particle swarm optimization for finding the optimal filter parameters of the multiscale Gaussian matched filter for achieving improved accuracy of retina vessel segmentation. The proposed approach is tested on DRIVE and STARE retina database and obtained better results when compared to other available retina vessel segmentation algorithms.Matched filter is well known in retina blood vessel segmentation. Since the widths of the vessels are varying across the image, matched filter with single scale does not provide acceptable performance figures. The proposed approach makes use of two matched filters. Particle swarm optimization based parameter selection is employed for selecting the right values of parameters of the multiscale matched filter. The approach is tested on DRIVE and STARE databases to demonstrate the performance advantages over existing approaches. A sensitivity of 71.32%, specificity of 98.66% and accuracy of 96.33% are obtained on DRIVE database, and a sensitivity of 71.72%, specificity of 96.87% and accuracy of 95% are obtained on STARE database. Results demonstrate that the multiscale matched filter works better than single scale matched filter for vessel segmentation. Future work proposes to introduce vessel morphology to improve the vessel segmentation results for achieving improved performances.row total; x column total, fit + fie + for + foo Eij = 4,9 € {1,2} (3)(fig ~ Bij)"" 2 = Mjya1 2d j= x (w) i=1,24j=1,2 EijPositive set Negative setWi * SO score(e) = MwiwiLOwi€s Fs, 6)Crawl Preprocessing a [i - Sentence type detection Opinion lexicon [> Coreference resolution Opinion rules Aggregate opinion =) (Lexicon-based method) Train sentiment classifier Learning-based method) a ie » Extract opinionated tweetsPreprocessing Sentence type detection Coreference resolution Aggregate opinion (Lexicon-based method) Crawl tweets Train sentiment classifier Learning-based method 7 ( g ) Extract opinionated tweets Classified tweets",Medical Data Analysis,"The concept of matched filter is commonly used in retina vessel segmentation, but single scale filters have limited performance. Multiscale matched filters are more effective due to their improved noise suppression features, but determining the right parameter values can be a challenge. To address this issue, the proposed approach uses particle swarm optimization to find the optimal filter parameters for achieving better accuracy in retina vessel segmentation. The approach was tested on two retina databases and showed better results compared to other available algorithms. The multiscale matched filter achieved a sensitivity of 71.32-71.72%, specificity of 96.87-98.66%, and accuracy of 95-96.33%. This approach demonstrated that the multiscale matched filter is more effective than the single scale matched filter for vessel segmentation. Future work proposes introducing vessel morphology to improve segmentation results and achieve better performance.",Medical Data Analysis,"row total; x column total, fit + fie + for + foo Eij = 4,9 € {1,2} (3)(fig ~ Bij)"" 2 = Mjya1 2d j= x (w) i=1,24j=1,2 EijPositive set Negative setWi * SO score(e) = MwiwiLOwi€s Fs, 6)Crawl Preprocessing a [i - Sentence type detection Opinion lexicon [> Coreference resolution Opinion rules Aggregate opinion =) (Lexicon-based method) Train sentiment classifier Learning-based method) a ie » Extract opinionated tweetsPreprocessing Sentence type detection Coreference resolution Aggregate opinion (Lexicon-based method) Crawl tweets Train sentiment classifier Learning-based method 7 ( g ) Extract opinionated tweets Classified tweets",Deep Learning and Machine Learning
263,"Clinical characteristics and changes of chest CT features in 307 patients with common COVID-19 pneumonia infected SARS-CoV -2: A multicenter study in Jiangsu, China","SARS-CoV-2 , COVID-19 , chest , CT , change, penumonia.","Objective: The study was aimed to describe the clinical characteristics and evaluate the dynamic changes of chest CT features in the first three weeks in the common type of COVID-19 pneumonia patients in Jiangsu Province. Methods: 307 patients infected SARS-CoV-2 classified as common type were enrolled in the study. 628 chest CT scans were divided into three groups based on the time interval between symptoms and chest CT scan. The clinical characteristics were descriptively analyzed. The chest CT features were quantitatively evaluated. Mann-Whitney U test was used to test the differences in three groups and between men and women. Spearman rank correlation was used to test the association between the arterial blood gas(ABG) analysis results and chest CT scores. Results: Fever (69.1%) and cough (62.8%) were common symptoms. 111(36.2%) patients were anorexia. GGO was the most common manifestation of COVID-19 pneumonia, which could be followed by consolidation and fibrosis. Lower lobe or subpleural region was the most common distribution form of lesion. More lung lobes were involved in the third week. Total chest CT scores in the second week were higher than the first week. Fibrosis Scores increased in the second and third week. Total CT score, GGO score and fibrosis score of male patients were significantly higher than female in the second week. Male patients had higher consolidation score and fibrosis score than female in the third week. Total CT score and GGO score had weak to moderate correlation with arterial blood gas indices. Conclusion: Changes in chest CT were difficult to assess quantitatively in the first third weeks. Male patients recovered slower than female in the second week. Although CT score had correlations with arterial blood gas indices, long-term follow-up of pulmonary function test is needed to determine the recovery of lung.","Chest CT played an important role in the diagnosis of COVID-19 pneumonia. Changes in chest CT were difficult to assess quantitatively in the first third weeks. Chest CT of male patients was more serious than female in the second. More consolidation and fibrosis lesions existed in male patients in the third week. Although CT score had correlations with arterial blood gas indices, long-term follow-up of pulmonary function test is needed to determine the recovery of lung.","Clinical characteristics and changes of chest CT features in 307 patients with common COVID-19 pneumonia infected SARS-CoV -2: A multicenter study in Jiangsu, ChinaSARS-CoV-2 , COVID-19 , chest , CT , change, penumonia.Objective: The study was aimed to describe the clinical characteristics and evaluate the dynamic changes of chest CT features in the first three weeks in the common type of COVID-19 pneumonia patients in Jiangsu Province. Methods: 307 patients infected SARS-CoV-2 classified as common type were enrolled in the study. 628 chest CT scans were divided into three groups based on the time interval between symptoms and chest CT scan. The clinical characteristics were descriptively analyzed. The chest CT features were quantitatively evaluated. Mann-Whitney U test was used to test the differences in three groups and between men and women. Spearman rank correlation was used to test the association between the arterial blood gas(ABG) analysis results and chest CT scores. Results: Fever (69.1%) and cough (62.8%) were common symptoms. 111(36.2%) patients were anorexia. GGO was the most common manifestation of COVID-19 pneumonia, which could be followed by consolidation and fibrosis. Lower lobe or subpleural region was the most common distribution form of lesion. More lung lobes were involved in the third week. Total chest CT scores in the second week were higher than the first week. Fibrosis Scores increased in the second and third week. Total CT score, GGO score and fibrosis score of male patients were significantly higher than female in the second week. Male patients had higher consolidation score and fibrosis score than female in the third week. Total CT score and GGO score had weak to moderate correlation with arterial blood gas indices. Conclusion: Changes in chest CT were difficult to assess quantitatively in the first third weeks. Male patients recovered slower than female in the second week. Although CT score had correlations with arterial blood gas indices, long-term follow-up of pulmonary function test is needed to determine the recovery of lung.Chest CT played an important role in the diagnosis of COVID-19 pneumonia. Changes in chest CT were difficult to assess quantitatively in the first third weeks. Chest CT of male patients was more serious than female in the second. More consolidation and fibrosis lesions existed in male patients in the third week. Although CT score had correlations with arterial blood gas indices, long-term follow-up of pulmonary function test is needed to determine the recovery of lung.",Medical Data Analysis,"The study aimed to describe the clinical characteristics and dynamic changes of chest CT features in the first three weeks of COVID-19 pneumonia patients in Jiangsu Province. The study enrolled 307 patients classified as common type and analyzed their symptoms and chest CT features. The most common manifestation was ground-glass opacities (GGO), followed by consolidation and fibrosis, mainly distributed in the lower lobe or subpleural region. Male patients had a slower recovery than females in the second week, with higher consolidation and fibrosis scores. Although chest CT scores had correlations with arterial blood gas indices, long-term follow-up of pulmonary function test is needed to determine lung recovery. Chest CT plays an important role in the diagnosis of COVID-19 pneumonia, but changes in chest CT are difficult to assess quantitatively in the first three weeks.",Medical Data Analysis,,Sentiment Analysis
264,Recovery of 3D rib motion from dynamic chest radiography and CT data using local contrast normalization and articular motion model,"2D–3D registration, Rib motion analysis, Articular motion model, Local contrast normalization.","Dynamic chest radiography (2D x-ray video) is a low-dose and cost-effective functional imaging method with high temporal resolution. While the analysis of rib-cage motion has been shown to be effective for evaluating respiratory function, it has been limited to 2D. We aim at 3D rib-motion analysis for high temporal resolution while keeping the radiation dose at a level comparable to conventional examination. To achieve this, we developed a method for automatically recovering 3D rib motion based on 2D-3D registration of x-ray video and single-time-phase computed tomography. We introduce the following two novel components into the conventional intensity-based 2D–3D registration pipeline: (1) a rib-motion model based on a uniaxial joint to constrain the search space and (2) local contrast normalization (LCN) as a pre-process of x-ray video to improve the cost function of the optimization parameters, which is often called the landscape. The effects of each component on the registration results were quantitatively evaluated through experiments using simulated images and real patients’ x-ray videos obtained in a clinical setting. The rotation-angle error of the rib and the mean projection contour distance (mPCD) were used as the error metrics. The simulation experiments indicate that the proposed uniaxial joint model improved registration accuracy. By searching the rotation axis along with the rotation angle of the ribs, the rotation-angle error and mPCD significantly decreased from 2.246 ± 1.839° and 1.148 ± 0.743 mm to 1.495 ± 0.993° and 0.742 ± 0.281 mm, compared to simply applying De Troyer’s model. The real-image experiments with eight patients demonstrated that LCN improved the cost function space; thus, robustness in optimization resulting in an average mPCD of 1.255 ± 0.615 mm. We demonstrated that an anatomical knowledge based constraint and an intensity normalization, LCN, significantly improved robustness and accuracy in rib-motion reconstruction using chest x-ray video.","We proposed a robust 2D-3D registration method for 3D rib-motion recovery from dynamic radiography. Specifically, the contributions of this paper are the introduction of (1) a uniaxial rib-motion model together with the optimization of the rotation axis to improve robustness in optimization while maintaining a high degree of accuracy, and (2) the LCN in the pre-processing of x-ray video to better condition the cost function space. We evaluated the proposed method through simulation experiments using the two-time-phase CT (i.e., inhale and exhale phases) and real-image experiments using x-ray videos acquired with a protocol used in a routine clinical setting. In both simulation and real-image experiments, the optimization of the rotation axis yielded statistically significant improvement (p < 0.01) compared to the scenario without rotation-axis search. We also confirmed the effectiveness of LCN in the real-image experiments.","Recovery of 3D rib motion from dynamic chest radiography and CT data using local contrast normalization and articular motion model2D–3D registration, Rib motion analysis, Articular motion model, Local contrast normalization.Dynamic chest radiography (2D x-ray video) is a low-dose and cost-effective functional imaging method with high temporal resolution. While the analysis of rib-cage motion has been shown to be effective for evaluating respiratory function, it has been limited to 2D. We aim at 3D rib-motion analysis for high temporal resolution while keeping the radiation dose at a level comparable to conventional examination. To achieve this, we developed a method for automatically recovering 3D rib motion based on 2D-3D registration of x-ray video and single-time-phase computed tomography. We introduce the following two novel components into the conventional intensity-based 2D–3D registration pipeline: (1) a rib-motion model based on a uniaxial joint to constrain the search space and (2) local contrast normalization (LCN) as a pre-process of x-ray video to improve the cost function of the optimization parameters, which is often called the landscape. The effects of each component on the registration results were quantitatively evaluated through experiments using simulated images and real patients’ x-ray videos obtained in a clinical setting. The rotation-angle error of the rib and the mean projection contour distance (mPCD) were used as the error metrics. The simulation experiments indicate that the proposed uniaxial joint model improved registration accuracy. By searching the rotation axis along with the rotation angle of the ribs, the rotation-angle error and mPCD significantly decreased from 2.246 ± 1.839° and 1.148 ± 0.743 mm to 1.495 ± 0.993° and 0.742 ± 0.281 mm, compared to simply applying De Troyer’s model. The real-image experiments with eight patients demonstrated that LCN improved the cost function space; thus, robustness in optimization resulting in an average mPCD of 1.255 ± 0.615 mm. We demonstrated that an anatomical knowledge based constraint and an intensity normalization, LCN, significantly improved robustness and accuracy in rib-motion reconstruction using chest x-ray video.We proposed a robust 2D-3D registration method for 3D rib-motion recovery from dynamic radiography. Specifically, the contributions of this paper are the introduction of (1) a uniaxial rib-motion model together with the optimization of the rotation axis to improve robustness in optimization while maintaining a high degree of accuracy, and (2) the LCN in the pre-processing of x-ray video to better condition the cost function space. We evaluated the proposed method through simulation experiments using the two-time-phase CT (i.e., inhale and exhale phases) and real-image experiments using x-ray videos acquired with a protocol used in a routine clinical setting. In both simulation and real-image experiments, the optimization of the rotation axis yielded statistically significant improvement (p < 0.01) compared to the scenario without rotation-axis search. We also confirmed the effectiveness of LCN in the real-image experiments.",Medical Data Analysis,"The paper proposes a method for automatically recovering 3D rib motion from 2D x-ray videos, while keeping the radiation dose low. The method involves 2D-3D registration of x-ray video and single-time-phase computed tomography, with the introduction of a uniaxial rib-motion model and local contrast normalization to improve the accuracy and robustness of the registration process. Simulation and real-image experiments were conducted to evaluate the proposed method, with statistically significant improvements observed when optimizing the rotation axis and using LCN. The paper concludes that the proposed method is a robust and accurate way to recover 3D rib motion from dynamic radiography.",Medical Data Analysis,,Medical Data Analysis
265,Improving dense conditional random field for retinal vessel segmentation by discriminative feature learning and thin-vessel enhancement,"Retinal vessel segmentation , Dense conditional random field , Convolutional neural network , Feature learning , Image enhancement.","Background and objectives: As retinal vessels in color fundus images are thin and elongated structures, standard pairwise based random fields, which always suffer the “shrinking bias” problem, are not competent for such segmentation task. Recently, a dense conditional random field (CRF) model has been successfully used in retinal vessel segmentation. Its corresponding energy function is formulated as a linear combination of several unary features and a pairwise term. However, the hand-crafted unary features can be suboptimal in terms of linear models. Here we propose to learn discriminative unary features and enhance thin vessels for pairwise potentials to further improve the segmentation performance. Methods: Our proposed method comprises four main steps: firstly, image preprocessing is applied to eliminate the strong edges around the field of view (FOV) and normalize the luminosity and contrast inside FOV; secondly, a convolutional neural network (CNN) is properly trained to generate discriminative features for linear models; thirdly, a combo of filters are applied to enhance thin vessels, reducing the intensity difference between thin and wide vessels; fourthly, by taking the discriminative features for unary potentials and the thin-vessel enhanced image for pairwise potentials, we adopt the dense CRF model to achieve the final retinal vessel segmentation. The segmentation performance is evaluated on four public datasets (i.e. DRIVE, STARE, CHASEDB1 and HRF). Results: Experimental results show that our proposed method improves the performance of the dense CRF model and outperforms other methods when evaluated in terms of F1-score, Matthews correlation coefficient (MCC) and G-mean, three effective metrics for the evaluation of imbalanced binary classification. Specifically, the F1-score, MCC and G-mean are 0.7942, 0.7656, 0.8835 for the DRIVE dataset respectively; 0.8017, 0.7830, 0.8859 for STARE respectively; 0.7644, 0.7398, 0.8579 for CHASEDB1 respectively; and 0.7627, 0.7402, 0.8812 for HRF respectively. Conclusions: The discriminative features learned in CNNs are more effective than hand-crafted ones. Our proposed method performs well in retinal vessel segmentation. The architecture of our method is trainable and can be integrated into computer-aided diagnostic (CAD) systems in the future.","In this paper, we propose a retinal vessel segmentation method based on the dense CRF model. Discriminative unary features are learned from a CNN model, which is properly trained using the superpixel-level balanced sample selection. A combo of filters are applied to generate a thin-vessel enhanced image used for pairwise potentials. Finally, the retinal vessel segmentation is achieved by efficient inference in the trained dense CRF model, which exploits the structural information of retinal vessels. Results show that our proposed method achieve better performance than the original dense CRF model with hand-crafted features, and that our method makes a general improvement in terms of F1-score, Matthews correlation coefficient and G-mean compared with other state-of-the-art methods. Our method can be further integrated into CAD systems to facilitate the retinal diagnosis. In future work, we will focus on the development of an end-to-end system like [54] to eliminate the weight gap between the CNN and dense CRF model to achieve optimal solutions.","Improving dense conditional random field for retinal vessel segmentation by discriminative feature learning and thin-vessel enhancementRetinal vessel segmentation , Dense conditional random field , Convolutional neural network , Feature learning , Image enhancement.Background and objectives: As retinal vessels in color fundus images are thin and elongated structures, standard pairwise based random fields, which always suffer the “shrinking bias” problem, are not competent for such segmentation task. Recently, a dense conditional random field (CRF) model has been successfully used in retinal vessel segmentation. Its corresponding energy function is formulated as a linear combination of several unary features and a pairwise term. However, the hand-crafted unary features can be suboptimal in terms of linear models. Here we propose to learn discriminative unary features and enhance thin vessels for pairwise potentials to further improve the segmentation performance. Methods: Our proposed method comprises four main steps: firstly, image preprocessing is applied to eliminate the strong edges around the field of view (FOV) and normalize the luminosity and contrast inside FOV; secondly, a convolutional neural network (CNN) is properly trained to generate discriminative features for linear models; thirdly, a combo of filters are applied to enhance thin vessels, reducing the intensity difference between thin and wide vessels; fourthly, by taking the discriminative features for unary potentials and the thin-vessel enhanced image for pairwise potentials, we adopt the dense CRF model to achieve the final retinal vessel segmentation. The segmentation performance is evaluated on four public datasets (i.e. DRIVE, STARE, CHASEDB1 and HRF). Results: Experimental results show that our proposed method improves the performance of the dense CRF model and outperforms other methods when evaluated in terms of F1-score, Matthews correlation coefficient (MCC) and G-mean, three effective metrics for the evaluation of imbalanced binary classification. Specifically, the F1-score, MCC and G-mean are 0.7942, 0.7656, 0.8835 for the DRIVE dataset respectively; 0.8017, 0.7830, 0.8859 for STARE respectively; 0.7644, 0.7398, 0.8579 for CHASEDB1 respectively; and 0.7627, 0.7402, 0.8812 for HRF respectively. Conclusions: The discriminative features learned in CNNs are more effective than hand-crafted ones. Our proposed method performs well in retinal vessel segmentation. The architecture of our method is trainable and can be integrated into computer-aided diagnostic (CAD) systems in the future.In this paper, we propose a retinal vessel segmentation method based on the dense CRF model. Discriminative unary features are learned from a CNN model, which is properly trained using the superpixel-level balanced sample selection. A combo of filters are applied to generate a thin-vessel enhanced image used for pairwise potentials. Finally, the retinal vessel segmentation is achieved by efficient inference in the trained dense CRF model, which exploits the structural information of retinal vessels. Results show that our proposed method achieve better performance than the original dense CRF model with hand-crafted features, and that our method makes a general improvement in terms of F1-score, Matthews correlation coefficient and G-mean compared with other state-of-the-art methods. Our method can be further integrated into CAD systems to facilitate the retinal diagnosis. In future work, we will focus on the development of an end-to-end system like [54] to eliminate the weight gap between the CNN and dense CRF model to achieve optimal solutions.(d) (e) () Figure 4 (a) and (d) Green channel of retina image from STARE database, (b) and (e) ground truth image, (c) and (f) the output of proposed method.(d) Figure 5 (a) The retinal image “01_test.tif ” from the DRIVE retina database, (b) ground truth, (c) the output image using GMF with Chaudhuri et al. [16] parameters, (d) the output image using OGMF with Al-Rawi et al. [21] parameters, (e) the output image using GA optimization [22] and (f) the output of proposed method.(a) (b) (c) Figure 3 (a) and (b) Matched filter responses at different scales, (c) is the scale production of (a) and (b).95| 90] 285| cy) 75| a a a Distance along profile (a) (b) Figure 2 (a) Green band of colored retina image “‘01_test.tif’ from DRIVE database, here vessel cross section is marked, (b) gray level intensity profile of marked region in |(b).Figure 1 (a) Color retina image, [9], (b) red channel, (c) green channel and (d) blue channel.",Medical Data Analysis,"The paper proposes a retinal vessel segmentation method based on the dense conditional random field (CRF) model. The proposed method learns discriminative unary features from a convolutional neural network (CNN) model and applies a combo of filters to enhance thin vessels for pairwise potentials. The retinal vessel segmentation is achieved by efficient inference in the trained dense CRF model. The proposed method outperforms other state-of-the-art methods in terms of F1-score, Matthews correlation coefficient, and G-mean on four public datasets. The method can be further integrated into computer-aided diagnostic (CAD) systems to facilitate retinal diagnosis.",Medical Data Analysis,"(d) (e) () Figure 4 (a) and (d) Green channel of retina image from STARE database, (b) and (e) ground truth image, (c) and (f) the output of proposed method.(d) Figure 5 (a) The retinal image “01_test.tif ” from the DRIVE retina database, (b) ground truth, (c) the output image using GMF with Chaudhuri et al. [16] parameters, (d) the output image using OGMF with Al-Rawi et al. [21] parameters, (e) the output image using GA optimization [22] and (f) the output of proposed method.(a) (b) (c) Figure 3 (a) and (b) Matched filter responses at different scales, (c) is the scale production of (a) and (b).95| 90] 285| cy) 75| a a a Distance along profile (a) (b) Figure 2 (a) Green band of colored retina image “‘01_test.tif’ from DRIVE database, here vessel cross section is marked, (b) gray level intensity profile of marked region in |(b).Figure 1 (a) Color retina image, [9], (b) red channel, (c) green channel and (d) blue channel.",Medical Data Analysis
266,Small lung nodules detection based on local variance analysis and probabilistic neural network,"Chest X-ray screening , Biomedical image processing , Automatic pathology recognition , Probabilistic neural network.","Background and objective: In medical examinations doctors use various techniques in order to provide to the patients an accurate analysis of their actual state of health. One of the commonly used methodologies is the x-ray screening. This examination very often help to diagnose some diseases of chest organs. The most frequent cause of wrong diagnosis lie in the radiologist’s difficulty in interpreting the presence of lungs carcinoma in chest X-ray. In such circumstances, an automated approach could be highly advantageous as it provides important help in medical diagnosis. Methods: In this paper we propose a new classification method of the lung carcinomas. This method start with the localization and extraction of the lung nodules by computing, for each pixel of the original image, the local variance obtaining an output image (variance image) with the same size of the original image. In the variance image we find the local maxima and then by using the locations of these maxima in the original image we found the contours of the possible nodules in lung tissues. However after this segmentation stage we find many false nodules. Therefore to discriminate the true ones we use a probabilistic neural network as classifier. Results: The performance of our approach is 92% of correct classifications, while the sensitivity is 95% and the specificity is 89.7%. The misclassification errors are due to the fact that network confuses false nodules with the true ones (6%) and true nodules with the false ones (2%). Conclusions: Several researchers have proposed automated algorithms to detect and classify pulmonary nodules but these methods fail to detect low-contrast nodules and have a high computational complexity, in contrast our method is relatively simple but at the same time provides good results and can detect low-contrast nodules. Furthermore, in this paper is presented a new algorithm for training the PNN neural networks that allows to obtain PNNs with many fewer neurons compared to the neural networks obtained by using the training algorithms present in the literature. So considerably lowering the computational burden of the trained network and at same time keeping the same performances.","The proposed method is relatively simple, but at the same time provides good results. In Table 1 we present a summary of similar approaches for lungs nodules detection. In comparison to other methods the main advantages of the proposed method are: a better correct classification rate (92%) and the fact that is capable to detect low-contrast nodules and lung cancers of minor or equal to 20 mm of diameter.","Small lung nodules detection based on local variance analysis and probabilistic neural networkChest X-ray screening , Biomedical image processing , Automatic pathology recognition , Probabilistic neural network.Background and objective: In medical examinations doctors use various techniques in order to provide to the patients an accurate analysis of their actual state of health. One of the commonly used methodologies is the x-ray screening. This examination very often help to diagnose some diseases of chest organs. The most frequent cause of wrong diagnosis lie in the radiologist’s difficulty in interpreting the presence of lungs carcinoma in chest X-ray. In such circumstances, an automated approach could be highly advantageous as it provides important help in medical diagnosis. Methods: In this paper we propose a new classification method of the lung carcinomas. This method start with the localization and extraction of the lung nodules by computing, for each pixel of the original image, the local variance obtaining an output image (variance image) with the same size of the original image. In the variance image we find the local maxima and then by using the locations of these maxima in the original image we found the contours of the possible nodules in lung tissues. However after this segmentation stage we find many false nodules. Therefore to discriminate the true ones we use a probabilistic neural network as classifier. Results: The performance of our approach is 92% of correct classifications, while the sensitivity is 95% and the specificity is 89.7%. The misclassification errors are due to the fact that network confuses false nodules with the true ones (6%) and true nodules with the false ones (2%). Conclusions: Several researchers have proposed automated algorithms to detect and classify pulmonary nodules but these methods fail to detect low-contrast nodules and have a high computational complexity, in contrast our method is relatively simple but at the same time provides good results and can detect low-contrast nodules. Furthermore, in this paper is presented a new algorithm for training the PNN neural networks that allows to obtain PNNs with many fewer neurons compared to the neural networks obtained by using the training algorithms present in the literature. So considerably lowering the computational burden of the trained network and at same time keeping the same performances.The proposed method is relatively simple, but at the same time provides good results. In Table 1 we present a summary of similar approaches for lungs nodules detection. In comparison to other methods the main advantages of the proposed method are: a better correct classification rate (92%) and the fact that is capable to detect low-contrast nodules and lung cancers of minor or equal to 20 mm of diameter.Figure 2. Transverse chest HRCT scans series from a 51-year-old man with common COVID-19 pneumonia. (a) Day 3 after symptom onset: multifocal round or patchy ground-glass opacity in bilateral lower lobes and the right middle lobe. (b)Day 9 after symptom onset: Progress of patchy ground-glass opacity in right middle lobe, mixed ground-glass opacity and consolidation in bilateral lower lobes. (c)Day 15 after symptom onset: multifocal irregular consolidation and fibrosis in bilateral lower lobes and the right middle lobe. (d)Day 26 after symptom onset: patchy ground-glass opacity in right middle lobe, mixed ground- glass opacity and consolidation in bilateral lower lobes.Figure 1. Transverse chest HRCT scans series from a 47-year-old woman with common COVID-19 pneumonia (a) Day 2 after symptom onset: multifocal round or patchy ground-glass opacity in bilateral lower lobes. (b)Day 8 after symptom onset: multifocal irregular consolidation and fibrosis in left upper lobe and bilateral lower lobes. (c)Day 17 after symptom onset: multifocal patchy ground-glass opacity in left upper lobe and bilateral lower lobes, fibrosis in right lower lobe. (d)Day 25 after symptom onset: multifocal patchy ground-glass opacity in left upper lobe and bilateral lower lobes.",Medical Data Analysis,"This paper proposes a new method for classifying lung nodules using x-ray screening. The method involves localizing and extracting the nodules using a variance image, followed by using a probabilistic neural network as a classifier to discriminate the true nodules. The performance of the approach is 92% correct classification with 95% sensitivity and 89.7% specificity. The proposed method is simpler and capable of detecting low-contrast nodules compared to other methods in the literature. Additionally, a new algorithm for training PNN neural networks is presented, allowing for fewer neurons and lower computational burden while maintaining performance.",Medical Data Analysis,"Figure 2. Transverse chest HRCT scans series from a 51-year-old man with common COVID-19 pneumonia. (a) Day 3 after symptom onset: multifocal round or patchy ground-glass opacity in bilateral lower lobes and the right middle lobe. (b)Day 9 after symptom onset: Progress of patchy ground-glass opacity in right middle lobe, mixed ground-glass opacity and consolidation in bilateral lower lobes. (c)Day 15 after symptom onset: multifocal irregular consolidation and fibrosis in bilateral lower lobes and the right middle lobe. (d)Day 26 after symptom onset: patchy ground-glass opacity in right middle lobe, mixed ground- glass opacity and consolidation in bilateral lower lobes.Figure 1. Transverse chest HRCT scans series from a 47-year-old woman with common COVID-19 pneumonia (a) Day 2 after symptom onset: multifocal round or patchy ground-glass opacity in bilateral lower lobes. (b)Day 8 after symptom onset: multifocal irregular consolidation and fibrosis in left upper lobe and bilateral lower lobes. (c)Day 17 after symptom onset: multifocal patchy ground-glass opacity in left upper lobe and bilateral lower lobes, fibrosis in right lower lobe. (d)Day 25 after symptom onset: multifocal patchy ground-glass opacity in left upper lobe and bilateral lower lobes.",Medical Data Analysis
267,Development of a child head analytical dynamic model considering cranial nonuniform thickness and curvature – Applying to children aged 0–1 years old,"Child head , Analytical model , Cranial thickness , Curvature , Dynamic response , Effect law.","Background and objective: Although analytical models have been used to quickly predict head response under impact condition, the existing models generally took the head as regular shell with uniform thickness which cannot account for the actual head geometry with varied cranial thickness and curvature at different locations. The objective of this study is to develop and validate an analytical model incorporating actual cranial thickness and curvature for child aged 0–1YO and investigate their effects on child head dynamic responses at different head locations. Methods: To develop the new analytical model, the child head was simplified into an irregular fluid-filled shell with non-uniform thickness and the cranial thickness and curvature at different locations were automatically obtained from CT scans using a procedure developed in this study. The implicit equation of maximum impact force was derived as a function of elastic modulus, thickness and radius of curvature of cranium. Results: The proposed analytical model are compared with cadaver test data of children aged 0–1 years old and it is shown to be accurate in predicting head injury metrics. According to this model, obvious difference in injury metrics were observed among subjects with the same age, but different cranial thickness and curvature; and the injury metrics at forehead location are significant higher than those at other locations due to large thickness it owns. Conclusions: The proposed model shows good biofidelity and can be used in quickly predicting the dynamics response at any location of head for child younger than 1 YO.","Some efforts and progress have been made to investigate the infant head dynamic responses through analytical method. The following conclusions can be drawn: • A theoretical dynamic model which can reflect the actual cranial thickness and curvature of child has been developed and preliminarily validated. The results show that the model has good biofidelity and can be used in quickly predicting the dynamics response at any location of head for child younger than 1 YO. • The cranial thickness and curvature at different impact locations for 5MO and 11MO child have been automatically calculated from CT scans through a procedure developed in this study. • The head injury metrics increased obviously with the increasing of cranial thickness and elastic modulus, relatively speaking, the head injury metrics decreased in a relatively slow-acting degree with the increasing of the radius of curvature. • The injury metrics of 11 MO child are apparently higher than that of the 5 MO child. Obvious difference in injury metrics were observed among subjects with the same age, but different cranial thickness and curvature. In addition, the injury metrics at forehead location are significant higher than those at other locations due to large thickness it owns.","Development of a child head analytical dynamic model considering cranial nonuniform thickness and curvature – Applying to children aged 0–1 years oldChild head , Analytical model , Cranial thickness , Curvature , Dynamic response , Effect law.Background and objective: Although analytical models have been used to quickly predict head response under impact condition, the existing models generally took the head as regular shell with uniform thickness which cannot account for the actual head geometry with varied cranial thickness and curvature at different locations. The objective of this study is to develop and validate an analytical model incorporating actual cranial thickness and curvature for child aged 0–1YO and investigate their effects on child head dynamic responses at different head locations. Methods: To develop the new analytical model, the child head was simplified into an irregular fluid-filled shell with non-uniform thickness and the cranial thickness and curvature at different locations were automatically obtained from CT scans using a procedure developed in this study. The implicit equation of maximum impact force was derived as a function of elastic modulus, thickness and radius of curvature of cranium. Results: The proposed analytical model are compared with cadaver test data of children aged 0–1 years old and it is shown to be accurate in predicting head injury metrics. According to this model, obvious difference in injury metrics were observed among subjects with the same age, but different cranial thickness and curvature; and the injury metrics at forehead location are significant higher than those at other locations due to large thickness it owns. Conclusions: The proposed model shows good biofidelity and can be used in quickly predicting the dynamics response at any location of head for child younger than 1 YO.Some efforts and progress have been made to investigate the infant head dynamic responses through analytical method. The following conclusions can be drawn: • A theoretical dynamic model which can reflect the actual cranial thickness and curvature of child has been developed and preliminarily validated. The results show that the model has good biofidelity and can be used in quickly predicting the dynamics response at any location of head for child younger than 1 YO. • The cranial thickness and curvature at different impact locations for 5MO and 11MO child have been automatically calculated from CT scans through a procedure developed in this study. • The head injury metrics increased obviously with the increasing of cranial thickness and elastic modulus, relatively speaking, the head injury metrics decreased in a relatively slow-acting degree with the increasing of the radius of curvature. • The injury metrics of 11 MO child are apparently higher than that of the 5 MO child. Obvious difference in injury metrics were observed among subjects with the same age, but different cranial thickness and curvature. In addition, the injury metrics at forehead location are significant higher than those at other locations due to large thickness it owns.Fig. 4. Error metric used in this study, mean projected contour distance (mPCD), and example registration results with different mPCD values. (a) X-ray image, (b) digitally reconstructed radiograph (DRR) at the estimated pose, (c) overlaid with the DRR edges (in red), (d)(e) enlarged view of the trials with different mPCD values. The yellow arrows indicate the true contour of the rib on the x-ray image. The result with mPCD of 0.7mm (d) exhibits almost no visually recognizable difference between the lines while 3.7mm (e) shows a clear discrepancy. (f) mPCD was defined as the distance between the manually traced contour (green dashed line) of each rib bone on the x-ray image and the automatically detected contour on the DRR (red dashed line). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).2D Input —— X-ray video : Xray(t) | = 3D Input Landmark identification __ by SSM mean +o Local contrast normalization(LCN) Geometry ) © landmark t.- rotation axis 2D-3D registration, —J— Similai | craien'Goraatonce) | me’ Gradient | craien'Goraatonce) | Optimization: CMA-ES argmax GC(LCN(Xray), Proj(@)) — Ag(@) threshold 8 Adri Idd; — Adry I? Movement of rib cage 0 di = Idi ~derl (a) Hierarchical optimization strategy 15t stage: 6 DOF 2""4 stage: 6+N DOF 3"" stage: 6+5N DOF n rib © RN global € Ré global € Ré (b) Fig. 1. (a) Workflow of the proposed method. The optimization parameter © represents the rigid transformation of the spine, and rotation axis and rotation angle of N ribs (note the rotation axis parameters are not optimized in the first stage). g(@) is a regularization term that penalizes the cost according to the distance between the ribs which acts like the intercostal muscles and facilitates a robust and anatomically feasible estimation. (b) Hierarchical optimization strategy in the proposed method. The first stage optimizes the global rigid transformation of the rib cage. The second stage jointly optimizes the local rigid transformation of the rib-rotation angle around the rotation axis n for each rib. The final stage jointly optimizes the rotation axis n within a small cylindrical region.X-ray detector Patient Fig. 3. Definition of projection geometry used in this study. Parameters associated with x-ray source and computed tomography (CT) and detector-coordinate systems are shown.Training data (j-th rib) Normalized geometry space (average bone shape) Manual Landmark Identification =) Non-tigid transformation Non-tigid transformation Target data Ly = Tare (Lave) Automated landmark identification Fig. 2. Workflow of computing the anatomical landmarks on the rib bone. The landmarks are manually identified on the average bone shape (Lave) and automatically mapped to the target shape (Lr) by statistical shape model (SSM) fitting.4 9.9 [sec] oee o (i) 0.0 [sec] 0.0 [sec] 3.3 [sec] 6.6 [sec] 9.9 [sec] Fig. 9. Visualization of the registration results at 4 representative frames of patient #1 in the real image experiment. (a-d) The original x-ray image, (e-h) overlaid with the contours of the ribs (green: manually identified ground truth, red: estimated) and the center of vertebrae (green cross: ground truth, red circle: estimated). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).{ Ir 1 Without local contrast normalization 1 With local contrast normalization = & a4 1 + g | : Ee. ! + EES a =e Wied anion tS Wie tn Wn axis search search +penalty axis search ‘search +penalty Mean [mm] 6.358 2.607 1.628 1.255 Std [mm] 6.636 1.798 0.851 0.615 | sp <oot Fig. 8. Quantitative evaluation results of the real-image experiments with 6 ribs (2nd, 3rd and 4th ribs of both sides) of 8 patients (ie., 48 data points in total). The mean and standard deviation for each scenario are listed in the table below. The error was significantly decreased by adding the rotation axis search and penalty term, and further decreased by adding the preprocessing with LCN.Patient #1 Patient #2 Patient #3 Patient #4 fips P0211 8625059 Ge) ff mPC0=1680-4001 ve] Gs 002819543201 [ol fs P01 38040012 [nl wo 3 ot 4 © © Ee * e < e : z Ex Em sh Em en 80 ge 820 £ 820 820 Z z > = z 4 z "" ry © © ” S a ° Mb SBS cos oF ons 02 pas 01 018 0? Ons 01 01s 07 Ons 01 015 07 3 feet simlaty silarty ‘lary 3500 ; f | L : 1 2500 < g Zz 8 $ | 3 = 2 = a 2 005 01 01s 02 00s 01 015 az O05 or ars 02 005 01 6 Sima Sinai Simin Sin 2 Patient #5 Patient #6 Patient #7 Patient #8 5 fe C027 2986262 [nm fie nb gp=4t22 18) gfe 90012300526 [nm df 902.9805 901 [nm = © © o of fo fo fo fo § 2 gn gn ga ge x £ 3 1” 0 1 0 i 2 3 A axe d d al Son 2 3 005 01 0x5 02 005 01 015 02 005 01 018 02 005 01 xs 02 & 3 Siar sity Siar Siar 5 2 - 3500 ag = & = 7 a N 1 i 2500 Eo Zz ° 1 -2.0 — s sffremPO=1 294202980, = o i! : aa cat 00st ois 02 00s Gv 018 02 Sir Sima Fig. 7. Analysis of similarity metric landscape to investigate the effectiveness of local contrast normalization (LCN). The mPCD is plotted as a function of the similarity ‘measure (GC) in the top and bottom rows of each patient. The color of each cross indicates the progression of iterations in the optimization process (i.e. the registration started from the blue cross and converged at the red cross). Thirty optimization trials were carried out for the first frame of each patient. One cross represents an average over 100 function evaluations (approximately 5 x 10* function evaluations were conducted in one trial). The red circle indicates the converged solution. Note that the red crosses in the bottom row (with LCN) are more concentrated and show lower mPCD than the top row (without LCN), indicating that LCN improved both robustness and accuracy. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).Right rib Left rib range of motion [deg] — 2nd —— 2nd 3rd -- 3rd 4th 4th range of motion: 7.01+0.44 [deg] 0 2 4 6 8 10 12 Patient #1 time[s] range of motion: 8.54+1.84 [deg] Bro g $5 € S 2 § o B° : : 0 5 10 time(s] range of motion: 6.57+1.00 [deg] “Sto x 8 = § 2 § a 2 a 3 0 5 10 time(s] range of motion: 6.46+1.82 [deg] “sto 0 *e = = ¢ 5 2 5 & g 0 5 10 time(s] range of motion: 9.34+2.08 [deg] “sto S 3 3 2 = Bs & . o S a BR? 2 0 5 10 time(s] range of motion: 4.77+0.62 [deg] g “sto = $ 2 s s i a 3 2 0 5 10 time(s] (a) (b) (c) (d) Fig. 10. Visualization of the registration result of all cases in the real image experiment. (a) The original x-ray image, (b,c) overlaid with the 3D rib model at the estimated pose. The color of each rib indicates the range of motion, which is defined as the difference between the maximum and minimum rotation angle in the ten seconds acquired by the video. The color difference between right and left ribs in patient #5 indicate left-right asymmetry motion, (d) The rib rotation angle around the rotation axis as a function of time. The 2nd, 3rd and 4th ribs of right and left sides were plotted for each patient. The reference of the rotation angle (0°) was the rotation angle estimated at the first frame of the x-ray video. The blue and red lines shows the ribs of right and left sides, respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).. *:p <0.01 sr - = + (1) True axis + (2) Anatomical axis without rotation axis search T + » (8) Anatomical axis with rotation axis search +penalty 35 a 0 8 3 _ 9 =25 z 1 = 8 € - . 7 E> g 2, a + i & . hist ! = B4 + i 5 6 5 + i | g ee 5 iH | | 2 5 x yy | 05; — 1 cc 8 i . et 24 Sit oe 3 aL. (1) True (2) Anatomical (3) Anatomical"" (1) True (2) Anatomical (2) Anatomical axis axis without axis with rotation axis. axis without axis with rotation rotation axis axis search rotation axis axis search 1 search +penalty search +penalty Mean {mm} | 0.698 | 1.148 | 0.742 | [Mean (deg]| 0.863 | 2.246 | 1.495 0 Std{mm] | 0.326 | 0.743 | 0.281 Std [deg] | 0.654 | 1.840 | 0.993 mPCD [mm] (a) (b) (c) Fig. 6. Results of simulation experiments. (a-b) box and whisker plots for different search scenarios. (a) is used mPCD and (b) is used rotation-angle error as error metric. Boxes denote the 1st/3rd quartiles, the median is marked with the horizontal line within each box, and outliers are marked with crosses. (c) The scatter plot showing the rotation angle error as a function of mPCD. The correlation between the two metrics suggested validity of using mPCD, the error metric observable in the projection as an indicator of the unobservable 3D rotation-angle error.Patient #1 Patient #7 Patient #14 12 Fe Fe & a © 2 J . ’ 8 & o Patient #18 Patient #20 Patient #21 6 & ° © oD 45 I 4r 2 Fig. 5. Rib cage and lungs of six cases in EMPIRE10 data set that were used in the evaluation experiments. CT images at inhale and exhale phases were analyzed. Color of each rib indicates the rotation angle between inhale and exhale phases (see the colormap on the right). The opaque and transparent ribs show the inhale and exhale phases, respectively (they overlap each other in some cases). The lungs at inhale and exhale phases are shown in red and blue. The larger rotation angle was observed at the ribs of the superior levels. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).",Medical Data Analysis,"The objective of this study is to develop and validate an analytical model incorporating actual cranial thickness and curvature for child aged 0–1YO and investigate their effects on child head dynamic responses at different head locations. The proposed model is compared with cadaver test data of children aged 0–1 years old and it is shown to be accurate in predicting head injury metrics. The results show that the injury metrics increased obviously with the increasing of cranial thickness and elastic modulus, and the injury metrics at forehead location are significantly higher than those at other locations due to large thickness it owns. The proposed model shows good biofidelity and can be used in quickly predicting the dynamics response at any location of head for child younger than 1 YO.",Medical Data Analysis,"Fig. 4. Error metric used in this study, mean projected contour distance (mPCD), and example registration results with different mPCD values. (a) X-ray image, (b) digitally reconstructed radiograph (DRR) at the estimated pose, (c) overlaid with the DRR edges (in red), (d)(e) enlarged view of the trials with different mPCD values. The yellow arrows indicate the true contour of the rib on the x-ray image. The result with mPCD of 0.7mm (d) exhibits almost no visually recognizable difference between the lines while 3.7mm (e) shows a clear discrepancy. (f) mPCD was defined as the distance between the manually traced contour (green dashed line) of each rib bone on the x-ray image and the automatically detected contour on the DRR (red dashed line). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).2D Input —— X-ray video : Xray(t) | = 3D Input Landmark identification __ by SSM mean +o Local contrast normalization(LCN) Geometry ) © landmark t.- rotation axis 2D-3D registration, —J— Similai | craien'Goraatonce) | me’ Gradient | craien'Goraatonce) | Optimization: CMA-ES argmax GC(LCN(Xray), Proj(@)) — Ag(@) threshold 8 Adri Idd; — Adry I? Movement of rib cage 0 di = Idi ~derl (a) Hierarchical optimization strategy 15t stage: 6 DOF 2""4 stage: 6+N DOF 3"" stage: 6+5N DOF n rib © RN global € Ré global € Ré (b) Fig. 1. (a) Workflow of the proposed method. The optimization parameter © represents the rigid transformation of the spine, and rotation axis and rotation angle of N ribs (note the rotation axis parameters are not optimized in the first stage). g(@) is a regularization term that penalizes the cost according to the distance between the ribs which acts like the intercostal muscles and facilitates a robust and anatomically feasible estimation. (b) Hierarchical optimization strategy in the proposed method. The first stage optimizes the global rigid transformation of the rib cage. The second stage jointly optimizes the local rigid transformation of the rib-rotation angle around the rotation axis n for each rib. The final stage jointly optimizes the rotation axis n within a small cylindrical region.X-ray detector Patient Fig. 3. Definition of projection geometry used in this study. Parameters associated with x-ray source and computed tomography (CT) and detector-coordinate systems are shown.Training data (j-th rib) Normalized geometry space (average bone shape) Manual Landmark Identification =) Non-tigid transformation Non-tigid transformation Target data Ly = Tare (Lave) Automated landmark identification Fig. 2. Workflow of computing the anatomical landmarks on the rib bone. The landmarks are manually identified on the average bone shape (Lave) and automatically mapped to the target shape (Lr) by statistical shape model (SSM) fitting.4 9.9 [sec] oee o (i) 0.0 [sec] 0.0 [sec] 3.3 [sec] 6.6 [sec] 9.9 [sec] Fig. 9. Visualization of the registration results at 4 representative frames of patient #1 in the real image experiment. (a-d) The original x-ray image, (e-h) overlaid with the contours of the ribs (green: manually identified ground truth, red: estimated) and the center of vertebrae (green cross: ground truth, red circle: estimated). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).{ Ir 1 Without local contrast normalization 1 With local contrast normalization = & a4 1 + g | : Ee. ! + EES a =e Wied anion tS Wie tn Wn axis search search +penalty axis search ‘search +penalty Mean [mm] 6.358 2.607 1.628 1.255 Std [mm] 6.636 1.798 0.851 0.615 | sp <oot Fig. 8. Quantitative evaluation results of the real-image experiments with 6 ribs (2nd, 3rd and 4th ribs of both sides) of 8 patients (ie., 48 data points in total). The mean and standard deviation for each scenario are listed in the table below. The error was significantly decreased by adding the rotation axis search and penalty term, and further decreased by adding the preprocessing with LCN.Patient #1 Patient #2 Patient #3 Patient #4 fips P0211 8625059 Ge) ff mPC0=1680-4001 ve] Gs 002819543201 [ol fs P01 38040012 [nl wo 3 ot 4 © © Ee * e < e : z Ex Em sh Em en 80 ge 820 £ 820 820 Z z > = z 4 z "" ry © © ” S a ° Mb SBS cos oF ons 02 pas 01 018 0? Ons 01 01s 07 Ons 01 015 07 3 feet simlaty silarty ‘lary 3500 ; f | L : 1 2500 < g Zz 8 $ | 3 = 2 = a 2 005 01 01s 02 00s 01 015 az O05 or ars 02 005 01 6 Sima Sinai Simin Sin 2 Patient #5 Patient #6 Patient #7 Patient #8 5 fe C027 2986262 [nm fie nb gp=4t22 18) gfe 90012300526 [nm df 902.9805 901 [nm = © © o of fo fo fo fo § 2 gn gn ga ge x £ 3 1” 0 1 0 i 2 3 A axe d d al Son 2 3 005 01 0x5 02 005 01 015 02 005 01 018 02 005 01 xs 02 & 3 Siar sity Siar Siar 5 2 - 3500 ag = & = 7 a N 1 i 2500 Eo Zz ° 1 -2.0 — s sffremPO=1 294202980, = o i! : aa cat 00st ois 02 00s Gv 018 02 Sir Sima Fig. 7. Analysis of similarity metric landscape to investigate the effectiveness of local contrast normalization (LCN). The mPCD is plotted as a function of the similarity ‘measure (GC) in the top and bottom rows of each patient. The color of each cross indicates the progression of iterations in the optimization process (i.e. the registration started from the blue cross and converged at the red cross). Thirty optimization trials were carried out for the first frame of each patient. One cross represents an average over 100 function evaluations (approximately 5 x 10* function evaluations were conducted in one trial). The red circle indicates the converged solution. Note that the red crosses in the bottom row (with LCN) are more concentrated and show lower mPCD than the top row (without LCN), indicating that LCN improved both robustness and accuracy. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).Right rib Left rib range of motion [deg] — 2nd —— 2nd 3rd -- 3rd 4th 4th range of motion: 7.01+0.44 [deg] 0 2 4 6 8 10 12 Patient #1 time[s] range of motion: 8.54+1.84 [deg] Bro g $5 € S 2 § o B° : : 0 5 10 time(s] range of motion: 6.57+1.00 [deg] “Sto x 8 = § 2 § a 2 a 3 0 5 10 time(s] range of motion: 6.46+1.82 [deg] “sto 0 *e = = ¢ 5 2 5 & g 0 5 10 time(s] range of motion: 9.34+2.08 [deg] “sto S 3 3 2 = Bs & . o S a BR? 2 0 5 10 time(s] range of motion: 4.77+0.62 [deg] g “sto = $ 2 s s i a 3 2 0 5 10 time(s] (a) (b) (c) (d) Fig. 10. Visualization of the registration result of all cases in the real image experiment. (a) The original x-ray image, (b,c) overlaid with the 3D rib model at the estimated pose. The color of each rib indicates the range of motion, which is defined as the difference between the maximum and minimum rotation angle in the ten seconds acquired by the video. The color difference between right and left ribs in patient #5 indicate left-right asymmetry motion, (d) The rib rotation angle around the rotation axis as a function of time. The 2nd, 3rd and 4th ribs of right and left sides were plotted for each patient. The reference of the rotation angle (0°) was the rotation angle estimated at the first frame of the x-ray video. The blue and red lines shows the ribs of right and left sides, respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).. *:p <0.01 sr - = + (1) True axis + (2) Anatomical axis without rotation axis search T + » (8) Anatomical axis with rotation axis search +penalty 35 a 0 8 3 _ 9 =25 z 1 = 8 € - . 7 E> g 2, a + i & . hist ! = B4 + i 5 6 5 + i | g ee 5 iH | | 2 5 x yy | 05; — 1 cc 8 i . et 24 Sit oe 3 aL. (1) True (2) Anatomical (3) Anatomical"" (1) True (2) Anatomical (2) Anatomical axis axis without axis with rotation axis. axis without axis with rotation rotation axis axis search rotation axis axis search 1 search +penalty search +penalty Mean {mm} | 0.698 | 1.148 | 0.742 | [Mean (deg]| 0.863 | 2.246 | 1.495 0 Std{mm] | 0.326 | 0.743 | 0.281 Std [deg] | 0.654 | 1.840 | 0.993 mPCD [mm] (a) (b) (c) Fig. 6. Results of simulation experiments. (a-b) box and whisker plots for different search scenarios. (a) is used mPCD and (b) is used rotation-angle error as error metric. Boxes denote the 1st/3rd quartiles, the median is marked with the horizontal line within each box, and outliers are marked with crosses. (c) The scatter plot showing the rotation angle error as a function of mPCD. The correlation between the two metrics suggested validity of using mPCD, the error metric observable in the projection as an indicator of the unobservable 3D rotation-angle error.Patient #1 Patient #7 Patient #14 12 Fe Fe & a © 2 J . ’ 8 & o Patient #18 Patient #20 Patient #21 6 & ° © oD 45 I 4r 2 Fig. 5. Rib cage and lungs of six cases in EMPIRE10 data set that were used in the evaluation experiments. CT images at inhale and exhale phases were analyzed. Color of each rib indicates the rotation angle between inhale and exhale phases (see the colormap on the right). The opaque and transparent ribs show the inhale and exhale phases, respectively (they overlap each other in some cases). The lungs at inhale and exhale phases are shown in red and blue. The larger rotation angle was observed at the ribs of the superior levels. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article).",Medical Data Analysis
268,Automatic delineation of ribs and clavicles in chest radiographs using fully convolutional DenseNets,"Chest radiograph , Rib and clavicle delineation , Fully convolutional DenseNet.","Background and Objective: In chest radiographs (CXRs), all bones and soft tissues are overlapping with each other, which raises issues for radiologists to read and interpret CXRs. Delineating the ribs and clavicles is helpful for suppressing them from chest radiographs so that their effects can be reduced for chest radiography analysis. However, delineating ribs and clavicles automatically is difficult by methods without deep learning models. Moreover, few of methods without deep learning models can delineate the anterior ribs effectively due to their faint rib edges in the posterior-anterior (PA) CXRs. Methods: In this work, we present an effective deep learning method for delineating posterior ribs, anterior ribs and clavicles automatically using a fully convolutional DenseNet (FC-DenseNet) as pixel classifier. We consider a pixel-weighted loss function to mitigate the uncertainty issue during manually delineating for robust prediction. Results: We conduct a comparative analysis with two other fully convolutional networks for edge detection and the state-of-the-art method without deep learning models. The proposed method significantly outperforms these methods in terms of quantitative evaluation metrics and visual perception. The average recall, precision and F-measure are 0.773 ± 0.030, 0.861 ± 0.043 and 0.814 ± 0.023 respectively, and the mean boundary distance (MBD) is 0.855 ± 0.642 pixels of the proposed method on the test dataset. The proposed method also performs well on JSRT and NIH Chest X-ray datasets, indicating its generalizability across multiple databases. Besides, a preliminary result of suppressing the bone components of CXRs has been produced by using our delineating system. Conclusions: The proposed method can automatically delineate ribs and clavicles in CXRs and produce accurate edge maps.","In this study, we propose a method for automatically delineating posterior ribs, anterior ribs, and clavicles in CXRs. Combined classic FC-DenseNet with well-designed pixel-weighted CE loss function, the proposed method could automatically delineate ribs and clavicles of CXRs in multiple public databases and produce accurate binary edge maps. Besides, the proposed method also was validated the usefulness for bone suppression of CXRs.","Automatic delineation of ribs and clavicles in chest radiographs using fully convolutional DenseNetsChest radiograph , Rib and clavicle delineation , Fully convolutional DenseNet.Background and Objective: In chest radiographs (CXRs), all bones and soft tissues are overlapping with each other, which raises issues for radiologists to read and interpret CXRs. Delineating the ribs and clavicles is helpful for suppressing them from chest radiographs so that their effects can be reduced for chest radiography analysis. However, delineating ribs and clavicles automatically is difficult by methods without deep learning models. Moreover, few of methods without deep learning models can delineate the anterior ribs effectively due to their faint rib edges in the posterior-anterior (PA) CXRs. Methods: In this work, we present an effective deep learning method for delineating posterior ribs, anterior ribs and clavicles automatically using a fully convolutional DenseNet (FC-DenseNet) as pixel classifier. We consider a pixel-weighted loss function to mitigate the uncertainty issue during manually delineating for robust prediction. Results: We conduct a comparative analysis with two other fully convolutional networks for edge detection and the state-of-the-art method without deep learning models. The proposed method significantly outperforms these methods in terms of quantitative evaluation metrics and visual perception. The average recall, precision and F-measure are 0.773 ± 0.030, 0.861 ± 0.043 and 0.814 ± 0.023 respectively, and the mean boundary distance (MBD) is 0.855 ± 0.642 pixels of the proposed method on the test dataset. The proposed method also performs well on JSRT and NIH Chest X-ray datasets, indicating its generalizability across multiple databases. Besides, a preliminary result of suppressing the bone components of CXRs has been produced by using our delineating system. Conclusions: The proposed method can automatically delineate ribs and clavicles in CXRs and produce accurate edge maps.In this study, we propose a method for automatically delineating posterior ribs, anterior ribs, and clavicles in CXRs. Combined classic FC-DenseNet with well-designed pixel-weighted CE loss function, the proposed method could automatically delineate ribs and clavicles of CXRs in multiple public databases and produce accurate binary edge maps. Besides, the proposed method also was validated the usefulness for bone suppression of CXRs.5 6 5 6 7 Index of weights 5 6 Index of weights Index of weights Fig. 9. Illustrations of the learned weights in CNN and in dense CRF for unary features on different datasets. The blue bar represents the learned weights wP in CNN, and the yellow bar represents the learned weights w,, in dense CRF. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)Fig. 5. Segmentation results with the worst F1-score case for DRIVE, STARE, CHASEDB1, and HI[ | DRIVE STARE CHASEDBI po eees Ground truth Fig. 4. Segmentation results with the best F1-score case for DRIVE, STARE, CHASEDB1, and HRF.Fig. 6. Examples of segmenting vessels with central reflex. Images in each column from left to right are: parts of Image_O1L in CHASEDBI, the corresponding ground truth, the results obtained in our method, and the results provided by Orlando et al.(e) (f) (g) Fig. 3. Illustration of the thin-vessel enhancement. (a) The original fundus image. (b) The enhanced image Ize of the inverted green channel after image preprocessing. (c) The image I; after morphological operations. (d) The image of line strength J}. (e) The image J, after background suppression. (f) The image I; after edge filtering. (g) The image I, combining I, and ;. (h) The final thin-vessel enhanced image lye. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)(a) (b) Fig. 8. An example of using FCNs for retinal vessel segmentation. (a) is an image from DRIVE dataset. (b) is the corresponding ground truth, (c) is our segmentation result. (d) is the vessel class map output from FCN-8s. (d) is the edge map output from HED.Trainin; Image patches Unary features Dense CRF model Image prepro- cessing Inference Green channel Pairwise term Thin-vessel enhancement Segmentation result Input image | = BN | ReLU BN | ReLU BN Lt) 1 sigmoid [- “max pooling SE max pooling Feature map 9549553 20x22x12 Lixtixt2 8x8x12 4x4x12 1000 10 14 size: input (b) Fig. 1. Overview of our proposed method. (a) Flowchart of the proposed method. (b) Structure of the CNN used for discriminative feature learning (BN - Batch Normalization layer, ReLU - Rectified Linear Units).0.796 0.794 ’ g 0.792 N 2 ' V\ 2 1 £ 2 079 1 i\ \ © 8 Vp\ 7 Vv 2 ® LT] \ 7 = I 0.788 ! 3 § I ° 0.786 ry Pyver £ 1 \4 - “\ 7 \ / 0.784 / Vl / / 0.782 oO 4 6 8 10 12 14 16 18 Number of learned unary features Fig. 7. The F1-score and time consumption for different number of unary features. The red dot shows the best F1-score at n = 10. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this arti- cle.)",Medical Data Analysis,"This study proposes an effective deep learning method using a fully convolutional DenseNet (FC-DenseNet) for automatically delineating posterior ribs, anterior ribs, and clavicles in chest radiographs (CXRs) to reduce their effects for chest radiography analysis. The proposed method significantly outperforms two other fully convolutional networks for edge detection and a state-of-the-art method without deep learning models in terms of quantitative evaluation metrics and visual perception. The proposed method is also validated for its usefulness in bone suppression of CXRs and shows generalizability across multiple databases.",Medical Data Analysis,"5 6 5 6 7 Index of weights 5 6 Index of weights Index of weights Fig. 9. Illustrations of the learned weights in CNN and in dense CRF for unary features on different datasets. The blue bar represents the learned weights wP in CNN, and the yellow bar represents the learned weights w,, in dense CRF. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)Fig. 5. Segmentation results with the worst F1-score case for DRIVE, STARE, CHASEDB1, and HI[ | DRIVE STARE CHASEDBI po eees Ground truth Fig. 4. Segmentation results with the best F1-score case for DRIVE, STARE, CHASEDB1, and HRF.Fig. 6. Examples of segmenting vessels with central reflex. Images in each column from left to right are: parts of Image_O1L in CHASEDBI, the corresponding ground truth, the results obtained in our method, and the results provided by Orlando et al.(e) (f) (g) Fig. 3. Illustration of the thin-vessel enhancement. (a) The original fundus image. (b) The enhanced image Ize of the inverted green channel after image preprocessing. (c) The image I; after morphological operations. (d) The image of line strength J}. (e) The image J, after background suppression. (f) The image I; after edge filtering. (g) The image I, combining I, and ;. (h) The final thin-vessel enhanced image lye. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)(a) (b) Fig. 8. An example of using FCNs for retinal vessel segmentation. (a) is an image from DRIVE dataset. (b) is the corresponding ground truth, (c) is our segmentation result. (d) is the vessel class map output from FCN-8s. (d) is the edge map output from HED.Trainin; Image patches Unary features Dense CRF model Image prepro- cessing Inference Green channel Pairwise term Thin-vessel enhancement Segmentation result Input image | = BN | ReLU BN | ReLU BN Lt) 1 sigmoid [- “max pooling SE max pooling Feature map 9549553 20x22x12 Lixtixt2 8x8x12 4x4x12 1000 10 14 size: input (b) Fig. 1. Overview of our proposed method. (a) Flowchart of the proposed method. (b) Structure of the CNN used for discriminative feature learning (BN - Batch Normalization layer, ReLU - Rectified Linear Units).0.796 0.794 ’ g 0.792 N 2 ' V\ 2 1 £ 2 079 1 i\ \ © 8 Vp\ 7 Vv 2 ® LT] \ 7 = I 0.788 ! 3 § I ° 0.786 ry Pyver £ 1 \4 - “\ 7 \ / 0.784 / Vl / / 0.782 oO 4 6 8 10 12 14 16 18 Number of learned unary features Fig. 7. The F1-score and time consumption for different number of unary features. The red dot shows the best F1-score at n = 10. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this arti- cle.)",Medical Data Analysis
269,Performance comparison of publicly available retinal blood vessel segmentation methods,"Fundus , Retinal imaging , Vessel segmentation.","Retinal blood vessel structure is an important indicator of many retinal and systemic diseases, which has motivated the development of various image segmentation methods for the blood vessels. In this study, two supervised and three unsupervised segmentation methods with a publicly available implementation are reviewed and quantitatively compared with each other on five public databases with ground truth segmentation of the vessels. Each method is tested under consistent conditions with two types of preprocessing, and the parameters of the methods are optimized for each database. Additionally, possibility to predict the parameters of the methods by the linear regression model is tested for each database. Resolution of the input images and amount of the vessel pixels in the ground truth are used as predictors. The results show the positive influence of preprocessing on the performance of the unsupervised methods. The methods show similar performance for segmentation accuracy, with the best performance achieved by the method by Azzopardi et al. (Acc 94.0) on ARIADB, the method by Soares et al. (Acc 94.6, 94.7) on CHASEDB1 and DRIVE, and the method by Nguyen et al. (Acc 95.8, 95.5) on HRF and STARE. The method by Soares et al. performed better with regard to the area under the ROC curve. Qualitative differences between the methods are discussed. Finally, it was possible to predict the parameter settings that give performance close to the optimized performance of each method.","This study reviewed retinal vessel segmentation methods with publicly available implementation and publicly available databases of color fundus photographs containing ground truth for vessel segmentation. Two supervised and three unsupervised methods were studied and quantitatively compared using five publicly available databases. Two types of image preprocessing approaches were tested and the method parameters were optimized for the best performance on each database. In addition, the studied methods were compared to recent state-of-the-art approaches. The results show that the parameter optimization does not significantly improve the segmentation performance of the methods when the original data is used. However, the performance of the methods in new image data differs significantly. The performance of the tested methods with respect to accuracy was very close; highest performance was achieved on ARIADB by the Azzopardi method (Acc 94.0), on CHASE and DRIVE by the Soares method (Acc 94.6, 94.7) and on HRF and STARE by the Nguyen method (Acc 95.8, 95.5). The Soares and Azzopardi methods usually provides higher area under the ROC curve than the other methods. Preprocessing of the images with CLAHE improved the overall performance of the unsupervised methods. Parameters yielding the reported performance are also provided to give reasonable parameter ranges and starting points to support optimization on new data. Finally, it was possible to predict parameters that give best segmentation performance for each method.","Performance comparison of publicly available retinal blood vessel segmentation methodsFundus , Retinal imaging , Vessel segmentation.Retinal blood vessel structure is an important indicator of many retinal and systemic diseases, which has motivated the development of various image segmentation methods for the blood vessels. In this study, two supervised and three unsupervised segmentation methods with a publicly available implementation are reviewed and quantitatively compared with each other on five public databases with ground truth segmentation of the vessels. Each method is tested under consistent conditions with two types of preprocessing, and the parameters of the methods are optimized for each database. Additionally, possibility to predict the parameters of the methods by the linear regression model is tested for each database. Resolution of the input images and amount of the vessel pixels in the ground truth are used as predictors. The results show the positive influence of preprocessing on the performance of the unsupervised methods. The methods show similar performance for segmentation accuracy, with the best performance achieved by the method by Azzopardi et al. (Acc 94.0) on ARIADB, the method by Soares et al. (Acc 94.6, 94.7) on CHASEDB1 and DRIVE, and the method by Nguyen et al. (Acc 95.8, 95.5) on HRF and STARE. The method by Soares et al. performed better with regard to the area under the ROC curve. Qualitative differences between the methods are discussed. Finally, it was possible to predict the parameter settings that give performance close to the optimized performance of each method.This study reviewed retinal vessel segmentation methods with publicly available implementation and publicly available databases of color fundus photographs containing ground truth for vessel segmentation. Two supervised and three unsupervised methods were studied and quantitatively compared using five publicly available databases. Two types of image preprocessing approaches were tested and the method parameters were optimized for the best performance on each database. In addition, the studied methods were compared to recent state-of-the-art approaches. The results show that the parameter optimization does not significantly improve the segmentation performance of the methods when the original data is used. However, the performance of the methods in new image data differs significantly. The performance of the tested methods with respect to accuracy was very close; highest performance was achieved on ARIADB by the Azzopardi method (Acc 94.0), on CHASE and DRIVE by the Soares method (Acc 94.6, 94.7) and on HRF and STARE by the Nguyen method (Acc 95.8, 95.5). The Soares and Azzopardi methods usually provides higher area under the ROC curve than the other methods. Preprocessing of the images with CLAHE improved the overall performance of the unsupervised methods. Parameters yielding the reported performance are also provided to give reasonable parameter ranges and starting points to support optimization on new data. Finally, it was possible to predict parameters that give best segmentation performance for each method.FUT Fig. 1. Samples of lung x-ray images, where pulmonary nodules were detected. Images with detailed description are available at http://radiologykey.com solitary-and-multiple- pulmonary-nodules/.Fig. 9. An original x-ray image (left), and its variance matrix (right), where we have detected a true pulmonary nodule and a false one.Confusion Matrix Output Class True nodules _—_ False nodules Target Class Fig. 8. Confusion matrix relative to the results of classification between true nod- ules and false nodules obtained by using the implemented probabilistic neural net- work.Class 2 N Fig. 6. An example of a two-dimensional classification performed by a pattern layer’s neuron. For centroids that coming from the set of true pulmonary nodules the class 1 represent the class of true positives while the class 2 the class of false positives. For centroids that coming from the set of false pulmonary nodules the class 1 represent the class of false positives while the class 2 the class of true pos- itives.12t+.tWy 579157 Class of Max (g,, 92) 8X)=W2 1 V2, 1+W2.2)22+ --- + W268 )2068 _[X=X26sll” % Tipit Laver Pattern Layer Summation Output P y (Training Set) Layer Layer Fig. 5. The implemented probabilistic neural network: The input layer estimates the distance of the input vector from the centroids of the pattern layer. As a result, a vector is produced whose elements indicate how close the input is to single classes in terms of Mahalanobis distance. By using this vector, the pattern layer computes a probability vector whose components define the belonging to the different classes. Finally, the output layer determines the belonging of an input vector to a particular class by selecting the maximum value of the probability vector to predict the target class.BEGIN | Calculus of the neurons centres and its smoothing parameters Calculus of the network’s weights Pruning: we eliminate the neuron witha minimum weight. Recalculation of the network’s weights The error with respect to the previous step is greater than 1% ? END | Fig. 7. Training procedure for the proposed neural network.—— - Fig. 4. Some examples of the extracted objects from original images after the preprocessing stage. The left and the right images shown true nodules while the middle one shown a false nodule.Ao A Fig. 3. Extraction of the possible diseased tissues by using the boundary tracking algorithm in examined x-ray images. We start to scan the image from the left upper corner to the bottom of the image.j pixel ij Window 7 pixel ij Output image Fig. 2. The 3 x 3 pixel processing window used to calculate the local variance.",Medical Data Analysis,"This study reviews and compares retinal vessel segmentation methods using publicly available databases of color fundus photographs containing ground truth for vessel segmentation. Two supervised and three unsupervised methods are studied and quantitatively compared using five publicly available databases. Two types of image preprocessing approaches were tested, and the method parameters were optimized for the best performance on each database. The results show that the parameter optimization does not significantly improve the segmentation performance of the methods when the original data is used, but the performance of the methods in new image data differs significantly. The performance of the tested methods with respect to accuracy was very close, with the highest performance achieved on ARIADB by the Azzopardi method, on CHASE and DRIVE by the Soares method, and on HRF and STARE by the Nguyen method. The Soares and Azzopardi methods usually provide higher area under the ROC curve than the other methods. Preprocessing of the images with CLAHE improved the overall performance of the unsupervised methods, and parameters yielding the reported performance are provided to support optimization on new data. Finally, it was possible to predict parameters that give the best segmentation performance for each method.",Medical Data Analysis,"FUT Fig. 1. Samples of lung x-ray images, where pulmonary nodules were detected. Images with detailed description are available at http://radiologykey.com solitary-and-multiple- pulmonary-nodules/.Fig. 9. An original x-ray image (left), and its variance matrix (right), where we have detected a true pulmonary nodule and a false one.Confusion Matrix Output Class True nodules _—_ False nodules Target Class Fig. 8. Confusion matrix relative to the results of classification between true nod- ules and false nodules obtained by using the implemented probabilistic neural net- work.Class 2 N Fig. 6. An example of a two-dimensional classification performed by a pattern layer’s neuron. For centroids that coming from the set of true pulmonary nodules the class 1 represent the class of true positives while the class 2 the class of false positives. For centroids that coming from the set of false pulmonary nodules the class 1 represent the class of false positives while the class 2 the class of true pos- itives.12t+.tWy 579157 Class of Max (g,, 92) 8X)=W2 1 V2, 1+W2.2)22+ --- + W268 )2068 _[X=X26sll” % Tipit Laver Pattern Layer Summation Output P y (Training Set) Layer Layer Fig. 5. The implemented probabilistic neural network: The input layer estimates the distance of the input vector from the centroids of the pattern layer. As a result, a vector is produced whose elements indicate how close the input is to single classes in terms of Mahalanobis distance. By using this vector, the pattern layer computes a probability vector whose components define the belonging to the different classes. Finally, the output layer determines the belonging of an input vector to a particular class by selecting the maximum value of the probability vector to predict the target class.BEGIN | Calculus of the neurons centres and its smoothing parameters Calculus of the network’s weights Pruning: we eliminate the neuron witha minimum weight. Recalculation of the network’s weights The error with respect to the previous step is greater than 1% ? END | Fig. 7. Training procedure for the proposed neural network.—— - Fig. 4. Some examples of the extracted objects from original images after the preprocessing stage. The left and the right images shown true nodules while the middle one shown a false nodule.Ao A Fig. 3. Extraction of the possible diseased tissues by using the boundary tracking algorithm in examined x-ray images. We start to scan the image from the left upper corner to the bottom of the image.j pixel ij Window 7 pixel ij Output image Fig. 2. The 3 x 3 pixel processing window used to calculate the local variance.",Medical Data Analysis
270,"Performance analysis of descriptive statistical features in retinal vessel segmentation via fuzzy logic, ANN, SVM, and classifier fusion","Retinal vessel segmentation , Statistical features , Classification .","Diabetic retinopathy is the most common diabetic eye disease and a leading cause of blindness in the world. Diagnosis of diabetic retinopathy at an early stage can be done through the segmentation of blood vessels of the retina. In this work, the performance of descriptive statistical features in retinal vessel segmentation is evaluated by using fuzzy logic, an artificial neural network classifier (ANN), a support vector machine (SVM), and classifier fusion. Newly constructed eight features are formed by statistical moments. Mean and median measurements of image pixels’ intensity values in four directions, horizontal, vertical, up-diagonal, and down-diagonal, are calculated. Features, F1, F2, F3, and F4 are calculated as the mean values and F5, F6, F7, and F8 are calculated as the median values of a processed pixel in each direction. A fuzzy rule-based classifier, an ANN, a SVM, and a classifier fusion are designed. The publicly available DRIVE and STARE databases are used for evaluation. The fuzzy classifier achieved 93.82% of an overall accuracy, 72.28% of sensitivity, and 97.04% of specificity. For the ANN classifier, 94.2% of overall accuracy, 67.7% of sensitivity, and 98.1% of specificity are achieved on the DRIVE database. For the STARE database, the fuzzy classifier achieved 92.4% of overall accuracy, 75% of sensitivity, and 94.3% of specificity. The ANN classifier achieved the overall accuracy, sensitivity, and specificity as 94.2%, 56.9%, and 98.4%, respectively. Although the overall accuracy of the SVM is calculated lower than the fuzzy and the ANN classifiers, it achieved higher sensitivity rates. Designed classifier fusion achieved the best performance among all by using the proposed statistical features. Its overall accuracy, sensitivity, and specificity are calculated as 95.10%, 74.09%, 98.35% for the DRIVE and 95.53%, 70.14%, 98.46 for the STARE database, respectively. The experimental results validate that the descriptive statistical features can be employed in retinal vessel segmentation and can be used in rule-based and supervised classifiers.","Computer-aided retinal blood vessel classification is important for early diabetic retinopathy detection, glaucoma, and age-related macular degeneration which are known as the most prevalent causes of blindness in the world. This study proposed newly constructed descriptive statistical features to segment retinal vessel structure. The features are formed by means and medians of the image pixels’ intensity values in four directions: horizontal, vertical, up diagonal, and down diagonal. The performance evaluation of the features is performed by a rule-based fuzzy classifier and two supervised methods, ANN and SVM classifiers, and a classifier fusion. Our experimental results show that all classifiers achieved compatible classification accuracies with sensitivity and specificity values. The advantage of the proposed descriptive features over the feature sets in literature lies in its simplicity. The calculation of the proposed features is straightforward and does not require complicated algorithms. Nevertheless, the fuzzy logic classifier achieved overall 93.82% of accuracy rate and 0.9419 of AUC which is one of the highest among the rule-based methods reported while the ANN classifier achieved 94.20% of accuracy for the DRIVE database. The classifier fusion showed the best performance among all four classifiers. While the fuzzy and ANN provided very close classification accuracies to the second human observer (94.7%) without a significant degradation of sensitivity and specificity, the classifier fusion exceeded the human observer’s performance for DRIVE database. Comparable performances have been observed for the STARE database. The overall accuracy, sensitivity, and specificity values are calculated as 92.3%, 75%, and 94.3% by the fuzzy classifier while they are calculated as 94.2%, 56.9%, and 98.4% by the ANN classifier. The classifier fusion achieved 95.53% of overall accuracy with 70.14% of sensitivity and 98.46% of specificity. Based on the experimental results, it is validated that the proposed statistical features hold valuable information to segment pixels that belong to retinal blood vessels. These features can easily be combined with other features to improve the segmentation results in rule-based or supervised classifiers.","Performance analysis of descriptive statistical features in retinal vessel segmentation via fuzzy logic, ANN, SVM, and classifier fusionRetinal vessel segmentation , Statistical features , Classification .Diabetic retinopathy is the most common diabetic eye disease and a leading cause of blindness in the world. Diagnosis of diabetic retinopathy at an early stage can be done through the segmentation of blood vessels of the retina. In this work, the performance of descriptive statistical features in retinal vessel segmentation is evaluated by using fuzzy logic, an artificial neural network classifier (ANN), a support vector machine (SVM), and classifier fusion. Newly constructed eight features are formed by statistical moments. Mean and median measurements of image pixels’ intensity values in four directions, horizontal, vertical, up-diagonal, and down-diagonal, are calculated. Features, F1, F2, F3, and F4 are calculated as the mean values and F5, F6, F7, and F8 are calculated as the median values of a processed pixel in each direction. A fuzzy rule-based classifier, an ANN, a SVM, and a classifier fusion are designed. The publicly available DRIVE and STARE databases are used for evaluation. The fuzzy classifier achieved 93.82% of an overall accuracy, 72.28% of sensitivity, and 97.04% of specificity. For the ANN classifier, 94.2% of overall accuracy, 67.7% of sensitivity, and 98.1% of specificity are achieved on the DRIVE database. For the STARE database, the fuzzy classifier achieved 92.4% of overall accuracy, 75% of sensitivity, and 94.3% of specificity. The ANN classifier achieved the overall accuracy, sensitivity, and specificity as 94.2%, 56.9%, and 98.4%, respectively. Although the overall accuracy of the SVM is calculated lower than the fuzzy and the ANN classifiers, it achieved higher sensitivity rates. Designed classifier fusion achieved the best performance among all by using the proposed statistical features. Its overall accuracy, sensitivity, and specificity are calculated as 95.10%, 74.09%, 98.35% for the DRIVE and 95.53%, 70.14%, 98.46 for the STARE database, respectively. The experimental results validate that the descriptive statistical features can be employed in retinal vessel segmentation and can be used in rule-based and supervised classifiers.Computer-aided retinal blood vessel classification is important for early diabetic retinopathy detection, glaucoma, and age-related macular degeneration which are known as the most prevalent causes of blindness in the world. This study proposed newly constructed descriptive statistical features to segment retinal vessel structure. The features are formed by means and medians of the image pixels’ intensity values in four directions: horizontal, vertical, up diagonal, and down diagonal. The performance evaluation of the features is performed by a rule-based fuzzy classifier and two supervised methods, ANN and SVM classifiers, and a classifier fusion. Our experimental results show that all classifiers achieved compatible classification accuracies with sensitivity and specificity values. The advantage of the proposed descriptive features over the feature sets in literature lies in its simplicity. The calculation of the proposed features is straightforward and does not require complicated algorithms. Nevertheless, the fuzzy logic classifier achieved overall 93.82% of accuracy rate and 0.9419 of AUC which is one of the highest among the rule-based methods reported while the ANN classifier achieved 94.20% of accuracy for the DRIVE database. The classifier fusion showed the best performance among all four classifiers. While the fuzzy and ANN provided very close classification accuracies to the second human observer (94.7%) without a significant degradation of sensitivity and specificity, the classifier fusion exceeded the human observer’s performance for DRIVE database. Comparable performances have been observed for the STARE database. The overall accuracy, sensitivity, and specificity values are calculated as 92.3%, 75%, and 94.3% by the fuzzy classifier while they are calculated as 94.2%, 56.9%, and 98.4% by the ANN classifier. The classifier fusion achieved 95.53% of overall accuracy with 70.14% of sensitivity and 98.46% of specificity. Based on the experimental results, it is validated that the proposed statistical features hold valuable information to segment pixels that belong to retinal blood vessels. These features can easily be combined with other features to improve the segmentation results in rule-based or supervised classifiers.120 120 Sample:11MO-2 Sample:11MO-2 100 oe 100 aie 3 _ Position:Forehead a Position:Right parietal S S 2 80 ~ 80 g - © 60 ® 60 3 - 8 40 8 40 2 2 20 20 0 0 Experiment [4] This model Young's model [18] Experiment [4] This model Young's model [18] Fig. 10. Comparison between the present model and Young’s model [18] for the forehead and right parietal location of 11MO-2 subject with similar curvature but obviously different cranial thickness. 2 3 Py 3 Sample:5MO-1 Position:Right Parietal ‘Sample:5MO-1 Position: Occiput a 3 a 3 s Ss 2 40 : 40 3 30 30 2 3 8 20 8 20 10 10 ° 0 Experiment [4] This model Young's model [18] Experiment [4] This model Young's model [18] Fig. 11. Comparison between the present model and Young's model [18] for the Occiput and right parietal location of SMO-1 subject with similar cranial thickness but obviously different curvature.(a) 80 (b) 120 100 & 60 aD 5 80 3 ° s 40 = 60 o 8 40 < 20 20 0 0 Ver. Occ. Rig. Lef. For. Ver. Occ. Rig. Lef. For. 1 (c)1°° Timo = 11MO-1 m11MO-2 m11MO-2 =. = 11MO-3 m= 11MO-3 5 60 © & 40 g ft 20 0 Ver. Occ. Rig. Lef. For. Ver. Occ. Rig. Lef. For. Fig. 9. Dynamic responses at 5 impact locations of child head at drop height of 15 cm predicted by analytical model (a) 5MO - acceleration (b) SMO - HIC (c) 11MO - acceleration (d) 11MO ~ HIC.(a) 100 80 60 40 Acceleration (g) 20 250 ™ Model_5MO-1 (b) = Model_5MO-2 200 = Model_5MO-3 m Experiment(Loyd 2071 1450 Q a 100 50 0 Drop height 15cm Drop height 30cm ™ Model_11MO-1 (d) 98° m= Model_11MO-2 300 @ Model_11MO-3 m Experiment(Loyd 2041 250 200 2 150 100 50 0 Drop height 15cm Drop height 30cm ™@ Model_5MO-1 = Model_5MO-2 = Model_5MO-3 mw Experiment(Loyd 2011) Drop height 15cm Drop height 30cm im Model_11MO-1 -|@ Model_11MO-2 m= Model_11MO-3 im Experiment(Loyd 2011) Drop height 15cm Drop height 30cm Fig. 7. Comparison results of head acceleration and HIC between analytical model and cadaver test at dropping height of 15 cm and 30 cm (a) 5MO - acceleration (b) 5MO. - HIC (c) 11MO - acceleration (d) 11MO - HIC.2000 1800 — Drop Height=30em 1600 800 600 400 200 300 400 500 600 700 800 900 1000 Elastic Modulus(MPa) (a) Elastic modulus 1400 Drop Height=15cm — Drop Height=30em 1200 1 15 2 25 3 35 4 45 5 5.5 6 Thickness(mm) (b) Cranial thickness 1000 — Drop Height=15¢m 900 — Drop Height=30em 800 700 600 Fmax(N) 500 400 300 200 100 30 40 50 60 70 80 90 100 110 120 Radius of curvature(mm) (c) Radius of curvature Fig. 8. Effect of elastic modulus, thickness and radius of curvature on maximum impact force.The discrete points around the impact point Fig. 6. Diagram of calculating the average curvature at impact point.Space: Imm 0.5mm 0.1mm 0.05mm 0.01mm Thickness: 2.2691mm 2.2500mm 2.2419mm 2.2417mm 2.2416mm Fig. 5. Different point densities and the corresponding calculation cranial thickness.moo Cranium reconstrction Posture adjusting Suture patching and smoothing Original CT scans Gantry tile correction Discrete points of Contour curve of inner and outer cranium cranial surfaces Fig. 3. Procedure for generating discrete feature points from child head CT scans.| “Calculation of cranial thickness and curvature section 2.2 for this item Geometry | a Child head parameters CT data definition Skull geometry quantification section 2.2.1 for this item 4 | Cranial thickness section 2.2.2 for this item Radius of cuRvature section 2.2.3 for this item design assumption modeling Impact Child head Mechanic scenario geometry }—» analysis and +» Formula derivation Analytical] | theory Modeling and derivation of the analytical theory model a a ale a See Sy i eet eee ee rma Reel section 2.1 for this item Derivation of dynamic impact responses —— a ‘section 2.3 for this item Fig. 1. Procedure of developing child head analytical model considering cranial thickness and curvature.Fixed solid infinite plane <7 \ Ven Msgi: Mass of solid object Msp: Head mass . Ew: Elastic modulus of cranial shell |g) ns —e Lsn: Poisson ratio of cranial shell OL SHCIOnISS! Ew: Elastic modulus of solid object Fig. 2. Scenario of child head impacting with a fixed rigid plane at speed of vs).The impact point ‘on cranial outer surface (A) i Enlarged figure of T + fitted cranial T+ . inner surface re- ie a ey { discretizing into T lt more points (B)) Fig. 4. Schematic of calculating cranial thickness at the impact location of child head.",Medical Data Analysis,"This study evaluates the performance of newly constructed descriptive statistical features in retinal vessel segmentation for the early detection of diabetic retinopathy, glaucoma, and age-related macular degeneration. The features are formed by statistical moments, mean and median measurements of image pixels’ intensity values in four directions. A fuzzy logic classifier, an artificial neural network classifier (ANN), a support vector machine (SVM), and classifier fusion are designed and evaluated on publicly available databases. The results show that all classifiers achieved compatible classification accuracies with sensitivity and specificity values, and the classifier fusion achieved the best performance among all four classifiers. The proposed statistical features are straightforward to calculate and hold valuable information to segment pixels that belong to retinal blood vessels.",Medical Data Analysis,"120 120 Sample:11MO-2 Sample:11MO-2 100 oe 100 aie 3 _ Position:Forehead a Position:Right parietal S S 2 80 ~ 80 g - © 60 ® 60 3 - 8 40 8 40 2 2 20 20 0 0 Experiment [4] This model Young's model [18] Experiment [4] This model Young's model [18] Fig. 10. Comparison between the present model and Young’s model [18] for the forehead and right parietal location of 11MO-2 subject with similar curvature but obviously different cranial thickness. 2 3 Py 3 Sample:5MO-1 Position:Right Parietal ‘Sample:5MO-1 Position: Occiput a 3 a 3 s Ss 2 40 : 40 3 30 30 2 3 8 20 8 20 10 10 ° 0 Experiment [4] This model Young's model [18] Experiment [4] This model Young's model [18] Fig. 11. Comparison between the present model and Young's model [18] for the Occiput and right parietal location of SMO-1 subject with similar cranial thickness but obviously different curvature.(a) 80 (b) 120 100 & 60 aD 5 80 3 ° s 40 = 60 o 8 40 < 20 20 0 0 Ver. Occ. Rig. Lef. For. Ver. Occ. Rig. Lef. For. 1 (c)1°° Timo = 11MO-1 m11MO-2 m11MO-2 =. = 11MO-3 m= 11MO-3 5 60 © & 40 g ft 20 0 Ver. Occ. Rig. Lef. For. Ver. Occ. Rig. Lef. For. Fig. 9. Dynamic responses at 5 impact locations of child head at drop height of 15 cm predicted by analytical model (a) 5MO - acceleration (b) SMO - HIC (c) 11MO - acceleration (d) 11MO ~ HIC.(a) 100 80 60 40 Acceleration (g) 20 250 ™ Model_5MO-1 (b) = Model_5MO-2 200 = Model_5MO-3 m Experiment(Loyd 2071 1450 Q a 100 50 0 Drop height 15cm Drop height 30cm ™ Model_11MO-1 (d) 98° m= Model_11MO-2 300 @ Model_11MO-3 m Experiment(Loyd 2041 250 200 2 150 100 50 0 Drop height 15cm Drop height 30cm ™@ Model_5MO-1 = Model_5MO-2 = Model_5MO-3 mw Experiment(Loyd 2011) Drop height 15cm Drop height 30cm im Model_11MO-1 -|@ Model_11MO-2 m= Model_11MO-3 im Experiment(Loyd 2011) Drop height 15cm Drop height 30cm Fig. 7. Comparison results of head acceleration and HIC between analytical model and cadaver test at dropping height of 15 cm and 30 cm (a) 5MO - acceleration (b) 5MO. - HIC (c) 11MO - acceleration (d) 11MO - HIC.2000 1800 — Drop Height=30em 1600 800 600 400 200 300 400 500 600 700 800 900 1000 Elastic Modulus(MPa) (a) Elastic modulus 1400 Drop Height=15cm — Drop Height=30em 1200 1 15 2 25 3 35 4 45 5 5.5 6 Thickness(mm) (b) Cranial thickness 1000 — Drop Height=15¢m 900 — Drop Height=30em 800 700 600 Fmax(N) 500 400 300 200 100 30 40 50 60 70 80 90 100 110 120 Radius of curvature(mm) (c) Radius of curvature Fig. 8. Effect of elastic modulus, thickness and radius of curvature on maximum impact force.The discrete points around the impact point Fig. 6. Diagram of calculating the average curvature at impact point.Space: Imm 0.5mm 0.1mm 0.05mm 0.01mm Thickness: 2.2691mm 2.2500mm 2.2419mm 2.2417mm 2.2416mm Fig. 5. Different point densities and the corresponding calculation cranial thickness.moo Cranium reconstrction Posture adjusting Suture patching and smoothing Original CT scans Gantry tile correction Discrete points of Contour curve of inner and outer cranium cranial surfaces Fig. 3. Procedure for generating discrete feature points from child head CT scans.| “Calculation of cranial thickness and curvature section 2.2 for this item Geometry | a Child head parameters CT data definition Skull geometry quantification section 2.2.1 for this item 4 | Cranial thickness section 2.2.2 for this item Radius of cuRvature section 2.2.3 for this item design assumption modeling Impact Child head Mechanic scenario geometry }—» analysis and +» Formula derivation Analytical] | theory Modeling and derivation of the analytical theory model a a ale a See Sy i eet eee ee rma Reel section 2.1 for this item Derivation of dynamic impact responses —— a ‘section 2.3 for this item Fig. 1. Procedure of developing child head analytical model considering cranial thickness and curvature.Fixed solid infinite plane <7 \ Ven Msgi: Mass of solid object Msp: Head mass . Ew: Elastic modulus of cranial shell |g) ns —e Lsn: Poisson ratio of cranial shell OL SHCIOnISS! Ew: Elastic modulus of solid object Fig. 2. Scenario of child head impacting with a fixed rigid plane at speed of vs).The impact point ‘on cranial outer surface (A) i Enlarged figure of T + fitted cranial T+ . inner surface re- ie a ey { discretizing into T lt more points (B)) Fig. 4. Schematic of calculating cranial thickness at the impact location of child head.",Medical Data Analysis
271,Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic review,"Sentiment analysis , COVID-19 , Opinion mining , Disease mitigation , Epidemic , Pandemic , Infectious disease.","The COVID-19 pandemic caused by the novel coronavirus SARS-CoV-2 occurred unexpectedly in China in December 2019. Tens of millions of confirmed cases and more than hundreds of thousands of confirmed deaths are reported worldwide according to the World Health Organization. News about the virus is spreading all over social media websites. Consequently, these social media outlets are experiencing and presenting different views, opinions and emotions during various outbreak-related incidents. For computer scientists and researchers, big data are valuable assets for understanding people’s sentiments regarding current events, especially those related to the pandemic. Therefore, analyzing these sentiments will yield remarkable findings. To the best of our knowledge, previous related studies have focused on one kind of infectious disease. No previous study has examined multiple diseases via sentiment analysis. Accordingly, this research aimed to review and analyze articles about the occurrence of different types of infectious diseases, such as epidemics, pandemics, viruses or outbreaks, during the last 10 years, understand the application of sentiment analysis and obtain the most important literature findings. Articles on related topics were systematically searched in five major databases, namely, ScienceDirect, PubMed, Web of Science, IEEE Xplore and Scopus, from 1 January 2010 to 30 June 2020. These indices were considered sufficiently extensive and reliable to cover our scope of the literature. Articles were selected based on our inclusion and exclusion criteria for the systematic review, with a total of n = 28 articles selected. All these articles were formed into a coherent taxonomy to describe the corresponding current standpoints in the literature in accordance with four main categories: lexicon-based models, machine learning based models, hybrid-based models and individuals. The obtained articles were categorized into motivations related to disease mitigation, data analysis and challenges faced by researchers with respect to data, social media platforms and community. Other aspects, such as the protocol being followed by the systematic review and demographic statistics of the literature distribution, were included in the review. Interesting patterns were observed in the literature, and the identified articles were grouped accordingly. This study emphasized the current standpoint and opportunities for research in this area and promoted additional efforts towards the understanding of this research field.","In this research, studies about sentiment analysis in the presence of infectious diseases, outbreaks, epidemics and pandemics over a 10-year period (1 January 2010 to 30 June 2020) were systematically reviewed. The research motivation of this work was the massive spread of COVID19. COVID-19 as an infectious disease remains vague as its literature and cases are proliferating massively; consequently, reporting updated information is nearly impossible. Furthermore, accurate information can only be obtained when the pandemic ends. Further studies should focus on the role of social media and sentiment analysis when a similar incident recurs. This systematic review addressed the main highlights: the protocol that explains how the last set of articles was chosen, a taxonomy analysis of current papers in the field and previous research efforts in the form of challenges and motivations. Despite the relatively low number of studies in this field, current data are essential for fighting similar outbreaks in the future and to stand in the face of these crises, not only as medical doctors and researchers but also as scientists from all domains, communities and decision-making bodies. In addition, studies should consider how our respected area can be an asset in the near future. From the perspectives of computer science, integrating other technologies, such as AI, ML and different analysis procedures, can contribute to making a difference. We all stand as one entity in the hope that technologies and science will help prevent a re-emergence of diseases such as COVID-19.","Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic reviewSentiment analysis , COVID-19 , Opinion mining , Disease mitigation , Epidemic , Pandemic , Infectious disease.The COVID-19 pandemic caused by the novel coronavirus SARS-CoV-2 occurred unexpectedly in China in December 2019. Tens of millions of confirmed cases and more than hundreds of thousands of confirmed deaths are reported worldwide according to the World Health Organization. News about the virus is spreading all over social media websites. Consequently, these social media outlets are experiencing and presenting different views, opinions and emotions during various outbreak-related incidents. For computer scientists and researchers, big data are valuable assets for understanding people’s sentiments regarding current events, especially those related to the pandemic. Therefore, analyzing these sentiments will yield remarkable findings. To the best of our knowledge, previous related studies have focused on one kind of infectious disease. No previous study has examined multiple diseases via sentiment analysis. Accordingly, this research aimed to review and analyze articles about the occurrence of different types of infectious diseases, such as epidemics, pandemics, viruses or outbreaks, during the last 10 years, understand the application of sentiment analysis and obtain the most important literature findings. Articles on related topics were systematically searched in five major databases, namely, ScienceDirect, PubMed, Web of Science, IEEE Xplore and Scopus, from 1 January 2010 to 30 June 2020. These indices were considered sufficiently extensive and reliable to cover our scope of the literature. Articles were selected based on our inclusion and exclusion criteria for the systematic review, with a total of n = 28 articles selected. All these articles were formed into a coherent taxonomy to describe the corresponding current standpoints in the literature in accordance with four main categories: lexicon-based models, machine learning based models, hybrid-based models and individuals. The obtained articles were categorized into motivations related to disease mitigation, data analysis and challenges faced by researchers with respect to data, social media platforms and community. Other aspects, such as the protocol being followed by the systematic review and demographic statistics of the literature distribution, were included in the review. Interesting patterns were observed in the literature, and the identified articles were grouped accordingly. This study emphasized the current standpoint and opportunities for research in this area and promoted additional efforts towards the understanding of this research field.In this research, studies about sentiment analysis in the presence of infectious diseases, outbreaks, epidemics and pandemics over a 10-year period (1 January 2010 to 30 June 2020) were systematically reviewed. The research motivation of this work was the massive spread of COVID19. COVID-19 as an infectious disease remains vague as its literature and cases are proliferating massively; consequently, reporting updated information is nearly impossible. Furthermore, accurate information can only be obtained when the pandemic ends. Further studies should focus on the role of social media and sentiment analysis when a similar incident recurs. This systematic review addressed the main highlights: the protocol that explains how the last set of articles was chosen, a taxonomy analysis of current papers in the field and previous research efforts in the form of challenges and motivations. Despite the relatively low number of studies in this field, current data are essential for fighting similar outbreaks in the future and to stand in the face of these crises, not only as medical doctors and researchers but also as scientists from all domains, communities and decision-making bodies. In addition, studies should consider how our respected area can be an asset in the near future. From the perspectives of computer science, integrating other technologies, such as AI, ML and different analysis procedures, can contribute to making a difference. We all stand as one entity in the hope that technologies and science will help prevent a re-emergence of diseases such as COVID-19.gradient field transformation ae Bone suppressed image Reconstructed Bone image Predicted delineation Fig. 11. Illustration of bone suppression by using the predicted delineation generated from our delineating system. Two patches in the red boxes from the CXR and the bone suppressed image at the same location are selected and zoomed for better view. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)JSRT NIH Chest X-ray 14 \ Wel won) Wow! LE Oe | Fig. 10. Examples of delineation results on the JSRT dataset and NIH Chest X-ray14 dataset by the proposed method.Canny Ogul et al.’s method Ours Ground Truth Fig. 9. Examples of delineation results by the traditional methods without deep learning models and the proposed method.1.0 0.9 - 0.8 - ° ° a N T T Precision ° ua 0.1 + [F=.662]class-balanced CE [F=.686]FL [F=.682]CE [F=.813]pixel-weighted FL [F=.814]pixel-weighted CE [F=.797]pixel-weighted class-balanced CE — 4 1 1 L 1 02 #03 04 05 06 O07 O8 09 10 Recall Fig. 5. Precision and recall curves of the trained FC-DenseNet model with different loss functions.channel concatenation €B Conv layer EEE Dense block =x Transiton down laecr=a Transiton up layer HxW H/2xW/2 H/4xW/4 1 48 48 Fig. 4. Architecture of the FC-DenseNet for delineating ribs and clavicles. Each cuboid represents the current feature map layer, generated from the preceding layer. The spatial resolution of each layer is printed above and the channel number is printed underneath. The top right corner presents comments on each colored rectangle, the concrete structure of each dense block, and the operator. Each colored rectangle corresponds to the cuboid with the same color. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)R Ground truth FC-DenseNet (Ours) i ; i Fig. 8. Comparison of the experimental results among different FCN models. Shown from left to right are the input CXRs, the ground truth, and the corresponding predictions of FC-DenseNet, HED, and CAN. i j1.0 0.9 0.85 0.7 0.65 0.5 0.4 evaluation metrics 0.3 0.2 0.1 | — recall ~~] = precision 0.04 —# F-measure 0.1 0.2 0.3 0.4 0.5 0.6 0.7 08 0.9 threshold T Fig. 6. Evaluation metrics of the trained FC-DenseNet model using the pixel- weighted CE with different thresholds.Original CXR Enhanced CXR Fig. 2. Example of manual delineation of the ribs and clavicles. The color coding is red: clavicles, green: posterior ribs, and blue: anterior ribs. (For interpretation of the Fig. 3. Illustration of preprocessing CXR using a guided filter. The left is the original references to color in this figure legend, the reader is referred to the web version CXR and the right is the enhanced CXR after preprocessing. of this article.)Fig. 1. Illustration of delineation results of different anatomical regions by the proposed method. Detected borders are outlined in white. Red rectangles are zoomed areas of clavicle, posterior ribs and anterior ribs respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)Our result ie / et Sees SESS BEE ERE Gound truth A \\ \\ Vo\\ KS Wx EO. : Ses the middle is the ground truth and the right is the Fig. 7. Examples of rib and clavicle delineations predicted by the proposed method. The left column is the input CXR, delineations predicted by the proposed method.",sentiment analysis,"This systematic review analyzes sentiment analysis in relation to infectious diseases, outbreaks, epidemics, and pandemics over a 10-year period from 2010 to 2020. The motivation for this study was the massive spread of COVID-19, and the review categorized the articles into four main categories: lexicon-based models, machine learning-based models, hybrid-based models, and individuals. The study emphasized the importance of social media and sentiment analysis in understanding people's sentiments regarding current events and the need for further research in this area. The authors also highlighted the potential for integrating other technologies, such as AI and ML, to make a difference in future outbreaks.",Object and Sentiment Recognition,"gradient field transformation ae Bone suppressed image Reconstructed Bone image Predicted delineation Fig. 11. Illustration of bone suppression by using the predicted delineation generated from our delineating system. Two patches in the red boxes from the CXR and the bone suppressed image at the same location are selected and zoomed for better view. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)JSRT NIH Chest X-ray 14 \ Wel won) Wow! LE Oe | Fig. 10. Examples of delineation results on the JSRT dataset and NIH Chest X-ray14 dataset by the proposed method.Canny Ogul et al.’s method Ours Ground Truth Fig. 9. Examples of delineation results by the traditional methods without deep learning models and the proposed method.1.0 0.9 - 0.8 - ° ° a N T T Precision ° ua 0.1 + [F=.662]class-balanced CE [F=.686]FL [F=.682]CE [F=.813]pixel-weighted FL [F=.814]pixel-weighted CE [F=.797]pixel-weighted class-balanced CE — 4 1 1 L 1 02 #03 04 05 06 O07 O8 09 10 Recall Fig. 5. Precision and recall curves of the trained FC-DenseNet model with different loss functions.channel concatenation €B Conv layer EEE Dense block =x Transiton down laecr=a Transiton up layer HxW H/2xW/2 H/4xW/4 1 48 48 Fig. 4. Architecture of the FC-DenseNet for delineating ribs and clavicles. Each cuboid represents the current feature map layer, generated from the preceding layer. The spatial resolution of each layer is printed above and the channel number is printed underneath. The top right corner presents comments on each colored rectangle, the concrete structure of each dense block, and the operator. Each colored rectangle corresponds to the cuboid with the same color. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)R Ground truth FC-DenseNet (Ours) i ; i Fig. 8. Comparison of the experimental results among different FCN models. Shown from left to right are the input CXRs, the ground truth, and the corresponding predictions of FC-DenseNet, HED, and CAN. i j1.0 0.9 0.85 0.7 0.65 0.5 0.4 evaluation metrics 0.3 0.2 0.1 | — recall ~~] = precision 0.04 —# F-measure 0.1 0.2 0.3 0.4 0.5 0.6 0.7 08 0.9 threshold T Fig. 6. Evaluation metrics of the trained FC-DenseNet model using the pixel- weighted CE with different thresholds.Original CXR Enhanced CXR Fig. 2. Example of manual delineation of the ribs and clavicles. The color coding is red: clavicles, green: posterior ribs, and blue: anterior ribs. (For interpretation of the Fig. 3. Illustration of preprocessing CXR using a guided filter. The left is the original references to color in this figure legend, the reader is referred to the web version CXR and the right is the enhanced CXR after preprocessing. of this article.)Fig. 1. Illustration of delineation results of different anatomical regions by the proposed method. Detected borders are outlined in white. Red rectangles are zoomed areas of clavicle, posterior ribs and anterior ribs respectively. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)Our result ie / et Sees SESS BEE ERE Gound truth A \\ \\ Vo\\ KS Wx EO. : Ses the middle is the ground truth and the right is the Fig. 7. Examples of rib and clavicle delineations predicted by the proposed method. The left column is the input CXR, delineations predicted by the proposed method.",Medical Data Analysis
272,One case of coronavirus disease 2019 (COVID-19) in a patient co-infected by HIV with a low CD4+ T-cell count,"COVID-19 , SARS-Cov-2 , HIV , Antibody.","The ongoing outbreak of COVID-19 that began in Wuhan, China, become an emergency of international concern when thousands of people were infected around the world. This study reports a case simultaneously infected by SARS-Cov-2 and HIV, which showed a longer disease course and slower generation of specific antibodies. This case highlights that a co-infection of SARS-Cov-2 and HIV may severely impair the immune system.","In conclusion, this paper reports the clinical features of a patient infected by SARS-Cov-2 and HIV. The case appeared to have a long disease course of > 2 months. It was not until later that the IgM in serum could be detected, which may have been due to the immune response being destroyed by the two viruses together.","One case of coronavirus disease 2019 (COVID-19) in a patient co-infected by HIV with a low CD4+ T-cell countCOVID-19 , SARS-Cov-2 , HIV , Antibody.The ongoing outbreak of COVID-19 that began in Wuhan, China, become an emergency of international concern when thousands of people were infected around the world. This study reports a case simultaneously infected by SARS-Cov-2 and HIV, which showed a longer disease course and slower generation of specific antibodies. This case highlights that a co-infection of SARS-Cov-2 and HIV may severely impair the immune system.In conclusion, this paper reports the clinical features of a patient infected by SARS-Cov-2 and HIV. The case appeared to have a long disease course of > 2 months. It was not until later that the IgM in serum could be detected, which may have been due to the immune response being destroyed by the two viruses together.Nguyen Azzopardi 50 Ww 0.6964x48.03 45 40 35 = » . 30 25 20 . 15 10 20 30 40 50 60 10 20 30 40 50 60 FOV diameter/FOV angle FOV diameter/FOV angle 1200 Bankhead Soares 16 7 & © Aeoot 18,8912x456.97| 1000 14 © Nsooz . b Neos —— 0.074x+0.862 800 0.245x 0.416 0.261x40.827 uF 600 ° 400 200 a oe e oO 10 20 30 40 50 60 20 30 40 50 60 FOV diameter/FOV angle FOV diameter/FOV angle Fig. 3. Illustration of the linear models. Selected parameters are correlated with angular resolution of the databases.100 9 as 5 AIRADB CHASEDB1 1» a — ARZ0p AIT in Aas * tanta pes Nguyen sae i sotto 8 » s » & from second observer * so ee ss woes » a a 1-Sp [%] DRIVE o in i | Zhang ot al. Gin press) 8 | | rnd ot Bax a8 [jane 9 °° 4 whe igs oo é 4 fe ct a a ‘eet 3 a, ng a 9) Br © 4 eae ore 5 Yon at 2001) Esti overt a. 013) 7 Te haces whores cont) a) & lS seen 19 » Spe nn « — ha 09 ~Odstcite et wl. (2013) S Se a 9 co “ peered Coit (2015) % oC CSCS 1Sp[%] 1-Sp [%] STARE 10 wf eee %» ss 0 — Odateltk ot al, 2013) = Ses Zs psn ~ Argo e204) You eal 2011) Lie at Haj. 2015) 0 Menon ea: 018) Xiao a. 2913) \\ \\ ata a 2010 \ \ Pere and Cale 2015) 1SpP%) Fig. 2. ROC characteristics of the studied methods. Manual segmentation by second observer is marked with an asterisk. The ROC curves correspond to parameters optimized by Acc (solid line) and AUC (dotted line) measures. The Bankhead method is different due to its postprocessing: one is the ROC curve of the IUWT response (solid line) and the other is the convex hull of all possible performances from the parameter search (dotted line). Legend relevant for all sub-figures is placed in the ARIADB sub-figure.Influence of the preprocessing type. Soares ae asAee @a ay @ Sofka aes a e e : Azzopardi i eo “2 oN a ea e i - Bankhead i @sa a ea s Nguyen Sek : ene : L ok, i ae, 93 93.5 94 94.5 95 95.5 96 Segmentation accuracy [%] Fig. 1. Comparison of the segmentation performance: ‘pad only’ (circles) and ‘CLAHE’ (triangles). The databases are marked with different colors. Solid dots and triangles mark the algorithm accuracy. Vertical lines mark the performance of the second observer when available. 2 See DRIVE = eer CHASEDB1 Pte RE 2 cir STARE 2 tie ARIADB performance of the second manual segm.",Medical Data Analysis,"This study reports a case of a patient who was simultaneously infected with SARS-Cov-2 and HIV, which resulted in a longer disease course and slower generation of specific antibodies. The case highlights the severe impairment of the immune system in patients with a co-infection of SARS-Cov-2 and HIV. The paper concludes that the immune response may be destroyed by the two viruses together and emphasizes the importance of monitoring patients with HIV for COVID-19.",Medical Data Analysis,"Nguyen Azzopardi 50 Ww 0.6964x48.03 45 40 35 = » . 30 25 20 . 15 10 20 30 40 50 60 10 20 30 40 50 60 FOV diameter/FOV angle FOV diameter/FOV angle 1200 Bankhead Soares 16 7 & © Aeoot 18,8912x456.97| 1000 14 © Nsooz . b Neos —— 0.074x+0.862 800 0.245x 0.416 0.261x40.827 uF 600 ° 400 200 a oe e oO 10 20 30 40 50 60 20 30 40 50 60 FOV diameter/FOV angle FOV diameter/FOV angle Fig. 3. Illustration of the linear models. Selected parameters are correlated with angular resolution of the databases.100 9 as 5 AIRADB CHASEDB1 1» a — ARZ0p AIT in Aas * tanta pes Nguyen sae i sotto 8 » s » & from second observer * so ee ss woes » a a 1-Sp [%] DRIVE o in i | Zhang ot al. Gin press) 8 | | rnd ot Bax a8 [jane 9 °° 4 whe igs oo é 4 fe ct a a ‘eet 3 a, ng a 9) Br © 4 eae ore 5 Yon at 2001) Esti overt a. 013) 7 Te haces whores cont) a) & lS seen 19 » Spe nn « — ha 09 ~Odstcite et wl. (2013) S Se a 9 co “ peered Coit (2015) % oC CSCS 1Sp[%] 1-Sp [%] STARE 10 wf eee %» ss 0 — Odateltk ot al, 2013) = Ses Zs psn ~ Argo e204) You eal 2011) Lie at Haj. 2015) 0 Menon ea: 018) Xiao a. 2913) \\ \\ ata a 2010 \ \ Pere and Cale 2015) 1SpP%) Fig. 2. ROC characteristics of the studied methods. Manual segmentation by second observer is marked with an asterisk. The ROC curves correspond to parameters optimized by Acc (solid line) and AUC (dotted line) measures. The Bankhead method is different due to its postprocessing: one is the ROC curve of the IUWT response (solid line) and the other is the convex hull of all possible performances from the parameter search (dotted line). Legend relevant for all sub-figures is placed in the ARIADB sub-figure.Influence of the preprocessing type. Soares ae asAee @a ay @ Sofka aes a e e : Azzopardi i eo “2 oN a ea e i - Bankhead i @sa a ea s Nguyen Sek : ene : L ok, i ae, 93 93.5 94 94.5 95 95.5 96 Segmentation accuracy [%] Fig. 1. Comparison of the segmentation performance: ‘pad only’ (circles) and ‘CLAHE’ (triangles). The databases are marked with different colors. Solid dots and triangles mark the algorithm accuracy. Vertical lines mark the performance of the second observer when available. 2 See DRIVE = eer CHASEDB1 Pte RE 2 cir STARE 2 tie ARIADB performance of the second manual segm.",Medical Data Analysis
273,Lymphopenia is associated with severe coronavirus disease 2019 (COVID-19) infections: A systemic review and meta-analysis,"COVID-2019 , lymphocyte count , lymphopenia.","Objectives: Coronavirus Disease 2019 (COVID-19) is a new respiratory and systemic disease which needs quick identification of potential critical patients. This meta-analysis aimed to explore the relationship between lymphocyte count and the severity of COVID-19. Methods: A comprehensive systematic literature search was carried out to find studies published from December 2019 to 22 March 2020 from five databases. The language of literatures included English and Chinese. Mean difference (MD) of lymphocyte count in COVID-19 patients with or without severe disease and odds ratio (OR) of lymphopenia for severe form of COVID-19 was evaluated with this meta-analysis. Results: Overall 13 case-series with a total of 2282 cases were included in the study. The pooled analysis showed that lymphocyte count was significantly lower in severe COVID-19 patients (MD -0.31 x 10^9 /L; 95%CI: -0.42 to -0.19 x 10^9 /L). The presence of lymphopenia was associated with nearly threefold increased risk of severe COVID-19 (Random effects model, OR = 2.99, 95% CI: 1.31-6.82). Conclusions: Lymphopenia is a prominent part of severe COVID-19 and a lymphocyte count of less than 1.5 x 10^9 /L may be useful in predicting the severity clinical outcomes.",Lymphopenia is a prominent part of severe COVID-19 and a lymphocyte count of less than 1.5 x 10^9 /L may be useful in predicting the severity of clinical outcomes. Further studies are needed to focus on lymphocyte changes in COVID-19 to confirm the predictive ability of lymphopenia in COVID-19.,"Lymphopenia is associated with severe coronavirus disease 2019 (COVID-19) infections: A systemic review and meta-analysisCOVID-2019 , lymphocyte count , lymphopenia.Objectives: Coronavirus Disease 2019 (COVID-19) is a new respiratory and systemic disease which needs quick identification of potential critical patients. This meta-analysis aimed to explore the relationship between lymphocyte count and the severity of COVID-19. Methods: A comprehensive systematic literature search was carried out to find studies published from December 2019 to 22 March 2020 from five databases. The language of literatures included English and Chinese. Mean difference (MD) of lymphocyte count in COVID-19 patients with or without severe disease and odds ratio (OR) of lymphopenia for severe form of COVID-19 was evaluated with this meta-analysis. Results: Overall 13 case-series with a total of 2282 cases were included in the study. The pooled analysis showed that lymphocyte count was significantly lower in severe COVID-19 patients (MD -0.31 x 10^9 /L; 95%CI: -0.42 to -0.19 x 10^9 /L). The presence of lymphopenia was associated with nearly threefold increased risk of severe COVID-19 (Random effects model, OR = 2.99, 95% CI: 1.31-6.82). Conclusions: Lymphopenia is a prominent part of severe COVID-19 and a lymphocyte count of less than 1.5 x 10^9 /L may be useful in predicting the severity clinical outcomes.Lymphopenia is a prominent part of severe COVID-19 and a lymphocyte count of less than 1.5 x 10^9 /L may be useful in predicting the severity of clinical outcomes. Further studies are needed to focus on lymphocyte changes in COVID-19 to confirm the predictive ability of lymphopenia in COVID-19.Fig. 3. (a) Adaptive histogram equalization (AHE), (b) Background image, (c) Normalized and masked image.Ee Fig. 2. (a) Original retinal fundus image, (b) Green channel image, (c) Complement of the green channel image. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)Retinal Fundus Image Green Channel Extraction Adaptive Histogram Equalization Pre-processing Background Extraction Creating Normalized Image and Masking Feature Extraction Fuzzy Logic (Descriptive Statistical Features) Algorithm feural Network SVM Algorithm Creating Fuzzy Sets Algorithm Setting Rules up C Evaluation ») C Evaluation ») Thresholding | — Classifier fusion |_____________p| >| Post-processing vy Vv vy (Segmented Fundus image _) (Segmented Fundus image ) + (_ Segmented Fundus image) (__ Segmented Fundus image _) Fig. 1. Block diagram of the proposed model.Original image Ground truth Segmented blood vessels. Fig. 11. Example results: segmented retinal blood vessels by the fuzzy classifier. (a-b) Images #3 and #19 of the DRIVE database (c-d) Images #0003 (with pathology) and #0319 of the STARE database.2 B 3 e g S Zz ° 1 Vessel Ce Ay NSS WW y A ge ij FI F2 F3 F4 F5 F6 F7 F8 Output Layer Hidden Layer Fig. 9. Architecture of the ANN classifier. Input LayerFig. 8. (a) After applying the fuzzy logic algorithm, (b) After thresholding, (c) Ground truth image.1 = os = 09 7 / oshre 08 5 3°77 S07 3 g Bos Eos gos Zos & 04 Boa : : Fos E03 02 02 o4 =~ DRIVE 04 + _2nd Observer —— STARE ° 0 01 02 03 04 05 06 OF 08 09 1 False Positive Fraction 01 02 03 04 05 06 O07 O08 09 1 False Positive Fraction Fig. 10. Receiver operating characteristic (ROC) curves for DRIVE and STARE database images by the fuzzy classifier. Manual segmentation by second observer is marked with an asterisk.horizontal (5) vertical (5) right (5) left (5) (mamdani) horizontal? (5) 10 rules output (5) vertical2 (5) right2 (5) left2 (5) Fig. 7. Fuzzy inference system (FIS).Crisp inputs Fl F2 F3 F4 FS F6 F7 F8 Wid Fuzzy Rule Base Fuzzifier Fig. 6. Basic configuration of fuzzy systems. Fuzzy Inference Engine Defuzzifier -—> Crisp outputMean Values of the Vectors Median Values of the Vectors Feature 1 Feature 2 Feature 3 Feature 4 Feature 5 Feature 6 Feature 7 Feature 8 ana YY Yy Fig. 4. Features extraction for inputs of the fuzzy system and neural networks. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)The box plot for the vessel pixels ‘The box plot for the non-vessel pixels Features Features oo 9 9 2 SemSuaU! [8X%Id Fig. 5. Box plot for the vessel and non-vessel pixels.",Medical Data Analysis,"This meta-analysis aimed to investigate the relationship between lymphocyte count and the severity of COVID-19. Thirteen case-series with a total of 2282 cases were included in the study, and the results showed that lymphocyte count was significantly lower in severe COVID-19 patients, and the presence of lymphopenia was associated with a nearly threefold increased risk of severe COVID-19. The study concluded that lymphopenia is a prominent part of severe COVID-19, and a lymphocyte count of less than 1.5 x 10^9/L may be useful in predicting the severity of clinical outcomes. However, further studies are needed to confirm the predictive ability of lymphopenia in COVID-19.",Medical Data Analysis,"Fig. 3. (a) Adaptive histogram equalization (AHE), (b) Background image, (c) Normalized and masked image.Ee Fig. 2. (a) Original retinal fundus image, (b) Green channel image, (c) Complement of the green channel image. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)Retinal Fundus Image Green Channel Extraction Adaptive Histogram Equalization Pre-processing Background Extraction Creating Normalized Image and Masking Feature Extraction Fuzzy Logic (Descriptive Statistical Features) Algorithm feural Network SVM Algorithm Creating Fuzzy Sets Algorithm Setting Rules up C Evaluation ») C Evaluation ») Thresholding | — Classifier fusion |_____________p| >| Post-processing vy Vv vy (Segmented Fundus image _) (Segmented Fundus image ) + (_ Segmented Fundus image) (__ Segmented Fundus image _) Fig. 1. Block diagram of the proposed model.Original image Ground truth Segmented blood vessels. Fig. 11. Example results: segmented retinal blood vessels by the fuzzy classifier. (a-b) Images #3 and #19 of the DRIVE database (c-d) Images #0003 (with pathology) and #0319 of the STARE database.2 B 3 e g S Zz ° 1 Vessel Ce Ay NSS WW y A ge ij FI F2 F3 F4 F5 F6 F7 F8 Output Layer Hidden Layer Fig. 9. Architecture of the ANN classifier. Input LayerFig. 8. (a) After applying the fuzzy logic algorithm, (b) After thresholding, (c) Ground truth image.1 = os = 09 7 / oshre 08 5 3°77 S07 3 g Bos Eos gos Zos & 04 Boa : : Fos E03 02 02 o4 =~ DRIVE 04 + _2nd Observer —— STARE ° 0 01 02 03 04 05 06 OF 08 09 1 False Positive Fraction 01 02 03 04 05 06 O07 O08 09 1 False Positive Fraction Fig. 10. Receiver operating characteristic (ROC) curves for DRIVE and STARE database images by the fuzzy classifier. Manual segmentation by second observer is marked with an asterisk.horizontal (5) vertical (5) right (5) left (5) (mamdani) horizontal? (5) 10 rules output (5) vertical2 (5) right2 (5) left2 (5) Fig. 7. Fuzzy inference system (FIS).Crisp inputs Fl F2 F3 F4 FS F6 F7 F8 Wid Fuzzy Rule Base Fuzzifier Fig. 6. Basic configuration of fuzzy systems. Fuzzy Inference Engine Defuzzifier -—> Crisp outputMean Values of the Vectors Median Values of the Vectors Feature 1 Feature 2 Feature 3 Feature 4 Feature 5 Feature 6 Feature 7 Feature 8 ana YY Yy Fig. 4. Features extraction for inputs of the fuzzy system and neural networks. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)The box plot for the vessel pixels ‘The box plot for the non-vessel pixels Features Features oo 9 9 2 SemSuaU! [8X%Id Fig. 5. Box plot for the vessel and non-vessel pixels.",Medical Data Analysis
274,Coronavirus 2019 and health systems affected by protracted conflict: The case of Syria,"Syria , COVID-19 , Communicable diseases , Preparedness Conflict.","Introduction: Two thirds of countries globally are unprepared to respond to a health emergency as per the International Health Regulations (2005), with conflict-affected countries like Syria being particularly vulnerable. Political influences on outbreak preparedness, response and reporting may also adversely affect control of SARS-CoV-2 in Syria. Syria reported its first case on 22 March 2020; however, concerns were raised that this was delayed and that underreporting continues. Discussion: Syria’s conflict has displaced more than half of its pre-war population, leaving 6.7 million people internally displaced. The consequent overcrowding – with insufficient water, sanitation and healthcare (including laboratory capacity) – could lead to conditions that are ideal for spread of SARSCoV-2 in Syria. Political changes have led to the formation of at least three health systems within Syria’s borders, each with its own governance, capacity and planning. This fragmentation, with little interaction between them, could lead to poor resource allocation and adversely affect control. As such, COVID-19 could overwhelm the health systems (particularly intensive care capacity), leading to high deaths across the population, particularly for the most vulnerable such as detainees. Conclusions: Locally implementable interventions that rapidly build WASH and health system capacity are required across Syria to ensure early detection and management of COVID-19 cases.","Countries affected by protracted conflict face numerous challenges with health systems that have already been decimated; as such, SARS-CoV-2 could spread rapidly through affected populations, particularly among those in the most vulnerable groups. In Syria, the multiple fragmented and increasingly politicized health systems within its borders present further challenges and the response requires locally appropriate interventions. Internationally recommended measures are unlikely to be enforceable or effective in areas where a lack of sanitation and overcrowding are rife; as such, rapid expansion of WASH and addressing shelter, particularly for internally displaced persons is needed. Ceasefires (as have occurred in Yemen), protection of health workers and health facilities, the expansion of humanitarian access through the remaining border crossings, and evacuation of critical cases for life-saving treatment are practicable measures that can support the response to COVID-19 in Syria.","Coronavirus 2019 and health systems affected by protracted conflict: The case of SyriaSyria , COVID-19 , Communicable diseases , Preparedness Conflict.Introduction: Two thirds of countries globally are unprepared to respond to a health emergency as per the International Health Regulations (2005), with conflict-affected countries like Syria being particularly vulnerable. Political influences on outbreak preparedness, response and reporting may also adversely affect control of SARS-CoV-2 in Syria. Syria reported its first case on 22 March 2020; however, concerns were raised that this was delayed and that underreporting continues. Discussion: Syria’s conflict has displaced more than half of its pre-war population, leaving 6.7 million people internally displaced. The consequent overcrowding – with insufficient water, sanitation and healthcare (including laboratory capacity) – could lead to conditions that are ideal for spread of SARSCoV-2 in Syria. Political changes have led to the formation of at least three health systems within Syria’s borders, each with its own governance, capacity and planning. This fragmentation, with little interaction between them, could lead to poor resource allocation and adversely affect control. As such, COVID-19 could overwhelm the health systems (particularly intensive care capacity), leading to high deaths across the population, particularly for the most vulnerable such as detainees. Conclusions: Locally implementable interventions that rapidly build WASH and health system capacity are required across Syria to ensure early detection and management of COVID-19 cases.Countries affected by protracted conflict face numerous challenges with health systems that have already been decimated; as such, SARS-CoV-2 could spread rapidly through affected populations, particularly among those in the most vulnerable groups. In Syria, the multiple fragmented and increasingly politicized health systems within its borders present further challenges and the response requires locally appropriate interventions. Internationally recommended measures are unlikely to be enforceable or effective in areas where a lack of sanitation and overcrowding are rife; as such, rapid expansion of WASH and addressing shelter, particularly for internally displaced persons is needed. Ceasefires (as have occurred in Yemen), protection of health workers and health facilities, the expansion of humanitarian access through the remaining border crossings, and evacuation of critical cases for life-saving treatment are practicable measures that can support the response to COVID-19 in Syria.Selected Studies (n=28) awe? “SS= a 7 “ > -— 7 Se ~~~ —_— -_~_ Len a Se ~n an Xx ~ >> Lexicon (n=10) Machine Learning (n=5) Hybrid (n=9) Individuals (n=4) a ants AW IINN AN niwWess. TIAN ATALIWSS VN THAN Ss TEAS LTT ANN 7i\N /! \ VSS a, / 4 ‘ \ AF EWN * YY / 6 ig A ACNORSSASSES ti 47 7 1 \ \ r 1 \ \ X \ VON / { fo \ \ SASS aif | \ \ ow / ul 1o\UN \, Ny é ! \ 7 Ni 2) xX 4 6 vi ig iy U0, Ht U2 13 1415 U6 i JS EE 20 2! 22) 2 24 125 26 \27 25) | ae eo age \ SSS v7 Cy xy ~3< A NX Utes= Tene ae 7 NINN BG 4 Ys a No? DK. a"" No her -- 2 -t ¢ SIO EK“ 7 MM MOF See ee te \ ¢ N <«K Y Uy de oe - - 7 \ sew > oe” ne \ 7 VYXOXN ON 4 L-2 25 - TB - \ SPRL Ne 2 tie 22-7 se yo \ / 1 VILLI SS UO BROS EO EEN \ 7 WO Se OO Sef = VN 4, VA 25 = BF. a7 7 IANS ~~ x \ “yt Zz Ag nae -1 “~~ \ ~~ vn \ Miyy7 27 oo eat / N \ ~ vyoN My? a = aN +7 en sy \ aor” Zee 7 “7 SN 17 Se fos \ \ -“— +4 Veer sv St Z Sah \ Covid-19 | Epidemic Outbreak. Ebola | Mers-Cov Infectious Disease| Pandemic = NIHI | Zika | ! | ! ML=2 L=2 L=l - = H H=3 H=! L=3 | ML=I op I=1 a [=I a D =! H=1 | Fig. 4. Taxonomy for related literature.2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 Science Direct Additional Sources Web of Science DISTRIBUTION BY YEAR ARTICLES SOURCE Scopus PubMed IEEEXplore 5 ~ CT eed —————— nn CT a LT | a TOPICS RELATED Covid-19 ST Epidemic ——— ———————— Outbreak Co Ebola —_—_—_—_—- Mers-Cov — Infectious Disease ———————y NIH ————— Pandemic mia «|| PUBLICATION TYPE Journal Article ___ a 18 NE 9 Conference Proceedings Book Section ml 1 Fig. 3. Demographic statistics.Queries: (Sentiment Analysis’ OR ‘Sentiment Classification’ OR ‘Opinion Analysis’ OR “Opinion Mining’) AND (Disease OR Infection OR Virus OR Epidemic OR Outbreak OR Pandemic) Records identified by searching five databases WOS, IEEE, SD, PubMed, Scopus and additional sources (Total = 2,754) Duplicate records (n= 28) Records excluded after Records screened titles and abstracts were (n= 2,726) reviewed (n= 2,652) Full-text stud «os judeed ine assessed forcligibilty [______}_ Studies judged ineligible (1-7) (n= 46) Studies included in qualitative synthesis, (n= 28) Fig. 2. SLR protocol.Data Collection owe o oo y Social Media e / News Platforms + Facebook + Twitter + Instagram + Youtube + Reddit Information Types + Wikipedia + Tweets + Posts > News + Texts Pre-processing Pre-processing Types Feature Extraction Tokenisati Cleaning Normalisation Language Detection Noise Removal Fig. 1. Sentiment analysis flow. @ FINISH Data Analysis + Polarity + Sentiment Analysis + Frequency Analysis _------!",Medical Data Analysis,"The article discusses the challenges that Syria faces in responding to the COVID-19 pandemic due to the country's protracted conflict, which has led to the fragmentation of its health systems and inadequate resources for water, sanitation, and healthcare. The article highlights the need for locally implementable interventions to rapidly build WASH and health system capacity to ensure early detection and management of COVID-19 cases. The article also suggests practicable measures such as ceasefires, protection of health workers and facilities, and expansion of humanitarian access to support the response to COVID-19 in Syria.",Medical Data Analysis,"Selected Studies (n=28) awe? “SS= a 7 “ > -— 7 Se ~~~ —_— -_~_ Len a Se ~n an Xx ~ >> Lexicon (n=10) Machine Learning (n=5) Hybrid (n=9) Individuals (n=4) a ants AW IINN AN niwWess. TIAN ATALIWSS VN THAN Ss TEAS LTT ANN 7i\N /! \ VSS a, / 4 ‘ \ AF EWN * YY / 6 ig A ACNORSSASSES ti 47 7 1 \ \ r 1 \ \ X \ VON / { fo \ \ SASS aif | \ \ ow / ul 1o\UN \, Ny é ! \ 7 Ni 2) xX 4 6 vi ig iy U0, Ht U2 13 1415 U6 i JS EE 20 2! 22) 2 24 125 26 \27 25) | ae eo age \ SSS v7 Cy xy ~3< A NX Utes= Tene ae 7 NINN BG 4 Ys a No? DK. a"" No her -- 2 -t ¢ SIO EK“ 7 MM MOF See ee te \ ¢ N <«K Y Uy de oe - - 7 \ sew > oe” ne \ 7 VYXOXN ON 4 L-2 25 - TB - \ SPRL Ne 2 tie 22-7 se yo \ / 1 VILLI SS UO BROS EO EEN \ 7 WO Se OO Sef = VN 4, VA 25 = BF. a7 7 IANS ~~ x \ “yt Zz Ag nae -1 “~~ \ ~~ vn \ Miyy7 27 oo eat / N \ ~ vyoN My? a = aN +7 en sy \ aor” Zee 7 “7 SN 17 Se fos \ \ -“— +4 Veer sv St Z Sah \ Covid-19 | Epidemic Outbreak. Ebola | Mers-Cov Infectious Disease| Pandemic = NIHI | Zika | ! | ! ML=2 L=2 L=l - = H H=3 H=! L=3 | ML=I op I=1 a [=I a D =! H=1 | Fig. 4. Taxonomy for related literature.2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 Science Direct Additional Sources Web of Science DISTRIBUTION BY YEAR ARTICLES SOURCE Scopus PubMed IEEEXplore 5 ~ CT eed —————— nn CT a LT | a TOPICS RELATED Covid-19 ST Epidemic ——— ———————— Outbreak Co Ebola —_—_—_—_—- Mers-Cov — Infectious Disease ———————y NIH ————— Pandemic mia «|| PUBLICATION TYPE Journal Article ___ a 18 NE 9 Conference Proceedings Book Section ml 1 Fig. 3. Demographic statistics.Queries: (Sentiment Analysis’ OR ‘Sentiment Classification’ OR ‘Opinion Analysis’ OR “Opinion Mining’) AND (Disease OR Infection OR Virus OR Epidemic OR Outbreak OR Pandemic) Records identified by searching five databases WOS, IEEE, SD, PubMed, Scopus and additional sources (Total = 2,754) Duplicate records (n= 28) Records excluded after Records screened titles and abstracts were (n= 2,726) reviewed (n= 2,652) Full-text stud «os judeed ine assessed forcligibilty [______}_ Studies judged ineligible (1-7) (n= 46) Studies included in qualitative synthesis, (n= 28) Fig. 2. SLR protocol.Data Collection owe o oo y Social Media e / News Platforms + Facebook + Twitter + Instagram + Youtube + Reddit Information Types + Wikipedia + Tweets + Posts > News + Texts Pre-processing Pre-processing Types Feature Extraction Tokenisati Cleaning Normalisation Language Detection Noise Removal Fig. 1. Sentiment analysis flow. @ FINISH Data Analysis + Polarity + Sentiment Analysis + Frequency Analysis _------!",Sentiment Analysis
275,Measuring rib cortical bone thickness and cross section from CT,"Cortical bone , Rib , Computed tomography , Osteoporosis.","This study assesses the ability to measure local cortical bone thickness, and to obtain mechanically relevant properties of rib cross-sections from clinical-resolution computed tomography (CT) scans of human ribs. The study utilized thirty-four sections of ribs published by Perz et al. (2015) in three modalities: standard clinical CT (clinCT), high-resolution clinical CT (HRclinCT), and microCT (μCT). Clinical-resolution images were processed using a Cortical Bone Mapping (CBM) algorithm applied to cross-cortex signals resampled perpendicularly to an initial smooth periosteal border. Geometric constraints were applied to remove outlier signals from consideration, and final predicted periosteal and endosteal borders from HRclinCT and clinCT were developed. Target values for local cortical thickness and for overall cross-sectional area and inertial properties were obtained from segmentation of the periosteal and endosteal borders on each corresponding μCT image. Errors in prediction (mean ± SD) of local cortical bone thickness for HRclinCT and clinCT resolutions were −0.03 ± 0.17 mm and −0.05 ± 0.22 mm, respectively, with R2 coefficients of determination from linear regression of 0.82 and 0.71 (p < 0.0001 for both). Predicted cortical shell measures derived from the periosteal and endosteal borders included total cross-sectional area (prediction errors of 6 ± 3% and −1 ± 5% respectively for HRclinCT and clinCT with R2 correlations of 0.99 and 0.96), cortical shell area (errors of −3 ± 8% and −8 ± 11% with R2 correlations of 0.91 and 0.87), and principal area moment of inertia (errors of 2 ± 8% and −3 ± 11% with R2 correlations of 0.98 and 0.95). Results here show substantial reductions in rib cross-sectional measurement error compared to past histogram-based thresholding methods and provide first validation of the CBM method when applied to rib bones. With the ubiquity of clinical CT scans covering the thorax and ribs, this study opens the door for individualized and population-wide quantification of rib structural properties and their corresponding effects on rib injury.","Numerical models of the thorax are increasingly being used to help understand and mitigate traumatic injuries. Advancement of these models depends on detailed and accurate anatomic information, particularly from key structural components such as ribs. Models are routinely generated from clinical CT imaging, yet the limited resolution of these images has restricted the geometric detail that can be directly obtained. This study has applied current Cortical Bone Mapping (CBM) techniques to rib sections to directly assess the accuracy of local cortex thickness measurement and detailed cross-sectional property measurement when taken using clinical-resolution CT scans. The accuracy of these techniques has been tested against target values (obtained from μCT) for rib overall cross-sectional area, rib cortical cross-sectional area, and rib local cortical bone thickness. Results show substantial reductions in measurement error of cross-sectional rib properties when using CBM compared to previous work. The accuracy and precision of rib cortical bone thickness measurements using the current methodology was also marginally higher when compared to bone thickness measures taken from other regions of the body. This accuracy allows researchers to obtain multiple geometric properties from clinical CT images without the need for more restrictive sources such as microCT.","Measuring rib cortical bone thickness and cross section from CTCortical bone , Rib , Computed tomography , Osteoporosis.This study assesses the ability to measure local cortical bone thickness, and to obtain mechanically relevant properties of rib cross-sections from clinical-resolution computed tomography (CT) scans of human ribs. The study utilized thirty-four sections of ribs published by Perz et al. (2015) in three modalities: standard clinical CT (clinCT), high-resolution clinical CT (HRclinCT), and microCT (μCT). Clinical-resolution images were processed using a Cortical Bone Mapping (CBM) algorithm applied to cross-cortex signals resampled perpendicularly to an initial smooth periosteal border. Geometric constraints were applied to remove outlier signals from consideration, and final predicted periosteal and endosteal borders from HRclinCT and clinCT were developed. Target values for local cortical thickness and for overall cross-sectional area and inertial properties were obtained from segmentation of the periosteal and endosteal borders on each corresponding μCT image. Errors in prediction (mean ± SD) of local cortical bone thickness for HRclinCT and clinCT resolutions were −0.03 ± 0.17 mm and −0.05 ± 0.22 mm, respectively, with R2 coefficients of determination from linear regression of 0.82 and 0.71 (p < 0.0001 for both). Predicted cortical shell measures derived from the periosteal and endosteal borders included total cross-sectional area (prediction errors of 6 ± 3% and −1 ± 5% respectively for HRclinCT and clinCT with R2 correlations of 0.99 and 0.96), cortical shell area (errors of −3 ± 8% and −8 ± 11% with R2 correlations of 0.91 and 0.87), and principal area moment of inertia (errors of 2 ± 8% and −3 ± 11% with R2 correlations of 0.98 and 0.95). Results here show substantial reductions in rib cross-sectional measurement error compared to past histogram-based thresholding methods and provide first validation of the CBM method when applied to rib bones. With the ubiquity of clinical CT scans covering the thorax and ribs, this study opens the door for individualized and population-wide quantification of rib structural properties and their corresponding effects on rib injury.Numerical models of the thorax are increasingly being used to help understand and mitigate traumatic injuries. Advancement of these models depends on detailed and accurate anatomic information, particularly from key structural components such as ribs. Models are routinely generated from clinical CT imaging, yet the limited resolution of these images has restricted the geometric detail that can be directly obtained. This study has applied current Cortical Bone Mapping (CBM) techniques to rib sections to directly assess the accuracy of local cortex thickness measurement and detailed cross-sectional property measurement when taken using clinical-resolution CT scans. The accuracy of these techniques has been tested against target values (obtained from μCT) for rib overall cross-sectional area, rib cortical cross-sectional area, and rib local cortical bone thickness. Results show substantial reductions in measurement error of cross-sectional rib properties when using CBM compared to previous work. The accuracy and precision of rib cortical bone thickness measurements using the current methodology was also marginally higher when compared to bone thickness measures taken from other regions of the body. This accuracy allows researchers to obtain multiple geometric properties from clinical CT images without the need for more restrictive sources such as microCT.Date 02.12 | 02.17 | 02.19 | 02.21 | 02.25 | 02.26 | 02.29 | 03.02 Day of admission 2 7 9 11 15 16 19 21 Fever(°C) 38.7 | 36.6 | 36.8 | 36.9 | 36.6 | 36.7 | 36.7 | 37.1 SPO2(%) 96 98 97 95 96 97 98 96 O2 support(L/min) 15 15 15 10 8 8 8 8 Mask - - - + + + + + White cell(109/L) 4.2 3.2 4.6 3.8 4.5 3.3 neutrophils(10°/L) 1.97 2.18 3.16 2.37 2.7 2.23 lymphocyte(10°/L) 1.55 0.6 0.91 0.84 1.3 0.56 CRP 96.51 | 42.7 26.1 | 11.14 | 8.89 11.65 ALB 33.2 28.7 28.8 26 AST 359 33.7 53.6 52.6 ALT 39 91 128 132 GGT 136 134 111 113 LDH 423 367 343 358 357 LDBH 318 316 277 286 272 RT-PCR + - - 7 Figure 1. Chest CT imaging changes. The first CT scan (A, C) showed bilateral diffuse ground glass appearance with some patchy consolidations. Another CT scan (B, D) performed after 9 days of treatment showed some improvement.",Medical Data Analysis,"This study evaluates the ability to measure local cortical bone thickness and obtain mechanically relevant properties of rib cross-sections from clinical-resolution CT scans of human ribs using a Cortical Bone Mapping (CBM) algorithm. The study compares three modalities: standard clinical CT (clinCT), high-resolution clinical CT (HRclinCT), and microCT (μCT). Results show that CBM substantially reduces measurement errors in cross-sectional rib properties compared to previous work, with accuracy and precision of rib cortical bone thickness measurements marginally higher than other regions of the body. This methodology enables researchers to obtain multiple geometric properties from clinical CT images without the need for more restrictive sources such as microCT, which can aid in understanding and mitigating traumatic injuries.",Medical Data Analysis,"Date 02.12 | 02.17 | 02.19 | 02.21 | 02.25 | 02.26 | 02.29 | 03.02 Day of admission 2 7 9 11 15 16 19 21 Fever(°C) 38.7 | 36.6 | 36.8 | 36.9 | 36.6 | 36.7 | 36.7 | 37.1 SPO2(%) 96 98 97 95 96 97 98 96 O2 support(L/min) 15 15 15 10 8 8 8 8 Mask - - - + + + + + White cell(109/L) 4.2 3.2 4.6 3.8 4.5 3.3 neutrophils(10°/L) 1.97 2.18 3.16 2.37 2.7 2.23 lymphocyte(10°/L) 1.55 0.6 0.91 0.84 1.3 0.56 CRP 96.51 | 42.7 26.1 | 11.14 | 8.89 11.65 ALB 33.2 28.7 28.8 26 AST 359 33.7 53.6 52.6 ALT 39 91 128 132 GGT 136 134 111 113 LDH 423 367 343 358 357 LDBH 318 316 277 286 272 RT-PCR + - - 7 Figure 1. Chest CT imaging changes. The first CT scan (A, C) showed bilateral diffuse ground glass appearance with some patchy consolidations. Another CT scan (B, D) performed after 9 days of treatment showed some improvement.",Medical Data Analysis
276,"Smartphone as a monitoring tool for bipolar disorder: a systematic review including data analysis, machine learning algorithms and predictive modelling","bipolar disorder , manic and depressive episode , smartphone-based monitoring , voice analysis , objective data collected via smartphone , systematic review.","Background: Bipolar disorder (BD) is a chronic illness with a high recurrence rate. Smartphones can be a useful tool for detecting prodromal symptoms of episode recurrence (through real-time monitoring) and providing options for early intervention between outpatient visits. Aims: The aim of this systematic review is to overview and discuss the studies on the smartphone-based systems that monitor or detect the phase change in BD. We also discuss the challenges concerning predictive modelling. Methods: Published studies were identified through searching the electronic databases. Predictive attributes reflecting illness activity were evaluated including data from patients' self-assessment ratings and objectively measured data collected via smartphone. Articles were reviewed according to PRISMA guidelines. Results: Objective data automatically collected using smartphones (voice data from phone calls and smartphone usage data reflecting social and physical activities) are valid markers of a mood state. The articles surveyed reported accuracies in the range of 67% to 97% in predicting mood status. Various machine learning approaches have been analyzed, however, there is no clear evidence about the superiority of any of the approach. Conclusions: The management of BD could be significantly improved by monitoring of illness activity via smartphone.","Smartphone-based monitoring has clear potential for facilitating timely and contextual care delivery for patients with bipolar disorder. Mobile apps using permanent, real-time monitoring of illness activity that could detect the phase change, would be very helpful to patients and clinicians. This could improve the diagnosis and provide options for early intervention on prodromal symptoms between outpatient visits in BD patients.","Smartphone as a monitoring tool for bipolar disorder: a systematic review including data analysis, machine learning algorithms and predictive modellingbipolar disorder , manic and depressive episode , smartphone-based monitoring , voice analysis , objective data collected via smartphone , systematic review.Background: Bipolar disorder (BD) is a chronic illness with a high recurrence rate. Smartphones can be a useful tool for detecting prodromal symptoms of episode recurrence (through real-time monitoring) and providing options for early intervention between outpatient visits. Aims: The aim of this systematic review is to overview and discuss the studies on the smartphone-based systems that monitor or detect the phase change in BD. We also discuss the challenges concerning predictive modelling. Methods: Published studies were identified through searching the electronic databases. Predictive attributes reflecting illness activity were evaluated including data from patients' self-assessment ratings and objectively measured data collected via smartphone. Articles were reviewed according to PRISMA guidelines. Results: Objective data automatically collected using smartphones (voice data from phone calls and smartphone usage data reflecting social and physical activities) are valid markers of a mood state. The articles surveyed reported accuracies in the range of 67% to 97% in predicting mood status. Various machine learning approaches have been analyzed, however, there is no clear evidence about the superiority of any of the approach. Conclusions: The management of BD could be significantly improved by monitoring of illness activity via smartphone.Smartphone-based monitoring has clear potential for facilitating timely and contextual care delivery for patients with bipolar disorder. Mobile apps using permanent, real-time monitoring of illness activity that could detect the phase change, would be very helpful to patients and clinicians. This could improve the diagnosis and provide options for early intervention on prodromal symptoms between outpatient visits in BD patients.2472 of records identified 1596 of records screened 48 of full-text articles assessed for eligibility 876 of records excluded for duplication 1548 of records excluded by reviewing title and abstract 35 articles excluded for no lymphocyte count or lymphopenia data; no comparison between severe and non-severe patients. 13 of studies included in qualitative synthesis Fig. 1. A flow diagram of the inclusion criteria of studies eligible for meta-analysis.",Medical Data Analysis,"This systematic review aimed to overview and discuss studies on smartphone-based systems for detecting phase changes in bipolar disorder. The review found that objective data collected via smartphones, such as voice data and smartphone usage data, are valid markers of mood state and can predict mood status with accuracies ranging from 67% to 97%. The use of mobile apps for real-time monitoring of illness activity has the potential to improve the management of bipolar disorder and provide options for early intervention between outpatient visits. The authors suggest that such monitoring could improve diagnosis and provide timely and contextual care delivery for patients with bipolar disorder.",Medical Data Analysis,2472 of records identified 1596 of records screened 48 of full-text articles assessed for eligibility 876 of records excluded for duplication 1548 of records excluded by reviewing title and abstract 35 articles excluded for no lymphocyte count or lymphopenia data; no comparison between severe and non-severe patients. 13 of studies included in qualitative synthesis Fig. 1. A flow diagram of the inclusion criteria of studies eligible for meta-analysis.,Medical Data Analysis
277,A new supervised retinal vessel segmentation method based on robust hybrid features,"Retinal blood vessels , Segmentation , Hybrid feature vector.","In this paper, we propose a new supervised retinal blood vessel segmentation method that combines a set of very robust features from different algorithms into a hybrid feature vector for pixel characterization. This 17-D feature vector consists of 13 Gabor filter responses computed at different configurations, contrast enhanced intensity, morphological top-hat transformed intensity, vessel-ness measure, and BCOSFIRE filter response. A random forest classifier, known for its speed, simplicity, and information fusion capability, is trained with the hybrid feature vector. The chosen combination of the different types of individually strong features results in increased local information with better discrimination for vessel and non-vessel pixels in both healthy and pathological retinal images. The proposed method is evaluated in detail on two publicly available databases DRIVE and STARE. Average classification accuracies of 0.9513 and 0.9605 on the DRIVE and STARE datasets, respectively, are achieved. When the majority of the common performance metrics are considered, our method is superior to the state-of-the-art methods. Performance results show that our method also outperforms the state-of-the-art methods in both cross training and pathological cases.","In this paper, we have proposed an effective method for the segmentation of the retinal blood vessels by pixel classification based on supervised learning. We have constructed a 17-D feature vector including contrast enhanced intensity, vessel-ness measure, intensity of morphologically transformed image, multi-scale response of Gabor wavelet and also B-COSFIRE filter response which has been used for the first time in a multi-feature supervised segmentation method. We have selected RF classifier which has shown ability for fusing the information from diverse features for labeling each pixel as a vessel or non-vessel pixel. The classifier has been trained by selecting 300,000 samples randomly from the DRIVE dataset and 60,000 samples from the STARE dataset. Despite that number of samples used by the proposed method for training on the STARE dataset is less than other multi-feature supervised methods, such as Soares et al. [24], Cheng et al. [4] and Fraz et al. [25] with 1 million, 200,000 and 75,000 samples, respectively, the average value of accuracy (0.9605) and area under ROC curve (0.9789) for our method are higher than all of the state-of-the-art methods. The performance evaluation results shows that the varied types of features selected for the method act complementary and the constructed hybrid feature vector boosts the accuracy and reliability of vessel segmentation. With regard to the cross training results, the proposed method achieves the highest value of average accuracy in both the DRIVE (0.9496) and STARE (0.9545) datasets compared to the other methods. These results shows that the proposed method is fairly robust to training set used, and still achieves the best accuracy while most of the other methods suffer from substantial accuracy decreases in cross training case. The property of training data independency of our method allows for its application on assorted datasets without the need for retraining, and this is definitely advantageous for wide screening programs. The performance of the proposed method on the pathological images is better than other methods. The average value of accuracy of the proposed method for 10 abnormal images from the STARE dataset is 0.9573 which is the highest value reported for images in this category. Thus,the robustness of our method against abnormal images is also verified.","A new supervised retinal vessel segmentation method based on robust hybrid featuresRetinal blood vessels , Segmentation , Hybrid feature vector.In this paper, we propose a new supervised retinal blood vessel segmentation method that combines a set of very robust features from different algorithms into a hybrid feature vector for pixel characterization. This 17-D feature vector consists of 13 Gabor filter responses computed at different configurations, contrast enhanced intensity, morphological top-hat transformed intensity, vessel-ness measure, and BCOSFIRE filter response. A random forest classifier, known for its speed, simplicity, and information fusion capability, is trained with the hybrid feature vector. The chosen combination of the different types of individually strong features results in increased local information with better discrimination for vessel and non-vessel pixels in both healthy and pathological retinal images. The proposed method is evaluated in detail on two publicly available databases DRIVE and STARE. Average classification accuracies of 0.9513 and 0.9605 on the DRIVE and STARE datasets, respectively, are achieved. When the majority of the common performance metrics are considered, our method is superior to the state-of-the-art methods. Performance results show that our method also outperforms the state-of-the-art methods in both cross training and pathological cases.In this paper, we have proposed an effective method for the segmentation of the retinal blood vessels by pixel classification based on supervised learning. We have constructed a 17-D feature vector including contrast enhanced intensity, vessel-ness measure, intensity of morphologically transformed image, multi-scale response of Gabor wavelet and also B-COSFIRE filter response which has been used for the first time in a multi-feature supervised segmentation method. We have selected RF classifier which has shown ability for fusing the information from diverse features for labeling each pixel as a vessel or non-vessel pixel. The classifier has been trained by selecting 300,000 samples randomly from the DRIVE dataset and 60,000 samples from the STARE dataset. Despite that number of samples used by the proposed method for training on the STARE dataset is less than other multi-feature supervised methods, such as Soares et al. [24], Cheng et al. [4] and Fraz et al. [25] with 1 million, 200,000 and 75,000 samples, respectively, the average value of accuracy (0.9605) and area under ROC curve (0.9789) for our method are higher than all of the state-of-the-art methods. The performance evaluation results shows that the varied types of features selected for the method act complementary and the constructed hybrid feature vector boosts the accuracy and reliability of vessel segmentation. With regard to the cross training results, the proposed method achieves the highest value of average accuracy in both the DRIVE (0.9496) and STARE (0.9545) datasets compared to the other methods. These results shows that the proposed method is fairly robust to training set used, and still achieves the best accuracy while most of the other methods suffer from substantial accuracy decreases in cross training case. The property of training data independency of our method allows for its application on assorted datasets without the need for retraining, and this is definitely advantageous for wide screening programs. The performance of the proposed method on the pathological images is better than other methods. The average value of accuracy of the proposed method for 10 abnormal images from the STARE dataset is 0.9573 which is the highest value reported for images in this category. Thus,the robustness of our method against abnormal images is also verified.",Medical Data Analysis,"The paper proposes a new method for retinal blood vessel segmentation based on supervised learning. The method combines a set of robust features into a 17-D hybrid feature vector for pixel characterization and uses a random forest classifier for information fusion. The proposed method achieves average classification accuracies of 0.9513 and 0.9605 on the DRIVE and STARE datasets, respectively, outperforming state-of-the-art methods in both cross-training and pathological cases. The varied types of features selected for the method act complementary, boosting the accuracy and reliability of vessel segmentation. The method's training data independency allows for its application on assorted datasets without retraining, making it advantageous for wide screening programs. The proposed method's performance on pathological images is better than other methods, verifying its robustness against abnormal images.",Medical Data Analysis,,Medical Data Analysis
278,Skull fracture prediction through subject-specific finite element modelling is highly sensitive to model parameters,"Finite element modelling, Head injury, Impact biomechanics, Skull fracture.","Reliable computer models are needed for a better understanding of the physical mechanisms of skull fracture in accidental hits, falls, bicycle - motor vehicle & car accidents and assaults. The performance and biofidelity of these models depend on the correct anatomical representation and material description of these structures. In literature, a strain energy criterion has been proposed to predict skull fractures. However, a broad range of values for this criterion has been reported. This study investigates if the impactor orientation, scalp thickness and material model of the skull could provide us with insight in the influencing factors of this criterion. 18 skull fracture experiments previously performed in our research group were reproduced in finite element simulations. Subject-specific skull geometries were derived from medical images and used to create high-quality finite element meshes. Based on local Hounsfield units, a subject-specific isotropic material model was assigned. The subject-specific models were able to predict fractures who matched visually with the corresponding experimental fracture patterns and provided detailed fracture patterns. The sensitivity study showed that small variations in impactor positioning as well as variations of the local geometry (frontal-temporal-occipital) strongly influenced the skull strain energy. Subject-specific modelling leads to a more accurate prediction of the force-displacement curve. The average error of the peak fracture force for all the 18 cases is 0.4190 for the subject-specific and 0.4538 for the homogeneous material model, for the displacement; 0.3368 versus 0.3844. But it should be carefully interpreted as small variations in the computational model significantly influence the outcome.","This study investigated the validity of an energy based skull fracture criterion with subject-specific FE head models. 18 different experimental impacts were reconstructed and each impact was simulated with a homogeneous material model as well as a subject-specific model based on local bone densities. The models using the subject-specific material model predicted the moment of fracture with a higher accuracy than the homogeneous material model. Furthermore, these models have the ability to predict the fracture lines with, to the authors knowledge, unprecedented precision. However, the modelling of the post-fracture behaviour is currently not satisfactory and should be improved upon. The findings in this study identified influencing factors on the skull strain energy such as contact area, scalp thickness and impactor positioning. The energy criterion is unable to take into account the effect of differences in geometry and bone properties. These results strongly invalidate the existence of an energy-based fracture criterion. We concluded that subject-specific modelling leads to a more accurate prediction of the force-displacement curve but it should be carefully interpreted as small variations in the computational model significantly influences the results.","Skull fracture prediction through subject-specific finite element modelling is highly sensitive to model parametersFinite element modelling, Head injury, Impact biomechanics, Skull fracture.Reliable computer models are needed for a better understanding of the physical mechanisms of skull fracture in accidental hits, falls, bicycle - motor vehicle & car accidents and assaults. The performance and biofidelity of these models depend on the correct anatomical representation and material description of these structures. In literature, a strain energy criterion has been proposed to predict skull fractures. However, a broad range of values for this criterion has been reported. This study investigates if the impactor orientation, scalp thickness and material model of the skull could provide us with insight in the influencing factors of this criterion. 18 skull fracture experiments previously performed in our research group were reproduced in finite element simulations. Subject-specific skull geometries were derived from medical images and used to create high-quality finite element meshes. Based on local Hounsfield units, a subject-specific isotropic material model was assigned. The subject-specific models were able to predict fractures who matched visually with the corresponding experimental fracture patterns and provided detailed fracture patterns. The sensitivity study showed that small variations in impactor positioning as well as variations of the local geometry (frontal-temporal-occipital) strongly influenced the skull strain energy. Subject-specific modelling leads to a more accurate prediction of the force-displacement curve. The average error of the peak fracture force for all the 18 cases is 0.4190 for the subject-specific and 0.4538 for the homogeneous material model, for the displacement; 0.3368 versus 0.3844. But it should be carefully interpreted as small variations in the computational model significantly influence the outcome.This study investigated the validity of an energy based skull fracture criterion with subject-specific FE head models. 18 different experimental impacts were reconstructed and each impact was simulated with a homogeneous material model as well as a subject-specific model based on local bone densities. The models using the subject-specific material model predicted the moment of fracture with a higher accuracy than the homogeneous material model. Furthermore, these models have the ability to predict the fracture lines with, to the authors knowledge, unprecedented precision. However, the modelling of the post-fracture behaviour is currently not satisfactory and should be improved upon. The findings in this study identified influencing factors on the skull strain energy such as contact area, scalp thickness and impactor positioning. The energy criterion is unable to take into account the effect of differences in geometry and bone properties. These results strongly invalidate the existence of an energy-based fracture criterion. We concluded that subject-specific modelling leads to a more accurate prediction of the force-displacement curve but it should be carefully interpreted as small variations in the computational model significantly influences the results.I HRclinCT- 1 clinCT 1 (microCT) (CT) Predicted (CT) Predicted (CT) Interpolated border Cortical shell periosteal border ~*—*—*—*~ endosteal border due to excluded signal Fig. 8. Rib section images (HRclinCT left, clinCT right) with predicted periosteal and endosteal border locations, and the corresponding borders from jCT (central).Fig. 6. The highlighted signal (with nearby signals removed for clarity) meets two criteria for exclusion: Signal spans two cortices, and Signal traverses cortex. Criteria are designed to detect signals that do not cross directly over the periosteal and endosteal borders of the cortex.HReclinCT prediction (Ct.Th, mm) 3.5 3.5 Error: -0.03+0.17 mm Error: -0.05+0.22 mm -2.2+30.3 % 3 7.4 Yo E € (25 & 2 2 < 2 215 2 a bE oS 1 & ° 05 oO 0.5 1 1.5 2 25 3 3.5 0 0.5 1 1.5 2 25 3.5 microCT (Ct.Th, mm) microCT (Ct.Th, mm) Fig. 7. Predicted local HRclinCT and clinCT cortical bone thickness compared to j4CT.HRelinCT error: + 9 > 30 F -0.6+1.5 Ct.Ar, mm? or? °. E -3.147.6 % * & ag g clinCT error: 9 00.9 a 25 | =1.642.0 Ct-Ar, mm? + oer L | -7.8:410.7 % al < oO Ps sg 48 +H 8 20 ores @ cies a ot * o ° s + £15 o @ oO © Q 8 10 6 © HRelinCT + elincT 10 15 20 25 30 microCT (Ct.Ar, mm?) Fig. 9. Predicted overall HRclinCT and clinCT cortical shell area compared to CT.HU 2000 1500 1000 = 500 -500 Periosteal ' Endosteal -1000 border (x)! border (x,) A 0 1 2 Smooth initial boundary os vat Cross-cortex signals Cortical signal position (mm) ~ Fitted step model Fig. 5. Cortical signals are sampled across the initial periosteal border at multiple locations, and the 6-parameter step model (shown prior to Gaussian smoothing) was fitted to each signal providing an optimal estimation of true periosteal and endosteal borders.¢: ua (Registration Faia — > As Fig. 4. Binary image mask registration between lower resolution scans and ,.CT to align the two image spaces. The final panel shows CT images with the superi posed jCT endosteal and periosteal borders~ HRclinCT + clinCT. > ~ HRclinCT + clinCT. > Largest bone region @ Nearby bone regions © Filled and closed regions Fig. 3. Segmented HRclinCT and clinCT rib section images using the segmentation routine to (1) extract the largest bone region in the image via thresholding, (2) include other nearby but smaller bone regions, and (3) perform morphological closing and filling operations until a single solid region is produced.Fig. 2. Typical segmented CT outer (periosteal) cortical border from histogram- based thresholding, and inner (endosteal) border from active contour segmentation.Background i> Bone pixels -0.5 ) 0.5 1 1.5 2 2.5 3 »CT pixel intensity «104 Fig. 1. Histogram of CT image pixel intensities along with the segmentation threshold used to separate cortical bone pixels on jzCT image from background pix- els.",Medical Data Analysis,"This study aims to investigate the validity of an energy-based skull fracture criterion using subject-specific finite element head models. 18 different experimental impacts were reconstructed, and each impact was simulated using a homogeneous material model and a subject-specific model based on local bone densities. The study found that subject-specific models predicted the moment of fracture more accurately than the homogeneous material model and had the ability to predict fracture lines with unprecedented precision. However, the modelling of post-fracture behaviour is currently unsatisfactory and needs improvement. The study identified influencing factors on skull strain energy, such as contact area, scalp thickness, and impactor positioning, and concluded that subject-specific modelling leads to a more accurate prediction of the force-displacement curve, but small variations in the computational model significantly influence the results. The study also highlighted the inability of the energy-based fracture criterion to account for differences in geometry and bone properties.",Medical Data Analysis,"I HRclinCT- 1 clinCT 1 (microCT) (CT) Predicted (CT) Predicted (CT) Interpolated border Cortical shell periosteal border ~*—*—*—*~ endosteal border due to excluded signal Fig. 8. Rib section images (HRclinCT left, clinCT right) with predicted periosteal and endosteal border locations, and the corresponding borders from jCT (central).Fig. 6. The highlighted signal (with nearby signals removed for clarity) meets two criteria for exclusion: Signal spans two cortices, and Signal traverses cortex. Criteria are designed to detect signals that do not cross directly over the periosteal and endosteal borders of the cortex.HReclinCT prediction (Ct.Th, mm) 3.5 3.5 Error: -0.03+0.17 mm Error: -0.05+0.22 mm -2.2+30.3 % 3 7.4 Yo E € (25 & 2 2 < 2 215 2 a bE oS 1 & ° 05 oO 0.5 1 1.5 2 25 3 3.5 0 0.5 1 1.5 2 25 3.5 microCT (Ct.Th, mm) microCT (Ct.Th, mm) Fig. 7. Predicted local HRclinCT and clinCT cortical bone thickness compared to j4CT.HRelinCT error: + 9 > 30 F -0.6+1.5 Ct.Ar, mm? or? °. E -3.147.6 % * & ag g clinCT error: 9 00.9 a 25 | =1.642.0 Ct-Ar, mm? + oer L | -7.8:410.7 % al < oO Ps sg 48 +H 8 20 ores @ cies a ot * o ° s + £15 o @ oO © Q 8 10 6 © HRelinCT + elincT 10 15 20 25 30 microCT (Ct.Ar, mm?) Fig. 9. Predicted overall HRclinCT and clinCT cortical shell area compared to CT.HU 2000 1500 1000 = 500 -500 Periosteal ' Endosteal -1000 border (x)! border (x,) A 0 1 2 Smooth initial boundary os vat Cross-cortex signals Cortical signal position (mm) ~ Fitted step model Fig. 5. Cortical signals are sampled across the initial periosteal border at multiple locations, and the 6-parameter step model (shown prior to Gaussian smoothing) was fitted to each signal providing an optimal estimation of true periosteal and endosteal borders.¢: ua (Registration Faia — > As Fig. 4. Binary image mask registration between lower resolution scans and ,.CT to align the two image spaces. The final panel shows CT images with the superi posed jCT endosteal and periosteal borders~ HRclinCT + clinCT. > ~ HRclinCT + clinCT. > Largest bone region @ Nearby bone regions © Filled and closed regions Fig. 3. Segmented HRclinCT and clinCT rib section images using the segmentation routine to (1) extract the largest bone region in the image via thresholding, (2) include other nearby but smaller bone regions, and (3) perform morphological closing and filling operations until a single solid region is produced.Fig. 2. Typical segmented CT outer (periosteal) cortical border from histogram- based thresholding, and inner (endosteal) border from active contour segmentation.Background i> Bone pixels -0.5 ) 0.5 1 1.5 2 2.5 3 »CT pixel intensity «104 Fig. 1. Histogram of CT image pixel intensities along with the segmentation threshold used to separate cortical bone pixels on jzCT image from background pix- els.",Medical Data Analysis
279,Comparison of modern and conventional imaging techniques in establishing multiple myeloma-related bone disease: a systematic review,"multiple myeloma, bone disease, magnetic resonance imaging, positron emission tomography - computerized tomography, X-ray.","This systematic review of studies compared magnetic resonance imaging (MRI), 18F-fluorodeoxyglucose positron emission tomography (FDG-PET), FDG-PET with computerized tomography (PET-CT) and CT with whole body X-Ray (WBXR) or (whole body) CT in order to provide evidence based diagnostic guidelines in multiple myeloma bone disease. A comprehensive search of 3 bibliographic databases was performed; methodological quality was assessed using Quality Assessment of Diagnostic Accuracy Studies (QUADAS) criteria (score 1–14). Data from 32 directly comparative studies were extracted. The mean QUADAS score was 7.1 (3–11), with quality hampered mainly by a poor description of selection and execution criteria. All index tests had a higher detection rate when compared to WBXR, with up to 80% more lesions detected by the newer imaging techniques; MRI (1.12–1.82) CT (1.04–1.33), PET (1.00–1.58) and PET-CT (1.27–1.45). However, the modern imaging techniques detected fewer lesions in the skull and ribs. In a direct comparison CT and MRI performed equally with respect to detection rate and sensitivity. This systematic review supports the International Myeloma Working Group guidelines, which recommend that WBCT can replace WBXR. In our opinion, the equal performance of MRI also indicates that it is a valuable alternative. As lesions of the skull and ribs are underdiagnosed by modern imaging techniques we advise additional X-rays of these regions. The consequences of this approach are discussed.","In conclusion, this systematic review showed that MRI, CT and FDG-PET(-CT) are sensitive techniques with a higher detection rate of MM-related bone disease in comparison with WBXR, with the exception of the skull and ribs. We therefore recommend additional X-ray of the ribs (chest X-ray) and the skull in cases where no bone lesions are detected with the newer imaging techniques, as omitting XR of the ribs and the skull in these cases could lead to the underdiagnosis of MM-related bone disease. A solid statement regarding the superiority of one of the newer imaging techniques to detect MM bone disease at diagnosis or relapse cannot be given based on our systematic review. However, given the equal diagnostic value of MRI and CT as compared to WBXR, we are of the opinion that MRI should be incorporated as an equal alternative for CT. MRI might be the preferred investigation at diagnosis because of its repeatedly reported prognostic value, which appeared not to hold true for CT. FDG-PET-CT is a promising alternative, especially during follow-up, as it depicts active lesions and normalization of FDG-uptake during treatment has been found to predict outcome. Future studies, focussing on prognostic value as well as the role of imaging in follow-up, will determine which technique, or combination of techniques should be the new gold standard.","Comparison of modern and conventional imaging techniques in establishing multiple myeloma-related bone disease: a systematic reviewmultiple myeloma, bone disease, magnetic resonance imaging, positron emission tomography - computerized tomography, X-ray.This systematic review of studies compared magnetic resonance imaging (MRI), 18F-fluorodeoxyglucose positron emission tomography (FDG-PET), FDG-PET with computerized tomography (PET-CT) and CT with whole body X-Ray (WBXR) or (whole body) CT in order to provide evidence based diagnostic guidelines in multiple myeloma bone disease. A comprehensive search of 3 bibliographic databases was performed; methodological quality was assessed using Quality Assessment of Diagnostic Accuracy Studies (QUADAS) criteria (score 1–14). Data from 32 directly comparative studies were extracted. The mean QUADAS score was 7.1 (3–11), with quality hampered mainly by a poor description of selection and execution criteria. All index tests had a higher detection rate when compared to WBXR, with up to 80% more lesions detected by the newer imaging techniques; MRI (1.12–1.82) CT (1.04–1.33), PET (1.00–1.58) and PET-CT (1.27–1.45). However, the modern imaging techniques detected fewer lesions in the skull and ribs. In a direct comparison CT and MRI performed equally with respect to detection rate and sensitivity. This systematic review supports the International Myeloma Working Group guidelines, which recommend that WBCT can replace WBXR. In our opinion, the equal performance of MRI also indicates that it is a valuable alternative. As lesions of the skull and ribs are underdiagnosed by modern imaging techniques we advise additional X-rays of these regions. The consequences of this approach are discussed.In conclusion, this systematic review showed that MRI, CT and FDG-PET(-CT) are sensitive techniques with a higher detection rate of MM-related bone disease in comparison with WBXR, with the exception of the skull and ribs. We therefore recommend additional X-ray of the ribs (chest X-ray) and the skull in cases where no bone lesions are detected with the newer imaging techniques, as omitting XR of the ribs and the skull in these cases could lead to the underdiagnosis of MM-related bone disease. A solid statement regarding the superiority of one of the newer imaging techniques to detect MM bone disease at diagnosis or relapse cannot be given based on our systematic review. However, given the equal diagnostic value of MRI and CT as compared to WBXR, we are of the opinion that MRI should be incorporated as an equal alternative for CT. MRI might be the preferred investigation at diagnosis because of its repeatedly reported prognostic value, which appeared not to hold true for CT. FDG-PET-CT is a promising alternative, especially during follow-up, as it depicts active lesions and normalization of FDG-uptake during treatment has been found to predict outcome. Future studies, focussing on prognostic value as well as the role of imaging in follow-up, will determine which technique, or combination of techniques should be the new gold standard.Records identif ed through Additional records database searching identif ed through reference (n=2422) lists (n=15) | Total records after duplicates removed and limits applied (abstracts, English language) (n= 202) Abstracts excluded: Abstract screened (n=165) (n=202) * Not Bipolar disorder * Not concerning | subject - Not mHealth Full- text articles Full- text articles assessed for eligibility — excluded: . (n=37) * Not empirical studies (n=7) | * Study protocols (n=3) Studies included in the review (n=27) Fig. 1. Flow chart: an overview of the study selection process.Psychiatric ‘Copenhagen No. of publications per city Single © Multiple No. of publications per country @. m2 mma =? 0 1000 2000 3000 km —— qv % Fig. 2. The geographical location of the most active research clusters that have published in the field of smartphone-based monitoring in bipolar disorder.",Medical Data Analysis,"This systematic review compared different imaging techniques for the diagnosis of multiple myeloma bone disease, including magnetic resonance imaging (MRI), 18F-fluorodeoxyglucose positron emission tomography (FDG-PET), FDG-PET with computerized tomography (PET-CT), and whole body X-Ray (WBXR) or (whole body) CT. The review found that all newer imaging techniques had a higher detection rate of MM-related bone disease compared to WBXR, except for lesions in the skull and ribs. CT and MRI were found to perform equally with respect to detection rate and sensitivity. The authors recommend additional X-rays of the ribs and skull in cases where no bone lesions are detected with newer imaging techniques to avoid underdiagnosis of MM-related bone disease. The authors suggest incorporating MRI as an equal alternative to CT for diagnosis, as MRI has repeatedly reported prognostic value. FDG-PET-CT is a promising alternative for follow-up as it depicts active lesions and normalization of FDG-uptake during treatment has been found to predict outcome. Further studies will determine which technique, or combination of techniques, should be the new gold standard.",Medical Data Analysis,"Records identif ed through Additional records database searching identif ed through reference (n=2422) lists (n=15) | Total records after duplicates removed and limits applied (abstracts, English language) (n= 202) Abstracts excluded: Abstract screened (n=165) (n=202) * Not Bipolar disorder * Not concerning | subject - Not mHealth Full- text articles Full- text articles assessed for eligibility — excluded: . (n=37) * Not empirical studies (n=7) | * Study protocols (n=3) Studies included in the review (n=27) Fig. 1. Flow chart: an overview of the study selection process.Psychiatric ‘Copenhagen No. of publications per city Single © Multiple No. of publications per country @. m2 mma =? 0 1000 2000 3000 km —— qv % Fig. 2. The geographical location of the most active research clusters that have published in the field of smartphone-based monitoring in bipolar disorder.",Medical Data Analysis
280,"Bone Loss, Weight Loss, and Weight Fluctuation Predict Mortality Risk in Elderly Men and Women","Bone loss, BMD, weight loss, weight fluctuation, mortality, fracture.","Introduction: Although low BMD has been shown to be associated with mortality in women, the effect of BMD is affected by weight and weight change and the contribution of these factors to mortality risk, particularly in men, is not known. This study examined the association between baseline BMD, rate of bone loss, weight loss, and weight fluctuation and all-cause mortality risk in elderly men and women. Materials and Methods: Data from 1059 women and 644 men, >=60 years of age (as of 1989), of white background who participated in the Dubbo Osteoporosis Epidemiology Study were analyzed. All-cause mortality was recorded annually between 1989 and 2004. BMD at the femoral neck was measured by DXA (GE-LUNAR) at baseline and at approximately every 2 yr afterward. Data on incident osteoporotic fractures and concomitant diseases, including cardiovascular diseases, all types of cancer, and type I/II diabetes mellitus, was also recorded. Results: In the multivariable Cox’s proportional hazards model with adjustment for age, incident fractures, and concomitant diseases, the following variables were independent risk factors of all-cause mortality in men: rate of BMD loss of at least 1%/yr, rate of weight loss of at least 1%/yr, and weight fluctuation (defined by the CV) of at least 3%. In women, in addition to the significant factors observed in men, lower baseline BMD was also an independent risk factor of mortality. In both sexes, baseline weight was not an independent and significant predictor of mortality risk. Approximately 36% and 22% of deaths in women and men, respectively, were attributable to the four risk factors. Conclusions: These data suggest that, although low BMD was a risk factor of mortality in women, it was not a risk factor of mortality in men. However, high rates of BMD loss, weight loss, and weight fluctuation were also independent predictors of all-cause mortality in elderly men and women, independent of age, incident fracture, and concomitant diseases.","In conclusion, these data suggest that lower BMD was an independent predictor of mortality in women but not in men. Furthermore, in addition to low baseline BMD, rate of BMD loss and weight fluctuation were also significant predictors of all-cause mortality in elderly men and women, independent from age and concomitant diseases. Although the risk factors found to be associated with mortality in this study probably reflect the underlying frailty or the presence of other wasting diseases in the elderly, these findings partly re-emphasize the public health burden of osteoporosis in the general population. Reducing bone loss, weight loss, and maintaining stable weight may have beneficial effects on the survival of elderly individuals.","Bone Loss, Weight Loss, and Weight Fluctuation Predict Mortality Risk in Elderly Men and WomenBone loss, BMD, weight loss, weight fluctuation, mortality, fracture.Introduction: Although low BMD has been shown to be associated with mortality in women, the effect of BMD is affected by weight and weight change and the contribution of these factors to mortality risk, particularly in men, is not known. This study examined the association between baseline BMD, rate of bone loss, weight loss, and weight fluctuation and all-cause mortality risk in elderly men and women. Materials and Methods: Data from 1059 women and 644 men, >=60 years of age (as of 1989), of white background who participated in the Dubbo Osteoporosis Epidemiology Study were analyzed. All-cause mortality was recorded annually between 1989 and 2004. BMD at the femoral neck was measured by DXA (GE-LUNAR) at baseline and at approximately every 2 yr afterward. Data on incident osteoporotic fractures and concomitant diseases, including cardiovascular diseases, all types of cancer, and type I/II diabetes mellitus, was also recorded. Results: In the multivariable Cox’s proportional hazards model with adjustment for age, incident fractures, and concomitant diseases, the following variables were independent risk factors of all-cause mortality in men: rate of BMD loss of at least 1%/yr, rate of weight loss of at least 1%/yr, and weight fluctuation (defined by the CV) of at least 3%. In women, in addition to the significant factors observed in men, lower baseline BMD was also an independent risk factor of mortality. In both sexes, baseline weight was not an independent and significant predictor of mortality risk. Approximately 36% and 22% of deaths in women and men, respectively, were attributable to the four risk factors. Conclusions: These data suggest that, although low BMD was a risk factor of mortality in women, it was not a risk factor of mortality in men. However, high rates of BMD loss, weight loss, and weight fluctuation were also independent predictors of all-cause mortality in elderly men and women, independent of age, incident fracture, and concomitant diseases.In conclusion, these data suggest that lower BMD was an independent predictor of mortality in women but not in men. Furthermore, in addition to low baseline BMD, rate of BMD loss and weight fluctuation were also significant predictors of all-cause mortality in elderly men and women, independent from age and concomitant diseases. Although the risk factors found to be associated with mortality in this study probably reflect the underlying frailty or the presence of other wasting diseases in the elderly, these findings partly re-emphasize the public health burden of osteoporosis in the general population. Reducing bone loss, weight loss, and maintaining stable weight may have beneficial effects on the survival of elderly individuals.| | Best case accuracy Worst case accuracy Input Image < g = S 2 = Ob g Dn 1 Human Observer Fig. 6. Segmentation results for STARE dataset.|| Best case accuracy Worst case accuracy Input Image g $ 3 3 $ & eo — op o a 1“ Human Observer Fig. 5. Segmentation results for DRIVE dataset.Fig. 3. Configuration of an asymmetric vertical bar-selective B-COSFIRE filter. The center point of the filter is on the end of the line.(a) (b) () (d) (e) (f) DRIVE dataset STARE dataset Fig. 4. Kinds of features used in the proposed method; (a) original input image, (b) CLAHE contrast enhanced image, (c) vesselness measurement on original image, (d) morphological top-hat transformation of original image, (e) Gabor filter response of (b) for scales b= 1, 4=6, 7 =0.50, and (f) B-COSFIRE filter response of original image (first row: the DRIVE image, second row: the STARE image).Fig. 2. Configuration of a vertical bar-selective B-COSFIRE filter. The points with the strongest DoG responses along the circles of given radii are labeled by numbers. The filter center point is denoted by ‘1’.6 Feature extraction ¢— Classifier Testing — Training = Train Data Test Data Fig. 1. General framework of the proposed method.",Medical Data Analysis,"This study aimed to examine the association between baseline bone mineral density (BMD), rate of bone loss, weight loss, and weight fluctuation, and all-cause mortality risk in elderly men and women. The study analyzed data from 1059 women and 644 men, aged 60 years or older, of white background who participated in the Dubbo Osteoporosis Epidemiology Study. The results showed that lower baseline BMD was an independent predictor of mortality in women, but not in men. High rates of BMD loss, weight loss, and weight fluctuation were significant predictors of all-cause mortality in both elderly men and women, independent of age, incident fracture, and concomitant diseases. The study emphasized the public health burden of osteoporosis and suggested that reducing bone loss, weight loss, and maintaining stable weight may have beneficial effects on the survival of elderly individuals.",Medical Data Analysis,"| | Best case accuracy Worst case accuracy Input Image < g = S 2 = Ob g Dn 1 Human Observer Fig. 6. Segmentation results for STARE dataset.|| Best case accuracy Worst case accuracy Input Image g $ 3 3 $ & eo — op o a 1“ Human Observer Fig. 5. Segmentation results for DRIVE dataset.Fig. 3. Configuration of an asymmetric vertical bar-selective B-COSFIRE filter. The center point of the filter is on the end of the line.(a) (b) () (d) (e) (f) DRIVE dataset STARE dataset Fig. 4. Kinds of features used in the proposed method; (a) original input image, (b) CLAHE contrast enhanced image, (c) vesselness measurement on original image, (d) morphological top-hat transformation of original image, (e) Gabor filter response of (b) for scales b= 1, 4=6, 7 =0.50, and (f) B-COSFIRE filter response of original image (first row: the DRIVE image, second row: the STARE image).Fig. 2. Configuration of a vertical bar-selective B-COSFIRE filter. The points with the strongest DoG responses along the circles of given radii are labeled by numbers. The filter center point is denoted by ‘1’.6 Feature extraction ¢— Classifier Testing — Training = Train Data Test Data Fig. 1. General framework of the proposed method.",Medical Data Analysis
281,Bone Disease in Thalassemia: A Frequent and Still Unresolved Problem,"DXA, BMD, fractures, vertebral morphometry, thalassemia.","Adults with b thalassemia major frequently have low BMD, fractures, and bone pain. The purpose of this study was to determine the prevalence of low BMD, fractures, and bone pain in all thalassemia syndromes in childhood, adolescence, and adulthood, associations of BMD with fractures and bone pain, and etiology of bone disease in thalassemia. Patients of all thalassemia syndromes in the Thalassemia Clinical Research Network, $6 yr of age, with no preexisting medical condition affecting bone mass or requiring steroids, participated. We measured spine and femur BMD and whole body BMC by DXA and assessed vertebral abnormalities by morphometric X-ray absorptiometry (MXA). Medical history by interview and review of medical records, physical examinations, and blood and urine collections were performed. Three hundred sixty-one subjects, 49% male, with a mean age of 23.2 yr (range, 6.1–75 yr), were studied. Spine and femur BMD Z-scores < 22 occurred in 46% and 25% of participants, respectively. Greater age, lower weight, hypogonadism, and increased bone turnover were strong independent predictors of low bone mass regardless of thalassemia syndrome. Peak bone mass was suboptimal. Thirty-six percent of patients had a history of fractures, and 34% reported bone pain. BMD was negatively associated with fractures but not with bone pain. Nine percent of participants had uniformly decreased height of several vertebrae by MXA, which was associated with the use of iron chelator deferoxamine before 6 yr of age. In patients with thalassemia, low BMD and fractures occur frequently and independently of the particular syndrome. Peak bone mass is suboptimal. Low BMD is associated with hypogonadism, increased bone turnover, and an increased risk for fractures.","In summary, in this large cohort of patients across all thalassemia syndromes, we observed a high prevalence of fractures and a strong association between bone mass and fractures. Therefore, strategies to improve BMD are very important in thalassemia management. We showed that bone disease in thalassemia is an adolescent problem with adult manifestations. Current transfusion and chelation practices seem insufficient to prevent the development of low bone mass. Our data highlight the need for randomized trials to determine the appropriate form of gonadal steroid replacement and vitamin D supplementation as well as additional strategies to optimize bone accrual in this disease. Further longitudinal studies are needed to address changes of bone mass during puberty. Finally, changes in bone turn over seem to be involved in the development of bone disease, although the factors that lead to increased bone resorption in thalassemia remain unclear and warrant further study.","Bone Disease in Thalassemia: A Frequent and Still Unresolved ProblemDXA, BMD, fractures, vertebral morphometry, thalassemia.Adults with b thalassemia major frequently have low BMD, fractures, and bone pain. The purpose of this study was to determine the prevalence of low BMD, fractures, and bone pain in all thalassemia syndromes in childhood, adolescence, and adulthood, associations of BMD with fractures and bone pain, and etiology of bone disease in thalassemia. Patients of all thalassemia syndromes in the Thalassemia Clinical Research Network, $6 yr of age, with no preexisting medical condition affecting bone mass or requiring steroids, participated. We measured spine and femur BMD and whole body BMC by DXA and assessed vertebral abnormalities by morphometric X-ray absorptiometry (MXA). Medical history by interview and review of medical records, physical examinations, and blood and urine collections were performed. Three hundred sixty-one subjects, 49% male, with a mean age of 23.2 yr (range, 6.1–75 yr), were studied. Spine and femur BMD Z-scores < 22 occurred in 46% and 25% of participants, respectively. Greater age, lower weight, hypogonadism, and increased bone turnover were strong independent predictors of low bone mass regardless of thalassemia syndrome. Peak bone mass was suboptimal. Thirty-six percent of patients had a history of fractures, and 34% reported bone pain. BMD was negatively associated with fractures but not with bone pain. Nine percent of participants had uniformly decreased height of several vertebrae by MXA, which was associated with the use of iron chelator deferoxamine before 6 yr of age. In patients with thalassemia, low BMD and fractures occur frequently and independently of the particular syndrome. Peak bone mass is suboptimal. Low BMD is associated with hypogonadism, increased bone turnover, and an increased risk for fractures.In summary, in this large cohort of patients across all thalassemia syndromes, we observed a high prevalence of fractures and a strong association between bone mass and fractures. Therefore, strategies to improve BMD are very important in thalassemia management. We showed that bone disease in thalassemia is an adolescent problem with adult manifestations. Current transfusion and chelation practices seem insufficient to prevent the development of low bone mass. Our data highlight the need for randomized trials to determine the appropriate form of gonadal steroid replacement and vitamin D supplementation as well as additional strategies to optimize bone accrual in this disease. Further longitudinal studies are needed to address changes of bone mass during puberty. Finally, changes in bone turn over seem to be involved in the development of bone disease, although the factors that lead to increased bone resorption in thalassemia remain unclear and warrant further study.HOMOGENEOUS SuBJECT-SPECIFIC Frontal Impact Occipital Impact Fig. 8. Comparison between the two material models, showing the simulated fracture lines (black) imposed on a picture of the skull post-impact with the experimental fracture lines (dotted red). The subject-specific material model is able to capture the fracture lines visually more accurate for all impacts. For the frontal impact, the point of initation is near the orbita, extending to the frontal skull bone (ID 2). Impact on the temporal site result in fractures extending from the impact site to the orbita (ID 11). Occipital impacts lead to complex fractures surrounding the impact area and extending anteriorly (ID 19).12000 t 7 Frontal site 4 ! [Bi temporal site -— ' ‘210000 } BB muttipte sites) s ' 8 1 t ® 8000 +> 4 2 WwW eg 5 L 4 = 6000 = g 1 uw ' = 4000 —— ! J ~ 1 an 0 = = 4 Q % Q o zZ % 5 % % 3, 3, 3 % ‘ % 2 S 2 2 2 2 % ~ & S 3 3 z a a D Q 3 & a z Fig. 11. The skull strain energy is investigated in numerous studies using the SUFEHM. The values found in these studies are in agreement with the values found in literature, especially the data from Marjoux et al.. The study by Asgharpour et al. which investigated frontal impacts is also in agreement, as the frontal impacts are in the whiskers of the boxplot (light-blue crosses). (Data estimated from (Marjoux et al., 2008), (Sahoo et al., 2016), (Sahoo et al., 2013), (Asgharpour et al., 2014)).12000 10000 8000 | zZ 2 6000 & 4000 Fore for subyoctupecito material O Subject-specific fracture point 2000 ©. Homoxeneous fracture point senses uperimental force 0 © Experimental fracture point 15 2 25 3 35 4 45 5 Impactor Displacement [mm] Fig. 6. For skull ID 2, the simulated force-displacement curve for the homo- geneous as the subject-specific material model follows the experimental data accurately, whereas it does not predict the more gradual phase which influences the accuracy of the calculated strain energy.25 _ _ N ro a 3 Total strain energy [J] uw 0 500 1000 1500 2000 Contact Area [mm?] Fig. 7. The total strain energy is strongly correlated with the contact area at the moment of fracture.Fig. 5. The scalp is modelled to fit the skull geometry perfectly. The scalp is rigidly coupled to one central node on the impactor. The impactor is modelled as a rigid body.Frontal Temporal Occipital Fig. 4. The impactor is rotated and translated for the frontal, temporal and occipital impact site whilst maintaining the impact location. The original position is delineated in the dashed line.40 35 30 25 20 15 10 Total Absorbed Energy [J] 5 — ; a 1.26 mm 2.52 mm 5.04 mm |3.06 mm 6.13 mm 12.26 “ 1.85 mm 3.77 mm 7.54mm Frontal Temporal Occipital @Scalp Energy [J] Skull Energy [J] Fig. 10. Bar graph showing the scalp and skull strain energy of the simulations with half, original and double scalp thicknesses for the frontal (ID 2), temporal (ID 11) and occipital (ID 19) cases. The absorbed scalp energy increases significantly for thicker scalps, so does the total absorbed energy.120 3 8 @ Ss S $ @ KTH material Ultimate Tensile Stress [MPa] 2 s N s — Interpolated values 0 . 1 1 0 0.5 1 15 2 Tissue Mineral Density [gHA/cm*] Fig. 3. The ultimate tensile stress is extrapolated for different tissue mineral densities based on Young's moduli and tensile stresses of the KTH FE model. The ultimate strain at fracture is assumed to be constant.10"" ws p Young's modulus [MPa] [Model parameters: lp, = 3.0 g/em> 05 lp, 1.4 g/cm* | IR, = 0.36 0 1 0 0.5 1 2 1.5 Tissue Mineral Density [gHA/cm*] Fig. 2. The elastic behaviour of skull bone is described by the relationship between tissue mineral density and Young's moduli. The curve is truncated at the lowest, from 0 to 0.2 gHA/cm’, and highest densities, from 1.2 gHA/cm?, to avoid inaccurate results (dotted line).0.95 0.9 0.85 True Positive Rate (Sensitivity) Oo nN a 0.7) 0.65 DRIVE: os © STARE: 0.55 3 2nd observer on DRIVE ++ 2nd observer on STARE 1 1 i i i 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 False Positive Rate (1-Specificity) 0.8 0.9 1 Fig. 7. ROC curves for DRIVE and STARE datasets. 005 ood vol i i Foul re 3 3 fool z oct Ope VE MOG? GIO GEG? G4 GO GID GS GS GIP GIT G2 Gt IN Oe MO VE GID Go CW GID Gr ce cs GIT GEG) IN GS G2 GI raver Fees (a) (b) Fig. 8. Predictor importance in classification for (a) DRIVE and (b) STARE datasets (BC is the B-COSFIRE filter response, MO is the morphological transformation, VE is the vesselness measure, G1-13 are the Gabor filter responses, and IN is the intensity).YS S\ 1 Q / 1. First pendulum (Aluminium) 3 <\\ 2. Rigid connection \Z\ 3. Second pendulum (Steel) YZ 4. Impactor with load cell XZ) 5. Additional weights YA) 6. Laser displacement sensor > Fig. 1. The schematic set-up with its components (adapted from (Monea, 2013)).",Medical Data Analysis,"This study aimed to determine the prevalence of low bone mineral density (BMD), fractures, and bone pain in patients with thalassemia across childhood, adolescence, and adulthood, as well as the etiology of bone disease in thalassemia. The study found that low BMD and fractures are common in thalassemia patients across all thalassemia syndromes, with suboptimal peak bone mass. Hypogonadism, increased bone turnover, and older age are independent predictors of low bone mass, and fractures are negatively associated with BMD. The study highlights the need for strategies to improve BMD in thalassemia management, including further research on appropriate forms of gonadal steroid replacement and vitamin D supplementation, and additional strategies to optimize bone accrual. Further longitudinal studies are also needed to address changes in bone mass during puberty and the factors that lead to increased bone resorption in thalassemia.",Medical Data Analysis,"HOMOGENEOUS SuBJECT-SPECIFIC Frontal Impact Occipital Impact Fig. 8. Comparison between the two material models, showing the simulated fracture lines (black) imposed on a picture of the skull post-impact with the experimental fracture lines (dotted red). The subject-specific material model is able to capture the fracture lines visually more accurate for all impacts. For the frontal impact, the point of initation is near the orbita, extending to the frontal skull bone (ID 2). Impact on the temporal site result in fractures extending from the impact site to the orbita (ID 11). Occipital impacts lead to complex fractures surrounding the impact area and extending anteriorly (ID 19).12000 t 7 Frontal site 4 ! [Bi temporal site -— ' ‘210000 } BB muttipte sites) s ' 8 1 t ® 8000 +> 4 2 WwW eg 5 L 4 = 6000 = g 1 uw ' = 4000 —— ! J ~ 1 an 0 = = 4 Q % Q o zZ % 5 % % 3, 3, 3 % ‘ % 2 S 2 2 2 2 % ~ & S 3 3 z a a D Q 3 & a z Fig. 11. The skull strain energy is investigated in numerous studies using the SUFEHM. The values found in these studies are in agreement with the values found in literature, especially the data from Marjoux et al.. The study by Asgharpour et al. which investigated frontal impacts is also in agreement, as the frontal impacts are in the whiskers of the boxplot (light-blue crosses). (Data estimated from (Marjoux et al., 2008), (Sahoo et al., 2016), (Sahoo et al., 2013), (Asgharpour et al., 2014)).12000 10000 8000 | zZ 2 6000 & 4000 Fore for subyoctupecito material O Subject-specific fracture point 2000 ©. Homoxeneous fracture point senses uperimental force 0 © Experimental fracture point 15 2 25 3 35 4 45 5 Impactor Displacement [mm] Fig. 6. For skull ID 2, the simulated force-displacement curve for the homo- geneous as the subject-specific material model follows the experimental data accurately, whereas it does not predict the more gradual phase which influences the accuracy of the calculated strain energy.25 _ _ N ro a 3 Total strain energy [J] uw 0 500 1000 1500 2000 Contact Area [mm?] Fig. 7. The total strain energy is strongly correlated with the contact area at the moment of fracture.Fig. 5. The scalp is modelled to fit the skull geometry perfectly. The scalp is rigidly coupled to one central node on the impactor. The impactor is modelled as a rigid body.Frontal Temporal Occipital Fig. 4. The impactor is rotated and translated for the frontal, temporal and occipital impact site whilst maintaining the impact location. The original position is delineated in the dashed line.40 35 30 25 20 15 10 Total Absorbed Energy [J] 5 — ; a 1.26 mm 2.52 mm 5.04 mm |3.06 mm 6.13 mm 12.26 “ 1.85 mm 3.77 mm 7.54mm Frontal Temporal Occipital @Scalp Energy [J] Skull Energy [J] Fig. 10. Bar graph showing the scalp and skull strain energy of the simulations with half, original and double scalp thicknesses for the frontal (ID 2), temporal (ID 11) and occipital (ID 19) cases. The absorbed scalp energy increases significantly for thicker scalps, so does the total absorbed energy.120 3 8 @ Ss S $ @ KTH material Ultimate Tensile Stress [MPa] 2 s N s — Interpolated values 0 . 1 1 0 0.5 1 15 2 Tissue Mineral Density [gHA/cm*] Fig. 3. The ultimate tensile stress is extrapolated for different tissue mineral densities based on Young's moduli and tensile stresses of the KTH FE model. The ultimate strain at fracture is assumed to be constant.10"" ws p Young's modulus [MPa] [Model parameters: lp, = 3.0 g/em> 05 lp, 1.4 g/cm* | IR, = 0.36 0 1 0 0.5 1 2 1.5 Tissue Mineral Density [gHA/cm*] Fig. 2. The elastic behaviour of skull bone is described by the relationship between tissue mineral density and Young's moduli. The curve is truncated at the lowest, from 0 to 0.2 gHA/cm’, and highest densities, from 1.2 gHA/cm?, to avoid inaccurate results (dotted line).0.95 0.9 0.85 True Positive Rate (Sensitivity) Oo nN a 0.7) 0.65 DRIVE: os © STARE: 0.55 3 2nd observer on DRIVE ++ 2nd observer on STARE 1 1 i i i 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 False Positive Rate (1-Specificity) 0.8 0.9 1 Fig. 7. ROC curves for DRIVE and STARE datasets. 005 ood vol i i Foul re 3 3 fool z oct Ope VE MOG? GIO GEG? G4 GO GID GS GS GIP GIT G2 Gt IN Oe MO VE GID Go CW GID Gr ce cs GIT GEG) IN GS G2 GI raver Fees (a) (b) Fig. 8. Predictor importance in classification for (a) DRIVE and (b) STARE datasets (BC is the B-COSFIRE filter response, MO is the morphological transformation, VE is the vesselness measure, G1-13 are the Gabor filter responses, and IN is the intensity).YS S\ 1 Q / 1. First pendulum (Aluminium) 3 <\\ 2. Rigid connection \Z\ 3. Second pendulum (Steel) YZ 4. Impactor with load cell XZ) 5. Additional weights YA) 6. Laser displacement sensor > Fig. 1. The schematic set-up with its components (adapted from (Monea, 2013)).",Medical Data Analysis
282,Relationship Between Low Bone Mineral Density and Fractures With Incident Cardiovascular Disease: A Systematic Review and Meta-Analysis,OSTEOPOROSIS; BONE MINERAL DENSITY; CARDIOVASCULAR DISEASE; META-ANALYSIS,"An increasing evidence base suggests that low bone mineral density (BMD) and fractures are associated with cardiovascular disease (CVD). We conducted a systematic review and meta-analysis summarizing the evidence of low BMD and fractures as risk factors for future CVD. Two independent authors searched major databases from inception to August 1, 2016, for longitudinal studies reporting data on CVD incidence (overall and specific CVD) and BMD status and fractures. The association between low BMD, fractures, and CVD across longitudinal studies was explored by calculating pooled adjusted hazard ratios (HRs) 95% confidence intervals (CIs) with a random-effects meta-analysis. Twenty-eight studies (18 regarding BMD and 10 fractures) followed a total of 1,107,885 participants for a median of 5 years. Taking those with higher BMD as the reference, people with low BMD were at increased risk of developing CVD during follow-up (11 studies; HR ¼ 1.33; 95%CI, 1.27 to 1.38; I 2 ¼ 53%), after adjusting for a median of eight confounders. This finding was confirmed using a decrease in one standard deviation of baseline BMD (9 studies; HR ¼ 1.16; 95% CI, 1.09 to 1.24; I 2¼ 69%). The presence of fractures at baseline was associated with an increased risk of developing CVD (HR ¼ 1.20; 95% CI, 1.06 to 1.37; I 2¼ 91%). Regarding specific CVDs, low BMD was associated with an increased risk of developing coronary artery disease, cerebrovascular conditions, and CVD-associated death. Fractures at baseline was associated with an increased risk of cerebrovascular conditions and death due to CVD. In conclusion, low BMD and fractures are associated with a small, but significant increased risk of CVD risk and possibly death.","In conclusion, low BMD and fractures appear to be associated with a higher risk (albeit of small-modest effect size) of future CVD. Although the data suggests a relationship, one should note there were some sources of potential bias in the literature. Nonetheless, because CVD events could accelerate the transition of people with bone diseases to greater disability and mortality, our meta-analysis suggests it may be important to consider the cardiovascular health of people with osteoporosis and fractures. Future studies are required to evaluate if addressing low BMD and fractures could positively influence CV outcomes. Such studies should ensure matching between cases and controls and attempt to disentangle underlying pathophysiological and behavioral (eg, physical activity, nutrition) mechanisms, which could potentially be targets for future preventative interventions.","Relationship Between Low Bone Mineral Density and Fractures With Incident Cardiovascular Disease: A Systematic Review and Meta-AnalysisOSTEOPOROSIS; BONE MINERAL DENSITY; CARDIOVASCULAR DISEASE; META-ANALYSISAn increasing evidence base suggests that low bone mineral density (BMD) and fractures are associated with cardiovascular disease (CVD). We conducted a systematic review and meta-analysis summarizing the evidence of low BMD and fractures as risk factors for future CVD. Two independent authors searched major databases from inception to August 1, 2016, for longitudinal studies reporting data on CVD incidence (overall and specific CVD) and BMD status and fractures. The association between low BMD, fractures, and CVD across longitudinal studies was explored by calculating pooled adjusted hazard ratios (HRs) 95% confidence intervals (CIs) with a random-effects meta-analysis. Twenty-eight studies (18 regarding BMD and 10 fractures) followed a total of 1,107,885 participants for a median of 5 years. Taking those with higher BMD as the reference, people with low BMD were at increased risk of developing CVD during follow-up (11 studies; HR ¼ 1.33; 95%CI, 1.27 to 1.38; I 2 ¼ 53%), after adjusting for a median of eight confounders. This finding was confirmed using a decrease in one standard deviation of baseline BMD (9 studies; HR ¼ 1.16; 95% CI, 1.09 to 1.24; I 2¼ 69%). The presence of fractures at baseline was associated with an increased risk of developing CVD (HR ¼ 1.20; 95% CI, 1.06 to 1.37; I 2¼ 91%). Regarding specific CVDs, low BMD was associated with an increased risk of developing coronary artery disease, cerebrovascular conditions, and CVD-associated death. Fractures at baseline was associated with an increased risk of cerebrovascular conditions and death due to CVD. In conclusion, low BMD and fractures are associated with a small, but significant increased risk of CVD risk and possibly death.In conclusion, low BMD and fractures appear to be associated with a higher risk (albeit of small-modest effect size) of future CVD. Although the data suggests a relationship, one should note there were some sources of potential bias in the literature. Nonetheless, because CVD events could accelerate the transition of people with bone diseases to greater disability and mortality, our meta-analysis suggests it may be important to consider the cardiovascular health of people with osteoporosis and fractures. Future studies are required to evaluate if addressing low BMD and fractures could positively influence CV outcomes. Such studies should ensure matching between cases and controls and attempt to disentangle underlying pathophysiological and behavioral (eg, physical activity, nutrition) mechanisms, which could potentially be targets for future preventative interventions.Reference test Index test user = wt ——— — — es —s (i wexR MRI gy wc —— PET fal ee se = —_—_—— (a wexr << PET fi —————— PET-CT | Gi wexr 60% 50% 40% -30% 10% 0% 10% 20% 30% 40% 50% 60% Fig. 2. Differences in detection of lesion between reference and index tests. The left side of the figure depicts the percentage of lesions that was only detected by the reference test (CT or WBXR) and the right side depicts the lesions that were only detected by the index test (MRI, PET, PET-CT or CT). The lesions detected by both techniques are not depicted in this figure. CT, computed tomography; MRI, magnetic resonance imaging; PET, positron emission tomography; WBXR, whole body X-ray.Potentially relevant studies identified and screened for retrieval after exclusion of doubles (n= 1819) Pubmed (n = 901); Embase (n = 1526); Cochrane (n = 15); Doubles (n = 623) Studies excluded (n = 1609) Exclusion was based on the abstract if the answer to one of the following questions is no: = Comparison of two imaging techniques * Containing at least WBXR or (WB)CT = Study concerns adult patients only * Study performed to (re)stage patients Studies retrieved for more detailed evaluation (n = 210) Studies excluded (n = 155): Not published in English (n = 53) No full report available (n = 22) No comparison of imaging results presented (n = 36) Imaging not performed for (re)staging of patients (n = 7) Other (review, case-report, guideline, editorial) (n = 37) Potentially appropriate studies (n = 55) Excluded from systematic review (n = 2: No results of reference test presented (n = 9) No direct comparison of index and reference test (n = 8) Results incompletely presented (n = 4) Results MM patients not presented separately (n = 1) Comparison of disease stadia, instead of imaging results (n = 1) Studies included in systematic review (n = 32) Fig 1. Flow of studies through selection process. WB, whole body; XR, x-ray; CT, computed tomography, MM, multiple myeloma.",Medical Data Analysis,"This systematic review and meta-analysis found that low bone mineral density (BMD) and fractures are associated with a small but significant increased risk of cardiovascular disease (CVD) and possibly death. The analysis included 28 studies that followed over 1 million participants for a median of 5 years. People with low BMD were at increased risk of developing overall CVD, coronary artery disease, cerebrovascular conditions, and CVD-associated death. The presence of fractures at baseline was associated with an increased risk of cerebrovascular conditions and death due to CVD. The authors suggest that addressing low BMD and fractures could positively influence cardiovascular outcomes and that future studies should evaluate potential preventative interventions. However, the authors note that there were some potential sources of bias in the literature.",Medical Data Analysis,"Reference test Index test user = wt ——— — — es —s (i wexR MRI gy wc —— PET fal ee se = —_—_—— (a wexr << PET fi —————— PET-CT | Gi wexr 60% 50% 40% -30% 10% 0% 10% 20% 30% 40% 50% 60% Fig. 2. Differences in detection of lesion between reference and index tests. The left side of the figure depicts the percentage of lesions that was only detected by the reference test (CT or WBXR) and the right side depicts the lesions that were only detected by the index test (MRI, PET, PET-CT or CT). The lesions detected by both techniques are not depicted in this figure. CT, computed tomography; MRI, magnetic resonance imaging; PET, positron emission tomography; WBXR, whole body X-ray.Potentially relevant studies identified and screened for retrieval after exclusion of doubles (n= 1819) Pubmed (n = 901); Embase (n = 1526); Cochrane (n = 15); Doubles (n = 623) Studies excluded (n = 1609) Exclusion was based on the abstract if the answer to one of the following questions is no: = Comparison of two imaging techniques * Containing at least WBXR or (WB)CT = Study concerns adult patients only * Study performed to (re)stage patients Studies retrieved for more detailed evaluation (n = 210) Studies excluded (n = 155): Not published in English (n = 53) No full report available (n = 22) No comparison of imaging results presented (n = 36) Imaging not performed for (re)staging of patients (n = 7) Other (review, case-report, guideline, editorial) (n = 37) Potentially appropriate studies (n = 55) Excluded from systematic review (n = 2: No results of reference test presented (n = 9) No direct comparison of index and reference test (n = 8) Results incompletely presented (n = 4) Results MM patients not presented separately (n = 1) Comparison of disease stadia, instead of imaging results (n = 1) Studies included in systematic review (n = 32) Fig 1. Flow of studies through selection process. WB, whole body; XR, x-ray; CT, computed tomography, MM, multiple myeloma.",Medical Data Analysis
283,"Melatonin effects on bone: potential use for the prevention and treatment for osteopenia, osteoporosis, and periodontal disease and for use in bone-grafting procedures","melatonin, menopause, osteoblasts, osteoclasts, osteopenia, osteoporosis, Per2","An important role for melatonin in bone formation and restructuring has emerged, and studies demonstrate the multiple mechanisms for these beneficial actions. Statistical analysis shows that even with existing osteoporotic therapies, bone-related disease, and mortality are on the rise, creating a huge financial burden for societies worldwide. These findings suggest that novel alternatives need to be developed to either prevent or reverse bone loss to combat osteoporosis-related fractures. The focus of this review describes melatonin’s role in bone physiology and discusses how disruption of melatonin rhythms by light exposure at night, shift work, and disease can adversely impact on bone. The signal transduction mechanisms underlying osteoblast and osteoclast differentiation and coupling with one another are discussed with a focus on how melatonin, through the regulation of RANKL and osteoprotegerin synthesis and release from osteoblasts, can induce osteoblastogenesis while inhibiting osteoclastogenesis. Also, melatonin’s free-radical scavenging and antioxidant properties of this indoleamine are discussed as yet an additional mechanism by which melatonin can maintain one’s bone health, especially oral health. The clinical use for melatonin in bone-grafting procedures, in reversing bone loss due to osteopenia and osteoporosis, and in managing periodontal disease is discussed.","Additional multicentered, randomized control trials (RCTs) in heterogeneous and susceptible populations (e.g., peri- or postmenopausal women, and elderly men and women) are required to assess the efficacy of melatonin to prevent and/or treat bone loss as described [38]. Presently, there are two ongoing RCTs registered with clinicaltrials.gov – one that assesses melatonin alone on treating osteopenia entitled ‘Treatment of Osteopenia With Melatonin (MelaOst)’ (clinical trial identifier: NCT01690000) and the other assessing melatonin in combination with other micronutrients to treat osteopenia entitled ‘Melatonin-Micronutrients for Osteopenia Treatment Study (MOTS)’ (clinical trial identifier: NCT01870115). Attention needs to be given to both the timing and dosing of melatonin as very high doses of melatonin (i.e., 50 mg/kg body weight) may reduce bone remodeling, thus substantially preventing fracture healing as shown in mice [118]. Also, the effect of melatonin on the ratio of osteoblast to osteoclast activity should be analyzed as osteoblasts and osteoclasts are tightly coupled (Figs 2 and 3), and melatonin affects both the osteoblast and osteoclast. As mentioned in the review by Amstrup et al. [38] and described in the first RCT conducted, the MOPS [26], women taking melatonin nightly for 6 months demonstrated no significant changes in their overall osteoclast or osteoblast activity when compared to each of their baseline values, respectively; however, a time-dependent decrease in the ratio of their osteoclast to osteoblast activity did occur, which was not observed in women taking placebo. These findings reinforce the important fact that in order for appropriate or normal bone remodeling to occur, both osteoclasts and osteoblasts need to be in equilibrium. It would be interesting to determine whether this melatonin-induced restoration of balance between bone-resorbing osteoclasts and bone-forming osteoblasts observed in the MOPS trial, if given over long periods (>2 yr), would decrease rates of osteopenia, osteoporosis, and bone fracture. These studies and more are warranted and should be pursued.","Melatonin effects on bone: potential use for the prevention and treatment for osteopenia, osteoporosis, and periodontal disease and for use in bone-grafting proceduresmelatonin, menopause, osteoblasts, osteoclasts, osteopenia, osteoporosis, Per2An important role for melatonin in bone formation and restructuring has emerged, and studies demonstrate the multiple mechanisms for these beneficial actions. Statistical analysis shows that even with existing osteoporotic therapies, bone-related disease, and mortality are on the rise, creating a huge financial burden for societies worldwide. These findings suggest that novel alternatives need to be developed to either prevent or reverse bone loss to combat osteoporosis-related fractures. The focus of this review describes melatonin’s role in bone physiology and discusses how disruption of melatonin rhythms by light exposure at night, shift work, and disease can adversely impact on bone. The signal transduction mechanisms underlying osteoblast and osteoclast differentiation and coupling with one another are discussed with a focus on how melatonin, through the regulation of RANKL and osteoprotegerin synthesis and release from osteoblasts, can induce osteoblastogenesis while inhibiting osteoclastogenesis. Also, melatonin’s free-radical scavenging and antioxidant properties of this indoleamine are discussed as yet an additional mechanism by which melatonin can maintain one’s bone health, especially oral health. The clinical use for melatonin in bone-grafting procedures, in reversing bone loss due to osteopenia and osteoporosis, and in managing periodontal disease is discussed.Additional multicentered, randomized control trials (RCTs) in heterogeneous and susceptible populations (e.g., peri- or postmenopausal women, and elderly men and women) are required to assess the efficacy of melatonin to prevent and/or treat bone loss as described [38]. Presently, there are two ongoing RCTs registered with clinicaltrials.gov – one that assesses melatonin alone on treating osteopenia entitled ‘Treatment of Osteopenia With Melatonin (MelaOst)’ (clinical trial identifier: NCT01690000) and the other assessing melatonin in combination with other micronutrients to treat osteopenia entitled ‘Melatonin-Micronutrients for Osteopenia Treatment Study (MOTS)’ (clinical trial identifier: NCT01870115). Attention needs to be given to both the timing and dosing of melatonin as very high doses of melatonin (i.e., 50 mg/kg body weight) may reduce bone remodeling, thus substantially preventing fracture healing as shown in mice [118]. Also, the effect of melatonin on the ratio of osteoblast to osteoclast activity should be analyzed as osteoblasts and osteoclasts are tightly coupled (Figs 2 and 3), and melatonin affects both the osteoblast and osteoclast. As mentioned in the review by Amstrup et al. [38] and described in the first RCT conducted, the MOPS [26], women taking melatonin nightly for 6 months demonstrated no significant changes in their overall osteoclast or osteoblast activity when compared to each of their baseline values, respectively; however, a time-dependent decrease in the ratio of their osteoclast to osteoblast activity did occur, which was not observed in women taking placebo. These findings reinforce the important fact that in order for appropriate or normal bone remodeling to occur, both osteoclasts and osteoblasts need to be in equilibrium. It would be interesting to determine whether this melatonin-induced restoration of balance between bone-resorbing osteoclasts and bone-forming osteoblasts observed in the MOPS trial, if given over long periods (>2 yr), would decrease rates of osteopenia, osteoporosis, and bone fracture. These studies and more are warranted and should be pursued.CUMULATIVE SURVIVAL PROBABILITY 107 09 08 o7 08 05 04 03 02 A. WOMEN. 0509 WEIGHT LOSS (96/7 10 og 08 07 06 Os c, WOMEN 6 7 8 8 WEIGHT FLUCTUATION (96 10 11 12:13: 14 15 012 107 09 08 o7 08 05 D. MEN 345 6 7 8 9 1011 12:13 4 15 WEIGHT FLUCTUATION (961 FIG. 2. Cumulative survival probability stratified by weight loss (%/yr) category, <0.5, 0.5-0.9, and =1.0 (A for women and B 04 6 1 8 ° 04 10 11:12:13 1415 012 TIME OF FOLLOW-UP (YEARS) 3 4 5 6 7 8 9 10 11 12 13 @ 18 formen), and by weight fluctuation (%), <3% and =3% (C for women and D for men).CUMULATIVE SURVIVAL PROBABILITY 10 os 08 08 05 o4 03 02+ 10 og 08 07 06 08 04 03 paseuncaMp: = 197 BASELINE BMD: 09 OSTEOPENA os or NORMAL 07 06 os 04 osTePoRosis 03 A. WOMEN, a .en OSTEOPOROSIS 02 0423456789 0112 1 4 15 O12 ss FHT TON DB 15 10 BONE LOSS (7 BONE LOSS (9 09 <05 08 o7 9509 06 21 os 0509 04 > 03 ©. WOMEN D. MEN 02 012345678 9011213 4 15 ‘TIME OF FOLLOW-UP (YEARS) o 1 2 3 4 5 7 8 101112 13 14 15 FIG. 1. Cumulative survival probability stratified by baseline BMD, osteoporosis, T- scores = —2.5; osteopenia, T-scores -2.4 to -1.1; and normal, T-scores = -1.0 (A for women and B for men), and by BMD change (%/yr) category, <0.5, 0.5-0.9, and =1.0 (C for women and D for men).",Medical Data Analysis,"This review article discusses the emerging role of melatonin in bone physiology and its potential as a therapeutic option for preventing or treating bone loss, particularly in conditions such as osteopenia and osteoporosis. The article highlights the multiple mechanisms by which melatonin can maintain bone health, including its ability to induce osteoblastogenesis while inhibiting osteoclastogenesis and its free-radical scavenging and antioxidant properties. The article also notes that disruption of melatonin rhythms, through exposure to light at night or shift work, can adversely impact bone health. The authors call for additional multicentered, randomized control trials to assess the efficacy of melatonin in preventing and treating bone loss.",Medical Data Analysis,"CUMULATIVE SURVIVAL PROBABILITY 107 09 08 o7 08 05 04 03 02 A. WOMEN. 0509 WEIGHT LOSS (96/7 10 og 08 07 06 Os c, WOMEN 6 7 8 8 WEIGHT FLUCTUATION (96 10 11 12:13: 14 15 012 107 09 08 o7 08 05 D. MEN 345 6 7 8 9 1011 12:13 4 15 WEIGHT FLUCTUATION (961 FIG. 2. Cumulative survival probability stratified by weight loss (%/yr) category, <0.5, 0.5-0.9, and =1.0 (A for women and B 04 6 1 8 ° 04 10 11:12:13 1415 012 TIME OF FOLLOW-UP (YEARS) 3 4 5 6 7 8 9 10 11 12 13 @ 18 formen), and by weight fluctuation (%), <3% and =3% (C for women and D for men).CUMULATIVE SURVIVAL PROBABILITY 10 os 08 08 05 o4 03 02+ 10 og 08 07 06 08 04 03 paseuncaMp: = 197 BASELINE BMD: 09 OSTEOPENA os or NORMAL 07 06 os 04 osTePoRosis 03 A. WOMEN, a .en OSTEOPOROSIS 02 0423456789 0112 1 4 15 O12 ss FHT TON DB 15 10 BONE LOSS (7 BONE LOSS (9 09 <05 08 o7 9509 06 21 os 0509 04 > 03 ©. WOMEN D. MEN 02 012345678 9011213 4 15 ‘TIME OF FOLLOW-UP (YEARS) o 1 2 3 4 5 7 8 101112 13 14 15 FIG. 1. Cumulative survival probability stratified by baseline BMD, osteoporosis, T- scores = —2.5; osteopenia, T-scores -2.4 to -1.1; and normal, T-scores = -1.0 (A for women and B for men), and by BMD change (%/yr) category, <0.5, 0.5-0.9, and =1.0 (C for women and D for men).",Medical Data Analysis
284,Comparing different supervised machine learning algorithms for disease prediction,"Machine learning, Supervised machine learning algorithm, Medical data, Disease prediction, Deep learning.","Background: Supervised machine learning algorithms have been a dominant method in the data mining field. Disease prediction using health data has recently shown a potential application area for these methods. This study aims to identify the key trends among different types of supervised machine learning algorithms, and their performance and usage for disease risk prediction. Methods: In this study, extensive research efforts were made to identify those studies that applied more than one supervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were searched for different types of search items. Thus, we selected 48 articles in total for the comparison among variants supervised machine learning algorithms for disease prediction. Results: We found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Naïve Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior accuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e., 53%. This was followed by SVM which topped in 41% of the studies it was considered. Conclusion: This study provides a wide overview of the relative performance of different variants of supervised machine learning algorithms for disease prediction. This important information of relative performance can be used to aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.","This research attempted to study comparative performances of different supervised machine learning algorithms in disease prediction. Since clinical data and research scope varies widely between disease prediction studies, a comparison was only possible when a common benchmark on the dataset and scope is established. Therefore, we only chose studies that implemented multiple machine learning methods on the same data and disease prediction for comparison. Regardless of the variations on frequency and performances, the results show the potential of these families of algorithms in the disease prediction.","Comparing different supervised machine learning algorithms for disease predictionMachine learning, Supervised machine learning algorithm, Medical data, Disease prediction, Deep learning.Background: Supervised machine learning algorithms have been a dominant method in the data mining field. Disease prediction using health data has recently shown a potential application area for these methods. This study aims to identify the key trends among different types of supervised machine learning algorithms, and their performance and usage for disease risk prediction. Methods: In this study, extensive research efforts were made to identify those studies that applied more than one supervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were searched for different types of search items. Thus, we selected 48 articles in total for the comparison among variants supervised machine learning algorithms for disease prediction. Results: We found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Naïve Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior accuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e., 53%. This was followed by SVM which topped in 41% of the studies it was considered. Conclusion: This study provides a wide overview of the relative performance of different variants of supervised machine learning algorithms for disease prediction. This important information of relative performance can be used to aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.This research attempted to study comparative performances of different supervised machine learning algorithms in disease prediction. Since clinical data and research scope varies widely between disease prediction studies, a comparison was only possible when a common benchmark on the dataset and scope is established. Therefore, we only chose studies that implemented multiple machine learning methods on the same data and disease prediction for comparison. Regardless of the variations on frequency and performances, the results show the potential of these families of algorithms in the disease prediction.SPINE BMD (g/cm*2) WHOLE BODY BMC (kg) AGE (YRS) FIG. 1. Bone mass vs. age. (A and B) Spine BMD (g/em’) with age-dependent reference norms for whites (solid line) and +2 SD (dashed lines) for males (A) and females (B). (C) Whole body BMC (kg) with locally weighted regressions for males (solid line) and females (dashed line). One individual age 75 yr is omitted (symbols: male, (J; female, ©).SERUM CTX URINARY NTX (nM BCE/mM CRT) URINARY D-PYR. BSAP g 100 (mcg/L) 100 (nM/mM CRT) 1000 100 (U/L) FEMALE HYPOGONADAL + NOTHYPOGONADAL O H *o - tof +, Ep 4, D +, + sot on ee ee Saas oo* 66 5 10 15 2 2 30 35 40 5 AGE (YRS) FIG. 4. Bone turnover markers vs. age stratified by sex and hypogonadal status (hy- pogonadal, +; not hypogonadal, O) with age- dependent upper and lower limits of normal (dashed lines). One individual age 75 yr is omitted.PERCENT OF TOTAL SPINE Z-SCORES FEMUR Z-SCORES 8 r 8 é 1. 8 ° co T Beta TM Beta Tl E-beta HbH DisHbH/CS+ (n= 236) (n = 43) (n = 43) (n= 19) (n= 20) r Beta TM Beta TI E-beta HbH DisHbH/CS+ (n= 236) (n = 42) (n= 43) (n= 19) (n= 20) - Z-SCORE NORMAL Z:-1to-2 Z:<-2 FIG. 3. Prevalence of low (Z-score < —2 SD), reduced (—2 SD = Z-score < —1 SD), and normal (Z-score = —1 SD) spine and femur Z-scores by thalassemia syndrome.HISTORY OF FRACTURES. BONE AND JOINT PAIN AGE6-10YRS_ Oo B AGE 11-19YRS & AGE 20+YRS ® fo | ""| { 1 | | i | i 0 ° PERCENT OF TOTAL betaTM betaTl E-betaalp/HbH_ beta TM beta Tl. E-beta_—_alp/HbH (n= 236) (n=42) (n= 42) (n= 39) (n= 236) (n= 43) (n= 42) (n= 39) FIG. 5. Prevalence of history of fracture (A) and recent bone or joint pain (B) by thalassemia syndrome, stratified by age group (6-10 yr, O; 11-19 yr, @; 20 or more yrs, @). Point estimates and exact 95% confidence intervals are indicated.BMD Z-SCORE 1 FEMUR SPINE WHOLE BODY FIG. 2. Spine, femur, and total body BMD Z-scores with a partial-linear spline (solid line) and 95% CI (dashed lines). Individuals AGE (YRS) >40 yr of age are omitted.",Deep Learning and Machine Learning,"This study aimed to compare the performance and usage of different types of supervised machine learning algorithms for disease risk prediction. The researchers conducted extensive research and selected 48 articles from Scopus and PubMed databases that applied more than one supervised machine learning algorithm on single disease prediction. The results showed that Support Vector Machine (SVM) was the most frequently applied algorithm, followed by Naïve Bayes, while Random Forest (RF) algorithm showed superior accuracy comparatively. RF had the highest accuracy in 53% of the studies where it was applied, followed by SVM which topped in 41% of the studies. The study provides important information on the relative performance of different supervised machine learning algorithms for disease prediction, which can aid researchers in selecting an appropriate algorithm for their studies.",Deep Learning and Machine Learning,"SPINE BMD (g/cm*2) WHOLE BODY BMC (kg) AGE (YRS) FIG. 1. Bone mass vs. age. (A and B) Spine BMD (g/em’) with age-dependent reference norms for whites (solid line) and +2 SD (dashed lines) for males (A) and females (B). (C) Whole body BMC (kg) with locally weighted regressions for males (solid line) and females (dashed line). One individual age 75 yr is omitted (symbols: male, (J; female, ©).SERUM CTX URINARY NTX (nM BCE/mM CRT) URINARY D-PYR. BSAP g 100 (mcg/L) 100 (nM/mM CRT) 1000 100 (U/L) FEMALE HYPOGONADAL + NOTHYPOGONADAL O H *o - tof +, Ep 4, D +, + sot on ee ee Saas oo* 66 5 10 15 2 2 30 35 40 5 AGE (YRS) FIG. 4. Bone turnover markers vs. age stratified by sex and hypogonadal status (hy- pogonadal, +; not hypogonadal, O) with age- dependent upper and lower limits of normal (dashed lines). One individual age 75 yr is omitted.PERCENT OF TOTAL SPINE Z-SCORES FEMUR Z-SCORES 8 r 8 é 1. 8 ° co T Beta TM Beta Tl E-beta HbH DisHbH/CS+ (n= 236) (n = 43) (n = 43) (n= 19) (n= 20) r Beta TM Beta TI E-beta HbH DisHbH/CS+ (n= 236) (n = 42) (n= 43) (n= 19) (n= 20) - Z-SCORE NORMAL Z:-1to-2 Z:<-2 FIG. 3. Prevalence of low (Z-score < —2 SD), reduced (—2 SD = Z-score < —1 SD), and normal (Z-score = —1 SD) spine and femur Z-scores by thalassemia syndrome.HISTORY OF FRACTURES. BONE AND JOINT PAIN AGE6-10YRS_ Oo B AGE 11-19YRS & AGE 20+YRS ® fo | ""| { 1 | | i | i 0 ° PERCENT OF TOTAL betaTM betaTl E-betaalp/HbH_ beta TM beta Tl. E-beta_—_alp/HbH (n= 236) (n=42) (n= 42) (n= 39) (n= 236) (n= 43) (n= 42) (n= 39) FIG. 5. Prevalence of history of fracture (A) and recent bone or joint pain (B) by thalassemia syndrome, stratified by age group (6-10 yr, O; 11-19 yr, @; 20 or more yrs, @). Point estimates and exact 95% confidence intervals are indicated.BMD Z-SCORE 1 FEMUR SPINE WHOLE BODY FIG. 2. Spine, femur, and total body BMD Z-scores with a partial-linear spline (solid line) and 95% CI (dashed lines). Individuals AGE (YRS) >40 yr of age are omitted.",Medical Data Analysis
285,Deep gluteal syndrome is defned as a non‑discogenic sciatic nerve disorder with entrapment in the deep gluteal space: a systematic review,"Hip , Deep gluteal syndrome , Disease defnition , Diagnostic pathway , Systematic review.","Purpose Clinicians are not confident in diagnosing deep gluteal syndrome (DGS) because of the ambiguity of the DGS disease definition and DGS diagnostic pathway. The purpose of this systematic review was to identify the DGS disease definition, and also to define a general DGS diagnostic pathway. Methods A systematic search was performed using four electronic databases: PubMed, MEDLINE, EMBASE, and Google Scholar. In eligibility criteria, studies in which cases were explicitly diagnosed with DGS were included, whereas review articles and commentary papers were excluded. Data are presented descriptively. Results The initial literature search yielded 359 articles, of which 14 studies met the eligibility criteria, pooling 853 patients with clinically diagnosed with DGS. In this review, it was discovered that the DGS disease definition was composed of three parts: (1) non-discogenic, (2) sciatic nerve disorder, and (3) nerve entrapment in the deep gluteal space. In the diagnosis of DGS, we found five diagnostic procedures: (1) history taking, (2) physical examination, (3) imaging tests, (4) response-to-injection, and (5) nerve-specific tests (electromyography). History taking (e.g. posterior hip pain, radicular pain, and difficulty sitting for 30 min), physical examination (e.g. tenderness in deep gluteal space, pertinent positive results with seated piriformis test, and positive Pace sign), and imaging tests (e.g. pelvic radiographs, spine and pelvic magnetic resonance imaging (MRI)) were generally performed in cases clinically diagnosed with DGS. Conclusion Existing literature suggests the DGS disease definition as being a non-discogenic sciatic nerve disorder with entrapment in the deep gluteal space. Also, the general diagnostic pathway for DGS was composed of history taking (posterior hip pain, radicular pain, and difficulty sitting for 30 min), physical examination (tenderness in deep gluteal space, positive seated piriformis test, and positive Pace sign), and imaging tests (pelvic radiographs, pelvic MRI, and spine MRI). This review helps clinicians diagnose DGS with more confidence.","The DGS disease defnition was identifed as being a non-discogenic sciatic nerve disorder with entrapment in the deep gluteal space. Also, we proposed the general diagnostic pathway for DGS using history taking (posterior hip pain, radicular pain, and difficulty sitting for more than 30 min), physical examination (tenderness in deep gluteal space, pertinent positive results with seated piriformis test, and positive Pace sign), and imaging tests (pelvic radiographs, pelvic MRI, and spine MRI).","Deep gluteal syndrome is defned as a non‑discogenic sciatic nerve disorder with entrapment in the deep gluteal space: a systematic reviewHip , Deep gluteal syndrome , Disease defnition , Diagnostic pathway , Systematic review.Purpose Clinicians are not confident in diagnosing deep gluteal syndrome (DGS) because of the ambiguity of the DGS disease definition and DGS diagnostic pathway. The purpose of this systematic review was to identify the DGS disease definition, and also to define a general DGS diagnostic pathway. Methods A systematic search was performed using four electronic databases: PubMed, MEDLINE, EMBASE, and Google Scholar. In eligibility criteria, studies in which cases were explicitly diagnosed with DGS were included, whereas review articles and commentary papers were excluded. Data are presented descriptively. Results The initial literature search yielded 359 articles, of which 14 studies met the eligibility criteria, pooling 853 patients with clinically diagnosed with DGS. In this review, it was discovered that the DGS disease definition was composed of three parts: (1) non-discogenic, (2) sciatic nerve disorder, and (3) nerve entrapment in the deep gluteal space. In the diagnosis of DGS, we found five diagnostic procedures: (1) history taking, (2) physical examination, (3) imaging tests, (4) response-to-injection, and (5) nerve-specific tests (electromyography). History taking (e.g. posterior hip pain, radicular pain, and difficulty sitting for 30 min), physical examination (e.g. tenderness in deep gluteal space, pertinent positive results with seated piriformis test, and positive Pace sign), and imaging tests (e.g. pelvic radiographs, spine and pelvic magnetic resonance imaging (MRI)) were generally performed in cases clinically diagnosed with DGS. Conclusion Existing literature suggests the DGS disease definition as being a non-discogenic sciatic nerve disorder with entrapment in the deep gluteal space. Also, the general diagnostic pathway for DGS was composed of history taking (posterior hip pain, radicular pain, and difficulty sitting for 30 min), physical examination (tenderness in deep gluteal space, positive seated piriformis test, and positive Pace sign), and imaging tests (pelvic radiographs, pelvic MRI, and spine MRI). This review helps clinicians diagnose DGS with more confidence.The DGS disease defnition was identifed as being a non-discogenic sciatic nerve disorder with entrapment in the deep gluteal space. Also, we proposed the general diagnostic pathway for DGS using history taking (posterior hip pain, radicular pain, and difficulty sitting for more than 30 min), physical examination (tenderness in deep gluteal space, pertinent positive results with seated piriformis test, and positive Pace sign), and imaging tests (pelvic radiographs, pelvic MRI, and spine MRI).Grou ‘Subgroup within study Cateaneus Coleaneus Distal radius Distal radios Femoral neck Femoral nek Femoral neck Femoral neck Femoral neck Femeral neck Osteoporosis Osteoporosis Osteoporosis Osteoporosis Spine Spine Spine Total body Total besy Total bosy Total hip Total hip Total hip Total hip Trochanter Trochanter Ulvacistal radius Ulvacistal radius Overatt Szuleet Zhou et al, 2018 ‘Shen et al, 2012 ‘Szulcet al, 2008 (femoral nest) Nordstrom et al, 2010 Chen etal. 2015, Lin et at, 2014 ‘Yuet al. 2015 ‘Szulcet al., 2009 (spine) Domiciano et a.. 2016 (lumbar) Matsubara et al, 2008 ‘Szuleet a., 2009 (whole body) ‘Szuleet a., 2009 (total hip) ‘Mussolino et al. 2007 Domiciano et al., 2016 (total hip) Szuleet 12009 (wochanter) ‘Szule et al, 2009 (ultracistal radius) Hazard ratio 2.170 2170 1810 3.940 2.510 1.320 1.820 1.890 2183 1.200 1.240 1.320 1.298 1.200 0,950 1.288 2800 1.780 2.020 1.590 1.280 2.70 1.325 Statistics for each study Lower Upper limit mit Z:Value 14873108 4.018 1987 3te8 4018 oss7 2870 1.818, oss7 2870 1.518 1004 © 9680 2.990 1940 6.380 4.181 077s 2288 1,034 331 2367 3st 1.182 142432883833 12271877 8.938 11084388 3,749 20114818781 12381388 11.237 oss 2722 1.736 os 1770 9,162 o7e1 2108 0,908 1222 6418 2.436 1048-302 2.123 12990372 3.108, 0928 2,729 sez 1.813 1289 -7.208 1043 2.698 oss 2.894 093 (2.596 0990 © 2780 0.990 © 2,750 1208 1,283 ot 02 Normal BMD Low BMD Fig. 1. Forrest plot of the association between low BMD at baseline and incident cardiovascular diseases. BMD = bone mineral density.Grou Subgroup within study Any Any Hip Hip Hip Hip Hip Hip Hip Hip Vertebral Vertebral Vertebral Overall Study name Varosy et al., 2003 Cameron et al., 2010 Chiang et al., 2013 Koh et al., 2013 Kuo et al., 2015 Tsai et al., 2015 von Friesendorff et al., Xu et al., 2013 Buckens et al., 2015 Chen et al., 2013 Statistics for each study Hazard Lower Upper limit limit Z-Valuep-Value ratio 0,750 0,750 1,990 8,150 1,360 1,090 1,510 1,840 1,530 1,484 1,200 1,620 1,257 1,203 0,572 0,983 -2,081 0,572 0,983 -2,081 0,570 6,944 1,079 2,927 22,695 4,015 0,902 2,050 1,469 1,013 1,173 2,297 1,386 1,645 9,421 0,966 3,505 1,854 1,463 1,600 18,632 1,220 1,804 3,954 0,995 1,447 1,907 0,959 2,737 1,803 1,017 1,553 2,112 1,060 1,366 2,857 0,037 0,037 0,280 0,000 0,142 0,022 0,000 0,064 0,000 0,000 0,056 0,071 0,035, 0,004 Hazard ratio and 95% Cl Controls Fig. 3. Forrest plot of the association between fractures at baseline and incident cardiovascular diseases. FractureGroup by ‘Subgroup within study Caleaneus Caleaneus Caleaneus Distal radius Distal radius Distal radius Femoral neck Femoral neck Femoral neck Femoral neck Femoral neck Proximal radius Proximal radius Radiographic absorptiometry of eft hand Radiographic absorptiometry oft hand Spine Spine Total hip Tota tip Tota tip Total ip Tota hip Trochanter Trochanter Overall Browner etal, 1991 (calcaneus) Browner etal, 1993 (calcaneus) Browner etal, 1991 (distal radius) vvon der Recke et al, 1999 Wiklund et al, 2012 (femoral neck) Farhat et al., 2007 (femoral neck) Pinheiro et al, 2006 (femoral neck) Zhou et al, 2015 Browner etal, 1991 (proximal radius) Mussolino et al, 2008 Pinheiro et al, 2006 (spine) ‘Wiklund et al, 2012 (total hip) Fathat et al, 2007 (total hip) Mussolino et al, 2007 ‘Trivedi et al, 2001 Pinheiro et al, 2006 (trochanter) Hazard ratio 4,120 1,310 4,188 4,010 1,560 4,172 4,430 4,120 1,280 1,690 1,387 1,220 1,220 4,080 41,060 1,370 4,370 1,480 4,090 4,080 1,390 4,199 1310 1,310 4,163 Statistics for each study Lower Upper limit limit Z-Value p-Value 0.930 1,349 1,192 0,233 1,029 1,688 2,190 0,029 1,024 1,378 2.268 0,023 0.833 1,226 0,101 0.919 0.931 2613 1,689 0.091 0,782 1,756 0,770 0,442 1214 1/685 4.272 0,000 0.947 1,326 1,320 0,187 0,725 2260 0851 0,395 1,480 1,930 7,745 0,000 4120 1,718 3,000 0,003 1011 1,472 2077 0,088 1011 1.472 2077 0,088 0.956 1,178 1,107 0,268 0.956 1,175 1,107 0,268 0786 2388 (1,111 0.267 0.786 2388 1,111 0,267 1,098 1,996 2571 0,010 0.958 1,240 1,310 0,190 0.895 1,303 0,804 0,422 1,080 1,789 2,555 O01 1,098 1,984 2.464 0,014 0,787 2,180 1,039 0,299 0,787 2180 1,039 0.299 1,092 1,239 4,680 0,000 Hazard ratio and 95% Cl on 02 Od Increase in one SD Decrease in one SD Fig. 2. Forrest plot of the association between a decrease of one standard deviation in BMD at baseline and incident cardiovascular diseases. BMD = bone mineral density.",Medical Data Analysis,"This systematic review aimed to identify the disease definition and diagnostic pathway for deep gluteal syndrome (DGS), a condition that clinicians may struggle to diagnose due to its ambiguous definition and diagnostic pathway. Four electronic databases were searched, yielding 14 studies that met the eligibility criteria, involving 853 patients with clinically diagnosed DGS. The review identified the DGS disease definition as a non-discogenic sciatic nerve disorder with entrapment in the deep gluteal space, and proposed a general diagnostic pathway using history taking, physical examination, and imaging tests. The history taking should include posterior hip pain, radicular pain, and difficulty sitting for more than 30 minutes, while the physical examination should involve tenderness in the deep gluteal space, pertinent positive results with the seated piriformis test, and positive Pace sign. The imaging tests should include pelvic radiographs, pelvic MRI, and spine MRI. This review provides useful information for clinicians to diagnose DGS with more confidence.",Medical Data Analysis,"Grou ‘Subgroup within study Cateaneus Coleaneus Distal radius Distal radios Femoral neck Femoral nek Femoral neck Femoral neck Femoral neck Femeral neck Osteoporosis Osteoporosis Osteoporosis Osteoporosis Spine Spine Spine Total body Total besy Total bosy Total hip Total hip Total hip Total hip Trochanter Trochanter Ulvacistal radius Ulvacistal radius Overatt Szuleet Zhou et al, 2018 ‘Shen et al, 2012 ‘Szulcet al, 2008 (femoral nest) Nordstrom et al, 2010 Chen etal. 2015, Lin et at, 2014 ‘Yuet al. 2015 ‘Szulcet al., 2009 (spine) Domiciano et a.. 2016 (lumbar) Matsubara et al, 2008 ‘Szuleet a., 2009 (whole body) ‘Szuleet a., 2009 (total hip) ‘Mussolino et al. 2007 Domiciano et al., 2016 (total hip) Szuleet 12009 (wochanter) ‘Szule et al, 2009 (ultracistal radius) Hazard ratio 2.170 2170 1810 3.940 2.510 1.320 1.820 1.890 2183 1.200 1.240 1.320 1.298 1.200 0,950 1.288 2800 1.780 2.020 1.590 1.280 2.70 1.325 Statistics for each study Lower Upper limit mit Z:Value 14873108 4.018 1987 3te8 4018 oss7 2870 1.818, oss7 2870 1.518 1004 © 9680 2.990 1940 6.380 4.181 077s 2288 1,034 331 2367 3st 1.182 142432883833 12271877 8.938 11084388 3,749 20114818781 12381388 11.237 oss 2722 1.736 os 1770 9,162 o7e1 2108 0,908 1222 6418 2.436 1048-302 2.123 12990372 3.108, 0928 2,729 sez 1.813 1289 -7.208 1043 2.698 oss 2.894 093 (2.596 0990 © 2780 0.990 © 2,750 1208 1,283 ot 02 Normal BMD Low BMD Fig. 1. Forrest plot of the association between low BMD at baseline and incident cardiovascular diseases. BMD = bone mineral density.Grou Subgroup within study Any Any Hip Hip Hip Hip Hip Hip Hip Hip Vertebral Vertebral Vertebral Overall Study name Varosy et al., 2003 Cameron et al., 2010 Chiang et al., 2013 Koh et al., 2013 Kuo et al., 2015 Tsai et al., 2015 von Friesendorff et al., Xu et al., 2013 Buckens et al., 2015 Chen et al., 2013 Statistics for each study Hazard Lower Upper limit limit Z-Valuep-Value ratio 0,750 0,750 1,990 8,150 1,360 1,090 1,510 1,840 1,530 1,484 1,200 1,620 1,257 1,203 0,572 0,983 -2,081 0,572 0,983 -2,081 0,570 6,944 1,079 2,927 22,695 4,015 0,902 2,050 1,469 1,013 1,173 2,297 1,386 1,645 9,421 0,966 3,505 1,854 1,463 1,600 18,632 1,220 1,804 3,954 0,995 1,447 1,907 0,959 2,737 1,803 1,017 1,553 2,112 1,060 1,366 2,857 0,037 0,037 0,280 0,000 0,142 0,022 0,000 0,064 0,000 0,000 0,056 0,071 0,035, 0,004 Hazard ratio and 95% Cl Controls Fig. 3. Forrest plot of the association between fractures at baseline and incident cardiovascular diseases. FractureGroup by ‘Subgroup within study Caleaneus Caleaneus Caleaneus Distal radius Distal radius Distal radius Femoral neck Femoral neck Femoral neck Femoral neck Femoral neck Proximal radius Proximal radius Radiographic absorptiometry of eft hand Radiographic absorptiometry oft hand Spine Spine Total hip Tota tip Tota tip Total ip Tota hip Trochanter Trochanter Overall Browner etal, 1991 (calcaneus) Browner etal, 1993 (calcaneus) Browner etal, 1991 (distal radius) vvon der Recke et al, 1999 Wiklund et al, 2012 (femoral neck) Farhat et al., 2007 (femoral neck) Pinheiro et al, 2006 (femoral neck) Zhou et al, 2015 Browner etal, 1991 (proximal radius) Mussolino et al, 2008 Pinheiro et al, 2006 (spine) ‘Wiklund et al, 2012 (total hip) Fathat et al, 2007 (total hip) Mussolino et al, 2007 ‘Trivedi et al, 2001 Pinheiro et al, 2006 (trochanter) Hazard ratio 4,120 1,310 4,188 4,010 1,560 4,172 4,430 4,120 1,280 1,690 1,387 1,220 1,220 4,080 41,060 1,370 4,370 1,480 4,090 4,080 1,390 4,199 1310 1,310 4,163 Statistics for each study Lower Upper limit limit Z-Value p-Value 0.930 1,349 1,192 0,233 1,029 1,688 2,190 0,029 1,024 1,378 2.268 0,023 0.833 1,226 0,101 0.919 0.931 2613 1,689 0.091 0,782 1,756 0,770 0,442 1214 1/685 4.272 0,000 0.947 1,326 1,320 0,187 0,725 2260 0851 0,395 1,480 1,930 7,745 0,000 4120 1,718 3,000 0,003 1011 1,472 2077 0,088 1011 1.472 2077 0,088 0.956 1,178 1,107 0,268 0.956 1,175 1,107 0,268 0786 2388 (1,111 0.267 0.786 2388 1,111 0,267 1,098 1,996 2571 0,010 0.958 1,240 1,310 0,190 0.895 1,303 0,804 0,422 1,080 1,789 2,555 O01 1,098 1,984 2.464 0,014 0,787 2,180 1,039 0,299 0,787 2180 1,039 0.299 1,092 1,239 4,680 0,000 Hazard ratio and 95% Cl on 02 Od Increase in one SD Decrease in one SD Fig. 2. Forrest plot of the association between a decrease of one standard deviation in BMD at baseline and incident cardiovascular diseases. BMD = bone mineral density.",Medical Data Analysis
286,"Leveraging Computational Intelligence Techniques for Diagnosing Degenerative Nerve Diseases: A Comprehensive Review, Open Challenges, and Future Research Directions",degenerative nerve diseases; neurodegenerative disorder; machine learning; progressive brain diseases; diagnosis,"Degenerative nerve diseases such as Alzheimer’s and Parkinson’s diseases have always been a global issue of concern. Approximately 1/6th of the world’s population suffers from these disorders, yet there are no definitive solutions to cure these diseases after the symptoms set in. The best way to treat these disorders is to detect them at an earlier stage. Many of these diseases are genetic; this enables machine learning algorithms to give inferences based on the patient’s medical records and history. Machine learning algorithms such as deep neural networks are also critical for the early identification of degenerative nerve diseases. The significant applications of machine learning and deep learning in early diagnosis and establishing potential therapies for degenerative nerve diseases have motivated us to work on this review paper. Through this review, we covered various machine learning and deep learning algorithms and their application in the diagnosis of degenerative nerve diseases, such as Alzheimer’s disease and Parkinson’s disease. Furthermore, we also included the recent advancements in each of these models, which improved their capabilities for classifying degenerative nerve diseases. The limitations of each of these methods are also discussed. In the conclusion, we mention open research challenges and various alternative technologies, such as virtual reality and Big data analytics, which can be useful for the diagnosis of degenerative nerve diseases.","Degenerative nerve diseases have been a popular topic of interest for a very long time. These disorders are untreatable and worsen the patient’s condition with time. The only measure we can currently take is to slow down the progression of these diseases. The early diagnosis of these diseases can enable patients to practice preventive measures before the disease progresses to an uncontrollable stage; hence, the early diagnosis and progression tracking of these disorders are crucial. Through this paper, we assessed the role of machine learning and deep learning in the diagnosis of these disorders and identified various algorithms that have shown promising results when used for the diagnosis of degenerative nerve diseases. Recent developments in each of these algorithms, such as the use of a hybrid clustering and DBN to improve the scalability and accuracy of classification of patients with Parkinson’s disease or a combination of sliding window approach and k-means clustering and dynamic network analyses for diagnosing dementia patients, show the immense potential that the various ML and DL algorithms have in the early diagnosis and tracking of these diseases. In the models we surveyed, we found that artificial neural networks and deep neural networks, including deep CNNs and deep belief networks, are the most promising algorithms for detecting degenerative nerve diseases. However, we found out that the use of ML and DL is still plagued by major challenges such as a lack of available data and lower scalability and accuracy. Hence, the future direction of research on using ML and DL to diagnose degenerative nerve diseases should be to overcome these challenges. Apart from machine learning and deep learning, there are a few other promising technologies such as the Internet of Things, digital twin, quantum computing, and Big data analytics, among others, that can potentially be helpful in the diagnosis of degenerative nerve diseases in the future.","Leveraging Computational Intelligence Techniques for Diagnosing Degenerative Nerve Diseases: A Comprehensive Review, Open Challenges, and Future Research Directionsdegenerative nerve diseases; neurodegenerative disorder; machine learning; progressive brain diseases; diagnosisDegenerative nerve diseases such as Alzheimer’s and Parkinson’s diseases have always been a global issue of concern. Approximately 1/6th of the world’s population suffers from these disorders, yet there are no definitive solutions to cure these diseases after the symptoms set in. The best way to treat these disorders is to detect them at an earlier stage. Many of these diseases are genetic; this enables machine learning algorithms to give inferences based on the patient’s medical records and history. Machine learning algorithms such as deep neural networks are also critical for the early identification of degenerative nerve diseases. The significant applications of machine learning and deep learning in early diagnosis and establishing potential therapies for degenerative nerve diseases have motivated us to work on this review paper. Through this review, we covered various machine learning and deep learning algorithms and their application in the diagnosis of degenerative nerve diseases, such as Alzheimer’s disease and Parkinson’s disease. Furthermore, we also included the recent advancements in each of these models, which improved their capabilities for classifying degenerative nerve diseases. The limitations of each of these methods are also discussed. In the conclusion, we mention open research challenges and various alternative technologies, such as virtual reality and Big data analytics, which can be useful for the diagnosis of degenerative nerve diseases.Degenerative nerve diseases have been a popular topic of interest for a very long time. These disorders are untreatable and worsen the patient’s condition with time. The only measure we can currently take is to slow down the progression of these diseases. The early diagnosis of these diseases can enable patients to practice preventive measures before the disease progresses to an uncontrollable stage; hence, the early diagnosis and progression tracking of these disorders are crucial. Through this paper, we assessed the role of machine learning and deep learning in the diagnosis of these disorders and identified various algorithms that have shown promising results when used for the diagnosis of degenerative nerve diseases. Recent developments in each of these algorithms, such as the use of a hybrid clustering and DBN to improve the scalability and accuracy of classification of patients with Parkinson’s disease or a combination of sliding window approach and k-means clustering and dynamic network analyses for diagnosing dementia patients, show the immense potential that the various ML and DL algorithms have in the early diagnosis and tracking of these diseases. In the models we surveyed, we found that artificial neural networks and deep neural networks, including deep CNNs and deep belief networks, are the most promising algorithms for detecting degenerative nerve diseases. However, we found out that the use of ML and DL is still plagued by major challenges such as a lack of available data and lower scalability and accuracy. Hence, the future direction of research on using ML and DL to diagnose degenerative nerve diseases should be to overcome these challenges. Apart from machine learning and deep learning, there are a few other promising technologies such as the Internet of Things, digital twin, quantum computing, and Big data analytics, among others, that can potentially be helpful in the diagnosis of degenerative nerve diseases in the future.Osteoblast ALP, BMPs, (4) Aa 0 Melatonin (B) %s RANK een eee Bone e a on Hematopoietic precursor Pre-osteoclast Osteoclast © T Melatonin Fig. 3. Mechanisms underlying melatonin’s actions on bone and bone cells. As shown, melatonin has been shown to act at numerous sites to regulate both osteoblastogenesis and osteoclastogenesis through its ability to (A) induce hMSC differentiation into osteoblasts via MT> melatonin receptors, (B) induce osteoprotegerin (OPG) expression in preosteoblasts which would inactive RANKL, leading to a suppres- sion of osteoclastogenesis, and (C) through its free-radical scavenging and antioxidant properties, leading to a protection against radical- induced loss of osteoblasts and osteoclasts. PTH (parathyroid hormone); Type I col (type I collagen); OSP (osteopontin); BMP-2 (bone morphogenetic protein 2); ALP (alkaline phosphatase); OCN (osteocalcin); TRAP (tartrate-resistant acid phosphatase); RANKL (recep- tor activator of NFxB ligand); OPG (osteoprotegerin).(+ Mature osteoblast | ———> (ors (ALP, BMP-2, OCN) Mature osteoclast (+) w) Bone formation Bone mineralization Bone resorption 2. Mechanisms underlying osteoblast-mediated regulation of osteoclastogenesis. Normal bone remodeling relies on constant commu- nication between bone-forming osteoblasts and bone-resorbing osteoclasts. The osteoblast is the main driver for regulating osteoclast dif- ferentiation, activity, and survival. Melatonin-mediated differentiation of mesenchymal stem cells into osteoblasts can occur through myriad signaling pathways, including ERK 1/2 and Wnt/-catenin. As shown, activation of MAPKs (ERK 1/2, p-38, or JNK) by melato- nin increases osteogenic gene expression starting with the transcription factor, RUNX2. For MEK1/2/ERK1/2, their activation is dependent on the activation of MT melatonin receptors and MT,R/Gi/-arrestin/MEK/ERK1/2 scaffolds, which leads to ALP activation. Melatonin-mediated osteoblast differentiation has also been shown to occur through the Wnt/f-catenin pathway where activation of frizzled (FZD) receptors by Wnts causes a down-regulation of glycogen synthase kinase (GSK); this results in a transloca- tion of f-catenin to the nucleus where it induces osteogenic gene expression, for example, Runx2, Bmp2, osterix, and osteocalcin. Also, as shown, osteoblast-mediated stimulation of osteoclastogenesis occurs through the release of M-CSF and RANKL from osteoblast/stromal cells. Activation of RANK on mononuclear cells by RANKL promotes osteoclastogenesis by causing mononuclear cells to fuse with one another to form multinucleated osteoclasts. RANKL exerts its effects by binding RANK on the surface of osteoclast precursors and recruits the adaptor protein, TRAF6. TRAF6 binding to RANK activates multiple signaling proteins, including NFxB, Akt/PKB, mTOR and the MAPKs, JNK, ERK, and p-38 involved in osteoclast differentiation, activity, and survival. In addition to RANKL, osteoblasts also secrete OPG, which acts as a decoy receptor for RANKL inhibiting osteoclastogenesis. TGF (transforming growth factor); MEK1/2 (MAP kinase/ERK kinase); ERK 1/2 (extracellular signal-regulated kinase); JNK (c-Jun N-terminal kinases); ALP (alkaline phosphatase); Runx2 (runt-related transcription factor 2); BMP-2 (bone morphogenetic protein 2); wnt (wingless type); M-CSF (macrophage colony-stimulating factor); c-Fms (colony-stimulating factor | receptor); RANKL (receptor activator of NF«B ligand); RANK (receptor activator of NF«B; OPG (osteoprotegerin); TRAF6 (TNF receptor-associated factor); mTOR (mammalian target of rapamycin); Akt/ PKB (protein kinase B), NFATc!1 (nuclear factor of activated t cells, cytoplasmic, calcineurin-dependent 1); TRAP (tartrate-resistant acid phosphatase); CTR (calcitonin receptor); GSK (glycogen synthase kinase); R-Smad (receptor-regulated Smad); Co-Smad (common mediator Smad); OCN (osteocalcin).Fig. 1. Depicted is a schematic demonstrating the relationship between melatonin secretion and bone resorption over a 24-hr cycle. As shown, both bone resorption (dotted line) and melatonin (solid line) display a diurnal rhythm with peaks occurring during the hours of darkness (filled in rectangles). A suppression of noc- turnal melatonin levels, either through light exposure at night (LAN) or through aging, increases bone resorption. Restoring nocturnal melatonin peaks over time may protect against bone loss by suppressing bone resorption.",Medical Data Analysis,"This review paper focuses on the role of machine learning (ML) and deep learning (DL) algorithms in the early diagnosis and tracking of degenerative nerve diseases such as Alzheimer's and Parkinson's disease. The paper covers various ML and DL algorithms and their recent advancements, such as artificial neural networks, deep CNNs, and deep belief networks, that have shown promising results in diagnosing these diseases. However, the paper also discusses the limitations of these methods, such as a lack of available data and lower scalability and accuracy. The conclusion highlights the importance of early diagnosis and progression tracking of these diseases and suggests alternative technologies such as IoT, digital twin, quantum computing, and Big data analytics that can potentially be helpful in the diagnosis of degenerative nerve diseases in the future.",Medical Data Analysis,"Osteoblast ALP, BMPs, (4) Aa 0 Melatonin (B) %s RANK een eee Bone e a on Hematopoietic precursor Pre-osteoclast Osteoclast © T Melatonin Fig. 3. Mechanisms underlying melatonin’s actions on bone and bone cells. As shown, melatonin has been shown to act at numerous sites to regulate both osteoblastogenesis and osteoclastogenesis through its ability to (A) induce hMSC differentiation into osteoblasts via MT> melatonin receptors, (B) induce osteoprotegerin (OPG) expression in preosteoblasts which would inactive RANKL, leading to a suppres- sion of osteoclastogenesis, and (C) through its free-radical scavenging and antioxidant properties, leading to a protection against radical- induced loss of osteoblasts and osteoclasts. PTH (parathyroid hormone); Type I col (type I collagen); OSP (osteopontin); BMP-2 (bone morphogenetic protein 2); ALP (alkaline phosphatase); OCN (osteocalcin); TRAP (tartrate-resistant acid phosphatase); RANKL (recep- tor activator of NFxB ligand); OPG (osteoprotegerin).(+ Mature osteoblast | ———> (ors (ALP, BMP-2, OCN) Mature osteoclast (+) w) Bone formation Bone mineralization Bone resorption 2. Mechanisms underlying osteoblast-mediated regulation of osteoclastogenesis. Normal bone remodeling relies on constant commu- nication between bone-forming osteoblasts and bone-resorbing osteoclasts. The osteoblast is the main driver for regulating osteoclast dif- ferentiation, activity, and survival. Melatonin-mediated differentiation of mesenchymal stem cells into osteoblasts can occur through myriad signaling pathways, including ERK 1/2 and Wnt/-catenin. As shown, activation of MAPKs (ERK 1/2, p-38, or JNK) by melato- nin increases osteogenic gene expression starting with the transcription factor, RUNX2. For MEK1/2/ERK1/2, their activation is dependent on the activation of MT melatonin receptors and MT,R/Gi/-arrestin/MEK/ERK1/2 scaffolds, which leads to ALP activation. Melatonin-mediated osteoblast differentiation has also been shown to occur through the Wnt/f-catenin pathway where activation of frizzled (FZD) receptors by Wnts causes a down-regulation of glycogen synthase kinase (GSK); this results in a transloca- tion of f-catenin to the nucleus where it induces osteogenic gene expression, for example, Runx2, Bmp2, osterix, and osteocalcin. Also, as shown, osteoblast-mediated stimulation of osteoclastogenesis occurs through the release of M-CSF and RANKL from osteoblast/stromal cells. Activation of RANK on mononuclear cells by RANKL promotes osteoclastogenesis by causing mononuclear cells to fuse with one another to form multinucleated osteoclasts. RANKL exerts its effects by binding RANK on the surface of osteoclast precursors and recruits the adaptor protein, TRAF6. TRAF6 binding to RANK activates multiple signaling proteins, including NFxB, Akt/PKB, mTOR and the MAPKs, JNK, ERK, and p-38 involved in osteoclast differentiation, activity, and survival. In addition to RANKL, osteoblasts also secrete OPG, which acts as a decoy receptor for RANKL inhibiting osteoclastogenesis. TGF (transforming growth factor); MEK1/2 (MAP kinase/ERK kinase); ERK 1/2 (extracellular signal-regulated kinase); JNK (c-Jun N-terminal kinases); ALP (alkaline phosphatase); Runx2 (runt-related transcription factor 2); BMP-2 (bone morphogenetic protein 2); wnt (wingless type); M-CSF (macrophage colony-stimulating factor); c-Fms (colony-stimulating factor | receptor); RANKL (receptor activator of NF«B ligand); RANK (receptor activator of NF«B; OPG (osteoprotegerin); TRAF6 (TNF receptor-associated factor); mTOR (mammalian target of rapamycin); Akt/ PKB (protein kinase B), NFATc!1 (nuclear factor of activated t cells, cytoplasmic, calcineurin-dependent 1); TRAP (tartrate-resistant acid phosphatase); CTR (calcitonin receptor); GSK (glycogen synthase kinase); R-Smad (receptor-regulated Smad); Co-Smad (common mediator Smad); OCN (osteocalcin).Fig. 1. Depicted is a schematic demonstrating the relationship between melatonin secretion and bone resorption over a 24-hr cycle. As shown, both bone resorption (dotted line) and melatonin (solid line) display a diurnal rhythm with peaks occurring during the hours of darkness (filled in rectangles). A suppression of noc- turnal melatonin levels, either through light exposure at night (LAN) or through aging, increases bone resorption. Restoring nocturnal melatonin peaks over time may protect against bone loss by suppressing bone resorption.",Medical Data Analysis
287,Gut Microbiota Are Related to Parkinson’s Disease and Clinical Phenotype,microbiome; gastrointestinal dysfunction; biomarker; gut-brain-axis; non-motor symptoms.,"In the course of Parkinson’s disease (PD), the enteric nervous system (ENS) and parasympathetic nerves are amongst the structures earliest and most frequently affected by alpha-synuclein pathology. Accordingly, gastrointestinal dysfunction, in particular constipation, is an important non-motor symptom in PD and often precedes the onset of motor symptoms by years. Recent research has shown that intestinal microbiota interact with the autonomic and central nervous system via diverse pathways including the ENS and vagal nerve. The gut microbiome in PD has not been previously investigated. We compared the fecal microbiomes of 72 PD patients and 72 control subjects by pyrosequencing the V1–V3 regions of the bacterial 16S ribosomal RNA gene. Associations between clinical parameters and microbiota were analyzed using generalized linear models, taking into account potential confounders. On average, the abundance of Prevotellaceae in feces of PD patients was reduced by 77.6% as compared with controls. Relative abundance of Prevotellaceae of 6.5% or less had 86.1% sensitivity and 38.9% specificity for PD. A logistic regression classifier based on the abundance of four bacterial families and the severity of constipation identified PD patients with 66.7% sensitivity and 90.3% specificity. The relative abundance of Enterobacteriaceae was positively associated with the severity of postural instability and gait difficulty. These findings suggest that the intestinal microbiome is altered in PD and is related to motor phenotype. Further studies are warranted to elucidate the temporal and causal relationships between gut microbiota and PD and the suitability of the microbiome as a biomarker.","Our findings shed new light on previous reports regarding gastrointestinal involvement in PD. Investigating whether high abundance of Prevotellaceae has protective effects against PD or whether low abundance is rather an indicator of disturbed mucosal barrier function will be important. Although very sensitive, low Prevotellaceae levels alone are not specific for PD. Inclusion of other bacterial families may increase accuracy, and further exploring the potential of fecal microbiome analysis as a biomarker for PD seems worthwhile. Further studies may elucidate the temporal and causal relationships between gut microbiota and PD and the mechanisms involved.","Gut Microbiota Are Related to Parkinson’s Disease and Clinical Phenotypemicrobiome; gastrointestinal dysfunction; biomarker; gut-brain-axis; non-motor symptoms.In the course of Parkinson’s disease (PD), the enteric nervous system (ENS) and parasympathetic nerves are amongst the structures earliest and most frequently affected by alpha-synuclein pathology. Accordingly, gastrointestinal dysfunction, in particular constipation, is an important non-motor symptom in PD and often precedes the onset of motor symptoms by years. Recent research has shown that intestinal microbiota interact with the autonomic and central nervous system via diverse pathways including the ENS and vagal nerve. The gut microbiome in PD has not been previously investigated. We compared the fecal microbiomes of 72 PD patients and 72 control subjects by pyrosequencing the V1–V3 regions of the bacterial 16S ribosomal RNA gene. Associations between clinical parameters and microbiota were analyzed using generalized linear models, taking into account potential confounders. On average, the abundance of Prevotellaceae in feces of PD patients was reduced by 77.6% as compared with controls. Relative abundance of Prevotellaceae of 6.5% or less had 86.1% sensitivity and 38.9% specificity for PD. A logistic regression classifier based on the abundance of four bacterial families and the severity of constipation identified PD patients with 66.7% sensitivity and 90.3% specificity. The relative abundance of Enterobacteriaceae was positively associated with the severity of postural instability and gait difficulty. These findings suggest that the intestinal microbiome is altered in PD and is related to motor phenotype. Further studies are warranted to elucidate the temporal and causal relationships between gut microbiota and PD and the suitability of the microbiome as a biomarker.Our findings shed new light on previous reports regarding gastrointestinal involvement in PD. Investigating whether high abundance of Prevotellaceae has protective effects against PD or whether low abundance is rather an indicator of disturbed mucosal barrier function will be important. Although very sensitive, low Prevotellaceae levels alone are not specific for PD. Inclusion of other bacterial families may increase accuracy, and further exploring the potential of fecal microbiome analysis as a biomarker for PD seems worthwhile. Further studies may elucidate the temporal and causal relationships between gut microbiota and PD and the mechanisms involved.ROC curve for Support vector machine ROC curve for Support vector machine 0.8 2 2 8 & 2° 2 8 04 B 3 5 8 Ey (a) for disease = 0 02 04 0.6 08 1 0 02 04 0.6 0.8 1 False positive rate False positive rate Fig. 12 Illustration of the superior performance of the Support vector machine using ROC graphs (based on the data from Table 4) - (a) for disease names that were modelled; and (b) for validation methods that were followedPredicted class Comparing ROC curves P N 1 True 08 Positives 06 (TP) 0.4 —®— Poor —®— Good —e— Excellent 0.2 Actual class False = | Positives True positive rate 0 0.2 0.4 0.6 0.8 1 False positive rate (a) (b) Fig. 11 a The basic framework of the confusion matrix; and (b) A presentation of the ROC curveSearch terms used in this study - “disease prediction” AND “machine learning” - “disease prediction” AND “data mining” - “disease risk prediction” AND “machine learning” - “disease risk prediction” AND “data mining” Scopus PubMed (305) (83) Unique articles (336) (Computer program for searching Final collection of articles (48) Manual inspection Heart disease (23) Liver disease (1) Diabetes @ Asthma] Breast | Cerebral (1) __ [cancer (5)] infarction (1) Hemoglobin variants (1) Hypertension a) Kidney disease (1) om s disease (3) Prostate Lot er(2)]_ 0) Fig. 9 The overall data collection procedure. It also shows the number of articles considered for each diseaseArticles per year yu 2e© 6 5 4 3 2 I > EE 1999 2000 2005 2008 2010 2012 2013 2014 2015 2016 2017 2018 Fig. 8 Number of articles published in different years> Fig. 5 An illustration of the Naive Bayes algorithm. The ‘white’ circle is the new sample instance which needs to be classified either to ‘red’ class or ‘green’ classHyperplane Fig. 2 A simplified illustration of how the support vector machine works. The SVM has identified a hyperplane (actually a line) which maximises the separation between the ‘star’ and ‘circle’ classesDataset Random subset Random subset Random subset (GiassA) (Class B) (Glass) (Class B)| | Giass/A) (Class B) (Giass/A) (Class B)| | (Giass/A) (Class B) (Glass A) (Class B) Fig. 4 An illustration of a Random forest which consists of three different decision trees. Each of those three decision trees was trained using a random subset of the training dataFig. 3 An illustration of a Decision tree. Each variable (C1, C2, and C3) is represented by a circle and the decision outcomes (Class A and Class B) are shown by rectangles. In order to successfully classify a sample to a class, each branch is labelled with either True’ or ‘False’ based on the outcome value from the test of its ancestor nodeutput layer Input layer Hidden layer 1 Hidden layer 2 Fig. 7 An illustration of the artificial neural network structure with two hidden layers. The arrows connect the output of nodes from one layer to the input of nodes of another layer_ p a Diabetic =>- Underlying Underlying algorithm algorithm 3 =e =e Labelled training data (Diabetic patients) Unlabelled test data De =e E08.01 e re Non- Diabetic _ ° E08.10 # Step!: Train the algorithm using Step2: Feed the trained algorithm on labelled training data unlabelled data Fig. 1 An illustration of how supervised machine learning algorithms work to categorise diabetic and non-diabetic patients based on abstract data> Fig. 6 A simplified illustration of the K-nearest neighbour algorithm. When K = 3, the sample object (‘star’) is classified as ‘black’ since it gets more ‘vote’ from the ‘black’ class. However, for K=5 the same sample object is classified as ‘red’ since it now gets more ‘vote’ from the ‘red’ classTotal articles (336) Used more than one supervised algorithm — 55 articles t After manual inspection — 48 articles (used in this study) Used either only one supervised algorithm or other algorithms (281) Used only one supervised algorithm (155) Used other data mining algorithms (126) 20 (14%) 4 ANN +> 47 (30%) 21(15%) & DT + 24 (16%) 13 (9%) |} KNN }>| 6 (4%) 20 (14%) ke! LR >| 6 (4%) 23 (16%) }e NB > 30 (19%) 17 (12%) }e{ RF >| 15 (10%) 29 (20%) 41 SVM +> 27 (17%) Fig. 10 Composition of initially selected 329 articles with respect to the seven supervised learning algorithms",Medical Data Analysis,This study investigated the fecal microbiomes of Parkinson's disease (PD) patients and control subjects and found that the abundance of Prevotellaceae was reduced in PD patients by 77.6% on average. The study also found a positive association between the abundance of Enterobacteriaceae and the severity of postural instability and gait difficulty in PD patients. The findings suggest that the intestinal microbiome is altered in PD and is related to motor phenotype. Further studies are warranted to explore the potential of fecal microbiome analysis as a biomarker for PD and elucidate the temporal and causal relationships between gut microbiota and PD.,Medical Data Analysis,"ROC curve for Support vector machine ROC curve for Support vector machine 0.8 2 2 8 & 2° 2 8 04 B 3 5 8 Ey (a) for disease = 0 02 04 0.6 08 1 0 02 04 0.6 0.8 1 False positive rate False positive rate Fig. 12 Illustration of the superior performance of the Support vector machine using ROC graphs (based on the data from Table 4) - (a) for disease names that were modelled; and (b) for validation methods that were followedPredicted class Comparing ROC curves P N 1 True 08 Positives 06 (TP) 0.4 —®— Poor —®— Good —e— Excellent 0.2 Actual class False = | Positives True positive rate 0 0.2 0.4 0.6 0.8 1 False positive rate (a) (b) Fig. 11 a The basic framework of the confusion matrix; and (b) A presentation of the ROC curveSearch terms used in this study - “disease prediction” AND “machine learning” - “disease prediction” AND “data mining” - “disease risk prediction” AND “machine learning” - “disease risk prediction” AND “data mining” Scopus PubMed (305) (83) Unique articles (336) (Computer program for searching Final collection of articles (48) Manual inspection Heart disease (23) Liver disease (1) Diabetes @ Asthma] Breast | Cerebral (1) __ [cancer (5)] infarction (1) Hemoglobin variants (1) Hypertension a) Kidney disease (1) om s disease (3) Prostate Lot er(2)]_ 0) Fig. 9 The overall data collection procedure. It also shows the number of articles considered for each diseaseArticles per year yu 2e© 6 5 4 3 2 I > EE 1999 2000 2005 2008 2010 2012 2013 2014 2015 2016 2017 2018 Fig. 8 Number of articles published in different years> Fig. 5 An illustration of the Naive Bayes algorithm. The ‘white’ circle is the new sample instance which needs to be classified either to ‘red’ class or ‘green’ classHyperplane Fig. 2 A simplified illustration of how the support vector machine works. The SVM has identified a hyperplane (actually a line) which maximises the separation between the ‘star’ and ‘circle’ classesDataset Random subset Random subset Random subset (GiassA) (Class B) (Glass) (Class B)| | Giass/A) (Class B) (Giass/A) (Class B)| | (Giass/A) (Class B) (Glass A) (Class B) Fig. 4 An illustration of a Random forest which consists of three different decision trees. Each of those three decision trees was trained using a random subset of the training dataFig. 3 An illustration of a Decision tree. Each variable (C1, C2, and C3) is represented by a circle and the decision outcomes (Class A and Class B) are shown by rectangles. In order to successfully classify a sample to a class, each branch is labelled with either True’ or ‘False’ based on the outcome value from the test of its ancestor nodeutput layer Input layer Hidden layer 1 Hidden layer 2 Fig. 7 An illustration of the artificial neural network structure with two hidden layers. The arrows connect the output of nodes from one layer to the input of nodes of another layer_ p a Diabetic =>- Underlying Underlying algorithm algorithm 3 =e =e Labelled training data (Diabetic patients) Unlabelled test data De =e E08.01 e re Non- Diabetic _ ° E08.10 # Step!: Train the algorithm using Step2: Feed the trained algorithm on labelled training data unlabelled data Fig. 1 An illustration of how supervised machine learning algorithms work to categorise diabetic and non-diabetic patients based on abstract data> Fig. 6 A simplified illustration of the K-nearest neighbour algorithm. When K = 3, the sample object (‘star’) is classified as ‘black’ since it gets more ‘vote’ from the ‘black’ class. However, for K=5 the same sample object is classified as ‘red’ since it now gets more ‘vote’ from the ‘red’ classTotal articles (336) Used more than one supervised algorithm — 55 articles t After manual inspection — 48 articles (used in this study) Used either only one supervised algorithm or other algorithms (281) Used only one supervised algorithm (155) Used other data mining algorithms (126) 20 (14%) 4 ANN +> 47 (30%) 21(15%) & DT + 24 (16%) 13 (9%) |} KNN }>| 6 (4%) 20 (14%) ke! LR >| 6 (4%) 23 (16%) }e NB > 30 (19%) 17 (12%) }e{ RF >| 15 (10%) 29 (20%) 41 SVM +> 27 (17%) Fig. 10 Composition of initially selected 329 articles with respect to the seven supervised learning algorithms",Deep Learning and Machine Learning
288,HIGH RESOLUTION ULTRASOUND IN POSTERIOR INTEROSSEOUS NERVE SYNDROME,arcade of Frohse; compression neuropathy; high-resolution ultrasound; posterior interosseous nerve; radial nerve; supinator muscle,"Introduction: Posterior interosseous nerve (PIN) syndrome is a rare compression neuropathy of the PIN in the region of the supinator muscle, most common by the arcade of Frohse. We aimed to specify ultrasonographic findings in patients with PIN syndrome in comparison to healthy volunteers. Methods: Ultrasound images and clinical data of 13 patients with PIN syndrome confirmed by neurological examination and electrophysiological testing were evaluated retrospectively. Anteroposterior nerve diameters measured at the arcade of Frohse were compared with those of 20 healthy volunteers. The echotexture and the presence of a caliber change of the PIN were additionally assessed. Results: Enlargement of the PIN was seen in all patients with PIN syndrome, but not in volunteers (statistically significant difference in mean diameter P < 0.05). Furthermore, edema and caliber change of the PIN were present in all patients. Conclusions: High-resolution ultrasound allows for differentiation between patients with PIN syndrome and healthy volunteers.","HRUS is an essential tool in the work-up of a patient with PIN syndrome. As the clinical and electrodiagnostic presentation of a PIN syndrome is straightforward, sonography is definitely not needed to define the syndrome itself, although it can confirm the syndrome in doubtful cases by demonstrating the typical findings of nerve compression, including fascicular loss due to edema, impaired echogenic texture, and caliber change. However, HRUS is indispensable for the diagnosis of a “primary” PIN syndrome by providing a quick, inexpensive, and reliable method to exclude other sources of PIN compression apart from the arcade of Frohse.","HIGH RESOLUTION ULTRASOUND IN POSTERIOR INTEROSSEOUS NERVE SYNDROMEarcade of Frohse; compression neuropathy; high-resolution ultrasound; posterior interosseous nerve; radial nerve; supinator muscleIntroduction: Posterior interosseous nerve (PIN) syndrome is a rare compression neuropathy of the PIN in the region of the supinator muscle, most common by the arcade of Frohse. We aimed to specify ultrasonographic findings in patients with PIN syndrome in comparison to healthy volunteers. Methods: Ultrasound images and clinical data of 13 patients with PIN syndrome confirmed by neurological examination and electrophysiological testing were evaluated retrospectively. Anteroposterior nerve diameters measured at the arcade of Frohse were compared with those of 20 healthy volunteers. The echotexture and the presence of a caliber change of the PIN were additionally assessed. Results: Enlargement of the PIN was seen in all patients with PIN syndrome, but not in volunteers (statistically significant difference in mean diameter P < 0.05). Furthermore, edema and caliber change of the PIN were present in all patients. Conclusions: High-resolution ultrasound allows for differentiation between patients with PIN syndrome and healthy volunteers.HRUS is an essential tool in the work-up of a patient with PIN syndrome. As the clinical and electrodiagnostic presentation of a PIN syndrome is straightforward, sonography is definitely not needed to define the syndrome itself, although it can confirm the syndrome in doubtful cases by demonstrating the typical findings of nerve compression, including fascicular loss due to edema, impaired echogenic texture, and caliber change. However, HRUS is indispensable for the diagnosis of a “primary” PIN syndrome by providing a quick, inexpensive, and reliable method to exclude other sources of PIN compression apart from the arcade of Frohse.Fig.3 Deep gluteal space definition. The most accepted definition of deep gluteal space in the included studies is as fol- lows: anterior border (posterior acetabular column), posterior border (gluteus maximus mus- cle), medial border (sacrotuber- ous ligament), lateral border (gluteal tuberosity), superior border (sciatic notch), inferior border (ischial tuberosity) Sacrotuberous ligament (medial border) Ischial tuberosity (inferior border)HPI Physical exam. Imaging Rectal ecucn [7 (Local anesthetic and corticoid) —*) Three principles of DGS aol ae ; i ccna 1. _non-discogenic Posterior hip pain Tenderness in buttock pelvic Xp a Radicular pain Positive seated piriformis test spine MRI Sciatic nerve specific exam. 3. Entrapment in deep gluteal space Worsen while sit > 30 min. Positive Pace sign pelvicMRI [> (Electromyography) Fig.4 DGS diagnostic pathway. The most prominent DGS disease examination, imaging tests, response-to-injection, and nerve-specific definition was non-discogenic entrapment of sciatic nerve in deep tests were conducted gluteal space. To confidently diagnose DGS, history taking, physicalRecords identified through database searching 359 studies EMBASE 33 MEDLINE 24 PUBMED 22 Google scholar 280 |—________+> Duplicate: 32 studies ¥v Titles assessed for eligibility 327 Studies Screening ——————— Abstracts assessed for eligibility 195 Studies y Full-text articles assessed for eligibility ase Studies Full-text articles excluded: 124 studies Reviews & commentaries: 66 studies References: 9 studies a5) No cases: 36 studies 3 Studies included in qualitative No DGS diagnosis: 9 studies re) synthesis Cadaver studies: 3 studies = 14 Studies Same cohort: 1 study Fig. 2 PRISMA flowchart. The initial literature search yielded 359 articles using four electronic databases: MEDLINE, EMBASE, PubMed, and Google Scholar. After a 3-step screening proc 14 studies met the eligibility criteria pooling 853 patients clearly diagnosed with DGSFig. 1 The number of publi- cations mentioning DGS in Google Scholar. More than 280 articles included the term “deep gluteal syndrome” and more than half of the articles have been published in the last 5 years Publications in google scholar 60 50 40 30 20 10 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 [years]",Medical Data Analysis,"This article discusses the use of high-resolution ultrasound (HRUS) to diagnose posterior interosseous nerve (PIN) syndrome, a rare compression neuropathy. The study evaluated ultrasound images and clinical data of 13 patients with PIN syndrome and 20 healthy volunteers. The results showed that HRUS allows for differentiation between patients with PIN syndrome and healthy volunteers. HRUS can confirm the syndrome in doubtful cases by demonstrating the typical findings of nerve compression. However, HRUS is also indispensable for the diagnosis of a ""primary"" PIN syndrome by providing a quick, inexpensive, and reliable method to exclude other sources of PIN compression apart from the arcade of Frohse.",Medical Data Analysis,"Fig.3 Deep gluteal space definition. The most accepted definition of deep gluteal space in the included studies is as fol- lows: anterior border (posterior acetabular column), posterior border (gluteus maximus mus- cle), medial border (sacrotuber- ous ligament), lateral border (gluteal tuberosity), superior border (sciatic notch), inferior border (ischial tuberosity) Sacrotuberous ligament (medial border) Ischial tuberosity (inferior border)HPI Physical exam. Imaging Rectal ecucn [7 (Local anesthetic and corticoid) —*) Three principles of DGS aol ae ; i ccna 1. _non-discogenic Posterior hip pain Tenderness in buttock pelvic Xp a Radicular pain Positive seated piriformis test spine MRI Sciatic nerve specific exam. 3. Entrapment in deep gluteal space Worsen while sit > 30 min. Positive Pace sign pelvicMRI [> (Electromyography) Fig.4 DGS diagnostic pathway. The most prominent DGS disease examination, imaging tests, response-to-injection, and nerve-specific definition was non-discogenic entrapment of sciatic nerve in deep tests were conducted gluteal space. To confidently diagnose DGS, history taking, physicalRecords identified through database searching 359 studies EMBASE 33 MEDLINE 24 PUBMED 22 Google scholar 280 |—________+> Duplicate: 32 studies ¥v Titles assessed for eligibility 327 Studies Screening ——————— Abstracts assessed for eligibility 195 Studies y Full-text articles assessed for eligibility ase Studies Full-text articles excluded: 124 studies Reviews & commentaries: 66 studies References: 9 studies a5) No cases: 36 studies 3 Studies included in qualitative No DGS diagnosis: 9 studies re) synthesis Cadaver studies: 3 studies = 14 Studies Same cohort: 1 study Fig. 2 PRISMA flowchart. The initial literature search yielded 359 articles using four electronic databases: MEDLINE, EMBASE, PubMed, and Google Scholar. After a 3-step screening proc 14 studies met the eligibility criteria pooling 853 patients clearly diagnosed with DGSFig. 1 The number of publi- cations mentioning DGS in Google Scholar. More than 280 articles included the term “deep gluteal syndrome” and more than half of the articles have been published in the last 5 years Publications in google scholar 60 50 40 30 20 10 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 [years]",Medical Data Analysis
289,New research advances of facial expression recognition,"Face recognition
,
Feature extraction
,
Facial features
,
Image motion analysis
,
Face detection
,
Nonlinear optics
,
Optical sensors
,
Data mining
,
Image sequences","As an important part of the technology for human-machine interface, facial expression recognition (FER) draws much attention recently and numerous methods are proposed. In this survey we present the facial expression recognition research and characteristics which focus on the development in the past five years. A new classification method of feature extraction is proposed in this paper, through which we can realize the development trend of feature extraction methods. Currently, the automatic and multi-scale methods of feature extraction are widely used. With the rapid development of the technology of three-dimensional model, some novel methods are used to feature extraction. At the same time, improved expression classification methods (ANN, SVM, HMM) are emphasized. Lastly, we demonstrate the state and possible challenges of the technology of FER and provide some valuable advices about the development trend of FER","Analysis of facial expression is an intriguing problem which humans solve with quite an apparent ease. But much work has to be done in the field of automatic facial expression, such as:

It is unrealistic in these application domains that frontal view image of faces without hair, glasses and young people without permanent wrinkles.

It is unrealistic that all facial expression can be displayed by the six basic emotion categories. Yet, none of these systems performs quantification of the facial action codes.

More work has to be done in the field of automatic facial expression interpretation with regard to the integration of other communication channels such as voice and gesture.

There are still much work to do in order that the system can analyze both static images and image sequences.

The technology of facial expression recognition can be applied greatly and has enormous market potential, such as, safety driving assistance, safety monitoring, interactive distance education and so on. The research of it has great significance and it needs to be further researched for a long time.","New research advances of facial expression recognitionFace recognition
,
Feature extraction
,
Facial features
,
Image motion analysis
,
Face detection
,
Nonlinear optics
,
Optical sensors
,
Data mining
,
Image sequencesAs an important part of the technology for human-machine interface, facial expression recognition (FER) draws much attention recently and numerous methods are proposed. In this survey we present the facial expression recognition research and characteristics which focus on the development in the past five years. A new classification method of feature extraction is proposed in this paper, through which we can realize the development trend of feature extraction methods. Currently, the automatic and multi-scale methods of feature extraction are widely used. With the rapid development of the technology of three-dimensional model, some novel methods are used to feature extraction. At the same time, improved expression classification methods (ANN, SVM, HMM) are emphasized. Lastly, we demonstrate the state and possible challenges of the technology of FER and provide some valuable advices about the development trend of FERAnalysis of facial expression is an intriguing problem which humans solve with quite an apparent ease. But much work has to be done in the field of automatic facial expression, such as:

It is unrealistic in these application domains that frontal view image of faces without hair, glasses and young people without permanent wrinkles.

It is unrealistic that all facial expression can be displayed by the six basic emotion categories. Yet, none of these systems performs quantification of the facial action codes.

More work has to be done in the field of automatic facial expression interpretation with regard to the integration of other communication channels such as voice and gesture.

There are still much work to do in order that the system can analyze both static images and image sequences.

The technology of facial expression recognition can be applied greatly and has enormous market potential, such as, safety driving assistance, safety monitoring, interactive distance education and so on. The research of it has great significance and it needs to be further researched for a long time.2013: 1.6% | [ 2022:4.1% 2014: 1.8% —— 2024: 12.2% 2015: 4.1 So 2016: sa 2017: 2.4% 2018: 21.1% —— 2020: 26.8% 2019: 22.8% M2022 M2021 | 2020 M2019 M2018 M2017 2016 | 2015 M2014 M2013 Figure 2. The distribution of the reviewed papers by year.Figure 1. PRISMA flow diagram for the selection process of research articles used in this review: computational intelligence techniques for diagnosing degenerative nerve diseases.Internet of ( Everything cd Wa : / Future Research _ \= Directions y a > Figure 7. Future research directions in degenerative nerve disease diagnosis.Increasing Dataset Size and Imputing Missing Values Predicting ignii Degenerative Open Challenges - Oe Nerve Diseases i i fomimaging Degenerative Nerve Degenerative Data in Real- Diseases Diagnosis Dataset time Figure 6. Open challenges in degenerative nerve disease diagnosis.Restricted Boltzmann Deep Belief Network Machines Deep Convolutional Neural Networks Recurrent Neural Networks Deep Neural Networks Figure 5. Deep learning models used for degenerative nerve disease diagnosis in this review as a tree illustration. Generative lagnosis Parkinson’s Disease eee) 7 =r Deep Learning Methods Di Long Short-Term Memory Degenerative Nerve Diseases Other Degenerative Nerve DiseasesGenerative Naive Bayes Neural Networks Support Vector Alzheimer’s Disease Dementia liagnosis Degenerative Nerve Diseases Di Semi-supervised Co-tre ig Learning Classification Figure 4. Machine learning models used for degenerative nerve disease diagnosis in this review: a tree illustration.1.1. Contributions of this Review 3.1, Dementia 3.2. Parkinson's disease 3. 3. Huntington's disease 3.4. Altheimer’s disease 3.5. Amyotrophic Lateral Sclerosis 3.6. Friedreich Ataxia 3.7. Spinal Muscular Atrophy 3. Common Degenerative Nerve Diseases 4.1. Machine Learning Techniques 4.2. Limitations - Machine Learning Techniques 4,3. Inferences — Machine Learning Techniques Leveraging Computational 4.4. Deep Learning Models Intelligence Techniques for 4. Computational Intelligence Techniques| for Diagnosing Degenerative Nerve Dizenosing pec Neve amen eee 4.6. Inferences - Deep Learning Models 4.7. Other Computational Intelligence Models 4.5. Limitations - Deep Learning Models Dataset size and Imputing Missing Values 5.2. Designing a Bias Free Degenerative Nerve Disease 5. Open Challenges - Degenerative Nerve Diseases Dataset Diagnosis 5.3. Privacy Preservation of Patients 5.4, Predicting Degenerative Nerve Disease from Imaging Data in Real-time. 6.1. Internet of Everything 6. Future Research Directions - Degenerative 6.2. Ubiquitous Computing Nerve Diseases Diagnosis 6.3. Augmented Reality (AR) and Virtual Reality (VR) 6.4, Robots and Machine Co-creativity 6.5. Big Data Analytics 6.6. Quantum Computing 6.7. Digital Twin 6.8. Privacy Enhancing Computation Figure 3. General structure of this review article.",Facial Emotion Recognition,"The article discusses the growing interest in facial expression recognition (FER) as a crucial aspect of human-machine interface technology, and presents an overview of FER research and characteristics over the past five years. The article proposes a new classification method of feature extraction, which emphasizes the automatic and multi-scale methods currently in use, as well as novel methods utilizing three-dimensional models. The article also emphasizes the need for improved expression classification methods and outlines possible challenges and future directions for FER development, including the integration of other communication channels and the analysis of static images and image sequences. The article concludes that FER has significant market potential in various applications, and further research is needed to advance the technology.



",Object and Sentiment Recognition,"2013: 1.6% | [ 2022:4.1% 2014: 1.8% —— 2024: 12.2% 2015: 4.1 So 2016: sa 2017: 2.4% 2018: 21.1% —— 2020: 26.8% 2019: 22.8% M2022 M2021 | 2020 M2019 M2018 M2017 2016 | 2015 M2014 M2013 Figure 2. The distribution of the reviewed papers by year.Figure 1. PRISMA flow diagram for the selection process of research articles used in this review: computational intelligence techniques for diagnosing degenerative nerve diseases.Internet of ( Everything cd Wa : / Future Research _ \= Directions y a > Figure 7. Future research directions in degenerative nerve disease diagnosis.Increasing Dataset Size and Imputing Missing Values Predicting ignii Degenerative Open Challenges - Oe Nerve Diseases i i fomimaging Degenerative Nerve Degenerative Data in Real- Diseases Diagnosis Dataset time Figure 6. Open challenges in degenerative nerve disease diagnosis.Restricted Boltzmann Deep Belief Network Machines Deep Convolutional Neural Networks Recurrent Neural Networks Deep Neural Networks Figure 5. Deep learning models used for degenerative nerve disease diagnosis in this review as a tree illustration. Generative lagnosis Parkinson’s Disease eee) 7 =r Deep Learning Methods Di Long Short-Term Memory Degenerative Nerve Diseases Other Degenerative Nerve DiseasesGenerative Naive Bayes Neural Networks Support Vector Alzheimer’s Disease Dementia liagnosis Degenerative Nerve Diseases Di Semi-supervised Co-tre ig Learning Classification Figure 4. Machine learning models used for degenerative nerve disease diagnosis in this review: a tree illustration.1.1. Contributions of this Review 3.1, Dementia 3.2. Parkinson's disease 3. 3. Huntington's disease 3.4. Altheimer’s disease 3.5. Amyotrophic Lateral Sclerosis 3.6. Friedreich Ataxia 3.7. Spinal Muscular Atrophy 3. Common Degenerative Nerve Diseases 4.1. Machine Learning Techniques 4.2. Limitations - Machine Learning Techniques 4,3. Inferences — Machine Learning Techniques Leveraging Computational 4.4. Deep Learning Models Intelligence Techniques for 4. Computational Intelligence Techniques| for Diagnosing Degenerative Nerve Dizenosing pec Neve amen eee 4.6. Inferences - Deep Learning Models 4.7. Other Computational Intelligence Models 4.5. Limitations - Deep Learning Models Dataset size and Imputing Missing Values 5.2. Designing a Bias Free Degenerative Nerve Disease 5. Open Challenges - Degenerative Nerve Diseases Dataset Diagnosis 5.3. Privacy Preservation of Patients 5.4, Predicting Degenerative Nerve Disease from Imaging Data in Real-time. 6.1. Internet of Everything 6. Future Research Directions - Degenerative 6.2. Ubiquitous Computing Nerve Diseases Diagnosis 6.3. Augmented Reality (AR) and Virtual Reality (VR) 6.4, Robots and Machine Co-creativity 6.5. Big Data Analytics 6.6. Quantum Computing 6.7. Digital Twin 6.8. Privacy Enhancing Computation Figure 3. General structure of this review article.",Medical Data Analysis
290,A Review on Facial Expression Recognition: Feature Extraction and Classification,":Dynamic image , sequence facial expression , facial expression classification , facial feature extraction , human–computer interaction , static images","Facial expression recognition (FER) is currently a very active research topic in the fields of computer vision, pattern recognition, artificial intelligence, and has drawn extensive attentions owing to its potential applications to natural human–computer interaction (HCI), human emotion analysis, interactive video, image indexing and retrieval, etc. This paper is a survey of FER addressing the most two important aspects of designing an FER system. The first one is facial feature extraction for static images and dynamic image sequences. The second one is facial expression classification. Conclusions and future work are finally discussed in the last section of this survey.","In conclusion, Facial Expression Recognition (FER) has gained significant attention and has become a prominent research topic in various fields such as computer vision, pattern recognition, and artificial intelligence. Its potential applications in natural human-computer interaction, human emotion analysis, and image indexing and retrieval make it an exciting area of research. This survey paper has presented an overview of FER, focusing on the crucial aspects of facial feature extraction and facial expression classification. The survey has highlighted various techniques used for feature extraction and expression classification, and their strengths and limitations. The future work in FER includes integration with other communication channels and the analysis of static images and image sequences, which would enhance the accuracy and usability of FER systems. Overall, this survey provides valuable insights into the current state of FER and offers suggestions for future research in this area.","A Review on Facial Expression Recognition: Feature Extraction and Classification:Dynamic image , sequence facial expression , facial expression classification , facial feature extraction , human–computer interaction , static imagesFacial expression recognition (FER) is currently a very active research topic in the fields of computer vision, pattern recognition, artificial intelligence, and has drawn extensive attentions owing to its potential applications to natural human–computer interaction (HCI), human emotion analysis, interactive video, image indexing and retrieval, etc. This paper is a survey of FER addressing the most two important aspects of designing an FER system. The first one is facial feature extraction for static images and dynamic image sequences. The second one is facial expression classification. Conclusions and future work are finally discussed in the last section of this survey.In conclusion, Facial Expression Recognition (FER) has gained significant attention and has become a prominent research topic in various fields such as computer vision, pattern recognition, and artificial intelligence. Its potential applications in natural human-computer interaction, human emotion analysis, and image indexing and retrieval make it an exciting area of research. This survey paper has presented an overview of FER, focusing on the crucial aspects of facial feature extraction and facial expression classification. The survey has highlighted various techniques used for feature extraction and expression classification, and their strengths and limitations. The future work in FER includes integration with other communication channels and the analysis of static images and image sequences, which would enhance the accuracy and usability of FER systems. Overall, this survey provides valuable insights into the current state of FER and offers suggestions for future research in this area.20% * ° 3 io % 5 8 a ° = ° 3 8 4x4 —_ 3 ° $ é * 20%4 + * ¥ 8 - im Parkinson Control FIG. 1. Box plots showing the distributions of Prevotellaceae abundance in both study groups. Black horizontal lines indicate the median values and the boxes around them delineate the IQR. Whiskers extend to the highest value within 1.5 IQR of the upper quartile. Circles represent out- liers beyond the whisker limit and asterisks represent extreme outliers beyond 3 IQR of the upper quartile. High levels of Prevotellaceae were rare in the PD group whereas low levels were found in both groups. Median [IQR]: Parkinson 0.16% [0.00%-1.66%]; Control: 0.77% [0.00%- 18.18%]",Facial Emotion Recognition,"Facial Expression Recognition (FER) is a highly active area of research in computer vision, pattern recognition, and artificial intelligence. FER has gained extensive attention due to its potential applications in natural human-computer interaction, human emotion analysis, and image indexing and retrieval. This survey paper provides an overview of FER, emphasizing two crucial aspects of designing an FER system, namely facial feature extraction and facial expression classification. The survey discusses various techniques used for feature extraction and expression classification, and their strengths and limitations. Additionally, future work in FER includes integrating with other communication channels and analyzing static images and image sequences to enhance the accuracy and usability of FER systems. Overall, this survey offers valuable insights into the current state of FER and presents suggestions for future research in this field.",Object and Sentiment Recognition,20% * ° 3 io % 5 8 a ° = ° 3 8 4x4 —_ 3 ° $ é * 20%4 + * ¥ 8 - im Parkinson Control FIG. 1. Box plots showing the distributions of Prevotellaceae abundance in both study groups. Black horizontal lines indicate the median values and the boxes around them delineate the IQR. Whiskers extend to the highest value within 1.5 IQR of the upper quartile. Circles represent out- liers beyond the whisker limit and asterisks represent extreme outliers beyond 3 IQR of the upper quartile. High levels of Prevotellaceae were rare in the PD group whereas low levels were found in both groups. Median [IQR]: Parkinson 0.16% [0.00%-1.66%]; Control: 0.77% [0.00%- 18.18%],Medical Data Analysis
291,Facial Expression Recognition Using Facial Movement Features,"Feature extraction
,
Face recognition
,
Training
,
Human factors
,
Shape analysis","Facial expression is an important channel for human communication and can be applied in many real applications. One critical step for facial expression recognition (FER) is to accurately extract emotional features. Current approaches on FER in static images have not fully considered and utilized the features of facial element and muscle movements, which represent static and dynamic, as well as geometric and appearance characteristics of facial expressions. This paper proposes an approach to solve this limitation using ""salient” distance features, which are obtained by extracting patch-based 3D Gabor features, selecting the ""salient” patches, and performing patch matching operations. The experimental results demonstrate high correct recognition rate (CRR), significant performance improvements due to the consideration of facial element and muscle movements, promising results under face registration errors, and fast processing time. Comparison with the state-of-the-art performance confirms that the proposed approach achieves the highest CRR on the JAFFE database and is among the top performers on the Cohn-Kanade (CK) database.","This paper explores the issue of facial expression recognition using facial movement features. The effectiveness of the proposed approach is testified by the recognition performance, computational time, and comparison with the state-of-the-art performance. The experimental results also demonstrate significant performance improvements due to the consideration of facial movement features and promising performance under face registration errors.

The results indicate that patch-based Gabor features show a better performance over point-based Gabor features in terms of extracting regional features, keeping the position information, achieving a better recognition performance, and requiring a less number. Different emotions have different “salient” areas; however, the majority of these areas are distributed around mouth and eyes. In addition, these “salient” areas for each emotion seem to not be influenced by the choice of using point-based or using patch-based features. The “salient” patches are distributed across all scales with an emphasis on the higher scales. For both the JAFFE and CK databases, DL2 performs the best among four distances. As for emotion, anger contributes most to the misrecognition. The JAFFE database requires larger sizes of patches than the CK database to keep useful information.

The proposed approach can be potentially applied into many applications, such as patient state detection, driver fatigue monitoring, and intelligent tutoring system. In our future work, we will extend our approach to a video-based FER system by combining patch-based Gabor features with motion information in multiframes. Recent progress on action recognition [47] and face recognition [48] has laid a foundation for using both appearance and motion features.","Facial Expression Recognition Using Facial Movement FeaturesFeature extraction
,
Face recognition
,
Training
,
Human factors
,
Shape analysisFacial expression is an important channel for human communication and can be applied in many real applications. One critical step for facial expression recognition (FER) is to accurately extract emotional features. Current approaches on FER in static images have not fully considered and utilized the features of facial element and muscle movements, which represent static and dynamic, as well as geometric and appearance characteristics of facial expressions. This paper proposes an approach to solve this limitation using ""salient” distance features, which are obtained by extracting patch-based 3D Gabor features, selecting the ""salient” patches, and performing patch matching operations. The experimental results demonstrate high correct recognition rate (CRR), significant performance improvements due to the consideration of facial element and muscle movements, promising results under face registration errors, and fast processing time. Comparison with the state-of-the-art performance confirms that the proposed approach achieves the highest CRR on the JAFFE database and is among the top performers on the Cohn-Kanade (CK) database.This paper explores the issue of facial expression recognition using facial movement features. The effectiveness of the proposed approach is testified by the recognition performance, computational time, and comparison with the state-of-the-art performance. The experimental results also demonstrate significant performance improvements due to the consideration of facial movement features and promising performance under face registration errors.

The results indicate that patch-based Gabor features show a better performance over point-based Gabor features in terms of extracting regional features, keeping the position information, achieving a better recognition performance, and requiring a less number. Different emotions have different “salient” areas; however, the majority of these areas are distributed around mouth and eyes. In addition, these “salient” areas for each emotion seem to not be influenced by the choice of using point-based or using patch-based features. The “salient” patches are distributed across all scales with an emphasis on the higher scales. For both the JAFFE and CK databases, DL2 performs the best among four distances. As for emotion, anger contributes most to the misrecognition. The JAFFE database requires larger sizes of patches than the CK database to keep useful information.

The proposed approach can be potentially applied into many applications, such as patient state detection, driver fatigue monitoring, and intelligent tutoring system. In our future work, we will extend our approach to a video-based FER system by combining patch-based Gabor features with motion information in multiframes. Recent progress on action recognition [47] and face recognition [48] has laid a foundation for using both appearance and motion features.FIGURE 7. Longitudinal ultrasound scan in a patient with poste- rior interosseous nerve (PIN) syndrome with an obvious caliber change and impaired fascicular structure due to edema of the PIN (arrow). Arcade of Frohse (arrowhead).0.35 0.25- PIN diameter 0.15 0.05 Healthy volunteers Patients PIN syndrome FIGURE 6. Diagram comparing the posterior interosseous nerve (PIN) diameters of healthy volunteers and patients. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com].FIGURE 4. Transverse ultrasound scan in a patient with poste- rior interosseous nerve (PIN) syndrome depicting an enlarged, hypoechogenic PIN (arrow).FIGURE 2. Transverse ultrasound scan of the bifurcation of the radial nerve into the posterior interosseous nerve (arrowhead) and superficial branch (arrow).0.35 ° 0.3 Bd 0.25 + a 3 0.2 + o a 5 o = ° Seog —_ 2 CC = = 01 0.05 Single patient FIGURE 5. Single anteroposterior diameters of the posterior in- terosseous nerve (PIN) of all patients (in centimeters). [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com].FIGURE 1. Transverse ultrasound scan at the dorsolateral as- pect of the distal upper arm depicting a normal radial nerve (arrow). H, humerus; L, lateral triceps muscle; M, medial triceps muscle.",Facial Emotion Recognition,"The paper proposes an approach to improve the accuracy of facial expression recognition (FER) in static images by considering facial element and muscle movements using salient distance features. The approach uses patch-based 3D Gabor features to extract features, select ""salient"" patches, and perform patch matching operations. The proposed approach shows high correct recognition rate (CRR), significant performance improvements, promising results under face registration errors, and fast processing time. The approach can potentially be applied in various real-world applications such as patient state detection and driver fatigue monitoring. The future work involves extending the approach to video-based FER systems by combining patch-based Gabor features with motion information.",Object and Sentiment Recognition,"FIGURE 7. Longitudinal ultrasound scan in a patient with poste- rior interosseous nerve (PIN) syndrome with an obvious caliber change and impaired fascicular structure due to edema of the PIN (arrow). Arcade of Frohse (arrowhead).0.35 0.25- PIN diameter 0.15 0.05 Healthy volunteers Patients PIN syndrome FIGURE 6. Diagram comparing the posterior interosseous nerve (PIN) diameters of healthy volunteers and patients. [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com].FIGURE 4. Transverse ultrasound scan in a patient with poste- rior interosseous nerve (PIN) syndrome depicting an enlarged, hypoechogenic PIN (arrow).FIGURE 2. Transverse ultrasound scan of the bifurcation of the radial nerve into the posterior interosseous nerve (arrowhead) and superficial branch (arrow).0.35 ° 0.3 Bd 0.25 + a 3 0.2 + o a 5 o = ° Seog —_ 2 CC = = 01 0.05 Single patient FIGURE 5. Single anteroposterior diameters of the posterior in- terosseous nerve (PIN) of all patients (in centimeters). [Color figure can be viewed in the online issue, which is available at wileyonlinelibrary.com].FIGURE 1. Transverse ultrasound scan at the dorsolateral as- pect of the distal upper arm depicting a normal radial nerve (arrow). H, humerus; L, lateral triceps muscle; M, medial triceps muscle.",Medical Data Analysis
292,Facial Expression Recognition: A Survey,Facial Expression Recognition(FER)LBPLDPLGCHOG ;,"Automatic facial expression recognition system has many applications including, but not limited to, human behavior understanding, detection of mental disorders, and synthetic human expressions. Two popular methods utilized mostly in the literature for the automatic FER systems are based on geometry and appearance. Even though there is lots of research using static images, the research is still going on for the development of new methods which would be quiet easy in computation and would have less memory usage as compared to previous methods. This paper presents a quick survey of facial expression recognition. A comparative study is also carried out using various feature extraction techniques on JAFFE dataset.","Facial Expression recognition has increasing application areas and requires more accurate and reliable FER
system. This paper has presented a survey on facial expression recognition. Recent feature extraction techniques are
covered along with comparison. The research is still going on (i) to increase the accuracy rate of predicting the
expressions, (ii) to have applications based on dynamic images/sequence of images/videos, (iii) to handle the
occlusion. ","Facial Expression Recognition: A SurveyFacial Expression Recognition(FER)LBPLDPLGCHOG ;Automatic facial expression recognition system has many applications including, but not limited to, human behavior understanding, detection of mental disorders, and synthetic human expressions. Two popular methods utilized mostly in the literature for the automatic FER systems are based on geometry and appearance. Even though there is lots of research using static images, the research is still going on for the development of new methods which would be quiet easy in computation and would have less memory usage as compared to previous methods. This paper presents a quick survey of facial expression recognition. A comparative study is also carried out using various feature extraction techniques on JAFFE dataset.Facial Expression recognition has increasing application areas and requires more accurate and reliable FER
system. This paper has presented a survey on facial expression recognition. Recent feature extraction techniques are
covered along with comparison. The research is still going on (i) to increase the accuracy rate of predicting the
expressions, (ii) to have applications based on dynamic images/sequence of images/videos, (iii) to handle the
occlusion. ",Deep Learning and Machine Learning,"In summary, automatic facial expression recognition has many applications, and there are two popular methods based on geometry and appearance. While there has been much research using static images, the development of new methods that are computationally efficient and have less memory usage is ongoing. This paper provides a survey of facial expression recognition and a comparative study of feature extraction techniques using the JAFFE dataset. The goal is to increase the accuracy rate, develop applications based on dynamic images, and handle occlusion.",Deep Learning and Machine Learning,,Object Recognition
293,A 3D facial expression database for facial behavior research,"Face recognition
,
Humans
,
Shape
,
Magnetic heads
,
Information analysis
,
Image databases
,
Video sequences
,
Facial animation
,
Space exploration
,
Pattern recognition","Traditionally, human facial expressions have been studied using either 2D static images or 2D video sequences. The 2D-based analysis is incapable of handing large pose variations. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition using 3D range data has been reported. A primary factor for preventing such research is the lack of a publicly available 3D facial expression database. In this paper, we present a newly developed 3D facial expression database, which includes both prototypical 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database available for the research community, with the ultimate goal of fostering the research on affective computing and increasing the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions. The new database can be a valuable resource for algorithm assessment, comparison and evaluation","The paper presents a new 3D facial expression database, which includes both 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This database is the first of its kind and can be used for research on affective computing and for increasing the general understanding of facial behavior and fine 3D structure in human facial expressions. The database can serve as a valuable resource for algorithm assessment, comparison, and evaluation.","A 3D facial expression database for facial behavior researchFace recognition
,
Humans
,
Shape
,
Magnetic heads
,
Information analysis
,
Image databases
,
Video sequences
,
Facial animation
,
Space exploration
,
Pattern recognitionTraditionally, human facial expressions have been studied using either 2D static images or 2D video sequences. The 2D-based analysis is incapable of handing large pose variations. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition using 3D range data has been reported. A primary factor for preventing such research is the lack of a publicly available 3D facial expression database. In this paper, we present a newly developed 3D facial expression database, which includes both prototypical 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database available for the research community, with the ultimate goal of fostering the research on affective computing and increasing the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions. The new database can be a valuable resource for algorithm assessment, comparison and evaluationThe paper presents a new 3D facial expression database, which includes both 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This database is the first of its kind and can be used for research on affective computing and for increasing the general understanding of facial behavior and fine 3D structure in human facial expressions. The database can serve as a valuable resource for algorithm assessment, comparison, and evaluation.",Facial Emotion Recognition,"This paper discusses the limitations of studying human facial expressions using 2D images or videos and the lack of research on 3D facial expression recognition using 3D range data. The authors present a newly developed 3D facial expression database, including both 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects, which is the first of its kind and can serve as a valuable resource for algorithm assessment, comparison, and evaluation. The ultimate goal of the database is to foster research on affective computing and increase the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions.



",Object and Sentiment Recognition,,Object Recognition
294,Facial Expression Recognition on Static Images,"Virtual reality, Facial expression recognition, Convolution neural networks.","Facial expression recognition (FER) is currently one of the most attractive and also the most challenging topics in the computer vision and artificial fields. FER applications are ranging from medical treatment, virtual reality, to driver fatigue surveillance, and many other human-machine interaction systems. Benefit from the recent success of deep learning techniques, especially the invention of convolution neural networks (CNN), various end-to-end deep learning-based FER systems have been proposed in the past few years. However, overfitting caused by a lack of training data is still the big challenge that almost all deep FER systems have to put into a concern to achieve high-performance accuracy. In this paper, we are going to build a FER model to recognize eight commons emotions: neutral, happiness, sadness, surprise, fear, disgust, anger, and contempt on the AffectNet dataset. In order to mitigate the effect of small training data, which is prone to overfitting, we proposed a thoughtful transfer learning framework. Specifically, we fine-tuning ResNet-50 model, which is pre-trained on ImageNet dataset for object detection task, on the AffectNet dataset to recognize eight above mentioned face emotions. Experiment results demonstrate the effectiveness of our proposed FER model.","Over the past several years, deep CNN has significantly boosted the performance of many systems in visual classification field. Likewise, deep CNNs have increasingly been used in FER task on static images. However, the lack of relatively enough emotion training data causes overfitting issues for almost FER system. In this paper, we proposed a deep transfer learning framework to mitigate the overfitting problem, which improves the discriminative power of our FER model. Nonetheless, the recognition accuracy is still not meet our expectation. This may be caused by the high imbalance of AffectNet data where the number of sample of major classes (e.g., Happy and Sad) are much larger than that of minor classes (e.g., Contempt and Disgust). Using different models and training processes as well as apply recently successful loss functions to further boost the performance is still in our future work.

","Facial Expression Recognition on Static ImagesVirtual reality, Facial expression recognition, Convolution neural networks.Facial expression recognition (FER) is currently one of the most attractive and also the most challenging topics in the computer vision and artificial fields. FER applications are ranging from medical treatment, virtual reality, to driver fatigue surveillance, and many other human-machine interaction systems. Benefit from the recent success of deep learning techniques, especially the invention of convolution neural networks (CNN), various end-to-end deep learning-based FER systems have been proposed in the past few years. However, overfitting caused by a lack of training data is still the big challenge that almost all deep FER systems have to put into a concern to achieve high-performance accuracy. In this paper, we are going to build a FER model to recognize eight commons emotions: neutral, happiness, sadness, surprise, fear, disgust, anger, and contempt on the AffectNet dataset. In order to mitigate the effect of small training data, which is prone to overfitting, we proposed a thoughtful transfer learning framework. Specifically, we fine-tuning ResNet-50 model, which is pre-trained on ImageNet dataset for object detection task, on the AffectNet dataset to recognize eight above mentioned face emotions. Experiment results demonstrate the effectiveness of our proposed FER model.Over the past several years, deep CNN has significantly boosted the performance of many systems in visual classification field. Likewise, deep CNNs have increasingly been used in FER task on static images. However, the lack of relatively enough emotion training data causes overfitting issues for almost FER system. In this paper, we proposed a deep transfer learning framework to mitigate the overfitting problem, which improves the discriminative power of our FER model. Nonetheless, the recognition accuracy is still not meet our expectation. This may be caused by the high imbalance of AffectNet data where the number of sample of major classes (e.g., Happy and Sad) are much larger than that of minor classes (e.g., Contempt and Disgust). Using different models and training processes as well as apply recently successful loss functions to further boost the performance is still in our future work.

",Facial Emotion Recognition,"The paper discusses the challenges and applications of facial expression recognition (FER) and proposes a deep transfer learning framework to recognize eight common emotions on the AffectNet dataset. The proposed framework fine-tunes a pre-trained ResNet-50 model on the AffectNet dataset to mitigate the overfitting problem caused by a lack of training data. The experiment results show the effectiveness of the proposed FER model, although the recognition accuracy is still not satisfactory due to the high imbalance of AffectNet data. The future work includes using different models and training processes and applying successful loss functions to further improve the performance of the FER model.",Object and Sentiment Recognition,,Object Recognition
295,Image based Static Facial Expression Recognition with Multiple Deep Network Learning,"facial expression recognition, static images, ensemble learning, convolutional neural networks, Emotion Recognition in the Wild Challenge, SFEW 2.0 dataset, face detection, classification module","We report our image based static facial expression recognition method for the Emotion Recognition in the Wild Challenge (EmotiW) 2015. We focus on the sub-challenge of the SFEW 2.0 dataset, where one seeks to automatically classify a set of static images into 7 basic emotions. The proposed method contains a face detection module based on the ensemble of three state-of-the-art face detectors, followed by a classification module with the ensemble of multiple deep convolutional neural networks (CNN). Each CNN model is initialized randomly and pre-trained on a larger dataset provided by the Facial Expression Recognition (FER) Challenge 2013. The pre-trained models are then fine-tuned on the training set of SFEW 2.0. To combine multiple CNN models, we present two schemes for learning the ensemble weights of the network responses: by minimizing the log likelihood loss, and by minimizing the hinge loss. Our proposed method generates state-of-the-art result on the FER dataset. It also achieves 55.96% and 61.29% respectively on the validation and test set of SFEW 2.0, surpassing the challenge baseline of 35.96% and 39.13% with significant gains.","The paper presents a method for static facial expression recognition in the Emotion Recognition in the Wild Challenge (EmotiW) 2015. The proposed method involves a face detection module based on an ensemble of three face detectors and a classification module based on an ensemble of multiple deep convolutional neural networks (CNN). Each CNN model is randomly initialized and pre-trained on a larger dataset provided by the Facial Expression Recognition (FER) Challenge 2013, and then fine-tuned on the training set of SFEW 2.0. To combine multiple CNN models, the paper presents two schemes for learning the ensemble weights of the network responses: by minimizing the log likelihood loss and by minimizing the hinge loss. The proposed method achieved state-of-the-art results on the FER dataset and outperformed the challenge baseline on the SFEW 2.0 dataset, achieving 55.96% and 61.29% accuracy on the validation and test sets, respectively.","Image based Static Facial Expression Recognition with Multiple Deep Network Learningfacial expression recognition, static images, ensemble learning, convolutional neural networks, Emotion Recognition in the Wild Challenge, SFEW 2.0 dataset, face detection, classification moduleWe report our image based static facial expression recognition method for the Emotion Recognition in the Wild Challenge (EmotiW) 2015. We focus on the sub-challenge of the SFEW 2.0 dataset, where one seeks to automatically classify a set of static images into 7 basic emotions. The proposed method contains a face detection module based on the ensemble of three state-of-the-art face detectors, followed by a classification module with the ensemble of multiple deep convolutional neural networks (CNN). Each CNN model is initialized randomly and pre-trained on a larger dataset provided by the Facial Expression Recognition (FER) Challenge 2013. The pre-trained models are then fine-tuned on the training set of SFEW 2.0. To combine multiple CNN models, we present two schemes for learning the ensemble weights of the network responses: by minimizing the log likelihood loss, and by minimizing the hinge loss. Our proposed method generates state-of-the-art result on the FER dataset. It also achieves 55.96% and 61.29% respectively on the validation and test set of SFEW 2.0, surpassing the challenge baseline of 35.96% and 39.13% with significant gains.The paper presents a method for static facial expression recognition in the Emotion Recognition in the Wild Challenge (EmotiW) 2015. The proposed method involves a face detection module based on an ensemble of three face detectors and a classification module based on an ensemble of multiple deep convolutional neural networks (CNN). Each CNN model is randomly initialized and pre-trained on a larger dataset provided by the Facial Expression Recognition (FER) Challenge 2013, and then fine-tuned on the training set of SFEW 2.0. To combine multiple CNN models, the paper presents two schemes for learning the ensemble weights of the network responses: by minimizing the log likelihood loss and by minimizing the hinge loss. The proposed method achieved state-of-the-art results on the FER dataset and outperformed the challenge baseline on the SFEW 2.0 dataset, achieving 55.96% and 61.29% accuracy on the validation and test sets, respectively.",Facial Emotion Recognition,"This paper presents a method for facial expression recognition in static images for the Emotion Recognition in the Wild Challenge (EmotiW) 2015. The method involves a face detection module based on an ensemble of three face detectors and a classification module based on an ensemble of multiple deep convolutional neural networks (CNN). The CNN models are pre-trained on the Facial Expression Recognition (FER) Challenge 2013 dataset and fine-tuned on the training set of SFEW 2.0. The paper presents two schemes for combining the CNN models: minimizing the log likelihood loss and minimizing the hinge loss. The proposed method achieved state-of-the-art results on the FER dataset and outperformed the challenge baseline on the SFEW 2.0 dataset, achieving 55.96% and 61.29% accuracy on the validation and test sets, respectively.",Object and Sentiment Recognition,,Object Recognition
296,"Facial Expression Recognition with Local Binary Patterns
and Linear Programming","facial expression recognition, local binary patterns, linear programming, classification, JAFFE database, texture classification, binary tree tournament scheme, performance","—In this work, we propose a novel approach to recognize facial expressions from static images. First,
the local binary patterns (LBP) are used to efficiently represent the facial images and then the linear programming (LP) technique is adopted to classify seven facial expressions—anger, disgust, fear, happiness, sadness,
surprise, and neutral. Experimental results demonstrate an average recognition accuracy of 93.8% in the JAFFE
database, which outperforms the rates of all other reported methods in the same database.","How to get a proper combination for face representation and classification is crucial for facial expression
recognition. The combination of the local binary pattern operator and the linear programming technique is
one solution to this question. The Local Binary Pattern
operator, which has demonstrated excellent performance in texture classification and face recognition, is
used in our method to describe a face efficiently for expression recognition. Then, 21 classifiers are produced based on linear programming technique and
classification is implemented with a binary tree tournament scheme. Experimental results demonstrate that
our method performs better than other methods on the
JAFFE database. These preliminary results are promising and it is of interest to experiment with different
facial expression databases.","Facial Expression Recognition with Local Binary Patterns
and Linear Programmingfacial expression recognition, local binary patterns, linear programming, classification, JAFFE database, texture classification, binary tree tournament scheme, performance—In this work, we propose a novel approach to recognize facial expressions from static images. First,
the local binary patterns (LBP) are used to efficiently represent the facial images and then the linear programming (LP) technique is adopted to classify seven facial expressions—anger, disgust, fear, happiness, sadness,
surprise, and neutral. Experimental results demonstrate an average recognition accuracy of 93.8% in the JAFFE
database, which outperforms the rates of all other reported methods in the same database.How to get a proper combination for face representation and classification is crucial for facial expression
recognition. The combination of the local binary pattern operator and the linear programming technique is
one solution to this question. The Local Binary Pattern
operator, which has demonstrated excellent performance in texture classification and face recognition, is
used in our method to describe a face efficiently for expression recognition. Then, 21 classifiers are produced based on linear programming technique and
classification is implemented with a binary tree tournament scheme. Experimental results demonstrate that
our method performs better than other methods on the
JAFFE database. These preliminary results are promising and it is of interest to experiment with different
facial expression databases.",Facial Emotion Recognition,"The paper proposes a novel approach to facial expression recognition using local binary patterns (LBP) for face representation and linear programming (LP) for classification. The method achieved an average recognition accuracy of 93.8% on the JAFFE database, outperforming all other reported methods. The paper highlights the importance of finding the right combination of face representation and classification for facial expression recognition and suggests that the combination of LBP and LP is a promising approach.",Object and Sentiment Recognition,,Object Recognition
297,A Model Based Method for Automatic Facial Expression Recognition,"Facial Expression
Face Image
Emotional Expression
Hide Neuron
Input Neuron","Automatic facial expression recognition is a research topic with interesting applications in the field of human-computer interaction, psychology and product marketing. The classification accuracy for an automatic system which uses static images as input is however largely limited by the image quality, lighting conditions and the orientation of the depicted face. These problems can be partially overcome by using a holistic model based approach called the Active Appearance Model. A system will be described that can classify expressions from one of the emotional categories joy, anger, sadness, surprise, fear and disgust with remarkable accuracy. It is also able to detect smaller, local facial features based on minimal muscular movements described by the Facial Action Coding System (FACS). Finally, we show how the system can be used for expression analysis and synthesis.","In conclusion, automatic facial expression recognition is an important research area with applications in various fields. However, the accuracy of classification for systems that use static images as input is limited by several factors such as lighting conditions, image quality and facial orientation. One approach to partially overcome these problems is the Active Appearance Model. A system is described that can classify expressions accurately from six emotional categories and can detect small, local facial features based on minimal muscular movements described by FACS. The system can also be used for expression analysis and synthesis.","A Model Based Method for Automatic Facial Expression RecognitionFacial Expression
Face Image
Emotional Expression
Hide Neuron
Input NeuronAutomatic facial expression recognition is a research topic with interesting applications in the field of human-computer interaction, psychology and product marketing. The classification accuracy for an automatic system which uses static images as input is however largely limited by the image quality, lighting conditions and the orientation of the depicted face. These problems can be partially overcome by using a holistic model based approach called the Active Appearance Model. A system will be described that can classify expressions from one of the emotional categories joy, anger, sadness, surprise, fear and disgust with remarkable accuracy. It is also able to detect smaller, local facial features based on minimal muscular movements described by the Facial Action Coding System (FACS). Finally, we show how the system can be used for expression analysis and synthesis.In conclusion, automatic facial expression recognition is an important research area with applications in various fields. However, the accuracy of classification for systems that use static images as input is limited by several factors such as lighting conditions, image quality and facial orientation. One approach to partially overcome these problems is the Active Appearance Model. A system is described that can classify expressions accurately from six emotional categories and can detect small, local facial features based on minimal muscular movements described by FACS. The system can also be used for expression analysis and synthesis.",Facial Emotion Recognition,"Automatic facial expression recognition is a significant research topic that has practical applications in human-computer interaction, psychology, and product marketing. However, classification accuracy for systems using static images as input is limited by factors such as image quality, lighting conditions, and facial orientation. The Active Appearance Model can partially overcome these issues, and a system is described that can accurately classify expressions from six emotional categories and detect small, local facial features based on FACS. The system can also be used for expression analysis and synthesis.",Object and Sentiment Recognition,,Object Recognition
298,Deep Learning for Emotion Recognition on Small Datasets using Transfer Learning,"Emotion recognition, static facial expression, transfer learning, deep CNN, fine-tuning, cascading, validation set, test set, challenge baseline.","This paper presents the techniques employed in our team's submissions to the 2015 Emotion Recognition in the Wild contest, for the sub-challenge of Static Facial Expression Recognition in the Wild. The objective of this sub-challenge is to classify the emotions expressed by the primary human subject in static images extracted from movies. We follow a transfer learning approach for deep Convolutional Neural Network (CNN) architectures. Starting from a network pre-trained on the generic ImageNet dataset, we perform supervised fine-tuning on the network in a two-stage process, first on datasets relevant to facial expressions, followed by the contest's dataset. Experimental results show that this cascading fine-tuning approach achieves better results, compared to a single stage fine-tuning with the combined datasets. Our best submission exhibited an overall accuracy of 48.5% in the validation set and 55.6% in the test set, which compares favorably to the respective 35.96% and 39.13% of the challenge baseline.","This paper describes the techniques used by the authors to participate in the 2015 Emotion Recognition in the Wild contest, specifically for the sub-challenge of Static Facial Expression Recognition in the Wild. The approach used is a transfer learning method for deep Convolutional Neural Network (CNN) architectures, where the network is fine-tuned in two stages, first on relevant facial expression datasets and then on the contest's dataset. The results show that this approach outperforms a single-stage fine-tuning with combined datasets. The best submission achieved an accuracy of 48.5% in the validation set and 55.6% in the test set, which is better than the challenge baseline.","Deep Learning for Emotion Recognition on Small Datasets using Transfer LearningEmotion recognition, static facial expression, transfer learning, deep CNN, fine-tuning, cascading, validation set, test set, challenge baseline.This paper presents the techniques employed in our team's submissions to the 2015 Emotion Recognition in the Wild contest, for the sub-challenge of Static Facial Expression Recognition in the Wild. The objective of this sub-challenge is to classify the emotions expressed by the primary human subject in static images extracted from movies. We follow a transfer learning approach for deep Convolutional Neural Network (CNN) architectures. Starting from a network pre-trained on the generic ImageNet dataset, we perform supervised fine-tuning on the network in a two-stage process, first on datasets relevant to facial expressions, followed by the contest's dataset. Experimental results show that this cascading fine-tuning approach achieves better results, compared to a single stage fine-tuning with the combined datasets. Our best submission exhibited an overall accuracy of 48.5% in the validation set and 55.6% in the test set, which compares favorably to the respective 35.96% and 39.13% of the challenge baseline.This paper describes the techniques used by the authors to participate in the 2015 Emotion Recognition in the Wild contest, specifically for the sub-challenge of Static Facial Expression Recognition in the Wild. The approach used is a transfer learning method for deep Convolutional Neural Network (CNN) architectures, where the network is fine-tuned in two stages, first on relevant facial expression datasets and then on the contest's dataset. The results show that this approach outperforms a single-stage fine-tuning with combined datasets. The best submission achieved an accuracy of 48.5% in the validation set and 55.6% in the test set, which is better than the challenge baseline.",Facial Emotion Recognition,"This paper describes the techniques used by the authors in the 2015 Emotion Recognition in the Wild contest for the sub-challenge of Static Facial Expression Recognition in the Wild. The approach used is a transfer learning method for deep Convolutional Neural Network (CNN) architectures, where the network is fine-tuned in two stages, first on relevant facial expression datasets and then on the contest's dataset. The results show that this approach outperforms a single-stage fine-tuning with combined datasets. The best submission achieved an accuracy of 48.5% in the validation set and 55.6% in the test set, which is better than the challenge baseline.",Object and Sentiment Recognition,,Object Recognition
299,Emotion recognition of static and dynamic faces in autism spectrum disorder,"AutismAsperger's disorder, Facial emotion recognition, Dynamic faces.","There is substantial evidence for facial emotion recognition (FER) deficits in autism spectrum disorder (ASD). The extent of this impairment, however, remains unclear, and there is some suggestion that clinical groups might benefit from the use of dynamic rather than static images. High-functioning individuals with ASD (n = 36) and typically developing controls (n = 36) completed a computerised FER task involving static and dynamic expressions of the six basic emotions. The ASD group showed poorer overall performance in identifying anger and disgust and were disadvantaged by dynamic (relative to static) stimuli when presented with sad expressions. Among both groups, however, dynamic stimuli appeared to improve recognition of anger. This research provides further evidence of specific impairment in the recognition of negative emotions in ASD, but argues against any broad advantages associated with the use of dynamic displays.","Individuals with ASD were impaired in the recognition of facial expressions showing anger and disgust. Although there were no overall group differences for the recognition of sad facial expressions, people with ASD were disadvantaged by the presentation of dynamic (compared with static) images of sad facial expressions. Lastly, although there were group differences for recognition of expressions of anger (with the ASD group performing worse), here there was an overall advantage for both groups when recognising dynamic images showing facial expressions of anger.

Impaired recognition of anger and disgust in ASD is somewhat consistent with studies involving similar patient groups (i.e., high-functioning ASD), where deficits have been found in the recognition of basic negative emotions such as fear, disgust, anger and sadness (Harms et al., Citation2010). These data further highlight that FER impairments in ASD do not extend to all emotions, but rather seem to be limited to negative emotions. From a clinical perspective, a failure to accurately recognise negative emotions in others may be substantially more disadvantageous than a failure to recognise positive emotions, and lead to greater interpersonal difficulties across a range of settings. For instance, a failure to recognise anger or disgust in another person, and a consequent failure to adjust behaviour accordingly, might exacerbate interpersonal difficulties or produce conflict across a range of settings. By contrast, if there was a failure to accurately recognise, for example, happiness, this would be unlikely to produce a similarly negative outcome. Interestingly, and despite the observed differences, there did not appear to be any differences in the pattern of errors for specific emotional expressions between the groups. There were no group differences for happy or surprise; while happy expressions were associated with a ceiling effect, that we did not detect a difference for surprise seems consistent with recent research. For instance, Law Smith et al. (Citation2010) reported deficits in recognising surprise in ASD but only at low intensities. The surprise stimuli used in the present study may have lacked the necessary subtlety to detect impairment in ASD.

While we were primarily interested in the influence of dynamic images, it appears that the use of dynamic facial expressions had relatively little effect. It is often suggested that dynamic stimuli have greater ecological validity and should be more easily recognised than static images, and this was the case, for all participants, for anger. Somewhat paradoxically, however, the presentation of dynamic images for the sad facial expression resulted in a reduction in accuracy (relative to static) among individuals with ASD (although overall group differences were not significant), suggesting that in some instances individuals with ASD might actually be disadvantaged by dynamic (relative to static) facial expressions. It is not immediately clear why individuals with ASD would be disadvantaged by a dynamic presentation for just one emotion. Neuroimaging research suggests that brain networks that process static and dynamic faces do differ somewhat (Kilts et al., Citation2003), and these different networks (or functions underlying these networks) could be differentially affected in ASD (at least in relation to the recognition of sadness). For example, structures thought to comprise the mirror neuron system, which is seemingly responsive to goal-directed and dynamic expressions of behaviour, have been shown as underactive in ASD (Dapretto et al., Citation2006). Recent research also implicates poor neural connectivity in ASD, particularly to and within prefrontal and parietal regions (Just, Cherkassky, Keller, Kana, & Minshew, Citation2007; Wicker et al., Citation2008). In this respect, functional magnetic resonance imaging (fMRI) studies of people with ASD using the current stimuli will be of great interest, as will similar studies with alternate stimulus sets. Lastly, it is possible that the duration of the dynamic images was too brief to benefit individuals with ASD (Gepner & Feron, Citation2009), although the extent to which this would apply to “high-functioning” individuals is not clear.
","Emotion recognition of static and dynamic faces in autism spectrum disorderAutismAsperger's disorder, Facial emotion recognition, Dynamic faces.There is substantial evidence for facial emotion recognition (FER) deficits in autism spectrum disorder (ASD). The extent of this impairment, however, remains unclear, and there is some suggestion that clinical groups might benefit from the use of dynamic rather than static images. High-functioning individuals with ASD (n = 36) and typically developing controls (n = 36) completed a computerised FER task involving static and dynamic expressions of the six basic emotions. The ASD group showed poorer overall performance in identifying anger and disgust and were disadvantaged by dynamic (relative to static) stimuli when presented with sad expressions. Among both groups, however, dynamic stimuli appeared to improve recognition of anger. This research provides further evidence of specific impairment in the recognition of negative emotions in ASD, but argues against any broad advantages associated with the use of dynamic displays.Individuals with ASD were impaired in the recognition of facial expressions showing anger and disgust. Although there were no overall group differences for the recognition of sad facial expressions, people with ASD were disadvantaged by the presentation of dynamic (compared with static) images of sad facial expressions. Lastly, although there were group differences for recognition of expressions of anger (with the ASD group performing worse), here there was an overall advantage for both groups when recognising dynamic images showing facial expressions of anger.

Impaired recognition of anger and disgust in ASD is somewhat consistent with studies involving similar patient groups (i.e., high-functioning ASD), where deficits have been found in the recognition of basic negative emotions such as fear, disgust, anger and sadness (Harms et al., Citation2010). These data further highlight that FER impairments in ASD do not extend to all emotions, but rather seem to be limited to negative emotions. From a clinical perspective, a failure to accurately recognise negative emotions in others may be substantially more disadvantageous than a failure to recognise positive emotions, and lead to greater interpersonal difficulties across a range of settings. For instance, a failure to recognise anger or disgust in another person, and a consequent failure to adjust behaviour accordingly, might exacerbate interpersonal difficulties or produce conflict across a range of settings. By contrast, if there was a failure to accurately recognise, for example, happiness, this would be unlikely to produce a similarly negative outcome. Interestingly, and despite the observed differences, there did not appear to be any differences in the pattern of errors for specific emotional expressions between the groups. There were no group differences for happy or surprise; while happy expressions were associated with a ceiling effect, that we did not detect a difference for surprise seems consistent with recent research. For instance, Law Smith et al. (Citation2010) reported deficits in recognising surprise in ASD but only at low intensities. The surprise stimuli used in the present study may have lacked the necessary subtlety to detect impairment in ASD.

While we were primarily interested in the influence of dynamic images, it appears that the use of dynamic facial expressions had relatively little effect. It is often suggested that dynamic stimuli have greater ecological validity and should be more easily recognised than static images, and this was the case, for all participants, for anger. Somewhat paradoxically, however, the presentation of dynamic images for the sad facial expression resulted in a reduction in accuracy (relative to static) among individuals with ASD (although overall group differences were not significant), suggesting that in some instances individuals with ASD might actually be disadvantaged by dynamic (relative to static) facial expressions. It is not immediately clear why individuals with ASD would be disadvantaged by a dynamic presentation for just one emotion. Neuroimaging research suggests that brain networks that process static and dynamic faces do differ somewhat (Kilts et al., Citation2003), and these different networks (or functions underlying these networks) could be differentially affected in ASD (at least in relation to the recognition of sadness). For example, structures thought to comprise the mirror neuron system, which is seemingly responsive to goal-directed and dynamic expressions of behaviour, have been shown as underactive in ASD (Dapretto et al., Citation2006). Recent research also implicates poor neural connectivity in ASD, particularly to and within prefrontal and parietal regions (Just, Cherkassky, Keller, Kana, & Minshew, Citation2007; Wicker et al., Citation2008). In this respect, functional magnetic resonance imaging (fMRI) studies of people with ASD using the current stimuli will be of great interest, as will similar studies with alternate stimulus sets. Lastly, it is possible that the duration of the dynamic images was too brief to benefit individuals with ASD (Gepner & Feron, Citation2009), although the extent to which this would apply to “high-functioning” individuals is not clear.
",Facial Emotion Recognition,"The study investigated the facial emotion recognition (FER) deficits in autism spectrum disorder (ASD) compared to typically developing individuals using static and dynamic expressions of the six basic emotions. The results showed that individuals with ASD were impaired in recognizing facial expressions of anger and disgust, and dynamic stimuli did not provide a significant advantage. Moreover, the study highlighted that FER impairments in ASD are limited to negative emotions and that the failure to recognize negative emotions in others may lead to greater interpersonal difficulties. The researchers suggest that neuroimaging studies of people with ASD using the current stimuli will be of great interest.",Object and Sentiment Recognition,,Object Recognition
300,Two-level attention with two-stage multi-task learning for facial emotion recognition,Facial emotion recognitionAttention mechanismMulti-task learningValence-arousal dimension,"Compared with facial emotion estimation on categorical model, dimensional emotion estimation can describe numerous emotions more accurately. Most prior works of dimensional emotion estimation only considered laboratory data and used video, speech or other multi-modal features. Compared with other modal data, static images has superiorities of accessibility, which is more conducive to the emotion estimation in real world. In this paper, a two-level attention with two-stage multi-task learning (2Att-2Mt) framework is proposed for facial emotion estimation on only static images. Firstly, the features of corresponding region (position level features) are extracted and enhanced automatically by first-level attention mechanism. Then, we utilize Bi-directional Recurrent Neural Network (Bi-RNN) with self-attention (second-level attention) to make full use of the relationship features of different layers (layer-level features) adaptively. And then, we propose a two-stage multi-task learning structure, which exploits categorical representations to ameliorate the dimensional representations and estimate valence and arousal simultaneously in view of the inherent complexity of dimensional representations and correlation of the two targets. The quantitative results conducted on AffectNet dataset show significant advancement on Concordance Correlation Coefficient(CCC) and Root Mean Square Error (RMSE), illustrating the superiority of the proposed framework. Besides, extensive comparative experiments have also fully demonstrated the effectiveness of different components (2Att and 2Mt) in our framework.","This paper proposes a new framework, called 2Att-2Mt, for accurately estimating emotions from static images. The framework uses a two-level attention mechanism and two-stage multi-task learning to extract and enhance features of corresponding regions and adaptively use relationship features of different layers. The proposed framework also exploits categorical representations to improve the estimation of valence and arousal simultaneously. The results on the AffectNet dataset show significant improvements in Concordance Correlation Coefficient and Root Mean Square Error compared to prior works, demonstrating the effectiveness of the proposed framework.","Two-level attention with two-stage multi-task learning for facial emotion recognitionFacial emotion recognitionAttention mechanismMulti-task learningValence-arousal dimensionCompared with facial emotion estimation on categorical model, dimensional emotion estimation can describe numerous emotions more accurately. Most prior works of dimensional emotion estimation only considered laboratory data and used video, speech or other multi-modal features. Compared with other modal data, static images has superiorities of accessibility, which is more conducive to the emotion estimation in real world. In this paper, a two-level attention with two-stage multi-task learning (2Att-2Mt) framework is proposed for facial emotion estimation on only static images. Firstly, the features of corresponding region (position level features) are extracted and enhanced automatically by first-level attention mechanism. Then, we utilize Bi-directional Recurrent Neural Network (Bi-RNN) with self-attention (second-level attention) to make full use of the relationship features of different layers (layer-level features) adaptively. And then, we propose a two-stage multi-task learning structure, which exploits categorical representations to ameliorate the dimensional representations and estimate valence and arousal simultaneously in view of the inherent complexity of dimensional representations and correlation of the two targets. The quantitative results conducted on AffectNet dataset show significant advancement on Concordance Correlation Coefficient(CCC) and Root Mean Square Error (RMSE), illustrating the superiority of the proposed framework. Besides, extensive comparative experiments have also fully demonstrated the effectiveness of different components (2Att and 2Mt) in our framework.This paper proposes a new framework, called 2Att-2Mt, for accurately estimating emotions from static images. The framework uses a two-level attention mechanism and two-stage multi-task learning to extract and enhance features of corresponding regions and adaptively use relationship features of different layers. The proposed framework also exploits categorical representations to improve the estimation of valence and arousal simultaneously. The results on the AffectNet dataset show significant improvements in Concordance Correlation Coefficient and Root Mean Square Error compared to prior works, demonstrating the effectiveness of the proposed framework.",Facial Emotion Recognition,"This paper proposes a new framework, called 2Att-2Mt, for accurately estimating emotions from static images. The framework uses a two-level attention mechanism and two-stage multi-task learning to extract and enhance features of corresponding regions and adaptively use relationship features of different layers. The proposed framework also exploits categorical representations to improve the estimation of valence and arousal simultaneously. The results on the AffectNet dataset show significant improvements in Concordance Correlation Coefficient and Root Mean Square Error compared to prior works, demonstrating the effectiveness of the proposed framework.",Object and Sentiment Recognition,,Object Recognition
301,"Facial Emotion Recognition in Parkinson’s Disease:
A Review and New Hypotheses","facial emotion recognition; Parkinson’s
disease; basal ganglia; dopamine; embodied simulation","Parkinson’s disease is a neurodegenerative disorder classically characterized by motor symptoms.
Among them, hypomimia affects facial expressiveness and
social communication and has a highly negative impact
on patients’ and relatives’ quality of life. Patients also
frequently experience nonmotor symptoms, including
emotional-processing impairments, leading to difficulty in
recognizing emotions from faces. Aside from its theoretical
importance, understanding the disruption of facial emotion
recognition in PD is crucial for improving quality of life for
both patients and caregivers, as this impairment is associated with heightened interpersonal difficulties. However,
studies assessing abilities in recognizing facial emotions in
PD still report contradictory outcomes. The origins of this
inconsistency are unclear, and several questions (regarding
the role of dopamine replacement therapy or the possible
consequences of hypomimia) remain unanswered. We
therefore undertook a fresh review of relevant articles
focusing on facial emotion recognition in PD to deepen current understanding of this nonmotor feature, exploring multiple significant potential confounding factors, both clinical
and methodological, and discussing probable pathophysiological mechanisms. This led us to examine recent proposals about the role of basal ganglia-based circuits in
emotion and to consider the involvement of facial mimicry
in this deficit from the perspective of embodied simulation theory.","Hypomimia has considerable repercussions on
patients’ (often perceived as bored, anxious or
cranky135-137) and relatives’ quality of life, damaging
interpersonal relationships and gradually increasing
social isolation.1,138 Patients, caregivers, and clinicians
can break this vicious cycle. It starts with awareness
of the patients’ difficulties in decoding, expressing,
and mimicking emotions, along with their attendant
social consequences. Research on these issues could
also improve medical management, as therapeutic
strategies can be adapted to patients’ symptoms, especially knowing that there are several PD subtypes with
3 separate phenotypes: mainly motor/slow progression, intermediate, and diffuse/malignant.84 Patients
with the latter are more likely to exhibit nonmotor
symptoms, including cognitive and mood disorders,
but patients with the main motor form may also
develop emotional impairments as a consequence of
impaired facial mimicry. Finally, these findings open
up a new line of inquiry into patients’ masked face
and its impact on socioemotional communication
among both patients and their caregivers","Facial Emotion Recognition in Parkinson’s Disease:
A Review and New Hypothesesfacial emotion recognition; Parkinson’s
disease; basal ganglia; dopamine; embodied simulationParkinson’s disease is a neurodegenerative disorder classically characterized by motor symptoms.
Among them, hypomimia affects facial expressiveness and
social communication and has a highly negative impact
on patients’ and relatives’ quality of life. Patients also
frequently experience nonmotor symptoms, including
emotional-processing impairments, leading to difficulty in
recognizing emotions from faces. Aside from its theoretical
importance, understanding the disruption of facial emotion
recognition in PD is crucial for improving quality of life for
both patients and caregivers, as this impairment is associated with heightened interpersonal difficulties. However,
studies assessing abilities in recognizing facial emotions in
PD still report contradictory outcomes. The origins of this
inconsistency are unclear, and several questions (regarding
the role of dopamine replacement therapy or the possible
consequences of hypomimia) remain unanswered. We
therefore undertook a fresh review of relevant articles
focusing on facial emotion recognition in PD to deepen current understanding of this nonmotor feature, exploring multiple significant potential confounding factors, both clinical
and methodological, and discussing probable pathophysiological mechanisms. This led us to examine recent proposals about the role of basal ganglia-based circuits in
emotion and to consider the involvement of facial mimicry
in this deficit from the perspective of embodied simulation theory.Hypomimia has considerable repercussions on
patients’ (often perceived as bored, anxious or
cranky135-137) and relatives’ quality of life, damaging
interpersonal relationships and gradually increasing
social isolation.1,138 Patients, caregivers, and clinicians
can break this vicious cycle. It starts with awareness
of the patients’ difficulties in decoding, expressing,
and mimicking emotions, along with their attendant
social consequences. Research on these issues could
also improve medical management, as therapeutic
strategies can be adapted to patients’ symptoms, especially knowing that there are several PD subtypes with
3 separate phenotypes: mainly motor/slow progression, intermediate, and diffuse/malignant.84 Patients
with the latter are more likely to exhibit nonmotor
symptoms, including cognitive and mood disorders,
but patients with the main motor form may also
develop emotional impairments as a consequence of
impaired facial mimicry. Finally, these findings open
up a new line of inquiry into patients’ masked face
and its impact on socioemotional communication
among both patients and their caregivers",Facial Emotion Recognition,"Parkinson's disease is a neurodegenerative disorder characterized by motor symptoms and nonmotor symptoms, including difficulties in recognizing emotions from faces. Understanding this impairment is crucial for improving the quality of life for patients and caregivers. However, studies on this topic have reported contradictory outcomes, and the origins of this inconsistency are unclear. This article undertakes a fresh review of relevant articles on facial emotion recognition in PD and explores potential confounding factors, both clinical and methodological, and probable pathophysiological mechanisms. The article also discusses the impact of hypomimia on patients' and relatives' quality of life and the importance of adapting therapeutic strategies to patients' symptoms. The findings open up a new line of inquiry into patients' masked face and its impact on socioemotional communication.",Object and Sentiment Recognition,,Object Recognition
302,"Emotion recognition through facial expression analysis
based on a neurofuzzy network","Facial expression analysis; MPEG-4 facial animation parameters; Activation evaluation emotion representation; Neurofuzzy network; Rule
extraction; Adaptation","Extracting and validating emotional cues through analysis of users’ facial expressions is of high importance for improving the level of
interaction in man machine communication systems. Extraction of appropriate facial features and consequent recognition of the user’s
emotional state that can be robust to facial expression variations among different users is the topic of this paper. Facial animation parameters
(FAPs) defined according to the ISO MPEG-4 standard are extracted by a robust facial analysis system, accompanied by appropriate
confidence measures of the estimation accuracy. A novel neurofuzzy system is then created, based on rules that have been defined through
analysis of FAP variations both at the discrete emotional space, as well as in the 2D continuous activation–evaluation one. The neurofuzzy
system allows for further learning and adaptation to specific users’ facial expression characteristics, measured though FAP estimation in real
life application of the system, using analysis by clustering of the obtained FAP values. Experimental studies with emotionally expressive
datasets, generated in the EC IST ERMIS project indicate the good performance and potential of the developed technologies.","This paper describes an emotion recognition system,
which combines psychological findings about emotion
representation with analysis and evaluation of facial
expressions. The performance of the proposed system has
been investigated with experimental real data. More
specifically, a neurofuzzy rule based system has been first
created and used to classify facial expressions using a
continuous 2D emotion space, obtaining high rates in
classification and clustering of data to quadrants of the
emotion representation space. To improve these rates for a
specific user, the initial set of rules that captured the a-priori
knowledge was then adapted via a learning procedure of the
neurofuzzy system (Tzouvaras, Stamou, & Kollias, 2004;
Wallace, Raouzaiou, Tsapatsoulis, & Kollias, 2004), so as
to capture unique expressivity instances. Future extensions
will include emotion recognition based on combined facial
and gesture analysis (Karpouzis et al., 2004; Balomenos
et al., 2004). These can provide the means to create systems
that combine analysis and synthesis of facial expressions,
for providing more expressive and friendly interactions
(Raouzaiou et al., 2004). Moreover, development of rulebased emotion recognition provides the possibility to
combine the results obtained within the framework of the
ERMIS project with current knowledge technologies, e.g. in
implementing an MPEG-4 visual ontology for emotion
recognition (Tzouvaras et al., 2004)","Emotion recognition through facial expression analysis
based on a neurofuzzy networkFacial expression analysis; MPEG-4 facial animation parameters; Activation evaluation emotion representation; Neurofuzzy network; Rule
extraction; AdaptationExtracting and validating emotional cues through analysis of users’ facial expressions is of high importance for improving the level of
interaction in man machine communication systems. Extraction of appropriate facial features and consequent recognition of the user’s
emotional state that can be robust to facial expression variations among different users is the topic of this paper. Facial animation parameters
(FAPs) defined according to the ISO MPEG-4 standard are extracted by a robust facial analysis system, accompanied by appropriate
confidence measures of the estimation accuracy. A novel neurofuzzy system is then created, based on rules that have been defined through
analysis of FAP variations both at the discrete emotional space, as well as in the 2D continuous activation–evaluation one. The neurofuzzy
system allows for further learning and adaptation to specific users’ facial expression characteristics, measured though FAP estimation in real
life application of the system, using analysis by clustering of the obtained FAP values. Experimental studies with emotionally expressive
datasets, generated in the EC IST ERMIS project indicate the good performance and potential of the developed technologies.This paper describes an emotion recognition system,
which combines psychological findings about emotion
representation with analysis and evaluation of facial
expressions. The performance of the proposed system has
been investigated with experimental real data. More
specifically, a neurofuzzy rule based system has been first
created and used to classify facial expressions using a
continuous 2D emotion space, obtaining high rates in
classification and clustering of data to quadrants of the
emotion representation space. To improve these rates for a
specific user, the initial set of rules that captured the a-priori
knowledge was then adapted via a learning procedure of the
neurofuzzy system (Tzouvaras, Stamou, & Kollias, 2004;
Wallace, Raouzaiou, Tsapatsoulis, & Kollias, 2004), so as
to capture unique expressivity instances. Future extensions
will include emotion recognition based on combined facial
and gesture analysis (Karpouzis et al., 2004; Balomenos
et al., 2004). These can provide the means to create systems
that combine analysis and synthesis of facial expressions,
for providing more expressive and friendly interactions
(Raouzaiou et al., 2004). Moreover, development of rulebased emotion recognition provides the possibility to
combine the results obtained within the framework of the
ERMIS project with current knowledge technologies, e.g. in
implementing an MPEG-4 visual ontology for emotion
recognition (Tzouvaras et al., 2004)",Facial Emotion Recognition,The paper describes a system for emotion recognition in man-machine communication. The system extracts facial animation parameters (FAPs) using a facial analysis system and uses a neurofuzzy rule-based system to classify facial expressions into a 2D emotion space with high accuracy. The system can also be adapted to individual users through a learning procedure. The paper suggests that future extensions could include combining facial and gesture analysis for more expressive interactions and implementing an MPEG-4 visual ontology for emotion recognition. Experimental studies with emotionally expressive datasets indicate the good performance and potential of the developed technologies.,Object and Sentiment Recognition,,Object Recognition
303,"EMOTION RECOGNITION USING FACIAL EXPRESSIONS WITH ACTIVE
APPEARANCE MODELS","Emotion, Facial Expression, Expression Recognition, Active Appeara","Recognizing emotion using facial expressions is a key element in human communication. In this paper we discuss a
framework for the classification of emotional states, based
on still images of the face. The technique we present involves the creation of an active appearance model (AAM)
trained on face images from a publicly available database
to represent shape and texture variation key to expression
recognition. Parameters from the AAM are used as features for a classification scheme that is able to successfully
identify faces related to the six universal emotions. The results of our study demonstrate the effectiveness of AAMs
in capturing the important facial structure for expression
identification and also help suggest a framework for future
development.","Using the AAM as a feature method has proven successful even with a simple Euclidean-distance classification
scheme. The capability of AAMs to model both the shape
and texture of faces makes them a strong tool to derive feature sets for emotion-based expression classification. It is
certainly likely that more sophisticated classifiers such as
SVMs will provide better results on this data set. Overall,
though, this initial work has shown potential for AAMs as
a feature set for expression classification.","EMOTION RECOGNITION USING FACIAL EXPRESSIONS WITH ACTIVE
APPEARANCE MODELSEmotion, Facial Expression, Expression Recognition, Active AppearaRecognizing emotion using facial expressions is a key element in human communication. In this paper we discuss a
framework for the classification of emotional states, based
on still images of the face. The technique we present involves the creation of an active appearance model (AAM)
trained on face images from a publicly available database
to represent shape and texture variation key to expression
recognition. Parameters from the AAM are used as features for a classification scheme that is able to successfully
identify faces related to the six universal emotions. The results of our study demonstrate the effectiveness of AAMs
in capturing the important facial structure for expression
identification and also help suggest a framework for future
development.Using the AAM as a feature method has proven successful even with a simple Euclidean-distance classification
scheme. The capability of AAMs to model both the shape
and texture of faces makes them a strong tool to derive feature sets for emotion-based expression classification. It is
certainly likely that more sophisticated classifiers such as
SVMs will provide better results on this data set. Overall,
though, this initial work has shown potential for AAMs as
a feature set for expression classification.",Facial Emotion Recognition,This paper proposes a framework for recognizing emotions through still images of faces using an active appearance model (AAM) trained on a public database. The AAM's parameters are used as features for a classification scheme that successfully identifies faces related to the six universal emotions. The paper demonstrates the effectiveness of AAMs in capturing important facial structure for expression identification and suggests a framework for future development. The AAM method has proven successful with a simple Euclidean-distance classification scheme and shows potential for expression classification.,Object and Sentiment Recognition,,Object Recognition
304,"Benchmarking commercial emotion detection systems using realistic
distortions of facial image datasets","Benchmarking commercial emotion detection systems using realistic
distortions of facial image datasets","Currently, there are several widely used commercial cloud-based services that attempt to recognize an individual’s emotions
based on their facial expressions. Most research into facial emotion recognition has used high-resolution, front-oriented,
full-face images. However, when images are collected in naturalistic settings (e.g., using smartphone’s frontal camera), these
images are likely to be far from ideal due to camera positioning, lighting conditions, and camera shake. The impact these
conditions have on the accuracy of commercial emotion recognition services has not been studied in full detail. To fill this
gap, we selected five prominent commercial emotion recognition systems—Amazon Rekognition, Baidu Research, Face++,
Microsoft Azure, and Affectiva—and evaluated their performance via two experiments. In Experiment 1, we compared the
systems’ accuracy at classifying images drawn from three standardized facial expression databases. In Experiment 2, we first
identified several common scenarios (e.g., partially visible face) that can lead to poor-quality pictures during smartphone
use, and manipulated the same set of images used in Experiment 1 to simulate these scenarios. We used the manipulated
images to again compare the systems’ classification performance, finding that the systems varied in how well they handled
manipulated images that simulate realistic image distortion. Based on our findings, we offer recommendations for developers
and researchers who would like to use commercial facial emotion recognition technologies in their applications.","In this study, we utilized three commonly used facial expression databases and proposed a series of manipulations to
simulate potential image quality reduction problems that
are likely to occur during real-world smartphone usage. We
conducted two experiments in order to assess the facial
emotion recognition capabilities of five popular commercial emotion recognition systems: Amazon, Baidu, Face++,
Microsoft Azure, and Affectiva. Through our experiments,
we found that each system has its own strengths and limitations under different distortion conditions (rotation, partial
face, brightness, blur, noise). Based on our findings, we offer
recommendations on how to achieve reliable facial emotion
recognition results for applications in the wild, by selecting
different systems depending on the nature of the captured
image. Finally, we recommend the use of our image manipulation methods for future testing of facial emotion recognition
system performance","Benchmarking commercial emotion detection systems using realistic
distortions of facial image datasetsBenchmarking commercial emotion detection systems using realistic
distortions of facial image datasetsCurrently, there are several widely used commercial cloud-based services that attempt to recognize an individual’s emotions
based on their facial expressions. Most research into facial emotion recognition has used high-resolution, front-oriented,
full-face images. However, when images are collected in naturalistic settings (e.g., using smartphone’s frontal camera), these
images are likely to be far from ideal due to camera positioning, lighting conditions, and camera shake. The impact these
conditions have on the accuracy of commercial emotion recognition services has not been studied in full detail. To fill this
gap, we selected five prominent commercial emotion recognition systems—Amazon Rekognition, Baidu Research, Face++,
Microsoft Azure, and Affectiva—and evaluated their performance via two experiments. In Experiment 1, we compared the
systems’ accuracy at classifying images drawn from three standardized facial expression databases. In Experiment 2, we first
identified several common scenarios (e.g., partially visible face) that can lead to poor-quality pictures during smartphone
use, and manipulated the same set of images used in Experiment 1 to simulate these scenarios. We used the manipulated
images to again compare the systems’ classification performance, finding that the systems varied in how well they handled
manipulated images that simulate realistic image distortion. Based on our findings, we offer recommendations for developers
and researchers who would like to use commercial facial emotion recognition technologies in their applications.In this study, we utilized three commonly used facial expression databases and proposed a series of manipulations to
simulate potential image quality reduction problems that
are likely to occur during real-world smartphone usage. We
conducted two experiments in order to assess the facial
emotion recognition capabilities of five popular commercial emotion recognition systems: Amazon, Baidu, Face++,
Microsoft Azure, and Affectiva. Through our experiments,
we found that each system has its own strengths and limitations under different distortion conditions (rotation, partial
face, brightness, blur, noise). Based on our findings, we offer
recommendations on how to achieve reliable facial emotion
recognition results for applications in the wild, by selecting
different systems depending on the nature of the captured
image. Finally, we recommend the use of our image manipulation methods for future testing of facial emotion recognition
system performanceNose in basal ganli-based cuit Intensity ating task iain ata Speen Sen |<] Dopamine depletion and amygdala ccutry etre et ee eames ton ent Se eee Fut ost na rg nvr ee rte tt ri ti in oh nec eee mar eer eh od 4 an What about the ear? ‘, Pte ce Fadl imix and Fn PO moereermmres | ci Sasa ees emcees ead ee sun [2] mpares Rin ro? ‘rence Se nt “Sete men mn dk oe woe scm OR ce [Negative emotions-specitcty? “ero Seascape eae err || amen crmmmncemencems perenne See shout a nh Vinca confounding factors c 4 1 Disease severity | [27 vspati/cogntie symptoms =] weosanorien | [J bopaminremiaton | er tz ns tonne || om at wt ne men a | | ama | | ates ay | Sess asa | acts mayne | | trecidanenere | Spee ern | eee | Stacie en ~ _ FIG. 1. Facial emotion recognition in Parkinson's disease: review and discussion. (1) level of impairment, (2) emotions-specifcity, (@~4) method- ‘ology, (6-6) underlying pathophysiological mechanisms with (5) recent considerations about basal ganglia-based circuits in emotions, (7) new hypothesis related to hypomimia, (8-11) potential clinical confounding factors.",Emotion detection using normal face,"The article discusses a study that evaluated the performance of five commercial emotion recognition systems (Amazon Rekognition, Baidu Research, Face++, Microsoft Azure, and Affectiva) in classifying facial expressions under different image distortion scenarios that simulate realistic image quality reduction problems likely to occur during real-world smartphone usage. The study found that each system has its own strengths and limitations and offers recommendations on how to achieve reliable facial emotion recognition results for applications in the wild, by selecting different systems depending on the nature of the captured image. Finally, the study recommends the use of their image manipulation methods for future testing of facial emotion recognition system performance.",Object and Sentiment Recognition,"Nose in basal ganli-based cuit Intensity ating task iain ata Speen Sen |<] Dopamine depletion and amygdala ccutry etre et ee eames ton ent Se eee Fut ost na rg nvr ee rte tt ri ti in oh nec eee mar eer eh od 4 an What about the ear? ‘, Pte ce Fadl imix and Fn PO moereermmres | ci Sasa ees emcees ead ee sun [2] mpares Rin ro? ‘rence Se nt “Sete men mn dk oe woe scm OR ce [Negative emotions-specitcty? “ero Seascape eae err || amen crmmmncemencems perenne See shout a nh Vinca confounding factors c 4 1 Disease severity | [27 vspati/cogntie symptoms =] weosanorien | [J bopaminremiaton | er tz ns tonne || om at wt ne men a | | ama | | ates ay | Sess asa | acts mayne | | trecidanenere | Spee ern | eee | Stacie en ~ _ FIG. 1. Facial emotion recognition in Parkinson's disease: review and discussion. (1) level of impairment, (2) emotions-specifcity, (@~4) method- ‘ology, (6-6) underlying pathophysiological mechanisms with (5) recent considerations about basal ganglia-based circuits in emotions, (7) new hypothesis related to hypomimia, (8-11) potential clinical confounding factors.",Object Recognition
305,"Facial expression megamix: Tests of dimensional and
category accounts of emotion recognition","Facial expressions, emotion recognition, categorical perception, morphed images, prototype, discrete categories, reaction time, forced-choice procedure.","We report four experiments investigating the perception of photographic quality continua
of interpolated (‘morphed’) facial expressions derived from prototypes of the 6 emotions in
the Ekman and Friesen (1976) series (happiness, surprise, fear, sadness, disgust and anger).
In Experiment 1, morphed images made from all possible pairwise combinations of
expressions were presented in random order; subjects identified these as belonging to
distinct expression categories corresponding to the prototypes at each end of the relevant
continuum. This result was replicated in Experiment 2, which also included morphs made
from a prototype with a neutral expression, and allowed ‘neutral’ as a response category.
These findings are inconsistent with the view that facial expressions are recognised by
locating them along two underlying dimensions, since such a view predicts that at least
some transitions between categories should involve neutral regions or identification as a
different emotion. Instead, they suggest that facial expressions of basic emotions are
recognised by their fit to discrete categories. Experiment 3 used continua involving 6
emotions to demonstrate best discrimination of pairs of stimuli falling across category
boundaries; this provides further evidence of categorical perception of facial expressions of
emotion. However, in both Experiment 1 and Experiment 2, reaction time data showed that
increasing distance from the prototype had a definite cost on ability to identify emotion in
the resulting morphed face. Moreover, Experiment 4 showed that subjects had some insight
into which emotions were blended to create specific morphed images. Hence, categorical
perception effects were found even though subjects were sensitive to physical properties of
these morphed facial expressions. We suggest that rapid classification of prototypes and
better across boundary discriminability reflect the underlying organisation of human
categorisation abilities.","The results of Experiment 4 show that when a prototype only contributes 10%
to a particular morph, subjects cannot reliably identify which prototype that 10%
has come from, but that for prototypes contributing 30% they begin to be able to
do this. This is interesting because in Experiment 1 all 30% morphs were
consistently identified as belonging to the category contributing 70%. However,
we noted that distance from the prototype affected reaction times for classifying
morphed expressions in Experiment 1, which implied that people can see
differences between these morphed expressions, even when they assign them to a
common category. Experiment 4 has taken this point a step further, by showing
not only that people are sensitive to differences between expressions which they
assign to a common overall category, but that they can have some insight into the
nature of these differences.
However, one should not overestimate the extent of this insight, which was
elicited by a forced-choice procedure, where subjects were made to select 3 labels
for each face. Etcoff and Magee (1992) had included a ‘free naming’ task in which
subjects were shown each picture and encouraged to describe the expression using
whatever words or labels they wished","Facial expression megamix: Tests of dimensional and
category accounts of emotion recognitionFacial expressions, emotion recognition, categorical perception, morphed images, prototype, discrete categories, reaction time, forced-choice procedure.We report four experiments investigating the perception of photographic quality continua
of interpolated (‘morphed’) facial expressions derived from prototypes of the 6 emotions in
the Ekman and Friesen (1976) series (happiness, surprise, fear, sadness, disgust and anger).
In Experiment 1, morphed images made from all possible pairwise combinations of
expressions were presented in random order; subjects identified these as belonging to
distinct expression categories corresponding to the prototypes at each end of the relevant
continuum. This result was replicated in Experiment 2, which also included morphs made
from a prototype with a neutral expression, and allowed ‘neutral’ as a response category.
These findings are inconsistent with the view that facial expressions are recognised by
locating them along two underlying dimensions, since such a view predicts that at least
some transitions between categories should involve neutral regions or identification as a
different emotion. Instead, they suggest that facial expressions of basic emotions are
recognised by their fit to discrete categories. Experiment 3 used continua involving 6
emotions to demonstrate best discrimination of pairs of stimuli falling across category
boundaries; this provides further evidence of categorical perception of facial expressions of
emotion. However, in both Experiment 1 and Experiment 2, reaction time data showed that
increasing distance from the prototype had a definite cost on ability to identify emotion in
the resulting morphed face. Moreover, Experiment 4 showed that subjects had some insight
into which emotions were blended to create specific morphed images. Hence, categorical
perception effects were found even though subjects were sensitive to physical properties of
these morphed facial expressions. We suggest that rapid classification of prototypes and
better across boundary discriminability reflect the underlying organisation of human
categorisation abilities.The results of Experiment 4 show that when a prototype only contributes 10%
to a particular morph, subjects cannot reliably identify which prototype that 10%
has come from, but that for prototypes contributing 30% they begin to be able to
do this. This is interesting because in Experiment 1 all 30% morphs were
consistently identified as belonging to the category contributing 70%. However,
we noted that distance from the prototype affected reaction times for classifying
morphed expressions in Experiment 1, which implied that people can see
differences between these morphed expressions, even when they assign them to a
common category. Experiment 4 has taken this point a step further, by showing
not only that people are sensitive to differences between expressions which they
assign to a common overall category, but that they can have some insight into the
nature of these differences.
However, one should not overestimate the extent of this insight, which was
elicited by a forced-choice procedure, where subjects were made to select 3 labels
for each face. Etcoff and Magee (1992) had included a ‘free naming’ task in which
subjects were shown each picture and encouraged to describe the expression using
whatever words or labels they wishedree || eee, +e. H =e rl ee tf : — i ] i aa TT eaaemas {Seve 1} ee |i} |e J 2 Lf ' Fig. 1. The facial feature extraction and facial expression analysis system,",Deep Learning and Machine Learning,"The article discusses four experiments that explore how people perceive facial expressions of emotions. The experiments used morphed images derived from prototypes of six emotions, and the results suggest that people categorize facial expressions of emotions into discrete categories rather than perceiving them along two underlying dimensions. The experiments also found that increasing distance from the prototype had a negative effect on the ability to identify emotions, and that people were sensitive to physical properties of these morphed facial expressions. Experiment 4 showed that people could have some insight into the differences between expressions, but this insight was elicited through a forced-choice procedure, and should not be overestimated.",Object and Sentiment Recognition,"ree || eee, +e. H =e rl ee tf : — i ] i aa TT eaaemas {Seve 1} ee |i} |e J 2 Lf ' Fig. 1. The facial feature extraction and facial expression analysis system,",Object Recognition
306,"Face masks reduce emotion-recognition
accuracy and perceived closeness","Face masks, coronavirus, social judgments, emotional recognition, trustworthiness, likability, closeness, psychological consequences.","Face masks became the symbol of the global fight against the coronavirus. While face
masks’ medical benefits are clear, little is known about their psychological consequences.
Drawing on theories of the social functions of emotions and rapid trait impressions, we
tested hypotheses on face masks’ effects on emotion-recognition accuracy and social judgments (perceived trustworthiness, likability, and closeness). Our preregistered study with
191 German adults revealed that face masks diminish people’s ability to accurately categorize an emotion expression and make target persons appear less close. Exploratory analyses further revealed that face masks buffered the negative effect of negative (vs. nonnegative) emotion expressions on perceptions of trustworthiness, likability, and closeness.
Associating face masks with the coronavirus’ dangers predicted higher perceptions of closeness for masked but not for unmasked faces. By highlighting face masks’ effects on social
functioning, our findings inform policymaking and point at contexts where alternatives to
face masks are needed.","While face masks effectively curb the spread of COVID-19 [3], they have collateral consequences for emotional inferences and social judgments. Face masks impair people’s ability to
accurately classify emotional expressions which tends to be exacerbated for older adults,
reduce perceived closeness, and increase perceptions of closeness, trustworthiness, and likability for targets expressing negative emotions. Together, our results highlight the relevance of
psychological factors in the context of the coronavirus and inform policymaking by illustrating
face masks’ (side) effects on social functioning","Face masks reduce emotion-recognition
accuracy and perceived closenessFace masks, coronavirus, social judgments, emotional recognition, trustworthiness, likability, closeness, psychological consequences.Face masks became the symbol of the global fight against the coronavirus. While face
masks’ medical benefits are clear, little is known about their psychological consequences.
Drawing on theories of the social functions of emotions and rapid trait impressions, we
tested hypotheses on face masks’ effects on emotion-recognition accuracy and social judgments (perceived trustworthiness, likability, and closeness). Our preregistered study with
191 German adults revealed that face masks diminish people’s ability to accurately categorize an emotion expression and make target persons appear less close. Exploratory analyses further revealed that face masks buffered the negative effect of negative (vs. nonnegative) emotion expressions on perceptions of trustworthiness, likability, and closeness.
Associating face masks with the coronavirus’ dangers predicted higher perceptions of closeness for masked but not for unmasked faces. By highlighting face masks’ effects on social
functioning, our findings inform policymaking and point at contexts where alternatives to
face masks are needed.While face masks effectively curb the spread of COVID-19 [3], they have collateral consequences for emotional inferences and social judgments. Face masks impair people’s ability to
accurately classify emotional expressions which tends to be exacerbated for older adults,
reduce perceived closeness, and increase perceptions of closeness, trustworthiness, and likability for targets expressing negative emotions. Together, our results highlight the relevance of
psychological factors in the context of the coronavirus and inform policymaking by illustrating
face masks’ (side) effects on social functioningPercentage Correct Subject 1 80.0% Subject 2 74.0% Subject 3 90.5% Subject 4 90.9% Subject 5 96.3% Subject 6 719.2% Subject 8 833% Subject 9 100% Subject 10 00.0% Subject 11 100% Subject 12 1% Subject 13 100% Subject 15 83.3% Subject 16 89.7% Subject 18 100% Total Average Correct O11% Table 2. Classification results by subject.Figure 2. Sample of faces used in training.",Emotion detection using normal face,"The study examined the psychological consequences of wearing face masks during the COVID-19 pandemic. The study found that face masks reduce people's ability to accurately categorize emotional expressions and make the target person appear less close. However, wearing a face mask buffered the negative effect of negative emotions on perceptions of trustworthiness, likability, and closeness. Moreover, associating face masks with the coronavirus predicted higher perceptions of closeness for masked faces. The findings suggest that policymakers need to consider the psychological factors when implementing policies that require face masks.",Object and Sentiment Recognition,Percentage Correct Subject 1 80.0% Subject 2 74.0% Subject 3 90.5% Subject 4 90.9% Subject 5 96.3% Subject 6 719.2% Subject 8 833% Subject 9 100% Subject 10 00.0% Subject 11 100% Subject 12 1% Subject 13 100% Subject 15 83.3% Subject 16 89.7% Subject 18 100% Total Average Correct O11% Table 2. Classification results by subject.Figure 2. Sample of faces used in training.,Object Recognition
307,Pattern Recognition Letters,"Facial Expression Recognition, Hybrid Convolution-Recurrent Neural Network, Human-Machine Interaction, Real-time Applications, Temporal Dependencies, Public Datasets, State-of-the-art Methods.","Deep Neural Networks (DNNs) outperform traditional models in numerous optical recognition missions
containing Facial Expression Recognition (FER) which is an imperative process in next-generation HumanMachine Interaction (HMI) for clinical practice and behavioral description. Existing FER methods do not
have high accuracy and are not sufficient practical in real-time applications. This work proposes a Hybrid
Convolution-Recurrent Neural Network method for FER in Images. The proposed network architecture
consists of Convolution layers followed by Recurrent Neural Network (RNN) which the combined model
extracts the relations within facial images and by using the recurrent network the temporal dependencies
which exist in the images can be considered during the classification. The proposed hybrid model is evaluated based on two public datasets and Promising experimental results have been obtained as compared
to the state-of-the-art methods.","In this paper, a model has been proposed for face emotion
recognition. We proposed a hybrid deep CNN and RNN model. In
addition, the proposed model evaluated under different circumstances and hyper parameters to properly tuning the proposed
model. Particularly, it has been found that the combination of the
two types of neural networks (CNN-RNN) cloud significantly improve the overall result of detection, which verified the efficiency
of the proposed model","Pattern Recognition LettersFacial Expression Recognition, Hybrid Convolution-Recurrent Neural Network, Human-Machine Interaction, Real-time Applications, Temporal Dependencies, Public Datasets, State-of-the-art Methods.Deep Neural Networks (DNNs) outperform traditional models in numerous optical recognition missions
containing Facial Expression Recognition (FER) which is an imperative process in next-generation HumanMachine Interaction (HMI) for clinical practice and behavioral description. Existing FER methods do not
have high accuracy and are not sufficient practical in real-time applications. This work proposes a Hybrid
Convolution-Recurrent Neural Network method for FER in Images. The proposed network architecture
consists of Convolution layers followed by Recurrent Neural Network (RNN) which the combined model
extracts the relations within facial images and by using the recurrent network the temporal dependencies
which exist in the images can be considered during the classification. The proposed hybrid model is evaluated based on two public datasets and Promising experimental results have been obtained as compared
to the state-of-the-art methods.In this paper, a model has been proposed for face emotion
recognition. We proposed a hybrid deep CNN and RNN model. In
addition, the proposed model evaluated under different circumstances and hyper parameters to properly tuning the proposed
model. Particularly, it has been found that the combination of the
two types of neural networks (CNN-RNN) cloud significantly improve the overall result of detection, which verified the efficiency
of the proposed modelx OR e Baid yy isi a | ea 2 Mauure 2 :) Affectiva ie 9 2h. Image Image Attribute Emotion Ground truth Selection Analysis Extraction Recognition ‘Comparison Fig.1 Overall procedureTable4 Augmented test set (static pictures) ‘Angry Disgust Fear Happy Sad Surprise —Neutral_—Total Rotation 732 74 20 720 720-708 74 5028 Partial Face 976 952 960 960 960 944 952 6704 Brightness 732 74 20 720 720-708 74 5028 Blur 366 357 360 360 360354 357 2514 Noise 366 387 360-360 360354 357 2514 Total 3172-3094 312031203120 3068 3094 21,788 2aasy 30 Fig.3 Example of rotation manipulations Original o=ns 45°",Facial Emotion Recognition,"The article discusses the development of a new deep neural network model for facial expression recognition in images, which combines convolutional and recurrent neural networks. The proposed hybrid model has been evaluated on two public datasets and has shown promising experimental results in comparison to state-of-the-art methods. The study suggests that the combination of the two neural network models can significantly improve the overall accuracy of detection, making it an efficient method for practical real-time applications.",Object and Sentiment Recognition,"x OR e Baid yy isi a | ea 2 Mauure 2 :) Affectiva ie 9 2h. Image Image Attribute Emotion Ground truth Selection Analysis Extraction Recognition ‘Comparison Fig.1 Overall procedureTable4 Augmented test set (static pictures) ‘Angry Disgust Fear Happy Sad Surprise —Neutral_—Total Rotation 732 74 20 720 720-708 74 5028 Partial Face 976 952 960 960 960 944 952 6704 Brightness 732 74 20 720 720-708 74 5028 Blur 366 357 360 360 360354 357 2514 Noise 366 387 360-360 360354 357 2514 Total 3172-3094 312031203120 3068 3094 21,788 2aasy 30 Fig.3 Example of rotation manipulations Original o=ns 45°",Object Recognition
308,Enhanced emotion detection and altered neural processing as faces become more iconic,"Iconic representations, emotional communication, cognitive psychology, low-level features, photorealistic, cartoonized images, P1 component, low-level visual features.","Iconic representations are ubiquitous; they fill children’s cartoons, add humor to newspapers, and bring emotional tone to online communication. Yet, the communicative function they serve remains unaddressed by cognitive psychology. Here, we examined the hypothesis that iconic representations communicate emotional information more efficiently than their realistic counterparts. In Experiment 1, we manipulated low-level features of emotional faces to create five sets of stimuli that ranged from photorealistic to fully iconic. Participants identified emotions on briefly presented faces. Results showed that, at short presentation times, accuracy for identifying emotion on more “cartoonized” images was enhanced. In addition, increasing contrast and decreasing featural complexity benefited accuracy. In Experiment 2, we examined an event-related potential component, the P1, which is sensitive to low-level visual stimulus features. Lower levels of contrast and complexity within schematic stimuli were also associated with lower P1 amplitudes. These findings support the hypothesis that iconic representations differ from realistic images in their ability to communicate specific information, including emotion, quickly and efficiently, and that this effect is driven by changes in low-level visual features in the stimuli.","Iconic faces can be viewed either as analogous to realistic images or as a distinct class of stimulus. Our findings support the view that iconic representations serve a distinct role – to impart specific information quickly and efficiently – and highlight the advantages of simplifying image features and increasing contrast to communicate emotion. In addition, our data suggest that the effects of iconization may not be specific to faces, but rather to any stimulus that has these low-level featural changes. It is thus important to consider that such features are not just potential low-level confounds but contribute to specific communicative functions. However, it is unknown if the discrimination of more subtle real-world types of emotional expression would also benefit from iconic representation (e.g., the ‘Duchenne’ smile, where genuine happiness is expressed with the wrinkling of the corners of the eyes) (Ekman, Davidson, & Friesen, 1990). It may be that iconic images have a communicative advantage only for simple visual information, a hypothesis that invites future research.

The effective communicative role of iconic images may underlie the ubiquity and popularity of iconic imagery and cartoons in popular culture. Better understanding of the factors that enhance their communicative role may help improve their use in various real-world applications such as emoticons, signs, and concept cartoons. We suggest that the communicative role of iconic imagery is an important area for further research, and its power would be better exploited than ignored.","Enhanced emotion detection and altered neural processing as faces become more iconicIconic representations, emotional communication, cognitive psychology, low-level features, photorealistic, cartoonized images, P1 component, low-level visual features.Iconic representations are ubiquitous; they fill children’s cartoons, add humor to newspapers, and bring emotional tone to online communication. Yet, the communicative function they serve remains unaddressed by cognitive psychology. Here, we examined the hypothesis that iconic representations communicate emotional information more efficiently than their realistic counterparts. In Experiment 1, we manipulated low-level features of emotional faces to create five sets of stimuli that ranged from photorealistic to fully iconic. Participants identified emotions on briefly presented faces. Results showed that, at short presentation times, accuracy for identifying emotion on more “cartoonized” images was enhanced. In addition, increasing contrast and decreasing featural complexity benefited accuracy. In Experiment 2, we examined an event-related potential component, the P1, which is sensitive to low-level visual stimulus features. Lower levels of contrast and complexity within schematic stimuli were also associated with lower P1 amplitudes. These findings support the hypothesis that iconic representations differ from realistic images in their ability to communicate specific information, including emotion, quickly and efficiently, and that this effect is driven by changes in low-level visual features in the stimuli.Iconic faces can be viewed either as analogous to realistic images or as a distinct class of stimulus. Our findings support the view that iconic representations serve a distinct role – to impart specific information quickly and efficiently – and highlight the advantages of simplifying image features and increasing contrast to communicate emotion. In addition, our data suggest that the effects of iconization may not be specific to faces, but rather to any stimulus that has these low-level featural changes. It is thus important to consider that such features are not just potential low-level confounds but contribute to specific communicative functions. However, it is unknown if the discrimination of more subtle real-world types of emotional expression would also benefit from iconic representation (e.g., the ‘Duchenne’ smile, where genuine happiness is expressed with the wrinkling of the corners of the eyes) (Ekman, Davidson, & Friesen, 1990). It may be that iconic images have a communicative advantage only for simple visual information, a hypothesis that invites future research.

The effective communicative role of iconic images may underlie the ubiquity and popularity of iconic imagery and cartoons in popular culture. Better understanding of the factors that enhance their communicative role may help improve their use in various real-world applications such as emoticons, signs, and concept cartoons. We suggest that the communicative role of iconic imagery is an important area for further research, and its power would be better exploited than ignored.(b) Fig. 3. Percentage identifications and reaction times (in ms) from Experiment 1. (a) Outer hexagon in Fig. Ib (happiness—surprise—fear-sadness~disgust-anger-happiness). (b) First inner triangle in Fig. 1b (happiness—fear~disgust-happiness). (c) Second inner triangle in Fig. 1b (surprise-sadness~anger~ surprise). (d) Diagonals in Fig. 1b (happiness~sadness; surprise~disgust; fear-anger)(a) LOVE, MT, HAPPINESS conTeMeT sunPRISE REIECTION ATTENTION oiscust FEAR, SUFFERING ANGER, ) HAPPINESS. ‘SURPRISE ‘SADNESS Fig. 1. Schematic representations of relation between different facial expressions. (a) As proposed by Woodworth and Schlosberg (1954). (b) Continua used in Experiment 1",Emotion detection using cartoon face,"The study examines the hypothesis that iconic representations communicate emotional information more efficiently than their realistic counterparts. Experiment 1 shows that cartoonized images enhance accuracy in identifying emotions compared to photorealistic images. Experiment 2 shows that lower levels of contrast and complexity within schematic stimuli are associated with lower P1 amplitudes. These findings suggest that iconic representations serve a distinct role in imparting specific information quickly and efficiently, and highlight the advantages of simplifying image features and increasing contrast to communicate emotion. Future research should explore if iconic images have a communicative advantage for more subtle emotional expressions. Understanding the factors that enhance their communicative role may improve their use in real-world applications.",Object and Sentiment Recognition,"(b) Fig. 3. Percentage identifications and reaction times (in ms) from Experiment 1. (a) Outer hexagon in Fig. Ib (happiness—surprise—fear-sadness~disgust-anger-happiness). (b) First inner triangle in Fig. 1b (happiness—fear~disgust-happiness). (c) Second inner triangle in Fig. 1b (surprise-sadness~anger~ surprise). (d) Diagonals in Fig. 1b (happiness~sadness; surprise~disgust; fear-anger)(a) LOVE, MT, HAPPINESS conTeMeT sunPRISE REIECTION ATTENTION oiscust FEAR, SUFFERING ANGER, ) HAPPINESS. ‘SURPRISE ‘SADNESS Fig. 1. Schematic representations of relation between different facial expressions. (a) As proposed by Woodworth and Schlosberg (1954). (b) Continua used in Experiment 1",Object Recognition
309,Human-Centered Emotion Recognition in Animated GIFs,"GIF, emotion recognition, facial attention module, Hierarchical Segment LSTM, human-centered, visual feature extraction, interpretability, keypoint-based.","As an intuitive way of expression emotion, the animated Graphical Interchange Format (GIF) images have been widely used on social media. Most previous studies on automated GIF emotion recognition fail to effectively utilize GIF's unique properties, and this potentially limits the recognition performance. In this study, we demonstrate the importance of human related information in GIFs and conduct human-centered GIF emotion recognition with a proposed Keypoint Attended Visual Attention Network (KAVAN). The framework consists of a facial attention module and a hierarchical segment temporal module. The facial attention module exploits the strong relationship between GIF contents and human characters, and extracts frame-level visual feature with a focus on human faces. The Hierarchical Segment LSTM (HS-LSTM) module is then proposed to better learn global GIF representations. Our proposed framework outperforms the state-of-the-art on the MIT GIFGIF dataset. Furthermore, the facial attention module provides reliable facial region mask predictions, which improves the model's interpretability.","Motivated by GIF’s unique properties, we focus on human-centered GIF emotion recognition and propose a Keypoint Attended Visual Attention Network (KAVAN). In the facial attention module, we learn facial region masks with estimated facial keypoints to guide the GIF frame representation extraction. In the temporal module, we propose a novel Hierarchical Segment LSTM (HS-LSTM) structure to better represent the temporal evolution and learn better global representations. Experiments on the GIFGIF dataset validate the effectiveness of the proposed framework.","Human-Centered Emotion Recognition in Animated GIFsGIF, emotion recognition, facial attention module, Hierarchical Segment LSTM, human-centered, visual feature extraction, interpretability, keypoint-based.As an intuitive way of expression emotion, the animated Graphical Interchange Format (GIF) images have been widely used on social media. Most previous studies on automated GIF emotion recognition fail to effectively utilize GIF's unique properties, and this potentially limits the recognition performance. In this study, we demonstrate the importance of human related information in GIFs and conduct human-centered GIF emotion recognition with a proposed Keypoint Attended Visual Attention Network (KAVAN). The framework consists of a facial attention module and a hierarchical segment temporal module. The facial attention module exploits the strong relationship between GIF contents and human characters, and extracts frame-level visual feature with a focus on human faces. The Hierarchical Segment LSTM (HS-LSTM) module is then proposed to better learn global GIF representations. Our proposed framework outperforms the state-of-the-art on the MIT GIFGIF dataset. Furthermore, the facial attention module provides reliable facial region mask predictions, which improves the model's interpretability.Motivated by GIF’s unique properties, we focus on human-centered GIF emotion recognition and propose a Keypoint Attended Visual Attention Network (KAVAN). In the facial attention module, we learn facial region masks with estimated facial keypoints to guide the GIF frame representation extraction. In the temporal module, we propose a novel Hierarchical Segment LSTM (HS-LSTM) structure to better represent the temporal evolution and learn better global representations. Experiments on the GIFGIF dataset validate the effectiveness of the proposed framework.idae-ape ce CETTE RATATAT TTT Table Malte regenion modes ceporing the et of coniton, valence, and mark rated sociation on sca adgment 1 Trnrotinas| T kb I comm [aan [eo pea lean eo bea [aso [eo [mer 23g | an (Swiy™ [335.357 | 06) [saad 327.350 [9.13 (006) | 4530 (2 000 [299.327 [non oy] 123caas) —|-035 008 | a2 aos) 24,001 [-025(@i0)| 236.0 |-044, 008 [ase oo [a0 <0) |-070, 059 | 074,000 [2427 001)"" | aso ase] 074(om)|-2548(< a0) [09m 068 I I I t t oon |-oar amy | — t t L 008) [ose (365 ae iascoony _[aocoar _[oxzyeas) [00 cay (004) [517 < ani 0 dai oi0) [2 0"" [enh xz 92) [-ag.021 [ | | | | | ox | [ | I I‘The person is... The person seems... © @ Neutral @ Proud @Fearful |] Tusworthy Se @ Surprised @ Amused @ Sad Likable Correct: 69.9% Correct: 48.9% |]©® Happy @ Angry @ Disgusted |} cose 10 me — iI I it— Fig 1. Trial structure of the emotion-recognition and social judgment task. Each square represents a single page on participants’ screen. Please note that the stimuli and the item wording are not accurate but serve illustrative purposes. For copyright reasons, we cannot include the stimuli used in the study. Percentages below the included stimuli indicate the number of correctly recognized emotional expressions across participants and trials in each condition. The depicted individual has given written informed consent (as outlined in the PLOS sasnaiat tiem) es oe loach tenis Gaia:",Emotion detection using normal face,This study proposes a framework called KAVAN for human-centered GIF emotion recognition. KAVAN consists of a facial attention module and a hierarchical segment temporal module. The facial attention module focuses on human faces to extract frame-level visual features. The HS-LSTM module learns global GIF representations. KAVAN outperforms the state-of-the-art on the MIT GIFGIF dataset and improves model interpretability. The proposed framework utilizes GIF's unique properties and provides reliable facial region mask predictions.,Object and Sentiment Recognition,"idae-ape ce CETTE RATATAT TTT Table Malte regenion modes ceporing the et of coniton, valence, and mark rated sociation on sca adgment 1 Trnrotinas| T kb I comm [aan [eo pea lean eo bea [aso [eo [mer 23g | an (Swiy™ [335.357 | 06) [saad 327.350 [9.13 (006) | 4530 (2 000 [299.327 [non oy] 123caas) —|-035 008 | a2 aos) 24,001 [-025(@i0)| 236.0 |-044, 008 [ase oo [a0 <0) |-070, 059 | 074,000 [2427 001)"" | aso ase] 074(om)|-2548(< a0) [09m 068 I I I t t oon |-oar amy | — t t L 008) [ose (365 ae iascoony _[aocoar _[oxzyeas) [00 cay (004) [517 < ani 0 dai oi0) [2 0"" [enh xz 92) [-ag.021 [ | | | | | ox | [ | I I‘The person is... The person seems... © @ Neutral @ Proud @Fearful |] Tusworthy Se @ Surprised @ Amused @ Sad Likable Correct: 69.9% Correct: 48.9% |]©® Happy @ Angry @ Disgusted |} cose 10 me — iI I it— Fig 1. Trial structure of the emotion-recognition and social judgment task. Each square represents a single page on participants’ screen. Please note that the stimuli and the item wording are not accurate but serve illustrative purposes. For copyright reasons, we cannot include the stimuli used in the study. Percentages below the included stimuli indicate the number of correctly recognized emotional expressions across participants and trials in each condition. The depicted individual has given written informed consent (as outlined in the PLOS sasnaiat tiem) es oe loach tenis Gaia:",Object Recognition
310,Understanding cartoon emotion using integrated deep neural network on large dataset,"emotion recognition, cartoon images, Mask R-CNN, ResNet-50, MobileNetV2, InceptionV3, VGG 16, character detection, facial expression, dataset, accuracy, segmentation, emotion classification, animators, illustrators, cartoonists, recommender system, body gestures, artificial intelligence","Emotion is an instinctive or intuitive feeling as distinguished from reasoning or knowledge. It varies over time, since it is a natural instinctive state of mind deriving from one’s circumstances, mood, or relationships with others. Since emotions vary over time, it is important to understand and analyze them appropriately. Existing works have mostly focused well on recognizing basic emotions from human faces. However, the emotion recognition from cartoon images has not been extensively covered. Therefore, in this paper, we present an integrated Deep Neural Network (DNN) approach that deals with recognizing emotions from cartoon images. Since state-of-works do not have large amount of data, we collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach, trained on a large dataset consisting of animations for both the characters (Tom and Jerry), correctly identifies the character, segments their face masks, and recognizes the consequent emotions with an accuracy score of 0.96. The approach utilizes Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16 for emotion classification. In our study, to classify emotions, VGG 16 outperforms others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN outperforms the state-of-the-art approaches.","Recognizing emotions from facial expressions of faces other than human beings is an interesting and challenging problem. Although the existing literature has endeavored to detect and recognize objects, however, recognizing emotions has not been extensively covered. Therefore, in this paper, we have presented an integrated Deep Neural Network (DNN) approach that has successfully recognized emotions from cartoon images. We have collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach has been trained on the large dataset and has correctly identified the character, segmented their face masks, and recognized the consequent emotions with an accuracy score of 0.96. The approach has utilized Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16, for emotion classification. The experimental analysis has depicted the outperformance of VGG 16 over others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN has also outperformed the state-of-the-art approaches.

The work would be beneficial to the animators, illustrators, and cartoonists. It can also be used to build a recommender system that allows users to associatively select emotion and cartoon pair. Studying emotions encased in cartoons also extracts other allied information, which if combined with artificial intelligence can open a plethora of opportunities, for instance, recognizing emotions from body gestures.","Understanding cartoon emotion using integrated deep neural network on large datasetemotion recognition, cartoon images, Mask R-CNN, ResNet-50, MobileNetV2, InceptionV3, VGG 16, character detection, facial expression, dataset, accuracy, segmentation, emotion classification, animators, illustrators, cartoonists, recommender system, body gestures, artificial intelligenceEmotion is an instinctive or intuitive feeling as distinguished from reasoning or knowledge. It varies over time, since it is a natural instinctive state of mind deriving from one’s circumstances, mood, or relationships with others. Since emotions vary over time, it is important to understand and analyze them appropriately. Existing works have mostly focused well on recognizing basic emotions from human faces. However, the emotion recognition from cartoon images has not been extensively covered. Therefore, in this paper, we present an integrated Deep Neural Network (DNN) approach that deals with recognizing emotions from cartoon images. Since state-of-works do not have large amount of data, we collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach, trained on a large dataset consisting of animations for both the characters (Tom and Jerry), correctly identifies the character, segments their face masks, and recognizes the consequent emotions with an accuracy score of 0.96. The approach utilizes Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16 for emotion classification. In our study, to classify emotions, VGG 16 outperforms others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN outperforms the state-of-the-art approaches.Recognizing emotions from facial expressions of faces other than human beings is an interesting and challenging problem. Although the existing literature has endeavored to detect and recognize objects, however, recognizing emotions has not been extensively covered. Therefore, in this paper, we have presented an integrated Deep Neural Network (DNN) approach that has successfully recognized emotions from cartoon images. We have collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach has been trained on the large dataset and has correctly identified the character, segmented their face masks, and recognized the consequent emotions with an accuracy score of 0.96. The approach has utilized Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16, for emotion classification. The experimental analysis has depicted the outperformance of VGG 16 over others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN has also outperformed the state-of-the-art approaches.

The work would be beneficial to the animators, illustrators, and cartoonists. It can also be used to build a recommender system that allows users to associatively select emotion and cartoon pair. Studying emotions encased in cartoons also extracts other allied information, which if combined with artificial intelligence can open a plethora of opportunities, for instance, recognizing emotions from body gestures.oarler 6 Fel Fe: Regression —AH-o ; mf | | ser ry 2 api Fig. 2. CNN Architecture, the network contains six convolutional layers containing. 8, 16, 32, 64, 128, and 256 filters; each of size 5 x5 and 3 x3 followed by ReLU ac- tivation functions. 3 x 3 max-pooling layers added just after every first five convolu- tional layers and average pooling at the last convolution layer. Every convolutional layer has two fully-connected layers and 200 hidden units. T = a Fig. 3. Hybrid CNN-RNN Network ArchitectureTable 3 Result of altering the number of hidden layers. sur sad mp. Fea Dis Method Prediction accuracy _ Loss HybridCNN-RNN, hidden layers=4 94.57% 428% HybridCNN-RNN, hidden layers=5 9473% ane Hybrid CNN-RNN, hidden layers=6 941% 3.98% =| [7 | = | i i= | oo | ow eel ae vr |e oe | | on i= | eo om cer | oer | om =e Bo ee = Bs eee «=| (/<|""|= Aug Dis Fea Hap, New Sad Sur (@) CNN model on testing st ay |= = > =o |= Ba 28 | 016 000) oa - ea) 000 in | oo 22 |e | oe = Be ee ~ | yl se cae Ave Dis Fen Hap New Sad Sur (&) CNN+RNN model on testing set Fig. 6. Confusion matrices on JAFFE Datasets.",Emotion detection using normal face,"The paper presents a Deep Neural Network (DNN) approach for recognizing emotions from cartoon images of characters Tom and Jerry, with four emotions: happy, sad, angry, and surprise. The approach utilizes Mask R-CNN for character detection and state-of-the-art deep learning models, including VGG 16, for emotion classification. VGG 16 outperforms the other models, achieving an accuracy of 96% and an F1 score of 0.85. The proposed approach outperforms state-of-the-art methods and has practical applications for animators, illustrators, and cartoonists. It also has the potential for developing a recommender system and recognizing emotions from body gestures.",Object and Sentiment Recognition,"oarler 6 Fel Fe: Regression —AH-o ; mf | | ser ry 2 api Fig. 2. CNN Architecture, the network contains six convolutional layers containing. 8, 16, 32, 64, 128, and 256 filters; each of size 5 x5 and 3 x3 followed by ReLU ac- tivation functions. 3 x 3 max-pooling layers added just after every first five convolu- tional layers and average pooling at the last convolution layer. Every convolutional layer has two fully-connected layers and 200 hidden units. T = a Fig. 3. Hybrid CNN-RNN Network ArchitectureTable 3 Result of altering the number of hidden layers. sur sad mp. Fea Dis Method Prediction accuracy _ Loss HybridCNN-RNN, hidden layers=4 94.57% 428% HybridCNN-RNN, hidden layers=5 9473% ane Hybrid CNN-RNN, hidden layers=6 941% 3.98% =| [7 | = | i i= | oo | ow eel ae vr |e oe | | on i= | eo om cer | oer | om =e Bo ee = Bs eee «=| (/<|""|= Aug Dis Fea Hap, New Sad Sur (@) CNN model on testing st ay |= = > =o |= Ba 28 | 016 000) oa - ea) 000 in | oo 22 |e | oe = Be ee ~ | yl se cae Ave Dis Fen Hap New Sad Sur (&) CNN+RNN model on testing set Fig. 6. Confusion matrices on JAFFE Datasets.",Object Recognition
311,Exploring Co-Training Strategies for Opinion Detection,"sentiment analysis, lexicon-based, corpus-based, semi-supervised learning, SSL, co-training, sentiment-labeled data, subdocument level, opinion, sentiment-bearing features, sentence level, unlabeled data, emotion classification","For the last decade or so, sentiment analysis, which aims to automatically identify opinions, polarities, or emotions from user-generated content (e.g., blogs, tweets), has attracted interest from both academic and industrial communities. Most sentiment analysis strategies fall into 2 categories: lexicon-based and corpusbased approaches. While the latter often requires sentiment-labeled data to build a machine learning model, both approaches need sentiment-labeled data for evaluation. Unfortunately, most data domains lack sufficient quantities of labeled data, especially at the subdocument level. Semisupervised learning (SSL), a machine learning technique that requires only a few labeled examples and can automatically label unlabeled data, is a promising strategy to deal with the issue of insufficient labeled data. Although previous studies have shown promising results of applying various SSL algorithms to solve a sentiment-analysis problem, co-training, an SSL algorithm, has not attracted much attention for sentiment analysis largely due to its restricted assumptions. Therefore, this study focuses on revisiting co-training in depth and discusses several co-training strategies for sentiment analysis following a loose assumption. Results suggest that co-training can be more effective than can other currently adopted SSL methods for sentiment analysis.","Sentiment is an important aspect of many types of information and being able to identify and organize sentiments or opinions is essential for information studies. Prior sentiment analysis studies have suggested that a large number of sentiment-bearing features are necessary for capturing subtle sentiments. However, researchers often face the challenge of limited amounts of sentiment-labeled data from which sentiment-bearing features can be extracted, especially at the sentence level. This shortage of labeled data has become a severe challenge for developing effective sentiment-analysis systems. While opinion labeled data are limited, researchers can easily access plenty of unlabeled opinions on the Web. Therefore, this research investigated several SSL algorithms, which require only a small amount of labeled data and a large amount of unlabeled data, to tackle this challenge. Specifically, this study examined co-training, a simple and effective SSL algorithm which has not received enough attention from sentiment-analysis researchers, mainly due to the restricted assumptions of the original co-training algorithms. Four different co-training strategies were developed under loose co-training assumptions and were tested against three data domains with different characteristics (i.e., movie reviews, news articles, and blog posts). For movie reviews, all co-training strategies attained state-ofthe-art results with a small number of labeled sentences. Due to the nature of the movie-review data, opinion detection in movie reviews is an “easy” problem because it involves genre classification and thus relies, strictly speaking, on distinguishing movie reviews from plot summaries. For other manually created data sets that are expected to reflect real opinion characteristics, the co-training approach was impeded by low baseline precision and demonstrated only limited improvement. For news articles, co-training was able to achieve slight improvement in performance over supervised learning using only labeled data. Blog posts are the most challenging domain, and blog data showed no benefits from implementing any co-training strategies. Nevertheless, strategies developed in this study can be easily adapted to other sentiment-analysis tasks and potentially solve the problem of insufficient quantities of labeled data, given that the initial supervised learning performance is not too low. Future study includes further investigation of co-training strategies that work for data domains where sentiment analysis is particularly difficult. Co-training strategies also will be examined for other sentiment-analysis tasks such as polarity detection and emotion classifications.","Exploring Co-Training Strategies for Opinion Detectionsentiment analysis, lexicon-based, corpus-based, semi-supervised learning, SSL, co-training, sentiment-labeled data, subdocument level, opinion, sentiment-bearing features, sentence level, unlabeled data, emotion classificationFor the last decade or so, sentiment analysis, which aims to automatically identify opinions, polarities, or emotions from user-generated content (e.g., blogs, tweets), has attracted interest from both academic and industrial communities. Most sentiment analysis strategies fall into 2 categories: lexicon-based and corpusbased approaches. While the latter often requires sentiment-labeled data to build a machine learning model, both approaches need sentiment-labeled data for evaluation. Unfortunately, most data domains lack sufficient quantities of labeled data, especially at the subdocument level. Semisupervised learning (SSL), a machine learning technique that requires only a few labeled examples and can automatically label unlabeled data, is a promising strategy to deal with the issue of insufficient labeled data. Although previous studies have shown promising results of applying various SSL algorithms to solve a sentiment-analysis problem, co-training, an SSL algorithm, has not attracted much attention for sentiment analysis largely due to its restricted assumptions. Therefore, this study focuses on revisiting co-training in depth and discusses several co-training strategies for sentiment analysis following a loose assumption. Results suggest that co-training can be more effective than can other currently adopted SSL methods for sentiment analysis.Sentiment is an important aspect of many types of information and being able to identify and organize sentiments or opinions is essential for information studies. Prior sentiment analysis studies have suggested that a large number of sentiment-bearing features are necessary for capturing subtle sentiments. However, researchers often face the challenge of limited amounts of sentiment-labeled data from which sentiment-bearing features can be extracted, especially at the sentence level. This shortage of labeled data has become a severe challenge for developing effective sentiment-analysis systems. While opinion labeled data are limited, researchers can easily access plenty of unlabeled opinions on the Web. Therefore, this research investigated several SSL algorithms, which require only a small amount of labeled data and a large amount of unlabeled data, to tackle this challenge. Specifically, this study examined co-training, a simple and effective SSL algorithm which has not received enough attention from sentiment-analysis researchers, mainly due to the restricted assumptions of the original co-training algorithms. Four different co-training strategies were developed under loose co-training assumptions and were tested against three data domains with different characteristics (i.e., movie reviews, news articles, and blog posts). For movie reviews, all co-training strategies attained state-ofthe-art results with a small number of labeled sentences. Due to the nature of the movie-review data, opinion detection in movie reviews is an “easy” problem because it involves genre classification and thus relies, strictly speaking, on distinguishing movie reviews from plot summaries. For other manually created data sets that are expected to reflect real opinion characteristics, the co-training approach was impeded by low baseline precision and demonstrated only limited improvement. For news articles, co-training was able to achieve slight improvement in performance over supervised learning using only labeled data. Blog posts are the most challenging domain, and blog data showed no benefits from implementing any co-training strategies. Nevertheless, strategies developed in this study can be easily adapted to other sentiment-analysis tasks and potentially solve the problem of insufficient quantities of labeled data, given that the initial supervised learning performance is not too low. Future study includes further investigation of co-training strategies that work for data domains where sentiment analysis is particularly difficult. Co-training strategies also will be examined for other sentiment-analysis tasks such as polarity detection and emotion classifications.Photo Rotoscoped Cartoon-- ‘Magnitude (uv) 95 90 3s 80 78 70 3 30 ns Ens $124 i gu 20 us ue —Photo Shocked Neutral Mid- rotoscope Rotoscope| Mid- cartoon Cartoon OO ® @ @: OO® @ @: QOOD@®@ OO ® @@Stimulus sets careon Mid-cartoon otoreoned ‘mig-rotoscoped Time course of a trial Which emotion? | rpisgusted — | + 8: Happy 8:Shocked | 0: Neutral 1000ms Until Responds:come PA 100 70 © 0 Contrast by featural complexit Je Cartoon Rotoscoped High continct ‘tow complexty Ign Comps ‘bein,Accuracy (%) 100 50 40 30 20 10 Stimulus type by presentation time ~®Cartoon —#-Mid-Cartoon © Rotoscope —#-Mid-Rotoscope Photo 7 33 50 66 Stimulus presentation time (ms)",sentiment analysis,"The text discusses sentiment analysis, which aims to identify opinions, emotions, and polarities from user-generated content. There are two main categories of sentiment analysis strategies, lexicon-based and corpus-based. However, both approaches require sentiment-labeled data for evaluation, which is often limited in quantity. Semisupervised learning (SSL) is a promising strategy for dealing with insufficient labeled data. This research focuses on co-training, an SSL algorithm that has not received much attention for sentiment analysis due to its restricted assumptions. The study examines four different co-training strategies and tests them against three data domains with different characteristics. The results suggest that co-training can be more effective than other currently adopted SSL methods for sentiment analysis, particularly for movie reviews. However, it showed limited improvement in other data domains, such as news articles and blog posts. Future research includes investigating co-training strategies for difficult sentiment analysis tasks and other sentiment-analysis tasks such as polarity detection and emotion classification.",Object and Sentiment Recognition,"Photo Rotoscoped Cartoon-- ‘Magnitude (uv) 95 90 3s 80 78 70 3 30 ns Ens $124 i gu 20 us ue —Photo Shocked Neutral Mid- rotoscope Rotoscope| Mid- cartoon Cartoon OO ® @ @: OO® @ @: QOOD@®@ OO ® @@Stimulus sets careon Mid-cartoon otoreoned ‘mig-rotoscoped Time course of a trial Which emotion? | rpisgusted — | + 8: Happy 8:Shocked | 0: Neutral 1000ms Until Responds:come PA 100 70 © 0 Contrast by featural complexit Je Cartoon Rotoscoped High continct ‘tow complexty Ign Comps ‘bein,Accuracy (%) 100 50 40 30 20 10 Stimulus type by presentation time ~®Cartoon —#-Mid-Cartoon © Rotoscope —#-Mid-Rotoscope Photo 7 33 50 66 Stimulus presentation time (ms)",Object Recognition
312,Combining local and global information for product feature extraction in opinion documents,Opinion mining Feature extraction Local context information Global context information Graph algorithms,"Product feature (feature in brief) extraction is one of important tasks in opinion mining as it enables an opinion mining system to provide feature level opinions. Most existing feature extraction methods use only local context information (LCI) in a clause or a sentence (such as co-occurrence or dependency relation) for extraction. But global context information (GCI) is also helpful. In this paper, we propose a combined approach, which integrates LCI and GCI to extract and rank features based on feature score and frequency. Experimental evaluation shows that the combined approach does a good job. It outperforms the baseline extraction methods individually.","In this paper, we proposed a combined approach for feature extraction, which exploits both the local (LCI) and global (GCI) L. Yang et al. / Information Processing Letters 116 (2016) 623–627 627 linking information. LCI uses the HITS algorithm, while GCI uses SimRank. LCI contains direct links between opinion words and nouns in a sentence. GCI contains the LCI (direct) links and also indirect links in the corpus. We integrated the LCI and GCI based approaches for the final extraction and ranking of features. Experimental results show that the approach does a good job, and it outperforms all the baseline methods.","Combining local and global information for product feature extraction in opinion documentsOpinion mining Feature extraction Local context information Global context information Graph algorithmsProduct feature (feature in brief) extraction is one of important tasks in opinion mining as it enables an opinion mining system to provide feature level opinions. Most existing feature extraction methods use only local context information (LCI) in a clause or a sentence (such as co-occurrence or dependency relation) for extraction. But global context information (GCI) is also helpful. In this paper, we propose a combined approach, which integrates LCI and GCI to extract and rank features based on feature score and frequency. Experimental evaluation shows that the combined approach does a good job. It outperforms the baseline extraction methods individually.In this paper, we proposed a combined approach for feature extraction, which exploits both the local (LCI) and global (GCI) L. Yang et al. / Information Processing Letters 116 (2016) 623–627 627 linking information. LCI uses the HITS algorithm, while GCI uses SimRank. LCI contains direct links between opinion words and nouns in a sentence. GCI contains the LCI (direct) links and also indirect links in the corpus. We integrated the LCI and GCI based approaches for the final extraction and ranking of features. Experimental results show that the approach does a good job, and it outperforms all the baseline methods.ram [3] ion [3] LSTM STM S-LSTM -LSTM -LSTM Loss nMSE nMSE nMSE nMSE nMSE nMSE MTL SOFRPRP RRRba la sl Se Se usal Calm Arousal Calm sure Pleasure Misery Misery Emotion Intensity Score",opinion mining,The text discusses the importance of product feature extraction in opinion mining and highlights the limitations of using only local context information (LCI) for feature extraction. The authors propose a combined approach that integrates both LCI and global context information (GCI) to extract and rank features based on feature score and frequency. The approach uses the HITS algorithm for LCI and SimRank for GCI. Experimental evaluation shows that the combined approach outperforms baseline methods for feature extraction.,Natural Language Processing,ram [3] ion [3] LSTM STM S-LSTM -LSTM -LSTM Loss nMSE nMSE nMSE nMSE nMSE nMSE MTL SOFRPRP RRRba la sl Se Se usal Calm Arousal Calm sure Pleasure Misery Misery Emotion Intensity Score,Object Recognition
313,"Polarity shift detection, elimination and ensemble: A three-stage model for document-level sentiment analysis",Sentiment analysis Sentiment classification Polarity shift,"The polarity shift problem is a major factor that affects classification performance of machinelearning-based sentiment analysis systems. In this paper, we propose a three-stage cascade model to address the polarity shift problem in the context of document-level sentiment classification. We first split each document into a set of subsentences and build a hybrid model that employs rules and statistical methods to detect explicit and implicit polarity shifts, respectively. Secondly, we propose a polarity shift elimination method, to remove polarity shift in negations. Finally, we train base classifiers on training subsets divided by different types of polarity shifts, and use a weighted combination of the component classifiers for sentiment classification. The results on a range of experiments illustrate that our approach significantly outperforms several alternative methods for polarity shift detection and elimination.","The work describes a cascade model, namely Polarity Shift Detection, Elimination and Ensemble (PSDEE), to address the polarity shift problem in document-level sentiment analysis. In the first stage, we propose a hybrid model that employs both rule-based and statistic-based methods to detect different types of polarity shifts. Specifically, we use a rule-based method to detect explicit negations and contrasts, and a statistical method to detect the implicit sentiment inconsistency. In the second stage, we introduce a novel method called antonym reversion to eliminate polarity shifts in negations. After the first two stages, a piece of text is separated into four subsets, namely the polarity-unshifted text, eliminated negations, explicit contrasts and sentiment inconsistency. In the third stage, a weighted ensemble of base classifiers trained on component text R. Xia et al. / Information Processing and Management 52 (2016) 36–45 45 subsets is employed as the final sentiment classifier, with the aim to leverage text with different types of polarity shifts. We conduct a range of experiments including four sentiment datasets, three classification algorithms and two types of features. The results demonstrate the effect of our PSDEE approach compared to several relate work that addresses polarity shift in documentlevel sentiment classification.","Polarity shift detection, elimination and ensemble: A three-stage model for document-level sentiment analysisSentiment analysis Sentiment classification Polarity shiftThe polarity shift problem is a major factor that affects classification performance of machinelearning-based sentiment analysis systems. In this paper, we propose a three-stage cascade model to address the polarity shift problem in the context of document-level sentiment classification. We first split each document into a set of subsentences and build a hybrid model that employs rules and statistical methods to detect explicit and implicit polarity shifts, respectively. Secondly, we propose a polarity shift elimination method, to remove polarity shift in negations. Finally, we train base classifiers on training subsets divided by different types of polarity shifts, and use a weighted combination of the component classifiers for sentiment classification. The results on a range of experiments illustrate that our approach significantly outperforms several alternative methods for polarity shift detection and elimination.The work describes a cascade model, namely Polarity Shift Detection, Elimination and Ensemble (PSDEE), to address the polarity shift problem in document-level sentiment analysis. In the first stage, we propose a hybrid model that employs both rule-based and statistic-based methods to detect different types of polarity shifts. Specifically, we use a rule-based method to detect explicit negations and contrasts, and a statistical method to detect the implicit sentiment inconsistency. In the second stage, we introduce a novel method called antonym reversion to eliminate polarity shifts in negations. After the first two stages, a piece of text is separated into four subsets, namely the polarity-unshifted text, eliminated negations, explicit contrasts and sentiment inconsistency. In the third stage, a weighted ensemble of base classifiers trained on component text R. Xia et al. / Information Processing and Management 52 (2016) 36–45 45 subsets is employed as the final sentiment classifier, with the aim to leverage text with different types of polarity shifts. We conduct a range of experiments including four sentiment datasets, three classification algorithms and two types of features. The results demonstrate the effect of our PSDEE approach compared to several relate work that addresses polarity shift in documentlevel sentiment classification.+ A oputhaye, ‘Blocks cont (Com20) ‘up Shape: (Nowe, 24, 26,3) Cape Sap ene, 1414 af | bocca (conan) | | [ [eats conve (Cone) ‘oes con (Comz0) | vent Shape: one, 24, 24, 649 | — _Ourpu Shape: one, 6, 56,256), || caput spe: one, 14 14, 512), ‘Boo",sentiment analysis,"The paper proposes a three-stage cascade model, called Polarity Shift Detection, Elimination and Ensemble (PSDEE), to address the polarity shift problem in document-level sentiment analysis. The first stage uses a hybrid model to detect different types of polarity shifts, while the second stage introduces a novel method called antonym reversion to eliminate polarity shifts in negations. The final stage uses a weighted ensemble of base classifiers trained on component text subsets to perform sentiment classification. Experiments conducted on various sentiment datasets show that the PSDEE approach outperforms other related methods.",Object and Sentiment Recognition,"+ A oputhaye, ‘Blocks cont (Com20) ‘up Shape: (Nowe, 24, 26,3) Cape Sap ene, 1414 af | bocca (conan) | | [ [eats conve (Cone) ‘oes con (Comz0) | vent Shape: one, 24, 24, 649 | — _Ourpu Shape: one, 6, 56,256), || caput spe: one, 14 14, 512), ‘Boo",Object Recognition
314,OpinionFlow: Visual Analysis of Opinion Diffusion on Social Media,"Opinion visualization, opinion diffusion, opinion flow, influence estimation, kernel density estimation, level-of-detail","— It is important for many different applications such as government and business intelligence to analyze and explore the diffusion of public opinions on social media. However, the rapid propagation and great diversity of public opinions on social media pose great challenges to effective analysis of opinion diffusion. In this paper, we introduce a visual analysis system called OpinionFlow to empower analysts to detect opinion propagation patterns and glean insights. Inspired by the information diffusion model and the theory of selective exposure, we develop an opinion diffusion model to approximate opinion propagation among Twitter users. Accordingly, we design an opinion flow visualization that combines a Sankey graph with a tailored density map in one view to visually convey diffusion of opinions among many users. A stacked tree is used to allow analysts to select topics of interest at different levels. The stacked tree is synchronized with the opinion flow visualization to help users examine and compare diffusion patterns across topics. Experiments and case studies on Twitter data demonstrate the effectiveness and usability of OpinionFlow.","This paper presents a visual analysis system called OpinionFlow, which enables analysts to visually explore and trace opinion diffusion on Twitter. We enhance a model borrowed from information diffusion to estimate the diffusion of opinions among users. The model is integrated with a new visualization technique to display opinion diffusion. The proposed system allows a user to explore opinion diffusion across a relatively large number of users using a hierarchical topic structure built by BRT. By integrating data analysis models and interactive visualizations, the system allows users to unfold discovered patterns, to form various hypotheses regarding opinion diffusion patterns, and to validate hypotheses through interactions with the visualizations. In the future, we plan to improve system performance by implementing parallel algorithms of data analysis such as parallel BRT, so that we can deploy the system on the Web. Although it is designed for expert users, we believe the system can benefit users who are interested in opinions diffusion on social media. We intend to invite more users to use our system and conduct a formal user study in the future.","OpinionFlow: Visual Analysis of Opinion Diffusion on Social MediaOpinion visualization, opinion diffusion, opinion flow, influence estimation, kernel density estimation, level-of-detail— It is important for many different applications such as government and business intelligence to analyze and explore the diffusion of public opinions on social media. However, the rapid propagation and great diversity of public opinions on social media pose great challenges to effective analysis of opinion diffusion. In this paper, we introduce a visual analysis system called OpinionFlow to empower analysts to detect opinion propagation patterns and glean insights. Inspired by the information diffusion model and the theory of selective exposure, we develop an opinion diffusion model to approximate opinion propagation among Twitter users. Accordingly, we design an opinion flow visualization that combines a Sankey graph with a tailored density map in one view to visually convey diffusion of opinions among many users. A stacked tree is used to allow analysts to select topics of interest at different levels. The stacked tree is synchronized with the opinion flow visualization to help users examine and compare diffusion patterns across topics. Experiments and case studies on Twitter data demonstrate the effectiveness and usability of OpinionFlow.This paper presents a visual analysis system called OpinionFlow, which enables analysts to visually explore and trace opinion diffusion on Twitter. We enhance a model borrowed from information diffusion to estimate the diffusion of opinions among users. The model is integrated with a new visualization technique to display opinion diffusion. The proposed system allows a user to explore opinion diffusion across a relatively large number of users using a hierarchical topic structure built by BRT. By integrating data analysis models and interactive visualizations, the system allows users to unfold discovered patterns, to form various hypotheses regarding opinion diffusion patterns, and to validate hypotheses through interactions with the visualizations. In the future, we plan to improve system performance by implementing parallel algorithms of data analysis such as parallel BRT, so that we can deploy the system on the Web. Although it is designed for expert users, we believe the system can benefit users who are interested in opinions diffusion on social media. We intend to invite more users to use our system and conduct a formal user study in the future.",sentiment analysis,"The paper introduces a visual analysis system called OpinionFlow that allows analysts to trace and explore opinion diffusion on Twitter. The system integrates a diffusion model and a visualization technique to display opinion diffusion among users. It uses a hierarchical topic structure built by BRT to explore opinion diffusion across a large number of users. The system is designed for expert users, but it can benefit anyone interested in opinion diffusion on social media. The authors plan to improve the system's performance and conduct a formal user study in the future. The effectiveness and usability of OpinionFlow are demonstrated through experiments and case studies on Twitter data.",Object and Sentiment Recognition,,Object Recognition
315,"Polarity shift detection, elimination and ensemble: A three-stage model for document-level sentiment analysis",Sentiment analysis Sentiment classification Polarity shift,"The polarity shift problem is a major factor that affects classification performance of machinelearning-based sentiment analysis systems. In this paper, we propose a three-stage cascade model to address the polarity shift problem in the context of document-level sentiment classification. We first split each document into a set of subsentences and build a hybrid model that employs rules and statistical methods to detect explicit and implicit polarity shifts, respectively. Secondly, we propose a polarity shift elimination method, to remove polarity shift in negations. Finally, we train base classifiers on training subsets divided by different types of polarity shifts, and use a weighted combination of the component classifiers for sentiment classification. The results on a range of experiments illustrate that our approach significantly outperforms several alternative methods for polarity shift detection and elimination.","The work describes a cascade model, namely Polarity Shift Detection, Elimination and Ensemble (PSDEE), to address the polarity shift problem in document-level sentiment analysis. In the first stage, we propose a hybrid model that employs both rule-based and statistic-based methods to detect different types of polarity shifts. Specifically, we use a rule-based method to detect explicit negations and contrasts, and a statistical method to detect the implicit sentiment inconsistency. In the second stage, we introduce a novel method called antonym reversion to eliminate polarity shifts in negations. After the first two stages, a piece of text is separated into four subsets, namely the polarity-unshifted text, eliminated negations, explicit contrasts and sentiment inconsistency. In the third stage, a weighted ensemble of base classifiers trained on component text R. Xia et al. / Information Processing and Management 52 (2016) 36–45 45 subsets is employed as the final sentiment classifier, with the aim to leverage text with different types of polarity shifts. We conduct a range of experiments including four sentiment datasets, three classification algorithms and two types of features. The results demonstrate the effect of our PSDEE approach compared to several relate work that addresses polarity shift in documentlevel sentiment classification.","Polarity shift detection, elimination and ensemble: A three-stage model for document-level sentiment analysisSentiment analysis Sentiment classification Polarity shiftThe polarity shift problem is a major factor that affects classification performance of machinelearning-based sentiment analysis systems. In this paper, we propose a three-stage cascade model to address the polarity shift problem in the context of document-level sentiment classification. We first split each document into a set of subsentences and build a hybrid model that employs rules and statistical methods to detect explicit and implicit polarity shifts, respectively. Secondly, we propose a polarity shift elimination method, to remove polarity shift in negations. Finally, we train base classifiers on training subsets divided by different types of polarity shifts, and use a weighted combination of the component classifiers for sentiment classification. The results on a range of experiments illustrate that our approach significantly outperforms several alternative methods for polarity shift detection and elimination.The work describes a cascade model, namely Polarity Shift Detection, Elimination and Ensemble (PSDEE), to address the polarity shift problem in document-level sentiment analysis. In the first stage, we propose a hybrid model that employs both rule-based and statistic-based methods to detect different types of polarity shifts. Specifically, we use a rule-based method to detect explicit negations and contrasts, and a statistical method to detect the implicit sentiment inconsistency. In the second stage, we introduce a novel method called antonym reversion to eliminate polarity shifts in negations. After the first two stages, a piece of text is separated into four subsets, namely the polarity-unshifted text, eliminated negations, explicit contrasts and sentiment inconsistency. In the third stage, a weighted ensemble of base classifiers trained on component text R. Xia et al. / Information Processing and Management 52 (2016) 36–45 45 subsets is employed as the final sentiment classifier, with the aim to leverage text with different types of polarity shifts. We conduct a range of experiments including four sentiment datasets, three classification algorithms and two types of features. The results demonstrate the effect of our PSDEE approach compared to several relate work that addresses polarity shift in documentlevel sentiment classification.",sentiment analysis,"The text discusses the issue of polarity shift in sentiment analysis and proposes a three-stage cascade model called Polarity Shift Detection, Elimination and Ensemble (PSDEE) to address this problem in document-level sentiment classification. The first stage uses a hybrid model with rule-based and statistical methods to detect explicit and implicit polarity shifts. The second stage eliminates polarity shift in negations using a novel method called antonym reversion. Finally, a weighted ensemble of base classifiers trained on different subsets of text is used as the final sentiment classifier. The PSDEE approach outperforms several alternative methods for polarity shift detection and elimination in various experiments.",Object and Sentiment Recognition,,Deep Learning and Machine Learning
316,Challenges in Sentiment Analysis,"Sentiment analysis tasks • Sentiment of the writer, reader, and other entities • Sentiment towards aspects of an entity • Stance detection • Sentiment lexicons • Sentiment annotation • Multilingual sentiment analysis","t A vast majority of the work in Sentiment Analysis has been on developing more accurate sentiment classifiers, usually involving supervised machine learning algorithms and a battery of features. Surveys by Pang and Lee (Found Trends Inf Retr 2(1–2):1–135, 2008), Liu and Zhang (A survey of opinion mining and sentiment analysis. In: Aggarwal CC, Zhai C (eds) In: Mining text data. Springer, New York, pp 415–463, 2012), and Mohammad (Mohammad Sentiment analysis: detecting valence, emotions, and other effectual states from text. In: Meiselman H (ed) Emotion measurement. Elsevier, Amsterdam, 2016b) give summaries of the many automatic classifiers, features, and datasets used to detect sentiment. In this chapter, we flesh out some of the challenges that still remain, questions that have not been explored sufficiently, and new issues emerging from taking on new sentiment analysis problems. We also discuss proposals to deal with these challenges. The goal of this chapter is to equip researchers and practitioners with pointers to the latest developments in sentiment analysis and encourage more work in the diverse landscape of problems, especially those areas that are relatively less explored.","Applications of sentiment analysis benefit from the fact that even though systems are not extremely accurate at determining sentiment of individual sentences, they can accurately capture significant changes in the proportion of instances that are positive (or negative). It is also worth noting that such sentiment tracking systems are more effective when incorporating carefully chosen baselines. For example, knowing the percentage of tweets that are negative towards Russian President, Vladimir Putin, is less useful than, for instance, knowing: the percentage of tweets that are negative towards Putin before vs. after the invasion of Crimea; or, the percentage of tweets that are negative towards Putin in Russia vs. the rest of the world; or, the percentage of tweets negative towards Putin vs. Barack Obama (US president). Sentiment analysis is commonly applied in several areas including tracking sentiment towards products, movies, politicians, and companies (O’Connor et al. 2010; Pang and Lee 2008), improving customer relation models (Bougie et al. 2003), detecting happiness and well-being (Schwartz et al. 2013), tracking the stock market (Bollen et al. 2011), and improving automatic dialogue systems (Velásquez 1997; Ravaja et al. 2006). The sheer volume of work in this area precludes detailed summarization here. Nonetheless, it should be noted that often the desired application can help direct certain design choices in the sentiment analysis system. For example, the threshold between neutral and positive sentiment and the threshold between neutral and negative sentiment can be determined empirically by what 4 Challenges in Sentiment Analysis 77 is most suitable for the target application. Similarly, as suggested earlier, some applications may require only the identification of strongly positive and strongly negative instances.","Challenges in Sentiment AnalysisSentiment analysis tasks • Sentiment of the writer, reader, and other entities • Sentiment towards aspects of an entity • Stance detection • Sentiment lexicons • Sentiment annotation • Multilingual sentiment analysist A vast majority of the work in Sentiment Analysis has been on developing more accurate sentiment classifiers, usually involving supervised machine learning algorithms and a battery of features. Surveys by Pang and Lee (Found Trends Inf Retr 2(1–2):1–135, 2008), Liu and Zhang (A survey of opinion mining and sentiment analysis. In: Aggarwal CC, Zhai C (eds) In: Mining text data. Springer, New York, pp 415–463, 2012), and Mohammad (Mohammad Sentiment analysis: detecting valence, emotions, and other effectual states from text. In: Meiselman H (ed) Emotion measurement. Elsevier, Amsterdam, 2016b) give summaries of the many automatic classifiers, features, and datasets used to detect sentiment. In this chapter, we flesh out some of the challenges that still remain, questions that have not been explored sufficiently, and new issues emerging from taking on new sentiment analysis problems. We also discuss proposals to deal with these challenges. The goal of this chapter is to equip researchers and practitioners with pointers to the latest developments in sentiment analysis and encourage more work in the diverse landscape of problems, especially those areas that are relatively less explored.Applications of sentiment analysis benefit from the fact that even though systems are not extremely accurate at determining sentiment of individual sentences, they can accurately capture significant changes in the proportion of instances that are positive (or negative). It is also worth noting that such sentiment tracking systems are more effective when incorporating carefully chosen baselines. For example, knowing the percentage of tweets that are negative towards Russian President, Vladimir Putin, is less useful than, for instance, knowing: the percentage of tweets that are negative towards Putin before vs. after the invasion of Crimea; or, the percentage of tweets that are negative towards Putin in Russia vs. the rest of the world; or, the percentage of tweets negative towards Putin vs. Barack Obama (US president). Sentiment analysis is commonly applied in several areas including tracking sentiment towards products, movies, politicians, and companies (O’Connor et al. 2010; Pang and Lee 2008), improving customer relation models (Bougie et al. 2003), detecting happiness and well-being (Schwartz et al. 2013), tracking the stock market (Bollen et al. 2011), and improving automatic dialogue systems (Velásquez 1997; Ravaja et al. 2006). The sheer volume of work in this area precludes detailed summarization here. Nonetheless, it should be noted that often the desired application can help direct certain design choices in the sentiment analysis system. For example, the threshold between neutral and positive sentiment and the threshold between neutral and negative sentiment can be determined empirically by what 4 Challenges in Sentiment Analysis 77 is most suitable for the target application. Similarly, as suggested earlier, some applications may require only the identification of strongly positive and strongly negative instances.",sentiment analysis,"The text discusses the challenges and opportunities in sentiment analysis, which involves developing accurate classifiers to detect sentiment in text. Despite the vast amount of work in this area, there are still unresolved questions and new issues emerging. However, sentiment analysis has many applications, such as tracking sentiment towards products, movies, politicians, and companies, improving customer relations, detecting happiness and well-being, tracking the stock market, and improving automatic dialogue systems. While the desired application can guide certain design choices in sentiment analysis systems, it is important to incorporate carefully chosen baselines to accurately capture significant changes in sentiment.",Object and Sentiment Recognition,,Sentiment Analysis
317,Erratum to: Multilingual Sentiment Analysis: State of the Art and Independent Comparison of Techniques,Sentiment analysis Sentiment classification Polarity shift,"With the advent of the internet, people actively express their opinions about products, services, events, political parties, etc., in social media, blogs, and website comments. The amount of research work on sentiment analysis is growing explosively. However, the majority of research efforts are devoted to English language data, while a great share of information is available in other languages. We present a state-of-the-art review on multilingual sentiment analysis. More importantly, we compare our own implementation of existing state-of-the-art approaches on common data. Precision observed in our experiments is typically lower than that reported by the original authors, which we attribute to lack of detail in the original presentation of those approaches. Thus, we compare the existing works by what they really offer to the reader, including whether they allow for accurate implementation and for reliable reproduction of the reported results.","We gave an overview of state-of-the-art multilingual sentiment analysis methods. We described data pre-processing, typical features, and the main resources used for multilingual sentiment analysis. Then, we discussed different approaches applied by their authors to English and other languages. We have classified these approaches into corpus-based, lexicon-based, and hybrid ones. The real value of any sentiment analysis technique for the research community corresponds to the results that can be reproduced with it, not in the results its original authors reportedly obtained with it. To evaluate this real value, we have implemented eleven selected approaches","Erratum to: Multilingual Sentiment Analysis: State of the Art and Independent Comparison of TechniquesSentiment analysis Sentiment classification Polarity shiftWith the advent of the internet, people actively express their opinions about products, services, events, political parties, etc., in social media, blogs, and website comments. The amount of research work on sentiment analysis is growing explosively. However, the majority of research efforts are devoted to English language data, while a great share of information is available in other languages. We present a state-of-the-art review on multilingual sentiment analysis. More importantly, we compare our own implementation of existing state-of-the-art approaches on common data. Precision observed in our experiments is typically lower than that reported by the original authors, which we attribute to lack of detail in the original presentation of those approaches. Thus, we compare the existing works by what they really offer to the reader, including whether they allow for accurate implementation and for reliable reproduction of the reported results.We gave an overview of state-of-the-art multilingual sentiment analysis methods. We described data pre-processing, typical features, and the main resources used for multilingual sentiment analysis. Then, we discussed different approaches applied by their authors to English and other languages. We have classified these approaches into corpus-based, lexicon-based, and hybrid ones. The real value of any sentiment analysis technique for the research community corresponds to the results that can be reproduced with it, not in the results its original authors reportedly obtained with it. To evaluate this real value, we have implemented eleven selected approaches",sentiment analysis,"The article discusses the growth of sentiment analysis research, but notes that most efforts are focused on English language data despite a significant amount of information being available in other languages. The authors provide a review of multilingual sentiment analysis, comparing existing state-of-the-art approaches and implementing them on common data. The authors classify the approaches into corpus-based, lexicon-based, and hybrid ones and emphasize the importance of evaluating the real value of sentiment analysis techniques through reproducible results. The article concludes with a discussion of the authors' own experiments and observations.",Object and Sentiment Recognition,,Sentiment Analysis
318,eSAP: A decision support framework for enhanced sentiment analysis and polarity classification,Sentiment analysis SentiWordNet Movie reviews Text mining Polarity detection Sentiment orientation Social media,"Sentiment analysis or opinion mining is an imperative research area of natural language processing. It is used to determine the writer’s attitude or speaker’s opinion towards a particular person, product or topic. Polarity or subjectivity classification is the process of categorizing a piece of text into positive or negative classes. In recent years, various supervised and unsupervised methods have been presented to accomplish sentiment polarity detection. SentiWordNet (SWN) has been extensively used as a lexical resource for opinion mining. This research incorporates SWN as the labeled training corpus where the sentiment scores are extracted based on the part of speech information. A vocabulary SWNV with revised sentiment scores, generated from SWN, is then used for Support Vector Machines model learning and classification process. Based on this vocabulary, a framework named “Enhanced Sentiment Analysis and Polarity Classification (eSAP)” is proposed. Training, testing and evaluation of the proposed eSAP are conducted on seven benchmark datasets from various domains. 10-fold cross validated accuracy, precision, recall, and fmeasure results averaged over seven datasets for the proposed framework are 80.82%, 80.83%, 80.94% and 80.81% respectively. A notable performance improvement of 13.4% in accuracy, 14.2% in precision, 6.9% in recall and 11.1% in f-measure is observed on average by evaluating the proposed eSAP against the baseline SWN classifier. State of the art performance comparison is conducted which also verifies the superiority of the proposed eSAP framework.","The application of supervised learning has been the prime research focus for text classification. Labeled datasets are required in order to train supervised classifiers. This becomes the key concern as tagged datasets are not easily available and it takes huge effort and resources to build such datasets. In such cases, application of unsupervised approaches is nontrivial. Sentiment lexicons, such as SentiWordNet, provide an efficient way for unsupervised text categorization. However, there is a need to improve the performance of SentiWordNet. This research is focused to improve SentiWordNet performance and proposed a complete sentiment analysis and classification framework based on SentiWordNet based vocabulary. Seven publically available datasets are used for performance evaluation and state of the art comparison. A notable performance improvement of 13.4% in accuracy, 14.2% in precision, 6.9% in recall and 11.1% in f-measure is observed by evaluating the proposed eSAP against the baseline SWN classifier. State of the art performance comparison is conducted for Cornell Movie Review and Multi-Domain Sentiment datasets which also verifies the superiority of the proposed eSAP framework. In future, we plan to explore other approaches like cosine similarity, information gain and gain ratio in a transfer learning methodology to further improve SentiWordNet performance","eSAP: A decision support framework for enhanced sentiment analysis and polarity classificationSentiment analysis SentiWordNet Movie reviews Text mining Polarity detection Sentiment orientation Social mediaSentiment analysis or opinion mining is an imperative research area of natural language processing. It is used to determine the writer’s attitude or speaker’s opinion towards a particular person, product or topic. Polarity or subjectivity classification is the process of categorizing a piece of text into positive or negative classes. In recent years, various supervised and unsupervised methods have been presented to accomplish sentiment polarity detection. SentiWordNet (SWN) has been extensively used as a lexical resource for opinion mining. This research incorporates SWN as the labeled training corpus where the sentiment scores are extracted based on the part of speech information. A vocabulary SWNV with revised sentiment scores, generated from SWN, is then used for Support Vector Machines model learning and classification process. Based on this vocabulary, a framework named “Enhanced Sentiment Analysis and Polarity Classification (eSAP)” is proposed. Training, testing and evaluation of the proposed eSAP are conducted on seven benchmark datasets from various domains. 10-fold cross validated accuracy, precision, recall, and fmeasure results averaged over seven datasets for the proposed framework are 80.82%, 80.83%, 80.94% and 80.81% respectively. A notable performance improvement of 13.4% in accuracy, 14.2% in precision, 6.9% in recall and 11.1% in f-measure is observed on average by evaluating the proposed eSAP against the baseline SWN classifier. State of the art performance comparison is conducted which also verifies the superiority of the proposed eSAP framework.The application of supervised learning has been the prime research focus for text classification. Labeled datasets are required in order to train supervised classifiers. This becomes the key concern as tagged datasets are not easily available and it takes huge effort and resources to build such datasets. In such cases, application of unsupervised approaches is nontrivial. Sentiment lexicons, such as SentiWordNet, provide an efficient way for unsupervised text categorization. However, there is a need to improve the performance of SentiWordNet. This research is focused to improve SentiWordNet performance and proposed a complete sentiment analysis and classification framework based on SentiWordNet based vocabulary. Seven publically available datasets are used for performance evaluation and state of the art comparison. A notable performance improvement of 13.4% in accuracy, 14.2% in precision, 6.9% in recall and 11.1% in f-measure is observed by evaluating the proposed eSAP against the baseline SWN classifier. State of the art performance comparison is conducted for Cornell Movie Review and Multi-Domain Sentiment datasets which also verifies the superiority of the proposed eSAP framework. In future, we plan to explore other approaches like cosine similarity, information gain and gain ratio in a transfer learning methodology to further improve SentiWordNet performance",sentiment analysis,"The text discusses sentiment analysis or opinion mining, which involves determining the writer's attitude or speaker's opinion towards a particular person, product or topic. The use of SentiWordNet (SWN) as a lexical resource for opinion mining is explored, and a framework called ""Enhanced Sentiment Analysis and Polarity Classification (eSAP)"" is proposed. The framework is evaluated on seven benchmark datasets, and a notable performance improvement is observed compared to the baseline SWN classifier. The use of supervised learning for text classification is discussed, as well as the limitations of unsupervised approaches. Future work includes exploring other approaches to improve SWN performance.",Object and Sentiment Recognition,,Sentiment Analysis
319,A fuzzy computational model of emotion for cloud based sentiment analysis,Hybrid cloud Big data Emotion modeling Affective computing Adaptive fuzzy systems Social network sentiment analysis,"This paper presents a novel emotion modeling methodology for incorporating human emotion into intelligent computer systems. The proposed approach includes a method to elicit emotion information from users, a new representation of emotion (AV-AT model) that is modelled using a genetically optimized adaptive fuzzy logic technique, and a framework for predicting and tracking user’s affective trajectory over time. The fuzzy technique is evaluated in terms of its ability to model affective states in comparison to other existing machine learning approaches. The performance of the proposed affect modeling methodology is tested through the deployment of a personalised learning system, and series of offline and online experiments. A hybrid cloud intelligence infrastructure is used to conduct large-scale experiments to analyze user sentiments and associated emotions, using data from a million Facebook users. A performance analysis of the infrastructure on processing, analyzing, and data storage has been carried out, illustrating its viability for large-scale data processing tasks. A comparison of the proposed emotion categorizing approach with Facebook’s sentiment analysis API demonstrates that our approach can achieve comparable performance. Finally, discussions on research contributions to cloud intelligence using sentiment analysis, emotion modeling, big data, and comparisons with other approaches are presented in detail.","This paper introduced a methodology for incorporating emotion in the design of intelligent computer systems, and explored its applicability and performance, through carrying out a series of online and offline experiments. The approach presented, initially establishes the mixed AV-AT emotion model. In order for this model to be successfully utilized, an adaptive fuzzy modeling method was implemented, which used optimized parameters with the help of a GA. A framework and basic architecture was proposed, which integrates the suggested approach, so that it can be utilized by affective computing systems. Moreover, an AC system was developed to evaluate the performance of the suggested affect modeling methodology in a real setting, while at the same time promoting student learning and engagement within modern pedagogical contexts. We have demonstrated a cloud computational intelligence infrastructure, which can integrate the suggested emotion modeling approach. This was achieved by conducting large-scale experiments carrying out data processing, sentiment analysis, and storage on data comprising of one million Facebook users. The proposed emotion modeling approach can be used as part of a cloud intelligence framework through the use of hybrid cloud services. Explanations for our research impact have been justified to ensure that our work is unique and significant. Contributions for big data processing were explained to ensure that our work could bridge the gap between theory and practice. The main contributions of our research can be summarized as follows: • A novel emotion modeling methodology is proposed for incorporating human emotion as part of intelligent computer systems. • A new mixed representation of emotion called the AV-AT model is presented offering high recognition accuracy and enabling flexibility in choosing suitable sets of emotions. • The adaptive fuzzy method presented, achieved a satisfactory classification performance compared to other well-known ML approaches, while at the same time retaining a high degree of interpretability. • A personalized learning system was developed, specifically designed for assisting students in the context of PBL pedagogical framework that has been tested successfully in two practical tutorial sessions. • Research directions were presented for applying this methodology in various contexts. 462 C. Karyotis et al. / Information Sciences 433–434 (2018) 448–463 • We demonstrated cloud intelligence and provided evidence of the ability and effectiveness of a large-scale deployment. Our hybrid cloud intelligence service processed and performed sentiment analysis and stored the outputs, with competitive execution times at all sites. • The proposed computational intelligence based emotion modelling approach was used to implicitly classify emotion states, and achieved a high percentage of matching with Facebook’s sentiment analysis. By providing a novel computational methodology to represent and model emotion, we aim to enhance our understanding of the incorporation of emotion in the design of intelligent computing systems, resulting in the improvement of services provided by those systems to their users. Future work will involve the modification of our approach to account for the transition probabilities between affective states. We aim to achieve this by using dynamic modeling tools, such as the Fuzzy Cognitive Map (FCM) [51] methodology. Future developments of the work will also include analysis of more up-to-date, and larger scale user data, along with the deployment of state of the art bio-inspired optimization algorithms in order to improve specific parameters of the developed fuzzy model.","A fuzzy computational model of emotion for cloud based sentiment analysisHybrid cloud Big data Emotion modeling Affective computing Adaptive fuzzy systems Social network sentiment analysisThis paper presents a novel emotion modeling methodology for incorporating human emotion into intelligent computer systems. The proposed approach includes a method to elicit emotion information from users, a new representation of emotion (AV-AT model) that is modelled using a genetically optimized adaptive fuzzy logic technique, and a framework for predicting and tracking user’s affective trajectory over time. The fuzzy technique is evaluated in terms of its ability to model affective states in comparison to other existing machine learning approaches. The performance of the proposed affect modeling methodology is tested through the deployment of a personalised learning system, and series of offline and online experiments. A hybrid cloud intelligence infrastructure is used to conduct large-scale experiments to analyze user sentiments and associated emotions, using data from a million Facebook users. A performance analysis of the infrastructure on processing, analyzing, and data storage has been carried out, illustrating its viability for large-scale data processing tasks. A comparison of the proposed emotion categorizing approach with Facebook’s sentiment analysis API demonstrates that our approach can achieve comparable performance. Finally, discussions on research contributions to cloud intelligence using sentiment analysis, emotion modeling, big data, and comparisons with other approaches are presented in detail.This paper introduced a methodology for incorporating emotion in the design of intelligent computer systems, and explored its applicability and performance, through carrying out a series of online and offline experiments. The approach presented, initially establishes the mixed AV-AT emotion model. In order for this model to be successfully utilized, an adaptive fuzzy modeling method was implemented, which used optimized parameters with the help of a GA. A framework and basic architecture was proposed, which integrates the suggested approach, so that it can be utilized by affective computing systems. Moreover, an AC system was developed to evaluate the performance of the suggested affect modeling methodology in a real setting, while at the same time promoting student learning and engagement within modern pedagogical contexts. We have demonstrated a cloud computational intelligence infrastructure, which can integrate the suggested emotion modeling approach. This was achieved by conducting large-scale experiments carrying out data processing, sentiment analysis, and storage on data comprising of one million Facebook users. The proposed emotion modeling approach can be used as part of a cloud intelligence framework through the use of hybrid cloud services. Explanations for our research impact have been justified to ensure that our work is unique and significant. Contributions for big data processing were explained to ensure that our work could bridge the gap between theory and practice. The main contributions of our research can be summarized as follows: • A novel emotion modeling methodology is proposed for incorporating human emotion as part of intelligent computer systems. • A new mixed representation of emotion called the AV-AT model is presented offering high recognition accuracy and enabling flexibility in choosing suitable sets of emotions. • The adaptive fuzzy method presented, achieved a satisfactory classification performance compared to other well-known ML approaches, while at the same time retaining a high degree of interpretability. • A personalized learning system was developed, specifically designed for assisting students in the context of PBL pedagogical framework that has been tested successfully in two practical tutorial sessions. • Research directions were presented for applying this methodology in various contexts. 462 C. Karyotis et al. / Information Sciences 433–434 (2018) 448–463 • We demonstrated cloud intelligence and provided evidence of the ability and effectiveness of a large-scale deployment. Our hybrid cloud intelligence service processed and performed sentiment analysis and stored the outputs, with competitive execution times at all sites. • The proposed computational intelligence based emotion modelling approach was used to implicitly classify emotion states, and achieved a high percentage of matching with Facebook’s sentiment analysis. By providing a novel computational methodology to represent and model emotion, we aim to enhance our understanding of the incorporation of emotion in the design of intelligent computing systems, resulting in the improvement of services provided by those systems to their users. Future work will involve the modification of our approach to account for the transition probabilities between affective states. We aim to achieve this by using dynamic modeling tools, such as the Fuzzy Cognitive Map (FCM) [51] methodology. Future developments of the work will also include analysis of more up-to-date, and larger scale user data, along with the deployment of state of the art bio-inspired optimization algorithms in order to improve specific parameters of the developed fuzzy model.",sentiment analysis,"The paper proposes a novel methodology for incorporating human emotion into intelligent computer systems, which includes a method to elicit emotion information from users, a new representation of emotion called the AV-AT model, and a framework for predicting and tracking user's affective trajectory over time. The approach is evaluated using offline and online experiments and a hybrid cloud intelligence infrastructure is used to conduct large-scale experiments to analyze user sentiments and associated emotions. The paper demonstrates the effectiveness of the proposed approach in comparison to other machine learning approaches, and discusses its potential applications in various contexts. Future work will involve the modification of the approach to account for the transition probabilities between affective states and analysis of more up-to-date and larger scale user data.",Object and Sentiment Recognition,,Sentiment Analysis
320,Prediction of Terrorist Activities by Using Unsupervised Learning Techniques,Unsupervised learning; Distance Based Clustering; Density Based Clustering; Sentiments analysis,Terrorism now considered as a major threat to the world population. Terrorism is increasing day by day by different means. From the last decade terrorism rate is increasing exponentially. But there is no efficient way for prediction of terrorism activities. Our paper focuses on prediction of different terrorist attacks by using data mining techniques. In this paper we proposed prediction of attacks by using unsupervised clustering technique. We proposed a framework in which we do sentiments analysis of our data and then by using a combination of density based clustering and distance based clustering we assign classes to our data. Class labels help us to predict terrorism attacks. By research we come to know that combination of these two clustering techniques give accurate results. This proposed framework gives high level of accuracy and it is useful in prediction of attacks types. It gives us a way to deal with terrorism attacks in advance and makes our society peaceful.,"Terrorist attacks prediction by using unsupervised learning gives us a new path for extracting information from big data. Our proposed framework combines two main clustering techniques and gives best results. Data mining is a popular field widely Prediction of Terrorist Activities by Using Unsupervised Learning Techniques 60 J. Appl. Emerg. Sci., 2016, 6(2) used in biomedical and for security purposes. Our unsupervised clustering based model makes proper discrimination between normal messages and terrorist messages. It scans all the items of large data set and determines terrorist attacks. Our proposed model also gives comfort to security agencies and helps them to deal with uncertain conditions in advance. It increases trust of people on security agencies.","Prediction of Terrorist Activities by Using Unsupervised Learning TechniquesUnsupervised learning; Distance Based Clustering; Density Based Clustering; Sentiments analysisTerrorism now considered as a major threat to the world population. Terrorism is increasing day by day by different means. From the last decade terrorism rate is increasing exponentially. But there is no efficient way for prediction of terrorism activities. Our paper focuses on prediction of different terrorist attacks by using data mining techniques. In this paper we proposed prediction of attacks by using unsupervised clustering technique. We proposed a framework in which we do sentiments analysis of our data and then by using a combination of density based clustering and distance based clustering we assign classes to our data. Class labels help us to predict terrorism attacks. By research we come to know that combination of these two clustering techniques give accurate results. This proposed framework gives high level of accuracy and it is useful in prediction of attacks types. It gives us a way to deal with terrorism attacks in advance and makes our society peaceful.Terrorist attacks prediction by using unsupervised learning gives us a new path for extracting information from big data. Our proposed framework combines two main clustering techniques and gives best results. Data mining is a popular field widely Prediction of Terrorist Activities by Using Unsupervised Learning Techniques 60 J. Appl. Emerg. Sci., 2016, 6(2) used in biomedical and for security purposes. Our unsupervised clustering based model makes proper discrimination between normal messages and terrorist messages. It scans all the items of large data set and determines terrorist attacks. Our proposed model also gives comfort to security agencies and helps them to deal with uncertain conditions in advance. It increases trust of people on security agencies.",sentiment analysis,"The paper discusses the increasing threat of terrorism worldwide and the lack of efficient methods for predicting terrorist activities. The authors propose a framework for predicting different types of terrorist attacks using unsupervised clustering techniques, specifically a combination of density-based and distance-based clustering. This framework incorporates sentiment analysis and class labels to accurately predict terrorism attacks. The authors note that their proposed model has high accuracy and can be useful in dealing with uncertain conditions in advance, increasing trust in security agencies.",Object and Sentiment Recognition,,Sentiment Analysis
321,Like It or Not: A Survey of Twitter Sentiment Analysis Methods,": Sentiment analysis, opinion mining, microblogs, twitter","Sentiment analysis in Twitter is a field that has recently attracted research interest. Twitter is one of the most popular microblog platforms on which users can publish their thoughts and opinions. Sentiment analysis in Twitter tackles the problem of analyzing the tweets in terms of the opinion they express. This survey provides an overview of the topic by investigating and briefly describing the algorithms that have been proposed for sentiment analysis in Twitter. The presented studies are categorized according to the approach they follow. In addition, we discuss fields related to sentiment analysis in Twitter including Twitter opinion retrieval, tracking sentiments over time, irony detection, emotion detection, and tweet sentiment quantification, tasks that have recently attracted increasing attention. Resources that have been used in the Twitter sentiment analysis literature are also briefly presented. The main contributions of this survey include the presentation of the proposed approaches for sentiment analysis in Twitter, their categorization according to the technique they use, and the discussion of recent research trends of the topic and its related fields.","Recent years have witnessed an increasing research interest in analyzing tweets according to the sentiment they express. This interest is a result of the large amount of messages that are posted everyday in Twitter andthat contain valuable information for the public mood for a number of different topics. Methods from the machine-learning field are applied for TSA on a more frequent rate compared to the rest of the approaches, with the SVM and NB classifiers being the most prevalent. Unigram-based ACM Computing Surveys, Vol. 49, No. 2, Article 28, Publication date: June 2016. Like It or Not: A Survey of Twitter Sentiment Analysis Methods 28:35 SVM is usually considered as a baseline to which the proposed methods are compared. In addition, lexicons are utilized in a large set of proposed methods to support detecting words that indicate sentiment. SentiWordNet and MPQA are the most used lexicons that are usually extended with words that are used in Twitter. This survey presented an overview on the recent updates of TSA. More than 50 articles were categorized and briefly described. After the analysis, it is clear that TSA is still an open field for research. We reviewed the most prominent approaches for TSA and discussed prevalent methods for Twitter-based opinion retrieval, tracking sentiments over time, irony detection, emotion detection, and tweet sentiment quantification. In addition, we presented different resources (datasets and sentiment lexicons) that have been used in TSA. Our survey gave a comprehensive review of the proposed TSA methods, discussed related trends, identified interesting open problems, and indicated promising future research directions.","Like It or Not: A Survey of Twitter Sentiment Analysis Methods: Sentiment analysis, opinion mining, microblogs, twitterSentiment analysis in Twitter is a field that has recently attracted research interest. Twitter is one of the most popular microblog platforms on which users can publish their thoughts and opinions. Sentiment analysis in Twitter tackles the problem of analyzing the tweets in terms of the opinion they express. This survey provides an overview of the topic by investigating and briefly describing the algorithms that have been proposed for sentiment analysis in Twitter. The presented studies are categorized according to the approach they follow. In addition, we discuss fields related to sentiment analysis in Twitter including Twitter opinion retrieval, tracking sentiments over time, irony detection, emotion detection, and tweet sentiment quantification, tasks that have recently attracted increasing attention. Resources that have been used in the Twitter sentiment analysis literature are also briefly presented. The main contributions of this survey include the presentation of the proposed approaches for sentiment analysis in Twitter, their categorization according to the technique they use, and the discussion of recent research trends of the topic and its related fields.Recent years have witnessed an increasing research interest in analyzing tweets according to the sentiment they express. This interest is a result of the large amount of messages that are posted everyday in Twitter andthat contain valuable information for the public mood for a number of different topics. Methods from the machine-learning field are applied for TSA on a more frequent rate compared to the rest of the approaches, with the SVM and NB classifiers being the most prevalent. Unigram-based ACM Computing Surveys, Vol. 49, No. 2, Article 28, Publication date: June 2016. Like It or Not: A Survey of Twitter Sentiment Analysis Methods 28:35 SVM is usually considered as a baseline to which the proposed methods are compared. In addition, lexicons are utilized in a large set of proposed methods to support detecting words that indicate sentiment. SentiWordNet and MPQA are the most used lexicons that are usually extended with words that are used in Twitter. This survey presented an overview on the recent updates of TSA. More than 50 articles were categorized and briefly described. After the analysis, it is clear that TSA is still an open field for research. We reviewed the most prominent approaches for TSA and discussed prevalent methods for Twitter-based opinion retrieval, tracking sentiments over time, irony detection, emotion detection, and tweet sentiment quantification. In addition, we presented different resources (datasets and sentiment lexicons) that have been used in TSA. Our survey gave a comprehensive review of the proposed TSA methods, discussed related trends, identified interesting open problems, and indicated promising future research directions.",sentiment analysis,"The text discusses the recent research interest in sentiment analysis on Twitter, which involves analyzing tweets to determine the opinion they express. Various algorithms and techniques have been proposed and categorized, with machine learning methods being the most prevalent. Lexicons are also commonly used to support sentiment detection, and there are related fields such as opinion retrieval, emotion detection, and tweet sentiment quantification. The article presents a comprehensive overview of the current state of research in TSA, identifying open problems and future research directions.",Object and Sentiment Recognition,,Sentiment Analysis
322,Long Short-term Memory Network over Rhetorical Structure Theory for Sentence-level Sentiment Analysis,": LSTM, Rhetorical Structure Theory, sentiment analysis","Using deep learning models to solve sentiment analysis of sentences is still a challenging task. Long short-term memory (LSTM) network solves the gradient disappeared problem existed in recurrent neural network (RNN), but LSTM structure is linear chain-structure that can’t capture text structure information. Afterwards, Tree-LSTM is proposed, which uses LSTM forget gate to skip sub-trees that have little effect on the results to get good performance. It illustrates that the chain-structured LSTM more strongly depends on text structure. However, Tree-LSTM can’t clearly figure out which sub-trees are important and which sub-trees have little effect. We propose a simple model which uses Rhetorical Structure Theory (RST) for text parsing. By building LSTM network on RST parse structure, we make full use of LSTM structural characteristics to automatically enhance the nucleus information and filter the satellite information of text. Furthermore, this approach can make the representations concerning the relations between segments of text, which can improve text semantic representations. Experiment results show that this method not only has higher classification accuracy, but also trains quickly.","During learning process of text features, further emphasizing the structure of text helps to improve the feature representation, thereby improving the classification accuracy. We propose RST-LSTM model that build deep learning network on RST parse tree. Each node in RST parse tree represents the nucleus segment or the satellite segment. The model can selectively integrate information of child nodes to update cell state by using their corresponding forget gates and captures long-distance relationship. Introducing RST into the sentence-level sentiment classification task and deeply studying the relations features of text make semantic combination based on text structure more accurate. The model is trained on Stanford Sentiment Treebank dataset Socher et al. (2013). Experimental results show that the model not only has higher classification accuracy than the state-of-the-art methods, which improves the accuracy about 2 percent, but also trains quickly.","Long Short-term Memory Network over Rhetorical Structure Theory for Sentence-level Sentiment Analysis: LSTM, Rhetorical Structure Theory, sentiment analysisUsing deep learning models to solve sentiment analysis of sentences is still a challenging task. Long short-term memory (LSTM) network solves the gradient disappeared problem existed in recurrent neural network (RNN), but LSTM structure is linear chain-structure that can’t capture text structure information. Afterwards, Tree-LSTM is proposed, which uses LSTM forget gate to skip sub-trees that have little effect on the results to get good performance. It illustrates that the chain-structured LSTM more strongly depends on text structure. However, Tree-LSTM can’t clearly figure out which sub-trees are important and which sub-trees have little effect. We propose a simple model which uses Rhetorical Structure Theory (RST) for text parsing. By building LSTM network on RST parse structure, we make full use of LSTM structural characteristics to automatically enhance the nucleus information and filter the satellite information of text. Furthermore, this approach can make the representations concerning the relations between segments of text, which can improve text semantic representations. Experiment results show that this method not only has higher classification accuracy, but also trains quickly.During learning process of text features, further emphasizing the structure of text helps to improve the feature representation, thereby improving the classification accuracy. We propose RST-LSTM model that build deep learning network on RST parse tree. Each node in RST parse tree represents the nucleus segment or the satellite segment. The model can selectively integrate information of child nodes to update cell state by using their corresponding forget gates and captures long-distance relationship. Introducing RST into the sentence-level sentiment classification task and deeply studying the relations features of text make semantic combination based on text structure more accurate. The model is trained on Stanford Sentiment Treebank dataset Socher et al. (2013). Experimental results show that the model not only has higher classification accuracy than the state-of-the-art methods, which improves the accuracy about 2 percent, but also trains quickly.",sentiment analysis,"The text discusses the challenges of using deep learning models for sentiment analysis of sentences. It explains the limitations of the Long short-term memory (LSTM) network in capturing text structure information and how the Tree-LSTM network uses LSTM forget gate to skip sub-trees to achieve better performance. However, Tree-LSTM struggles to identify which sub-trees are important. The proposed RST-LSTM model builds a deep learning network on Rhetorical Structure Theory (RST) parse tree to automatically enhance nucleus information and filter satellite information while improving semantic representations of text. This approach achieves higher classification accuracy and trains quickly compared to the state-of-the-art methods. The model is trained on the Stanford Sentiment Treebank dataset.",Object and Sentiment Recognition,,Sentiment Analysis
323,Combine HowNet lexicon to train phrase recursive autoencoder for sentence-level sentiment analysis,Sentiment analysis Recursive autoencoder HowNet lexicon Phrase structure tree,"Detecting sentiment of sentences in online reviews is still a challenging task. Traditional machine learning methods often use bag-of-words representations which cannot properly capture complex linguistic phenomena in sentiment analysis. Recently, recursive autoencoder (RAE) methods have been proposed for sentence-level sentiment analysis. They use word embedding to represent each word, and learn compositional vector representation of phrases and sentences with recursive autoencoders. Although RAE methods outperform other state-of-the-art sentiment prediction approaches on commonly used datasets, they tend to generate very deep parse trees, and need a large amount of labeled data for each node during the process of learning compositional vector representations. Furthermore, RAE methods mainly combine adjacent words in sequence with a greedy strategy, which make capturing semantic relations between distant words difficult. To solve these issues, we propose a semi-supervised method which combines HowNet lexicon to train phrase recursive autoencoders (we call it CHL-PRAE). CHL-PRAE constructs the phrase recursive autoencoder (PRAE) model at first. Then the model calculates the sentiment orientation of each node with the HowNet lexicon, which acts as sentiment labels, when we train the softmax classifier of PRAE. Furthermore, our CHL-PRAE model conducts bidirectional training to capture global information. Compared with RAE and some supervised methods such as support vector machine (SVM) and naïve Bayesian on English and Chinese datasets, the experiment results show that CHL-PRAE can provide the best performance for sentence-level sentiment analysis.","In this paper, we propose a novel method that combines HowNet lexicon to train bidirectional PRAE model for sentiment analysis on different datasets. Compared with the existing methods, the experimental results show that our model outperforms other approaches in sentiment classification tasks. What is more, the training time and computational complexity are reduced because of the construction of phrase-level binary tree. Despite the success of the proposed method, Bi-CHL-PRAE model still requires further improvement, such as the algorithm of converting phrase dependency tree to phrase-level binary tree. In future works, we will consider some other approaches and validate them through experiments. Besides, the Bi-CHL-PRAE model construct deep learning network based on the parse tree of text and HowNet Lexicon, which lead that good or bad feature representations largely depend on syntax tree structure and the quality of HowNet Lexicon which both have some limits. So the optimization of syntax parse tree and HowNet Lexicon are important exploring directions in the future. In addition, we will study our model on more datasets.","Combine HowNet lexicon to train phrase recursive autoencoder for sentence-level sentiment analysisSentiment analysis Recursive autoencoder HowNet lexicon Phrase structure treeDetecting sentiment of sentences in online reviews is still a challenging task. Traditional machine learning methods often use bag-of-words representations which cannot properly capture complex linguistic phenomena in sentiment analysis. Recently, recursive autoencoder (RAE) methods have been proposed for sentence-level sentiment analysis. They use word embedding to represent each word, and learn compositional vector representation of phrases and sentences with recursive autoencoders. Although RAE methods outperform other state-of-the-art sentiment prediction approaches on commonly used datasets, they tend to generate very deep parse trees, and need a large amount of labeled data for each node during the process of learning compositional vector representations. Furthermore, RAE methods mainly combine adjacent words in sequence with a greedy strategy, which make capturing semantic relations between distant words difficult. To solve these issues, we propose a semi-supervised method which combines HowNet lexicon to train phrase recursive autoencoders (we call it CHL-PRAE). CHL-PRAE constructs the phrase recursive autoencoder (PRAE) model at first. Then the model calculates the sentiment orientation of each node with the HowNet lexicon, which acts as sentiment labels, when we train the softmax classifier of PRAE. Furthermore, our CHL-PRAE model conducts bidirectional training to capture global information. Compared with RAE and some supervised methods such as support vector machine (SVM) and naïve Bayesian on English and Chinese datasets, the experiment results show that CHL-PRAE can provide the best performance for sentence-level sentiment analysis.In this paper, we propose a novel method that combines HowNet lexicon to train bidirectional PRAE model for sentiment analysis on different datasets. Compared with the existing methods, the experimental results show that our model outperforms other approaches in sentiment classification tasks. What is more, the training time and computational complexity are reduced because of the construction of phrase-level binary tree. Despite the success of the proposed method, Bi-CHL-PRAE model still requires further improvement, such as the algorithm of converting phrase dependency tree to phrase-level binary tree. In future works, we will consider some other approaches and validate them through experiments. Besides, the Bi-CHL-PRAE model construct deep learning network based on the parse tree of text and HowNet Lexicon, which lead that good or bad feature representations largely depend on syntax tree structure and the quality of HowNet Lexicon which both have some limits. So the optimization of syntax parse tree and HowNet Lexicon are important exploring directions in the future. In addition, we will study our model on more datasets.",sentiment analysis,"The text discusses the challenges of detecting sentiment in online reviews using traditional machine learning methods and the potential of recursive autoencoder (RAE) methods for sentence-level sentiment analysis. However, RAE methods have limitations such as generating deep parse trees and requiring a large amount of labeled data for each node. To overcome these limitations, the authors propose a semi-supervised method called CHL-PRAE that combines HowNet lexicon to train a bidirectional PRAE model for sentiment analysis. The experimental results show that CHL-PRAE outperforms other approaches in sentiment classification tasks with reduced training time and computational complexity. However, the Bi-CHL-PRAE model still requires further improvement, and the optimization of syntax parse tree and HowNet Lexicon are important exploring directions in the future.",Object and Sentiment Recognition,,Sentiment Analysis
324,New term weighting schemes with combination of multiple classifiers for sentiment analysis,Sentiment classification Openion mining Term weighting schemes,"The rapid growth of social media on the Web, such as forum discussions, reviews, blogs, micro-blogs, social networks and Twitter has created huge volume of opinionated data in digital forms. Therefore, last decade showed growth of sentiment analysis task to be one of the most active research areas in natural language processing. In this work, the problem of classifying documents based on overall sentiment is investigated. The main goal of this work is to present comprehensive investigation of different proposed new term weighting schemes for sentiment classification. The proposed new term weighting schemes exploit the class space density based on the class distribution in the whole documents set as well as in the class documents set. The proposed approaches provide positive discrimination on frequent and infrequent terms. We have compared our new term weighting schemes with traditional and state of art term weighting schemes. Some of our proposed terms weighting schemes outperform the traditional and state of art term weighting schemes results.","In this paper, different approaches for automatic sentiment classification have been investigated based on different proposed new term weighting schemes. We have proposed new term weighting schemes that exploit the class space density based on the class distribution in the whole document set as well as the class documents set. The proposed approaches provided positive discrimination on frequent and infrequent terms. Some of our proposed term weighting schemes outperformed the traditional and state of art term weighting schemes results. In the future work, some other new term weighting schemes will be created to improve automatic sentiment classification. A certain optimization approach such as genetic algorithm could be exploited to get the optimum combination of all term weighting schemes.","New term weighting schemes with combination of multiple classifiers for sentiment analysisSentiment classification Openion mining Term weighting schemesThe rapid growth of social media on the Web, such as forum discussions, reviews, blogs, micro-blogs, social networks and Twitter has created huge volume of opinionated data in digital forms. Therefore, last decade showed growth of sentiment analysis task to be one of the most active research areas in natural language processing. In this work, the problem of classifying documents based on overall sentiment is investigated. The main goal of this work is to present comprehensive investigation of different proposed new term weighting schemes for sentiment classification. The proposed new term weighting schemes exploit the class space density based on the class distribution in the whole documents set as well as in the class documents set. The proposed approaches provide positive discrimination on frequent and infrequent terms. We have compared our new term weighting schemes with traditional and state of art term weighting schemes. Some of our proposed terms weighting schemes outperform the traditional and state of art term weighting schemes results.In this paper, different approaches for automatic sentiment classification have been investigated based on different proposed new term weighting schemes. We have proposed new term weighting schemes that exploit the class space density based on the class distribution in the whole document set as well as the class documents set. The proposed approaches provided positive discrimination on frequent and infrequent terms. Some of our proposed term weighting schemes outperformed the traditional and state of art term weighting schemes results. In the future work, some other new term weighting schemes will be created to improve automatic sentiment classification. A certain optimization approach such as genetic algorithm could be exploited to get the optimum combination of all term weighting schemes.",sentiment analysis,"The article discusses the growth of sentiment analysis in natural language processing due to the massive amount of opinionated data generated by social media. The focus is on classifying documents based on overall sentiment, and the article proposes new term weighting schemes that exploit the class space density based on the class distribution in the whole document set as well as the class documents set. The proposed approaches outperformed traditional and state-of-the-art term weighting schemes, and future work will involve creating more new term weighting schemes and optimizing them with a genetic algorithm.",Object and Sentiment Recognition,,Sentiment Analysis
325,Recognizing Opinion Sources Based on a New Categorization of Opinion Types,"sentiment analysis, opinion categorization, participant opinion, non-participant opinion, transductive learning, phrase-level embeddings, Probabilistic Soft Logic, joint prediction model, recognizing sources of opinions, SVM, state-of-the-art works.","Recognizing sources of opinions is an important task in sentiment analysis. Different from previous works which categorize an opinion according to whether the source is the writer or the source is a noun phrase, we propose a new categorization of opinions according to the role that the source plays. The source of a participant opinion is a participant in the event that triggers the opinion. On the contrary, the source of a non-participant opinion is not a participant. Based on this new categorization, we classify an opinion using phrase-level embeddings. A transductive learning method is used for the classifier since there is no existing annotated corpora of this new categorization. A joint prediction model of Probabilistic Soft Logic then recognizes the sources of the two types of opinions in a single model. The experiments have shown that our model improves recognizing sources of opinions over baselines and several state-of-the-art works.",This work improves recognizing sources of opinions based on a new categorization of opinions: non-participant opinion or participant opinion. A transductive SVM is built to classify an opinion utilizing existing limited resources. The categorization information is then utilized by a Probabilistic Soft Logic model to jointly recognize sources of the two types of opinions in a single model. The experiments have shown that the model based on this new categorization of opinions achieves better performances over baselines and several state-of-theart works in recognizing sources.,"Recognizing Opinion Sources Based on a New Categorization of Opinion Typessentiment analysis, opinion categorization, participant opinion, non-participant opinion, transductive learning, phrase-level embeddings, Probabilistic Soft Logic, joint prediction model, recognizing sources of opinions, SVM, state-of-the-art works.Recognizing sources of opinions is an important task in sentiment analysis. Different from previous works which categorize an opinion according to whether the source is the writer or the source is a noun phrase, we propose a new categorization of opinions according to the role that the source plays. The source of a participant opinion is a participant in the event that triggers the opinion. On the contrary, the source of a non-participant opinion is not a participant. Based on this new categorization, we classify an opinion using phrase-level embeddings. A transductive learning method is used for the classifier since there is no existing annotated corpora of this new categorization. A joint prediction model of Probabilistic Soft Logic then recognizes the sources of the two types of opinions in a single model. The experiments have shown that our model improves recognizing sources of opinions over baselines and several state-of-the-art works.This work improves recognizing sources of opinions based on a new categorization of opinions: non-participant opinion or participant opinion. A transductive SVM is built to classify an opinion utilizing existing limited resources. The categorization information is then utilized by a Probabilistic Soft Logic model to jointly recognize sources of the two types of opinions in a single model. The experiments have shown that the model based on this new categorization of opinions achieves better performances over baselines and several state-of-theart works in recognizing sources.",sentiment analysis,"The text describes a new approach to sentiment analysis that categorizes opinions based on the role of the source, rather than whether the source is a writer or noun phrase. The approach uses a transductive learning method and a joint prediction model to recognize sources of participant and non-participant opinions. The approach outperforms previous methods in recognizing sources of opinions.",Object and Sentiment Recognition,,Sentiment Analysis
326,Sentiment Analysis in Social Media and Its Application,Sentiment analysis; Big data; Social media,"Twitter and sentiment analysis application can be seen in world events, healthcare, politics and busines its application. Social media contain a large amount of raw data that has been uploaded by users in the form of text, videos, photos 
and audio. The data can be converted into valuable information by using sentiment analysis. A systematic review of studies 
published between 2014 to 2019 was undertaken using the following trusted and credible database including ACM, Emerald 
Insight, IEEE Xplore, Science Direct and Scopus. After the initial and in-depth screening of paper, 24 out of 77 articles have been 
chosen from the review process. The articles have been reviewed based on the aim of the study. The result shows most of the 
articles applied opinion-lexicon method to analyses text sentiment in social media, extracted data on microblogging site mainlyTwitter and sentiment analysis application can be seen in world events, healthcare, politics and business","The conducted systematic literature review provides information on studies on sentiment analysis in social media. 
The paper makes the following three contributions. First, we show what is the method used in analyzing sentiment in 
social media. There is various method introduced by researches, still, the most common method uses in Lexicon based 
method is SentiWordnet and TF-IDF while for machine learning is Naïve Bayes and SVM. Choosing the appropriate 
method of sentiment analysis is depending on the data itself. Both methods demonstrated a similar accuracy. The 
things that need to take into consideration is the structure of the text, time and amount of data. If the data structure is 
messy, a small amount of data and limited time available to analyses, it is recommended to go for lexicon-based 
method. Bigger data is suitable for machine learning based method as it requires more time and data to train. In order 
to improve the quality and accuracy of the result, it is suggested to combine both lexicon and machine learning method. 
 Second, we identify what is the most common type of social media site to extract information for sentiment 
analysis. The most popular social media site to extract information is Twitter. Most of the reviewed paper use twitter 
as their social media context. This is due to the availability, accessibility and richness of Twitter content. There are 
millions of tweets every day on almost any topic. This indicates that social media is becoming a precious source of 
information. However, less attention is given to other social media sources such as blogs, WordPress, YouTube and 
others. The content of each social media might be different, and it is worth exploring other sources might open to new 
knowledge and findings. 
 Third, we demonstrate the application of sentiment analysis in social media. Sentiment analysis has a broad 
application and can be utilized in different areas such as improving quality and strategy in business, political 
forecasting an election result, monitor disease outbreak, create awareness on the importance of data security, 
perception towards a particular sport, and improve locate and response to the disaster. This shows that sentiment 
analysis plays a huge role to understand people perception and helps in decision making. For future recommendation, 
further investigation is needed to develop a universal model of sentiment analysis that can be applied to a different 
type of data, explores other potential social networking sites to obtain users opinion and expanding the context of 
sentiment analysis application.","Sentiment Analysis in Social Media and Its ApplicationSentiment analysis; Big data; Social mediaTwitter and sentiment analysis application can be seen in world events, healthcare, politics and busines its application. Social media contain a large amount of raw data that has been uploaded by users in the form of text, videos, photos 
and audio. The data can be converted into valuable information by using sentiment analysis. A systematic review of studies 
published between 2014 to 2019 was undertaken using the following trusted and credible database including ACM, Emerald 
Insight, IEEE Xplore, Science Direct and Scopus. After the initial and in-depth screening of paper, 24 out of 77 articles have been 
chosen from the review process. The articles have been reviewed based on the aim of the study. The result shows most of the 
articles applied opinion-lexicon method to analyses text sentiment in social media, extracted data on microblogging site mainlyTwitter and sentiment analysis application can be seen in world events, healthcare, politics and businessThe conducted systematic literature review provides information on studies on sentiment analysis in social media. 
The paper makes the following three contributions. First, we show what is the method used in analyzing sentiment in 
social media. There is various method introduced by researches, still, the most common method uses in Lexicon based 
method is SentiWordnet and TF-IDF while for machine learning is Naïve Bayes and SVM. Choosing the appropriate 
method of sentiment analysis is depending on the data itself. Both methods demonstrated a similar accuracy. The 
things that need to take into consideration is the structure of the text, time and amount of data. If the data structure is 
messy, a small amount of data and limited time available to analyses, it is recommended to go for lexicon-based 
method. Bigger data is suitable for machine learning based method as it requires more time and data to train. In order 
to improve the quality and accuracy of the result, it is suggested to combine both lexicon and machine learning method. 
 Second, we identify what is the most common type of social media site to extract information for sentiment 
analysis. The most popular social media site to extract information is Twitter. Most of the reviewed paper use twitter 
as their social media context. This is due to the availability, accessibility and richness of Twitter content. There are 
millions of tweets every day on almost any topic. This indicates that social media is becoming a precious source of 
information. However, less attention is given to other social media sources such as blogs, WordPress, YouTube and 
others. The content of each social media might be different, and it is worth exploring other sources might open to new 
knowledge and findings. 
 Third, we demonstrate the application of sentiment analysis in social media. Sentiment analysis has a broad 
application and can be utilized in different areas such as improving quality and strategy in business, political 
forecasting an election result, monitor disease outbreak, create awareness on the importance of data security, 
perception towards a particular sport, and improve locate and response to the disaster. This shows that sentiment 
analysis plays a huge role to understand people perception and helps in decision making. For future recommendation, 
further investigation is needed to develop a universal model of sentiment analysis that can be applied to a different 
type of data, explores other potential social networking sites to obtain users opinion and expanding the context of 
sentiment analysis application.",sentiment analysis,"The article discusses how sentiment analysis is used to convert social media data into valuable information, and how it is applied in world events, healthcare, politics, and business. A systematic literature review of studies on sentiment analysis in social media published between 2014 and 2019 was conducted, and 24 articles were chosen. The most common method used for sentiment analysis in social media was the opinion-lexicon method, and Twitter was the main microblogging site used for data extraction. The article also discusses the different methods used in sentiment analysis, including lexicon-based methods and machine learning methods such as Naïve Bayes and SVM. The appropriate method of sentiment analysis depends on the data structure, time, and amount of data available.",Object and Sentiment Recognition,,Sentiment Analysis
327,Expert Systems With Applications ,"Sentiment analysis 
Social media 
Twitter 
Causality 
Temporal sentiment analysis 
Professional and academic methodologies 
Reproducibility studies 
Causal rule prediction","Sentiment analysis has proven to be a valuable tool to gauge public opinion in different disciplines. It has been 
successfully employed in financial market prediction, health issues, customer analytics, commercial valuation 
assessment, brand marketing, politics, crime prediction, and emergency management. Many of the published 
studies have focused on sentiment analysis of Twitter messages, mainly because a large and diverse population 
expresses opinions about almost any topic daily on this platform. This paper proposes a comprehensive review of 
the multifaceted reality of sentiment analysis in social networks. We not only review the existing methods for 
sentiment analysis in social networks from an academic perspective, but also explore new aspects such as 
temporal dynamics, causal relationships, and applications in industry. We also study domains where these 
techniques have been applied, and discuss the practical applicability of emerging Artificial Intelligence methods. 
This paper emphasizes the importance of temporal characterization and causal effects in sentiment analysis in 
social networks, and explores their applications in different contexts such as stock market value, politics, and 
cyberbullying in educational centers. A strong interest from industry in this discipline can be inferred by the 
intense activity we observe in the field of intellectual protection, with more than 8,000 patents issued on the 
topic in only five years. This interest compares positively with the effort from academia, with more than 2,300 
articles published in 15 years. But these papers are unevenly split across domains: there is a strong presence in 
marketing, politics, economics, and health, but less activity in other domains such as emergencies. Regarding the 
techniques employed, traditional techniques such as dictionaries, neural networks, or Support Vector Machines 
are widely represented. In contrast, we could still not find a comparable representation of advanced state-of-theart techniques such as Transformers-based systems like BERT, T5, T0++, or GPT-2/3. This reality is consistent 
with the results found by the authors of this work, where computationally expensive tools such as GPT-3 are 
challenging to apply to achieve competitive results compared to those from simpler, lighter and more conventional techniques. These results, together with the interest shown by industry and academia, suggest that there is 
still ample room for research opportunities on domains, techniques and practical applications, and we expect to 
keep observing a sustained cadence in the number of published papers, patents and commercial tools made 
available.","Regarding technologies, it should be noted that the traditional ones 
(lexicons, tokens, Bayesian methods, bag of words) are still widely used 
whereas the newest methods (auto-regressive and encoder-decoder 
transformers) are not yet in widespread use. This may be due to the 
fact that the classic techniques are well established and are more 
affordable or more approachable for multidisciplinary teams. If we 
compare techniques across domains, we don’t see a significant difference in terms of the mix of techniques applied in each of the domains. 
Regarding the specific techniques applied, the use of lexicons, Word2vec, and n-grams stands out. Transformer-based techniques emerge, but 
to a lesser extent, and newer and more complex models such as T5, 
T0++, GPT-3 or GPT-J are infrequently cited. We believe that the use of 
these large pre-trained models will represent a future paradigm in 
sentiment analysis, as in many other disciplines, especially when zeroshot, one-shot or few-shot learning models are applicable. While this 
appears to be the case, preliminary outcomes of the reproducibility 
analyses performed in this work still yield very limited results. In 
particular, for the simple experiments carried out using GPT-J and GPT3 over the TweetEval dataset, we scored only 57.0 and 58.4 (macroaveraged recall), respectively, which compare poorly with the state of 
the art from traditional methods, and suggest that careful domain 
adaptation is still needed","Expert Systems With Applications Sentiment analysis 
Social media 
Twitter 
Causality 
Temporal sentiment analysis 
Professional and academic methodologies 
Reproducibility studies 
Causal rule predictionSentiment analysis has proven to be a valuable tool to gauge public opinion in different disciplines. It has been 
successfully employed in financial market prediction, health issues, customer analytics, commercial valuation 
assessment, brand marketing, politics, crime prediction, and emergency management. Many of the published 
studies have focused on sentiment analysis of Twitter messages, mainly because a large and diverse population 
expresses opinions about almost any topic daily on this platform. This paper proposes a comprehensive review of 
the multifaceted reality of sentiment analysis in social networks. We not only review the existing methods for 
sentiment analysis in social networks from an academic perspective, but also explore new aspects such as 
temporal dynamics, causal relationships, and applications in industry. We also study domains where these 
techniques have been applied, and discuss the practical applicability of emerging Artificial Intelligence methods. 
This paper emphasizes the importance of temporal characterization and causal effects in sentiment analysis in 
social networks, and explores their applications in different contexts such as stock market value, politics, and 
cyberbullying in educational centers. A strong interest from industry in this discipline can be inferred by the 
intense activity we observe in the field of intellectual protection, with more than 8,000 patents issued on the 
topic in only five years. This interest compares positively with the effort from academia, with more than 2,300 
articles published in 15 years. But these papers are unevenly split across domains: there is a strong presence in 
marketing, politics, economics, and health, but less activity in other domains such as emergencies. Regarding the 
techniques employed, traditional techniques such as dictionaries, neural networks, or Support Vector Machines 
are widely represented. In contrast, we could still not find a comparable representation of advanced state-of-theart techniques such as Transformers-based systems like BERT, T5, T0++, or GPT-2/3. This reality is consistent 
with the results found by the authors of this work, where computationally expensive tools such as GPT-3 are 
challenging to apply to achieve competitive results compared to those from simpler, lighter and more conventional techniques. These results, together with the interest shown by industry and academia, suggest that there is 
still ample room for research opportunities on domains, techniques and practical applications, and we expect to 
keep observing a sustained cadence in the number of published papers, patents and commercial tools made 
available.Regarding technologies, it should be noted that the traditional ones 
(lexicons, tokens, Bayesian methods, bag of words) are still widely used 
whereas the newest methods (auto-regressive and encoder-decoder 
transformers) are not yet in widespread use. This may be due to the 
fact that the classic techniques are well established and are more 
affordable or more approachable for multidisciplinary teams. If we 
compare techniques across domains, we don’t see a significant difference in terms of the mix of techniques applied in each of the domains. 
Regarding the specific techniques applied, the use of lexicons, Word2vec, and n-grams stands out. Transformer-based techniques emerge, but 
to a lesser extent, and newer and more complex models such as T5, 
T0++, GPT-3 or GPT-J are infrequently cited. We believe that the use of 
these large pre-trained models will represent a future paradigm in 
sentiment analysis, as in many other disciplines, especially when zeroshot, one-shot or few-shot learning models are applicable. While this 
appears to be the case, preliminary outcomes of the reproducibility 
analyses performed in this work still yield very limited results. In 
particular, for the simple experiments carried out using GPT-J and GPT3 over the TweetEval dataset, we scored only 57.0 and 58.4 (macroaveraged recall), respectively, which compare poorly with the state of 
the art from traditional methods, and suggest that careful domain 
adaptation is still needed",sentiment analysis,"The article discusses the importance of sentiment analysis in various fields and its application in social networks. It reviews existing methods and explores new aspects such as temporal dynamics, causal relationships, and industry applications. The paper highlights the importance of temporal characterization and causal effects in sentiment analysis and explores their applications in different contexts. The paper also notes the uneven distribution of research across domains and techniques. Traditional techniques such as lexicons, Bayesian methods, and bag of words are still widely used, while newer techniques such as transformer-based systems are infrequently cited. The authors believe that the use of large pre-trained models will represent a future paradigm in sentiment analysis. However, preliminary outcomes of reproducibility analyses suggest that careful domain adaptation is still needed.",Object and Sentiment Recognition,,Sentiment Analysis
328,Sentiment Analysis for Social Media,"Sentiment Analysis, Data Mining, Twitter.","Sentiment analysis, the automated extraction of 
expressions of positive or negative attitudes from text has received 
considerable attention from researchers during the past decade.
In addition, the popularity of internet users has been growing fast
parallel to emerging technologies; that actively use online review 
sites, social networks and personal blogs to express their opinions. 
They harbor positive and negative attitudes about people, 
organizations, places, events, and ideas. The tools provided by 
natural language processing and machine learning along with 
other approaches to work with large volumes of text, makes it 
possible to begin extracting sentiments from social media. In this 
paper we discuss some of the challenges in sentiment extraction, 
some of the approaches that have been taken to address these 
challenges and our approach that analyses sentiments from 
Twitter social media which gives the output beyond just the 
polarity but use those polarities in product profiling, trend 
analysis and forecasting. Promising results has shown that the 
approach can be further developed to cater business environment 
needs through sentiment analysis in social media","using the sentiment scores for sentiments regarding 
particular product or service with the user’s information, it 
could successfully profile the products, analyze trends and 
forecasting. So, as overall, the system is capable of saying that 
how a set of people of a particular age range, a particular area 
with a particular profession think about a particular product or 
service and how it will change it the future which are most 
useful information when it comes to business world","Sentiment Analysis for Social MediaSentiment Analysis, Data Mining, Twitter.Sentiment analysis, the automated extraction of 
expressions of positive or negative attitudes from text has received 
considerable attention from researchers during the past decade.
In addition, the popularity of internet users has been growing fast
parallel to emerging technologies; that actively use online review 
sites, social networks and personal blogs to express their opinions. 
They harbor positive and negative attitudes about people, 
organizations, places, events, and ideas. The tools provided by 
natural language processing and machine learning along with 
other approaches to work with large volumes of text, makes it 
possible to begin extracting sentiments from social media. In this 
paper we discuss some of the challenges in sentiment extraction, 
some of the approaches that have been taken to address these 
challenges and our approach that analyses sentiments from 
Twitter social media which gives the output beyond just the 
polarity but use those polarities in product profiling, trend 
analysis and forecasting. Promising results has shown that the 
approach can be further developed to cater business environment 
needs through sentiment analysis in social mediausing the sentiment scores for sentiments regarding 
particular product or service with the user’s information, it 
could successfully profile the products, analyze trends and 
forecasting. So, as overall, the system is capable of saying that 
how a set of people of a particular age range, a particular area 
with a particular profession think about a particular product or 
service and how it will change it the future which are most 
useful information when it comes to business world",sentiment analysis,"Sentiment analysis is the automated extraction of positive or negative attitudes from text and has gained attention from researchers due to the increasing popularity of online review sites, social networks, and personal blogs where people express their opinions. Natural language processing and machine learning tools are used to extract sentiments from social media, but there are challenges to this process. In this paper, the authors discuss their approach to sentiment analysis using Twitter, which goes beyond just polarity and includes product profiling, trend analysis, and forecasting. The system is capable of providing useful information for the business world, such as how people of a particular age range, area, and profession feel about a particular product or service and how it will change in the future.",Object and Sentiment Recognition,,Sentiment Analysis
329,A review on sentiment analysis and emotion detection from text,Afective computing · Opinion mining · Pre-processing · Word embedding,"Social networking platforms have become an essential means for communicating feelings to the entire world due to rapid 
expansion in the Internet era. Several people use textual content, pictures, audio, and video to express their feelings or 
viewpoints. Text communication via Web-based networking media, on the other hand, is somewhat overwhelming. Every 
second, a massive amount of unstructured data is generated on the Internet due to social media platforms. The data must be 
processed as rapidly as generated to comprehend human psychology, and it can be accomplished using sentiment analysis, 
which recognizes polarity in texts. It assesses whether the author has a negative, positive, or neutral attitude toward an item, 
administration, individual, or location. In some applications, sentiment analysis is insufcient and hence requires emotion 
detection, which determines an individual’s emotional/mental state precisely. This review paper provides understanding into 
levels of sentiment analysis, various emotion models, and the process of sentiment analysis and emotion detection from text. 
Finally, this paper discusses the challenges faced during sentiment and emotion analysis","In this paper, a review of the existing techniques for both 
emotion and sentiment detection is presented. As per the 
paper’s review, it has been analyzed that the lexicon-based 
technique performs well in both sentiment and emotion 
analysis. However, the dictionary-based approach is quite 
adaptable and straightforward to apply, whereas the corpusbased method is built on rules that function efectively in a 
certain domain. As a result, corpus-based approaches are 
more accurate but lack generalization. The performance of 
machine learning algorithms and deep learning algorithms 
depends on the pre-processing and size of the dataset. Nonetheless, in some cases, machine learning models fail to 
extract some implicit features or aspects of the text. In situations where the dataset is vast, the deep learning approach 
performs better than machine learning. Recurrent neural 
networks, especially the LSTM model, are prevalent in sentiment and emotion analysis, as they can cover long-term 
dependencies and extract features very well.","A review on sentiment analysis and emotion detection from textAfective computing · Opinion mining · Pre-processing · Word embeddingSocial networking platforms have become an essential means for communicating feelings to the entire world due to rapid 
expansion in the Internet era. Several people use textual content, pictures, audio, and video to express their feelings or 
viewpoints. Text communication via Web-based networking media, on the other hand, is somewhat overwhelming. Every 
second, a massive amount of unstructured data is generated on the Internet due to social media platforms. The data must be 
processed as rapidly as generated to comprehend human psychology, and it can be accomplished using sentiment analysis, 
which recognizes polarity in texts. It assesses whether the author has a negative, positive, or neutral attitude toward an item, 
administration, individual, or location. In some applications, sentiment analysis is insufcient and hence requires emotion 
detection, which determines an individual’s emotional/mental state precisely. This review paper provides understanding into 
levels of sentiment analysis, various emotion models, and the process of sentiment analysis and emotion detection from text. 
Finally, this paper discusses the challenges faced during sentiment and emotion analysisIn this paper, a review of the existing techniques for both 
emotion and sentiment detection is presented. As per the 
paper’s review, it has been analyzed that the lexicon-based 
technique performs well in both sentiment and emotion 
analysis. However, the dictionary-based approach is quite 
adaptable and straightforward to apply, whereas the corpusbased method is built on rules that function efectively in a 
certain domain. As a result, corpus-based approaches are 
more accurate but lack generalization. The performance of 
machine learning algorithms and deep learning algorithms 
depends on the pre-processing and size of the dataset. Nonetheless, in some cases, machine learning models fail to 
extract some implicit features or aspects of the text. In situations where the dataset is vast, the deep learning approach 
performs better than machine learning. Recurrent neural 
networks, especially the LSTM model, are prevalent in sentiment and emotion analysis, as they can cover long-term 
dependencies and extract features very well.“Author Title Method / Tools Application / Result Context Akter, ‘Sentiment analysis on facebook group Machine Determine recent trends and Facebook ‘diz 8.Tareq using the lexicon-based approach learning characteristics of people Group - food habit. Foodbank 2016) Mahtab, Islam Sentiment Analysis on Bangladesh Cricket Lexicon-based Analyze people sentiment Facebook ne with Support Vector Machine and machine expressed towards cricket Group ~ learning Bangladesh 01s) Cricket Chedia Social media sentiment analysis: lexicon Lexicon-based Sentiment analysis on Facebook ee versus machine leaming and consumer generated content brand pages Tan (2017) Musing leaming ElRahman, AlOtaibi Sentiment Analysis of Twitter Data Machine Popularity between two Twitter & AlShebri leaming restaurant - KFC and e019) — Abd El-Jawad, ‘Sentiment Analysis of Social Media Machine System to provide insight on Twitter Hodhod Networks Using Machine Leaming Leaming how people perception & Omar ois) Fatyanosa & Classification method comparison on Lexicon-based Sentiment on Jakarta Twitter Bashar 017) Bdosin soca meisine and govemor election aa Machine leaming Karamollaoglu, Sentiment Analysis of Turkish Social Lexicon-based Measure the perception or Twitter Dogru, Dérterler, Media Shares through Lexicon Based influences of the phenomena Uta & Yaldiz Approach 01s) Poccze, Ebster, Social media metrics and sentiment Machine Optimize brand Facebook Strauss & Christine al¥Sis to evaluate the effectiveness of _leaming ‘communication and page of social media posts understanding consumer YouTube 018) feedback Gamers Ragini, Anand & Big data analytics for disaster response Lexicon-based sentiment towards the needs Twitter Bhaskar (2018) and recovery through sentiment analysis. and machine —_of affected people during leaming disaster Ramanathan & ‘Twitter Text Mining for Sentiment Lexicon-based Feedbackon Oman tourism Twitter Meyysppan ‘Analysis on People’s Feedback about eon; Oman Tourism Shahare Sentiment analysis for the new data based Machine Process and identify emotion News from lenin) ‘on social media learning level from news data blogs Vishal & Uma ‘An Extensive study of Sentiment Analysis Machine Identify an efficient tool Twitter Gi) tools and Binary Classification of tweets learning which can help an enterprise using Rapid Miner Suman, Gupta & “Analysis of Stock Price Flow Based on ‘Machine Relate the flow of stock price Stock Twists Sharma (2017) Social Media Sentiments leamingTable 1. Summary of reviewed literature. ‘Author Title Method Tools Application / Result Context Yuliyanti, Djaina & Sentiment Mining of Community Lexicon-based Success level of the Twitter Sukoco. (2017) Development Program Evaluation Based on and machine community development Social Media learning program Martin-Domingo, Social media as a resource for sentiment Machine ‘Analyse airport service Twitter Martin, & analysis of Airport Service Quality learning quality account Mandsberg. (2019) Mansour. (2018) Social Media Analysis of User's Responses Lexicon-based Most user view ISIS as a Twitter to terrorism using sentiment analysis and threat and fear text mining Saragih & Girsang. Sentiment Analysis of Customer Lexicon-based Evaluate the business Facebook and (2017) Engagement on Social Media in Transport performance of online Twitter Online transport. ‘comments, Hassan, Hussain, Sentiment Analysis of Social Networking Machine Find the depression level ofa Twitter and Husain, Sadiq, Lee. Sites (SNS) Data using Machine Leaming _leaming person newsgroup (2017) Approach for the Measurement of Depression Joyce & Deng. Sentiment Analysis of Tweets forthe 2016 Lexicon-based Calculate sentiment Twitter 2017) US Presidential Election andmachine expressed and compare with learning polling data to sce the correlation Ikoro, Harmina, Analyzing Sentiments Expressed on Lexicon-based Analyze energy provider Twitter Malik, & Batista- Twitter by UK Energy Company company and the sentiment ‘Navarro. (2018) Consumers that users show Hao & Dai. (2016) Social media content and sentiment Lexicon-based Security breaches can be Twitter analysis on consumer security breaches detected in the early stages and prevent further destruction Shayaa, Wai, Social Media Sentiment Analysis on Lexicon-based Negative sentiment score on Multiple Chung, Sulaiman, Employment in Malaysia employment ‘channel social Jaafar & Zakaria, media (2017) Isah, Trundle & Social Media Analysis for Product Safety __Lexicon-based Monitor brand in order to act Facebook Neagu. (2014) using Text Mining and Sentiment Analysis and machine inven ofa sudden rise in comment and learning negative sentiment. Twitter Ali, Dong, Sentiment Analysis as a Service: A social Machine Identify the location of Twitter, Bouguettaya, ‘media-based sentiment analysis framework Leaming discase outbreaks Reddit, Enradi & Hadjidj Instagram, 2017) znews forum.",sentiment analysis,"Social networking platforms have become a popular way for people to express their feelings and opinions using textual content, pictures, audio, and video. Sentiment analysis is a technique used to recognize polarity in texts, assessing whether the author has a negative, positive, or neutral attitude toward a specific item, administration, individual, or location. Emotion detection goes beyond sentiment analysis, determining an individual's emotional/mental state accurately. This paper reviews various techniques for sentiment and emotion analysis, including lexicon-based, dictionary-based, and corpus-based approaches, as well as machine learning and deep learning algorithms. The review suggests that the lexicon-based technique performs well in both sentiment and emotion analysis, while the deep learning approach performs better in situations where the dataset is vast. Recurrent neural networks, particularly the LSTM model, are prevalent in sentiment and emotion analysis.",Object and Sentiment Recognition,"“Author Title Method / Tools Application / Result Context Akter, ‘Sentiment analysis on facebook group Machine Determine recent trends and Facebook ‘diz 8.Tareq using the lexicon-based approach learning characteristics of people Group - food habit. Foodbank 2016) Mahtab, Islam Sentiment Analysis on Bangladesh Cricket Lexicon-based Analyze people sentiment Facebook ne with Support Vector Machine and machine expressed towards cricket Group ~ learning Bangladesh 01s) Cricket Chedia Social media sentiment analysis: lexicon Lexicon-based Sentiment analysis on Facebook ee versus machine leaming and consumer generated content brand pages Tan (2017) Musing leaming ElRahman, AlOtaibi Sentiment Analysis of Twitter Data Machine Popularity between two Twitter & AlShebri leaming restaurant - KFC and e019) — Abd El-Jawad, ‘Sentiment Analysis of Social Media Machine System to provide insight on Twitter Hodhod Networks Using Machine Leaming Leaming how people perception & Omar ois) Fatyanosa & Classification method comparison on Lexicon-based Sentiment on Jakarta Twitter Bashar 017) Bdosin soca meisine and govemor election aa Machine leaming Karamollaoglu, Sentiment Analysis of Turkish Social Lexicon-based Measure the perception or Twitter Dogru, Dérterler, Media Shares through Lexicon Based influences of the phenomena Uta & Yaldiz Approach 01s) Poccze, Ebster, Social media metrics and sentiment Machine Optimize brand Facebook Strauss & Christine al¥Sis to evaluate the effectiveness of _leaming ‘communication and page of social media posts understanding consumer YouTube 018) feedback Gamers Ragini, Anand & Big data analytics for disaster response Lexicon-based sentiment towards the needs Twitter Bhaskar (2018) and recovery through sentiment analysis. and machine —_of affected people during leaming disaster Ramanathan & ‘Twitter Text Mining for Sentiment Lexicon-based Feedbackon Oman tourism Twitter Meyysppan ‘Analysis on People’s Feedback about eon; Oman Tourism Shahare Sentiment analysis for the new data based Machine Process and identify emotion News from lenin) ‘on social media learning level from news data blogs Vishal & Uma ‘An Extensive study of Sentiment Analysis Machine Identify an efficient tool Twitter Gi) tools and Binary Classification of tweets learning which can help an enterprise using Rapid Miner Suman, Gupta & “Analysis of Stock Price Flow Based on ‘Machine Relate the flow of stock price Stock Twists Sharma (2017) Social Media Sentiments leamingTable 1. Summary of reviewed literature. ‘Author Title Method Tools Application / Result Context Yuliyanti, Djaina & Sentiment Mining of Community Lexicon-based Success level of the Twitter Sukoco. (2017) Development Program Evaluation Based on and machine community development Social Media learning program Martin-Domingo, Social media as a resource for sentiment Machine ‘Analyse airport service Twitter Martin, & analysis of Airport Service Quality learning quality account Mandsberg. (2019) Mansour. (2018) Social Media Analysis of User's Responses Lexicon-based Most user view ISIS as a Twitter to terrorism using sentiment analysis and threat and fear text mining Saragih & Girsang. Sentiment Analysis of Customer Lexicon-based Evaluate the business Facebook and (2017) Engagement on Social Media in Transport performance of online Twitter Online transport. ‘comments, Hassan, Hussain, Sentiment Analysis of Social Networking Machine Find the depression level ofa Twitter and Husain, Sadiq, Lee. Sites (SNS) Data using Machine Leaming _leaming person newsgroup (2017) Approach for the Measurement of Depression Joyce & Deng. Sentiment Analysis of Tweets forthe 2016 Lexicon-based Calculate sentiment Twitter 2017) US Presidential Election andmachine expressed and compare with learning polling data to sce the correlation Ikoro, Harmina, Analyzing Sentiments Expressed on Lexicon-based Analyze energy provider Twitter Malik, & Batista- Twitter by UK Energy Company company and the sentiment ‘Navarro. (2018) Consumers that users show Hao & Dai. (2016) Social media content and sentiment Lexicon-based Security breaches can be Twitter analysis on consumer security breaches detected in the early stages and prevent further destruction Shayaa, Wai, Social Media Sentiment Analysis on Lexicon-based Negative sentiment score on Multiple Chung, Sulaiman, Employment in Malaysia employment ‘channel social Jaafar & Zakaria, media (2017) Isah, Trundle & Social Media Analysis for Product Safety __Lexicon-based Monitor brand in order to act Facebook Neagu. (2014) using Text Mining and Sentiment Analysis and machine inven ofa sudden rise in comment and learning negative sentiment. Twitter Ali, Dong, Sentiment Analysis as a Service: A social Machine Identify the location of Twitter, Bouguettaya, ‘media-based sentiment analysis framework Leaming discase outbreaks Reddit, Enradi & Hadjidj Instagram, 2017) znews forum.",Sentiment Analysis
330,Sentiment Analysis of Twitter Data,"Sentiment analysis, social media, Twitter, tweets","Nowadays, people from all around the world use 
social media sites to share information. Twitter for example is 
a platform in which users send, read posts known as ‘tweets’ 
and interact with different communities. Users share their 
daily lives, post their opinions on everything such as brands 
and places. Companies can benefit from this massive platform 
by collecting data related to opinions on them. The aim of this 
paper is to present a model that can perform sentiment 
analysis of real data collected from Twitter. Data in Twitter is 
highly unstructured which makes it difficult to analyze. 
However, our proposed model is different from prior work in 
this field because it combined the use of supervised and 
unsupervised machine learning algorithms. The process of 
performing sentiment analysis as follows: Tweet extracted 
directly from Twitter API, then cleaning and discovery of data 
performed. After that the data were fed into several models for 
the purpose of training. Each tweet extracted classified based 
on its sentiment whether it is a positive, negative or neutral. 
Data were collected on two subjects McDonalds and KFC to 
show which restaurant has more popularity. Different machine 
learning algorithms were used. The result from these models 
were tested using various testing metrics like cross validation 
and f-score. Moreover, our model demonstrates strong 
performance on mining texts extracted directly from Twitter","Sentiment analysis is a field of study for analyzing
opinions expressed in text in several social media sites. Our 
proposed model used several algorithms to enhance the 
accuracy of classifying tweets as positive, negative and 
neutral. Our presented methodology combined the use of 
unsupervised machine learning algorithm where previously 
labeled data were not exist at first using lexicon-based 
algorithm. After that data were fed into several supervised 
model. For testing various metrics used, and it is shown that 
based on cross validation, maximum entropy has the highest 
accuracy. As a result, McDonalds is more popular than KFC 
in terms of both negative and positive reviews. Same 
methodology can be used in various fields, detecting rumors ","Sentiment Analysis of Twitter DataSentiment analysis, social media, Twitter, tweetsNowadays, people from all around the world use 
social media sites to share information. Twitter for example is 
a platform in which users send, read posts known as ‘tweets’ 
and interact with different communities. Users share their 
daily lives, post their opinions on everything such as brands 
and places. Companies can benefit from this massive platform 
by collecting data related to opinions on them. The aim of this 
paper is to present a model that can perform sentiment 
analysis of real data collected from Twitter. Data in Twitter is 
highly unstructured which makes it difficult to analyze. 
However, our proposed model is different from prior work in 
this field because it combined the use of supervised and 
unsupervised machine learning algorithms. The process of 
performing sentiment analysis as follows: Tweet extracted 
directly from Twitter API, then cleaning and discovery of data 
performed. After that the data were fed into several models for 
the purpose of training. Each tweet extracted classified based 
on its sentiment whether it is a positive, negative or neutral. 
Data were collected on two subjects McDonalds and KFC to 
show which restaurant has more popularity. Different machine 
learning algorithms were used. The result from these models 
were tested using various testing metrics like cross validation 
and f-score. Moreover, our model demonstrates strong 
performance on mining texts extracted directly from TwitterSentiment analysis is a field of study for analyzing
opinions expressed in text in several social media sites. Our 
proposed model used several algorithms to enhance the 
accuracy of classifying tweets as positive, negative and 
neutral. Our presented methodology combined the use of 
unsupervised machine learning algorithm where previously 
labeled data were not exist at first using lexicon-based 
algorithm. After that data were fed into several supervised 
model. For testing various metrics used, and it is shown that 
based on cross validation, maximum entropy has the highest 
accuracy. As a result, McDonalds is more popular than KFC 
in terms of both negative and positive reviews. Same 
methodology can be used in various fields, detecting rumors No of oouments & on, 216 = cragat Hil Processing Tecniques 2015, 214 a ag 4 es ce ee a Nm Wi ae ct 400% 90% 80% = 70% 60% 50% Lb ove : E 30% 20% 10% — = ~ 0% Article Book Chapter Conference Review Paper ECONOMIC EMERGENCIES GENERAL sHEALTH =MARKETING =POLITICAL Fig. 7. Document type by domain from 2008 to 2021 related to Sentiment Analysis in Social Networks obtained from Scopus DataBase.3000 2500 2168 2000 1691 1500 1131 1000 899 500 17 ° 2016 2017 2018 2019 2020 2021 |PATENTS Fig. 2. Issued patents from 2016 to 2021 related to Sentiment Analysis in Social Networks. “-ECONOMIC = EMERGENCIES GENERAL HEALTH MARKETING -®POLITICAL 140 120 100 ° PD oO wr We PES LS S Fig. 8. Sentiment Analysis in Social Networks publications published between 2008 and 2021, according to Scopus‘SENTIMENT ANALYSIS REVIEWS 2002 ~ 2021 <— 2t0pics —e SENTIMENTANALYSIS, SENTIMENT ANALYSIS FROM TECHNIQUES SPECIFIC DOMAINS. T MACHINE LEXICON ore — Economic | [MARKETING ] [ HeaLTH | [ EMERGENCIES | [ POLITICAL ‘TECHNIQUES TECHNIQUES EXAMPLES EXAMPLES co, o> Rav & Rav Balazs& Rajalakshim Chaturvedi Hemmatian 2002-2015, Velazquer 2016 2017 2018 2019 “The challenges we approach in our paper © terol ma © cauesiverenert oa © seine taining © stint nti tt oma Fig. 1. Sentiment Analysis reviews and new challenges.100% 90% 80% 70% 60% 50% 40% 30% \ 20% | 10% o% V 2008 2009 2010 2011 2012 2019 2014 2018 2018 2017 2018 2019 2020 2021 ECONOMIC EMERGENCIES GENERAL SHEALTH SMARKETING POLITICAL Fig. 4. Distribution of papers from 2008 to 2021 related to Sentiment Analysis in Social Networks obtained from Scopus Datafiase. Not ocumants AUTOREGRESSIvE Fig. 5. Feature Extraction Technologies used in documents from 2008 to 2021 related to Sentiment Analysis in Social Networks obtained from Scopus DataBase.",sentiment analysis,"This paper presents a model for sentiment analysis of data collected from Twitter, where users share information and opinions. The proposed model combines supervised and unsupervised machine learning algorithms to classify tweets as positive, negative, or neutral. The data is highly unstructured, but the model performs well in extracting sentiment. The study focused on analyzing opinions about McDonald's and KFC, and the results show that McDonald's is more popular than KFC in terms of both negative and positive reviews. This methodology can be used in various fields for detecting rumors and analyzing opinions expressed on social media.",Object and Sentiment Recognition,"No of oouments & on, 216 = cragat Hil Processing Tecniques 2015, 214 a ag 4 es ce ee a Nm Wi ae ct 400% 90% 80% = 70% 60% 50% Lb ove : E 30% 20% 10% — = ~ 0% Article Book Chapter Conference Review Paper ECONOMIC EMERGENCIES GENERAL sHEALTH =MARKETING =POLITICAL Fig. 7. Document type by domain from 2008 to 2021 related to Sentiment Analysis in Social Networks obtained from Scopus DataBase.3000 2500 2168 2000 1691 1500 1131 1000 899 500 17 ° 2016 2017 2018 2019 2020 2021 |PATENTS Fig. 2. Issued patents from 2016 to 2021 related to Sentiment Analysis in Social Networks. “-ECONOMIC = EMERGENCIES GENERAL HEALTH MARKETING -®POLITICAL 140 120 100 ° PD oO wr We PES LS S Fig. 8. Sentiment Analysis in Social Networks publications published between 2008 and 2021, according to Scopus‘SENTIMENT ANALYSIS REVIEWS 2002 ~ 2021 <— 2t0pics —e SENTIMENTANALYSIS, SENTIMENT ANALYSIS FROM TECHNIQUES SPECIFIC DOMAINS. T MACHINE LEXICON ore — Economic | [MARKETING ] [ HeaLTH | [ EMERGENCIES | [ POLITICAL ‘TECHNIQUES TECHNIQUES EXAMPLES EXAMPLES co, o> Rav & Rav Balazs& Rajalakshim Chaturvedi Hemmatian 2002-2015, Velazquer 2016 2017 2018 2019 “The challenges we approach in our paper © terol ma © cauesiverenert oa © seine taining © stint nti tt oma Fig. 1. Sentiment Analysis reviews and new challenges.100% 90% 80% 70% 60% 50% 40% 30% \ 20% | 10% o% V 2008 2009 2010 2011 2012 2019 2014 2018 2018 2017 2018 2019 2020 2021 ECONOMIC EMERGENCIES GENERAL SHEALTH SMARKETING POLITICAL Fig. 4. Distribution of papers from 2008 to 2021 related to Sentiment Analysis in Social Networks obtained from Scopus Datafiase. Not ocumants AUTOREGRESSIvE Fig. 5. Feature Extraction Technologies used in documents from 2008 to 2021 related to Sentiment Analysis in Social Networks obtained from Scopus DataBase.",Sentiment Analysis
331,"Sentiment Analysis of Big Data: Methods,
Applications, and Open Challenges"," Opinion mining, sentiment analysis, big data, applications, opinionated data, social media,
online social network.","The development of IoT technologies and the massive admiration and acceptance of social
media tools and applications, new doors of opportunity have been opened for using data analytics in
gaining meaningful insights from unstructured information. The application of opinion mining and sentiment
analysis (OMSA) in the era of big data have been used a useful way in categorizing the opinion into different
sentiment and in general evaluating the mood of the public. Moreover, different techniques of OMSA have
been developed over the years in different data sets and applied to various experimental settings. In this
regard, this paper presents a comprehensive systematic literature review, aims to discuss both technical aspect
of OMSA (techniques and types) and non-technical aspect in the form of application areas are discussed.
Furthermore, this paper also highlighted both technical aspects of OMSA in the form of challenges in the
development of its technique and non-technical challenges mainly based on its application. These challenges
are presented as a future direction for research.","A few conclusions can be made based on the findings of
the systematic literature have been conducted on articles
published in the Web of Science from 2000-2016 on OMSA.
In addition, more articles have been published on sentiment
analysis as compared to opinion mining since 2015. The
emergence of the significance of sentiment analysis matches
the development of social media usage, including reviews,
forum blogs, micro-blogs, Facebook, Twitter, and other social
networks. Interestingly, presently we have access to a huge
amount of opinionated data which can further be used for different methods of analysis. Typically more than 80% of social
media data can be monitored for analysis purposes","Sentiment Analysis of Big Data: Methods,
Applications, and Open Challenges Opinion mining, sentiment analysis, big data, applications, opinionated data, social media,
online social network.The development of IoT technologies and the massive admiration and acceptance of social
media tools and applications, new doors of opportunity have been opened for using data analytics in
gaining meaningful insights from unstructured information. The application of opinion mining and sentiment
analysis (OMSA) in the era of big data have been used a useful way in categorizing the opinion into different
sentiment and in general evaluating the mood of the public. Moreover, different techniques of OMSA have
been developed over the years in different data sets and applied to various experimental settings. In this
regard, this paper presents a comprehensive systematic literature review, aims to discuss both technical aspect
of OMSA (techniques and types) and non-technical aspect in the form of application areas are discussed.
Furthermore, this paper also highlighted both technical aspects of OMSA in the form of challenges in the
development of its technique and non-technical challenges mainly based on its application. These challenges
are presented as a future direction for research.A few conclusions can be made based on the findings of
the systematic literature have been conducted on articles
published in the Web of Science from 2000-2016 on OMSA.
In addition, more articles have been published on sentiment
analysis as compared to opinion mining since 2015. The
emergence of the significance of sentiment analysis matches
the development of social media usage, including reviews,
forum blogs, micro-blogs, Facebook, Twitter, and other social
networks. Interestingly, presently we have access to a huge
amount of opinionated data which can further be used for different methods of analysis. Typically more than 80% of social
media data can be monitored for analysis purposesFig I: Top level architectural diagram of the systemFig 4: Product profiling Fig 5: Trend analysis and forecastingTABLE | EVALUATION OF DIFFERENT FEATURE EXTRACION METHODS Feature ‘Acoura | Positiv | Positive | Negati_] Negati Extraction cy © Recall | ve ve Method Precisi Precisi_ | Recall Unigrams 0.7483 0.8058 | 0.6543 | 0.7090 | 0.8424 Unigrams 0.7436 | 0.7605 | 0.7112 | 0.7288 | 0.7761 except Stop Words Bigram 07630 | OTS | 0.6814 | 0.7261 | 08447 Collocation Most OT7T8 | 0.8581 | 0.6658 | 0.7269 | 0.8899 Informative Unigrams Most O7785 | 08350 | 0.6881 | 0.7347 | O.8GAT Informative Unigrams and Bigram Collocations, TABLE 2 DIFFERENT EVALUATION METHODS Evaluation | Classifier Training | Test Set Method Set Size _| Size 1 Naive Bayes 70000. 10000 2 Naive Bayes, 70000 720000 3 Naive Bayes, 70000 30000 4 Maximum Entropy 70000 10000 3 ‘Maximum Entropy 70000 20000 6 ‘Maximum Entropy 70000 30000 TABLE 3 EVALUATION RESULTS FOR ABOVE EVALUATION METHODS Evaluati_] Acura | Positive | Positive | Negative | Negative on °y Precision | Recall | Precisio | Recall Method a T O77s1_[ 08357 | O6R4s | 0.7330__| 08658 2 0.7758 | 0.8336 | 0.6891 | 0.7350 _| 0.8625 3 0.7806 | 0.8388 | 0.6946 | 0.7304 | 0.8666 4 07730 | 0.8373 | 0.6780 | 0.7204 | 0.8680 5 0.7765 | 0.8388 | 0.6846 | 0.7335 _| O.868S 6 0.7799 | 0.8422 | 0.6889 | 0.7368 | 08710",sentiment analysis,"The rise of IoT technologies and social media has led to the development of new opportunities to use data analytics for gaining insights from unstructured information. Opinion mining and sentiment analysis (OMSA) have been useful in categorizing opinions and evaluating public mood. This systematic literature review discusses both the technical and non-technical aspects of OMSA, including techniques, types, application areas, and challenges. More articles have been published on sentiment analysis compared to opinion mining since 2015. The significance of sentiment analysis matches the development of social media usage, and more than 80% of social media data can be monitored for analysis purposes.",Object and Sentiment Recognition,"Fig I: Top level architectural diagram of the systemFig 4: Product profiling Fig 5: Trend analysis and forecastingTABLE | EVALUATION OF DIFFERENT FEATURE EXTRACION METHODS Feature ‘Acoura | Positiv | Positive | Negati_] Negati Extraction cy © Recall | ve ve Method Precisi Precisi_ | Recall Unigrams 0.7483 0.8058 | 0.6543 | 0.7090 | 0.8424 Unigrams 0.7436 | 0.7605 | 0.7112 | 0.7288 | 0.7761 except Stop Words Bigram 07630 | OTS | 0.6814 | 0.7261 | 08447 Collocation Most OT7T8 | 0.8581 | 0.6658 | 0.7269 | 0.8899 Informative Unigrams Most O7785 | 08350 | 0.6881 | 0.7347 | O.8GAT Informative Unigrams and Bigram Collocations, TABLE 2 DIFFERENT EVALUATION METHODS Evaluation | Classifier Training | Test Set Method Set Size _| Size 1 Naive Bayes 70000. 10000 2 Naive Bayes, 70000 720000 3 Naive Bayes, 70000 30000 4 Maximum Entropy 70000 10000 3 ‘Maximum Entropy 70000 20000 6 ‘Maximum Entropy 70000 30000 TABLE 3 EVALUATION RESULTS FOR ABOVE EVALUATION METHODS Evaluati_] Acura | Positive | Positive | Negative | Negative on °y Precision | Recall | Precisio | Recall Method a T O77s1_[ 08357 | O6R4s | 0.7330__| 08658 2 0.7758 | 0.8336 | 0.6891 | 0.7350 _| 0.8625 3 0.7806 | 0.8388 | 0.6946 | 0.7304 | 0.8666 4 07730 | 0.8373 | 0.6780 | 0.7204 | 0.8680 5 0.7765 | 0.8388 | 0.6846 | 0.7335 _| O.868S 6 0.7799 | 0.8422 | 0.6889 | 0.7368 | 08710",Sentiment Analysis
332,"Social media analytics: a survey of techniques, tools and platforms","Social media Scraping  Behavior
economics Sentiment analysis  Opinion mining  Toolkits  Software platforms","This paper is written for (social science) researchers seeking to analyze the wealth of social media now available. It presents a comprehensive review of
software tools for social networking media, wikis, really
simple syndication feeds, blogs, newsgroups, chat and
news feeds. For completeness, it also includes introductions to social media scraping, storage, data cleaning and
sentiment analysis. Although principally a review, the
paper also provides a methodology and a critique of social
media tools. Analyzing social media, in particular Twitter
feeds for sentiment analysis, has become a major research
and business activity due to the availability of web-based
application programming interfaces (APIs) provided by
Twitter, Facebook and News services. This has led to an
‘explosion’ of data services, software tools for scraping and
analysis and social media analytics platforms. It is also a
research area undergoing rapid change and evolution due to
commercial pressures and the potential for using social
media data for computational (social science) research.
Using a simple taxonomy, this paper provides a review of
leading software tools and how to use them to scrape,
cleanse and analyze the spectrum of social media. In
addition, it discussed the requirement of an experimental
computational environment for social media research and
presents as an illustration the system architecture of a
social media (analytics) platform built by University College London. The principal contribution of this paper is to
provide an overview (including code fragments) for scientists seeking to utilize social media scraping and
analytics either in their research or business. The data
retrieval techniques that are presented in this paper are
valid at the time of writing this paper (June 2014), but they
are subject to change since social media data scraping APIs
are rapidly changing","As discussed, the easy availability of APIs provided by
Twitter, Facebook and News services has led to an
‘explosion’ of data services and software tools for scraping
and sentiment analysis, and social media analytics platforms. This paper surveys some of the social media software tools, and for completeness introduced social media
scraping, data cleaning and sentiment analysis.
Perhaps, the biggest concern is that companies are
increasingly restricting access to their data to monetize
their content. It is important that researchers have access to
computational environments and especially ‘big’ social
media data for experimentation. Otherwise, computational
social science could become the exclusive domain of major
companies, government agencies and a privileged set of
academic researchers presiding over private data from
which they produce papers that cannot be critiqued or
replicated. Arguably what is required are public-domain
computational environments and data facilities for quantitative social science, which can be accessed by researchers
via a cloud-based facility","Social media analytics: a survey of techniques, tools and platformsSocial media Scraping  Behavior
economics Sentiment analysis  Opinion mining  Toolkits  Software platformsThis paper is written for (social science) researchers seeking to analyze the wealth of social media now available. It presents a comprehensive review of
software tools for social networking media, wikis, really
simple syndication feeds, blogs, newsgroups, chat and
news feeds. For completeness, it also includes introductions to social media scraping, storage, data cleaning and
sentiment analysis. Although principally a review, the
paper also provides a methodology and a critique of social
media tools. Analyzing social media, in particular Twitter
feeds for sentiment analysis, has become a major research
and business activity due to the availability of web-based
application programming interfaces (APIs) provided by
Twitter, Facebook and News services. This has led to an
‘explosion’ of data services, software tools for scraping and
analysis and social media analytics platforms. It is also a
research area undergoing rapid change and evolution due to
commercial pressures and the potential for using social
media data for computational (social science) research.
Using a simple taxonomy, this paper provides a review of
leading software tools and how to use them to scrape,
cleanse and analyze the spectrum of social media. In
addition, it discussed the requirement of an experimental
computational environment for social media research and
presents as an illustration the system architecture of a
social media (analytics) platform built by University College London. The principal contribution of this paper is to
provide an overview (including code fragments) for scientists seeking to utilize social media scraping and
analytics either in their research or business. The data
retrieval techniques that are presented in this paper are
valid at the time of writing this paper (June 2014), but they
are subject to change since social media data scraping APIs
are rapidly changingAs discussed, the easy availability of APIs provided by
Twitter, Facebook and News services has led to an
‘explosion’ of data services and software tools for scraping
and sentiment analysis, and social media analytics platforms. This paper surveys some of the social media software tools, and for completeness introduced social media
scraping, data cleaning and sentiment analysis.
Perhaps, the biggest concern is that companies are
increasingly restricting access to their data to monetize
their content. It is important that researchers have access to
computational environments and especially ‘big’ social
media data for experimentation. Otherwise, computational
social science could become the exclusive domain of major
companies, government agencies and a privileged set of
academic researchers presiding over private data from
which they produce papers that cannot be critiqued or
replicated. Arguably what is required are public-domain
computational environments and data facilities for quantitative social science, which can be accessed by researchers
via a cloud-based facilityDetecting Polarity/Emotion states CNN) Dictionary based NANED ved LST™M Use of pre- Corpus Based Hated giro trained models Random Forest a ete. Fig.4 Techniques for sentiment analysis and emotion detectionLack of Resources Compararive Sentences CEE Es Sarcasm and Multiple Tony Aspects: Sentences Implicit Aspects Fig.5 Challenges in sentiment analysis and emotion detectionfe} Interest Seimism oe ” ceptance bd ° Aggressive Submission hae ‘Apprehension O° Disgust © O ° ‘Oo Bowe ° Pensiveness ° Distraction Disapprove Remorse Fig. 2. Illustration of various emotional models with some psychological states, Fig.3 Basic steps to perform sentiment analysis and emotion detection Plutchik- O Ekman- Tzard- © Shaver- @ Tomkins-Fig. 1 Dimensional model of High Arousal emotions (Exciting) Annoyed Miracle Negative (Unpleasant) Positive (Pleasant) Death Comfort Tired ‘Serene Low Arousal (Relax)",sentiment analysis,"This paper provides a review of software tools for social media analysis, including social networking media, wikis, RSS feeds, blogs, newsgroups, chat, and news feeds. It also covers social media scraping, data cleaning, and sentiment analysis. The paper includes a methodology and a critique of social media tools and presents an illustration of a social media analytics platform built by University College London. The paper aims to provide an overview for researchers seeking to utilize social media scraping and analytics in their research or business. However, the rapid change in social media data scraping APIs is a concern, and companies are increasingly restricting access to their data. The paper suggests the need for public-domain computational environments and data facilities for quantitative social science that can be accessed by researchers via a cloud-based facility",Object and Sentiment Recognition,"Detecting Polarity/Emotion states CNN) Dictionary based NANED ved LST™M Use of pre- Corpus Based Hated giro trained models Random Forest a ete. Fig.4 Techniques for sentiment analysis and emotion detectionLack of Resources Compararive Sentences CEE Es Sarcasm and Multiple Tony Aspects: Sentences Implicit Aspects Fig.5 Challenges in sentiment analysis and emotion detectionfe} Interest Seimism oe ” ceptance bd ° Aggressive Submission hae ‘Apprehension O° Disgust © O ° ‘Oo Bowe ° Pensiveness ° Distraction Disapprove Remorse Fig. 2. Illustration of various emotional models with some psychological states, Fig.3 Basic steps to perform sentiment analysis and emotion detection Plutchik- O Ekman- Tzard- © Shaver- @ Tomkins-Fig. 1 Dimensional model of High Arousal emotions (Exciting) Annoyed Miracle Negative (Unpleasant) Positive (Pleasant) Death Comfort Tired ‘Serene Low Arousal (Relax)",Sentiment Analysis
333,"Optimization of sentiment analysis using 
machine learning classifers","Sentiment analysis, Social media text, Movie reviews, Product reviews.","Words and phrases bespeak the perspectives of people about products, services, 
governments and events on social media. Extricating positive or negative polarities 
from social media text denominates task of sentiment analysis in the feld of natural 
language processing. The exponential growth of demands for business organizations and governments, impel researchers to accomplish their research in sentiment 
analysis. This paper leverages four state-of-the-art machine learning classifers viz. Naïve 
Bayes, J48, BFTree and OneR for optimization of sentiment analysis. The experiments 
are performed using three manually compiled datasets; two of them are captured 
from Amazon and one dataset is assembled from IMDB movie reviews. The efcacies 
of these four classifcation techniques are examined and compared. The Naïve Bayes 
found to be quite fast in learning whereas OneR seems more promising in generating 
the accuracy of 91.3% in precision, 97% in F-measure and 92.34% in correctly classifed 
instances","Tis paper exploits four machine learning classifers for sentiment analysis using three 
manually annotated datasets. Te mean of 29 epochs of experimentation recorded in 
Table 4 shows that OneR is more precise in terms of percentage of correctly classifed 
instances. On the other hand, Naïve Bayes exhibits faster learning rate and J48 reveals 
adequacy in the true positive and false positive rates. Table 5 reveals the truth that J48 
and OneR are better for smaller dataset of woodland’s wallet reviews. Te preprocessing 
of proposed methodology is limited to extract foreign words, emoticons and elongated 
words with their appropriate sentiments. Te future work in the task of sentiment analysis has scope to improve preprocessing with word embeddings using deep neural networks and can also extend this study through convolution","Optimization of sentiment analysis using 
machine learning classifersSentiment analysis, Social media text, Movie reviews, Product reviews.Words and phrases bespeak the perspectives of people about products, services, 
governments and events on social media. Extricating positive or negative polarities 
from social media text denominates task of sentiment analysis in the feld of natural 
language processing. The exponential growth of demands for business organizations and governments, impel researchers to accomplish their research in sentiment 
analysis. This paper leverages four state-of-the-art machine learning classifers viz. Naïve 
Bayes, J48, BFTree and OneR for optimization of sentiment analysis. The experiments 
are performed using three manually compiled datasets; two of them are captured 
from Amazon and one dataset is assembled from IMDB movie reviews. The efcacies 
of these four classifcation techniques are examined and compared. The Naïve Bayes 
found to be quite fast in learning whereas OneR seems more promising in generating 
the accuracy of 91.3% in precision, 97% in F-measure and 92.34% in correctly classifed 
instancesTis paper exploits four machine learning classifers for sentiment analysis using three 
manually annotated datasets. Te mean of 29 epochs of experimentation recorded in 
Table 4 shows that OneR is more precise in terms of percentage of correctly classifed 
instances. On the other hand, Naïve Bayes exhibits faster learning rate and J48 reveals 
adequacy in the true positive and false positive rates. Table 5 reveals the truth that J48 
and OneR are better for smaller dataset of woodland’s wallet reviews. Te preprocessing 
of proposed methodology is limited to extract foreign words, emoticons and elongated 
words with their appropriate sentiments. Te future work in the task of sentiment analysis has scope to improve preprocessing with word embeddings using deep neural networks and can also extend this study through convolutionFig. 3."" Word cloud from textFrequeny of eons word etd ing ay ‘Tete sins et cots peg ny stu res » s s eve eis esa Soe Dil Te ETH Fig, 1.” Frequency of tweets about MeDonald’s. Frequency ofc word tweeted during the day ‘Wes ett pee ys ua weet went verse west tne Sor aac ote ESTA att Fig. 2” Frequency of tweets about KFC",sentiment analysis,"This paper focuses on sentiment analysis in the field of natural language processing, which involves extracting positive or negative polarities from social media text. The paper examines the efficacy of four state-of-the-art machine learning classifiers, including Naïve Bayes, J48, BFTree, and OneR, for optimizing sentiment analysis. Three manually compiled datasets, two from Amazon and one from IMDB movie reviews, are used in the experiments. OneR is found to be the most promising classifier, achieving an accuracy of 91.3% in precision, 97% in F-measure, and 92.34% in correctly classified instances. Naïve Bayes is faster in learning, and J48 exhibits adequacy in true positive and false positive rates. The preprocessing methodology involves extracting foreign words, emoticons, and elongated words with their appropriate sentiments. Future work in sentiment analysis could improve preprocessing with word embeddings using deep neural networks and extend the study through convolution",Object and Sentiment Recognition,"Fig. 3."" Word cloud from textFrequeny of eons word etd ing ay ‘Tete sins et cots peg ny stu res » s s eve eis esa Soe Dil Te ETH Fig, 1.” Frequency of tweets about MeDonald’s. Frequency ofc word tweeted during the day ‘Wes ett pee ys ua weet went verse west tne Sor aac ote ESTA att Fig. 2” Frequency of tweets about KFC",Sentiment Analysis
334,"A Study on Sentiment Analysis Techniques of Twitter 
Data","Twitter; sentiment; Web data; text mining; SVM; 
Bayesian algorithm; hybrid; ensembles","The entire world is transforming quickly under the 
present innovations. The Internet has become a basic 
requirement for everybody with the Web being utilized in every 
field. With the rapid increase in social network applications, 
people are using these platforms to voice them their opinions 
with regard to daily issues. Gathering and analyzing peoples’ 
reactions toward buying a product, public services, and so on are 
vital. Sentiment analysis (or opinion mining) is a common 
dialogue preparing task that aims to discover the sentiments 
behind opinions in texts on varying subjects. In recent years, 
researchers in the field of sentiment analysis have been 
concerned with analyzing opinions on different topics such as 
movies, commercial products, and daily societal issues. Twitter is 
an enormously popular microblog on which clients may voice 
their opinions. Opinion investigation of Twitter data is a field 
that has been given much attention over the last decade and 
involves dissecting “tweets” (comments) and the content of these 
expressions. As such, this paper explores the various sentiment 
analysis applied to Twitter data and their outcomes.","In this article, diverse techniques for Twitter sentiment 
analysis methods were discussed, including machine learning, 
ensemble approaches and dictionary (lexicon) based 
approaches. In addition, hybrid and ensemble Twitter 
sentiment analysis techniques were explored. Research 
outcomes demonstrated that machine learning techniques; for 
example, the SVM and MNB produced the greatest precision, 
especially when multiple features were included. SVM 
classifiers may be viewed as standard learning strategies, while
dictionary (lexicon) based techniques are extremely viable at 
times, requiring little efforts in the human-marked archive. 
Machine learning algorithms, such as The Naive Bayes, 
Maximum Entropy, and SVM, achieved an accuracy of 
approximately 80% when n-gram and bigram model were 
utilized. Ensemble and hybrid-based Twitter sentiment analysis 
algorithms tended to perform better than supervised machine 
learning techniques, as they were able to achieve a 
classification accuracy of approximately 85%.
In general, it was expected that ensemble Twitter 
sentiment-analysis methods would perform better than 
supervised machine learning algorithms, as they combined 
multiple classifiers and occasionally various features models. 
However, hybrid methods also performed well and obtained 
reasonable classification accuracy scores, since they were able 
to take advantage of both machine learning classifiers and 
lexicon-based Twitter sentiment-analysis approaches","A Study on Sentiment Analysis Techniques of Twitter 
DataTwitter; sentiment; Web data; text mining; SVM; 
Bayesian algorithm; hybrid; ensemblesThe entire world is transforming quickly under the 
present innovations. The Internet has become a basic 
requirement for everybody with the Web being utilized in every 
field. With the rapid increase in social network applications, 
people are using these platforms to voice them their opinions 
with regard to daily issues. Gathering and analyzing peoples’ 
reactions toward buying a product, public services, and so on are 
vital. Sentiment analysis (or opinion mining) is a common 
dialogue preparing task that aims to discover the sentiments 
behind opinions in texts on varying subjects. In recent years, 
researchers in the field of sentiment analysis have been 
concerned with analyzing opinions on different topics such as 
movies, commercial products, and daily societal issues. Twitter is 
an enormously popular microblog on which clients may voice 
their opinions. Opinion investigation of Twitter data is a field 
that has been given much attention over the last decade and 
involves dissecting “tweets” (comments) and the content of these 
expressions. As such, this paper explores the various sentiment 
analysis applied to Twitter data and their outcomes.In this article, diverse techniques for Twitter sentiment 
analysis methods were discussed, including machine learning, 
ensemble approaches and dictionary (lexicon) based 
approaches. In addition, hybrid and ensemble Twitter 
sentiment analysis techniques were explored. Research 
outcomes demonstrated that machine learning techniques; for 
example, the SVM and MNB produced the greatest precision, 
especially when multiple features were included. SVM 
classifiers may be viewed as standard learning strategies, while
dictionary (lexicon) based techniques are extremely viable at 
times, requiring little efforts in the human-marked archive. 
Machine learning algorithms, such as The Naive Bayes, 
Maximum Entropy, and SVM, achieved an accuracy of 
approximately 80% when n-gram and bigram model were 
utilized. Ensemble and hybrid-based Twitter sentiment analysis 
algorithms tended to perform better than supervised machine 
learning techniques, as they were able to achieve a 
classification accuracy of approximately 85%.
In general, it was expected that ensemble Twitter 
sentiment-analysis methods would perform better than 
supervised machine learning algorithms, as they combined 
multiple classifiers and occasionally various features models. 
However, hybrid methods also performed well and obtained 
reasonable classification accuracy scores, since they were able 
to take advantage of both machine learning classifiers and 
lexicon-based Twitter sentiment-analysis approachespee Selection of study and evaluation ee ee porting the results FIGURE 1. Research methodology of systematic literature review.—_— [FIGURE 4. Steps for constructing machine learning based method.",sentiment analysis,"This article discusses the importance of sentiment analysis in the context of social media and how it has become increasingly popular to gather and analyze people's reactions towards various topics. The paper explores various sentiment analysis techniques applied to Twitter data, including machine learning, ensemble approaches, and dictionary-based methods. The research outcomes demonstrate that machine learning techniques, particularly SVM and MNB, produce the greatest precision, while ensemble and hybrid-based Twitter sentiment analysis algorithms tend to perform better than supervised machine learning techniques. The article concludes that hybrid methods can obtain reasonable classification accuracy scores by combining both machine learning classifiers and lexicon-based Twitter sentiment analysis approaches.",Object and Sentiment Recognition,pee Selection of study and evaluation ee ee porting the results FIGURE 1. Research methodology of systematic literature review.—_— [FIGURE 4. Steps for constructing machine learning based method.,Sentiment Analysis
335,"The evolution of sentiment analysis—A review of research topics,
venues, and top cited papers","Sentiment analysis
Opinion mining
Bibliometric study
Text mining
Literature review
Topic modeling
Latent Dirichlet Allocation
Qualitative analysis","Sentiment analysis is one of the fastest growing research areas in computer science, making it challenging
to keep track of all the activities in the area. We present a computer-assisted literature review, where
we utilize both text mining and qualitative coding, and analyze 6996 papers from Scopus. We find that
the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th
century and in the text subjectivity analysis performed by the computational linguistics community in
1990’s. However, the outbreak of computer-based sentiment analysis only occurred with the availability
of subjective texts on theWeb. Consequently, 99% of the papers have been published after 2004. Sentiment
analysis papers are scattered to multiple publication venues, and the combined number of papers in the
top-15 venues only represent ca. 30% of the papers in total. We present the top-20 cited papers from
Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has
shifted from analyzing online product reviews to social media texts from Twitter and Facebook. Many
topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering
and cyberbullying extend the utilization of sentiment analysis","In this article, we presented a computer-assisted literature
review, using automated text clustering with manual qualitative
analysis, and a bibliometric study of sentiment analysis of 6,996 papers. We investigated the history of sentiment analysis, evaluated
the impact of sentiment analysis and its trends through a citation
and bibliometric study, delimited the communities of sentiment
analysis by finding the most popular publication venue, discovered which research topics have been investigated in sentiment
analysis, and reviewed the most cited original works and literature
reviews in sentiment analysis.
We found that the science of sentiment analysis and opinion
mining has deep roots in the studies on public opinion analysis
at the start of 20th century. First papers that matched our search
strings were post-World War II studies that investigated the public
opinion, for example towards communism, in countries recovering
from the devastations of the war. Still, the topic was in hibernation
until the mid-2000’s when it finally emerged as an important
research topic due to the need and availability of online product
reviews. In 2005, only 101 papers about this topic were published
while in 2015 the number was nearly 5699. This gives us a nearly
50-fold increase in a decade making sentiment analysis undoubtedly one of the fastest growing research areas of the previous years.
We found that the citation counts have increased along with
the paper counts. We found for example that the top-cited paper
of sentiment analysis exceeds the citation counts of any paper
published in a much mature and larger research area of software
engineering. It is notable that the pool of papers used for sentiment
analysis was only roughly 5000, while our past work on software
engineering had nearly 70,000 papers in the pool. Thus, sentiment
analysis is also making an impact at least when measured by the
number of citations.","The evolution of sentiment analysis—A review of research topics,
venues, and top cited papersSentiment analysis
Opinion mining
Bibliometric study
Text mining
Literature review
Topic modeling
Latent Dirichlet Allocation
Qualitative analysisSentiment analysis is one of the fastest growing research areas in computer science, making it challenging
to keep track of all the activities in the area. We present a computer-assisted literature review, where
we utilize both text mining and qualitative coding, and analyze 6996 papers from Scopus. We find that
the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th
century and in the text subjectivity analysis performed by the computational linguistics community in
1990’s. However, the outbreak of computer-based sentiment analysis only occurred with the availability
of subjective texts on theWeb. Consequently, 99% of the papers have been published after 2004. Sentiment
analysis papers are scattered to multiple publication venues, and the combined number of papers in the
top-15 venues only represent ca. 30% of the papers in total. We present the top-20 cited papers from
Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has
shifted from analyzing online product reviews to social media texts from Twitter and Facebook. Many
topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering
and cyberbullying extend the utilization of sentiment analysisIn this article, we presented a computer-assisted literature
review, using automated text clustering with manual qualitative
analysis, and a bibliometric study of sentiment analysis of 6,996 papers. We investigated the history of sentiment analysis, evaluated
the impact of sentiment analysis and its trends through a citation
and bibliometric study, delimited the communities of sentiment
analysis by finding the most popular publication venue, discovered which research topics have been investigated in sentiment
analysis, and reviewed the most cited original works and literature
reviews in sentiment analysis.
We found that the science of sentiment analysis and opinion
mining has deep roots in the studies on public opinion analysis
at the start of 20th century. First papers that matched our search
strings were post-World War II studies that investigated the public
opinion, for example towards communism, in countries recovering
from the devastations of the war. Still, the topic was in hibernation
until the mid-2000’s when it finally emerged as an important
research topic due to the need and availability of online product
reviews. In 2005, only 101 papers about this topic were published
while in 2015 the number was nearly 5699. This gives us a nearly
50-fold increase in a decade making sentiment analysis undoubtedly one of the fastest growing research areas of the previous years.
We found that the citation counts have increased along with
the paper counts. We found for example that the top-cited paper
of sentiment analysis exceeds the citation counts of any paper
published in a much mature and larger research area of software
engineering. It is notable that the pool of papers used for sentiment
analysis was only roughly 5000, while our past work on software
engineering had nearly 70,000 papers in the pool. Thus, sentiment
analysis is also making an impact at least when measured by the
number of citations.ree 2 ord pris e¥ero Brokers Sees Cee con Ess sox ery cot pote Fig. 20 Environment System Architecture and Modulesie Fig. 15 Graphical Reports with Si Insights,ie Fig. 15 Graphical Reports with Si Insights,Fig. 2 Google Trends Google Trends Search Trend Tip: Use commas to compare muttiple search terms. Searches Websites - Scale is based on the average worldwide trafic of libor in all years. Leam more - An improvement to our geographical assignment was applied retroactively from 1/1/2011. Learn more libor 1.00 Search Vole index Google Trends 4.00 E 2.00 F jusajriadadainisininteiaiaiejesarProducts YY Twiaer-Powariack BB oayneten se Dvticous Bocas Oh Worse a) ° ; a 4 ° ° ° WF wiver- search AP sewn a ~ ee ok Twit -Hictrcal PowerTrack Subseroton (i Pome bin ~ — (B vnat omer ata sources are vali rom Gris? @ Estmize Bi Gooale Pus “© Newsgator KG Fecevock © vawrnca © Panoramio 2 Pie {© wowgam B Prowmictet & Fourie © trioreedooate rm @ owoue 4 Meszcte i Peaat 1 SackOvertow P Seovwes @ ter Twit TREE Read Fig. 3 Gnip Dashboard, Publishers and FeedsTeen Learning Fig. 13 Machine Learning Overview",sentiment analysis,"The article discusses a computer-assisted literature review of sentiment analysis, analyzing 6,996 papers from Scopus. The roots of sentiment analysis can be traced back to public opinion analysis and text subjectivity analysis. The majority of sentiment analysis papers have been published after 2004, with a significant increase in the number of papers published in recent years. Sentiment analysis has shifted from analyzing online product reviews to social media texts from Twitter and Facebook. The article provides a taxonomy of research topics and the top-cited papers from Google Scholar and Scopus. The impact of sentiment analysis is evaluated through a citation and bibliometric study, which shows that sentiment analysis has become one of the fastest growing research areas in computer science.",Object and Sentiment Recognition,"ree 2 ord pris e¥ero Brokers Sees Cee con Ess sox ery cot pote Fig. 20 Environment System Architecture and Modulesie Fig. 15 Graphical Reports with Si Insights,ie Fig. 15 Graphical Reports with Si Insights,Fig. 2 Google Trends Google Trends Search Trend Tip: Use commas to compare muttiple search terms. Searches Websites - Scale is based on the average worldwide trafic of libor in all years. Leam more - An improvement to our geographical assignment was applied retroactively from 1/1/2011. Learn more libor 1.00 Search Vole index Google Trends 4.00 E 2.00 F jusajriadadainisininteiaiaiejesarProducts YY Twiaer-Powariack BB oayneten se Dvticous Bocas Oh Worse a) ° ; a 4 ° ° ° WF wiver- search AP sewn a ~ ee ok Twit -Hictrcal PowerTrack Subseroton (i Pome bin ~ — (B vnat omer ata sources are vali rom Gris? @ Estmize Bi Gooale Pus “© Newsgator KG Fecevock © vawrnca © Panoramio 2 Pie {© wowgam B Prowmictet & Fourie © trioreedooate rm @ owoue 4 Meszcte i Peaat 1 SackOvertow P Seovwes @ ter Twit TREE Read Fig. 3 Gnip Dashboard, Publishers and FeedsTeen Learning Fig. 13 Machine Learning Overview",Sentiment Analysis
336,"A Deep Learning Sentiment Analyser for Social Media
Comments in Low-Resource Languages","sentiment analysis; 1D-CNN; BiLSTM; attention mechanism; Facebook comments; COVID-19.
","During the pandemic, when people needed to physically distance, social media platforms
have been one of the outlets where people expressed their opinions, thoughts, sentiments, and
emotions regarding the pandemic situation. The core object of this research study is the sentiment
analysis of peoples’ opinions expressed on Facebook regarding the current pandemic situation in lowresource languages. To do this, we have created a large-scale dataset comprising of 10,742 manually
classified comments in the Albanian language. Furthermore, in this paper we report our efforts on the
design and development of a sentiment analyser that relies on deep learning. As a result, we report
the experimental findings obtained from our proposed sentiment analyser using various classifier
models with static and contextualized word embeddings, that is, fastText and BERT, trained and
validated on our collected and curated dataset. Specifically, the findings reveal that combining the
BiLSTM with an attention mechanism achieved the highest performance on our sentiment analysis
task, with an F1 score of 72.09","This article presented a sentiment analyser for extracting opinions, thoughts and
attitudes of people expressed on social media related to the COVID-19 pandemic. Three
deep neural networks utilizing an attention mechanism and a pre-trained embedding
model, that is, fastText, are trained and validated on a real-life large-scale dataset collected
for this purpose. The dataset consisted of users’ comments in the Albanian language
posted on NIPHK Facebook page during the period of March to August 2020. Our findings
showed that our proposed sentiment analyser performed pretty well, even outperforming
the baseline classifier on the collected dataset. Specifically, an F1 score of 72.09% is achieved
by integrating a local attention mechanism with BiLSTM. These results are very promising
considering the fact that the dataset is composed of social media user-generated reviews
which are typically written in an informal manner—without any regard to standards
of the Albanian language and also consisting of informal words and phrases like slang
words, emoticons, acronyms, etc. The findings validated the usefulness of our proposed
approach as an effective solution for handling users’ sentiment expressed in social media
in low-resource languages","A Deep Learning Sentiment Analyser for Social Media
Comments in Low-Resource Languagessentiment analysis; 1D-CNN; BiLSTM; attention mechanism; Facebook comments; COVID-19.
During the pandemic, when people needed to physically distance, social media platforms
have been one of the outlets where people expressed their opinions, thoughts, sentiments, and
emotions regarding the pandemic situation. The core object of this research study is the sentiment
analysis of peoples’ opinions expressed on Facebook regarding the current pandemic situation in lowresource languages. To do this, we have created a large-scale dataset comprising of 10,742 manually
classified comments in the Albanian language. Furthermore, in this paper we report our efforts on the
design and development of a sentiment analyser that relies on deep learning. As a result, we report
the experimental findings obtained from our proposed sentiment analyser using various classifier
models with static and contextualized word embeddings, that is, fastText and BERT, trained and
validated on our collected and curated dataset. Specifically, the findings reveal that combining the
BiLSTM with an attention mechanism achieved the highest performance on our sentiment analysis
task, with an F1 score of 72.09This article presented a sentiment analyser for extracting opinions, thoughts and
attitudes of people expressed on social media related to the COVID-19 pandemic. Three
deep neural networks utilizing an attention mechanism and a pre-trained embedding
model, that is, fastText, are trained and validated on a real-life large-scale dataset collected
for this purpose. The dataset consisted of users’ comments in the Albanian language
posted on NIPHK Facebook page during the period of March to August 2020. Our findings
showed that our proposed sentiment analyser performed pretty well, even outperforming
the baseline classifier on the collected dataset. Specifically, an F1 score of 72.09% is achieved
by integrating a local attention mechanism with BiLSTM. These results are very promising
considering the fact that the dataset is composed of social media user-generated reviews
which are typically written in an informal manner—without any regard to standards
of the Albanian language and also consisting of informal words and phrases like slang
words, emoticons, acronyms, etc. The findings validated the usefulness of our proposed
approach as an effective solution for handling users’ sentiment expressed in social media
in low-resource languagesARFE Dataset [class Labelled Feature loader ‘Assigner: Dataset Selector features used (2 Festus ee Dataset Classification Naive Bayes, 148, ji 5 i BeTree and [e_(Train: Test) aningand Normalization (80:20) est set splitter Module OneR Algorithm Normalized Dataset Batch Classifier Performance Output Evaluator Text Files Fig. 2 Proposed methodology Sentiment PredictionTable 2 Initial four reviews of training set and two reviews test set Set Sentence Review ‘Class Train 1 Horrible acting and bad writing Neg 2 I never did work out what the dog death scene was all about! Neg 3 The trailer is exciting but very misleading Neg 4 The basic structure of storyline is good Pos Test 1 The effective start and a very detective story Pos 2 ‘An evening locked up in the toilet is more enjoyable Pos Second review of test set is negative but Naive Bayes is lacking in context based sentiment classification “Gad Writing” “Detective Story” Fig. 1 148 Decision Tree for terms of Example in Table 2",sentiment analysis,"The article discusses the development of a sentiment analyzer using deep learning to analyze opinions and attitudes expressed on Facebook regarding the COVID-19 pandemic in low-resource languages. The study collected and classified 10,742 comments in Albanian and trained three deep neural networks using fastText pre-trained embedding model, achieving an F1 score of 72.09% by combining BiLSTM with an attention mechanism. The study demonstrates the effectiveness of the proposed approach in handling sentiment analysis on user-generated social media comments in low-resource languages.",Object and Sentiment Recognition,"ARFE Dataset [class Labelled Feature loader ‘Assigner: Dataset Selector features used (2 Festus ee Dataset Classification Naive Bayes, 148, ji 5 i BeTree and [e_(Train: Test) aningand Normalization (80:20) est set splitter Module OneR Algorithm Normalized Dataset Batch Classifier Performance Output Evaluator Text Files Fig. 2 Proposed methodology Sentiment PredictionTable 2 Initial four reviews of training set and two reviews test set Set Sentence Review ‘Class Train 1 Horrible acting and bad writing Neg 2 I never did work out what the dog death scene was all about! Neg 3 The trailer is exciting but very misleading Neg 4 The basic structure of storyline is good Pos Test 1 The effective start and a very detective story Pos 2 ‘An evening locked up in the toilet is more enjoyable Pos Second review of test set is negative but Naive Bayes is lacking in context based sentiment classification “Gad Writing” “Detective Story” Fig. 1 148 Decision Tree for terms of Example in Table 2",Sentiment Analysis
337,Twitter Sentiment Analysis on Coronavirus: Machine Learning Approach,"sentiment analysis, COVID-19, social media, microblogging, NLP, Logistic Regression, public opinion, positive feelings, negative feelings, universal approach, data source.","In machine learning, a fundamental challenge is the analysis of data to identify 
feelings using algorithms that allow us to determine the positive or negative emotions that people 
have regarding a topic. Social networks and microblogging are a valuable source of information, 
being mostly used to express personal points of view and thoughts. Based on this knowledge we 
propose a sentiment analysis of English tweets during the pandemic COVID-19 in 2020. The 
tweets were classified as positive or negative by applying the Logistic Regression algorithm,
using this method we got a classification accuracy of 78.5%.","Taking into account that the COVID-19 disease is global health problem and has affected most countries 
and their economies, this paper focuses on analysing people’s reaction to the pandemic. The main goal 
of the research is to deduce whether the sentiment of the public opinion is positive or negative by 
applying machine learning algorithms and NLP techniques. Despite the fact that the analysis found
variation of opinions, it seems that people mostly remain positive about the pandemic, January is the 
only month in which negative thoughts predominated, March is the month when the COVID-19 disease 
was declared as a pandemic and many countries started to apply care measures and safety protocols, 
which coincides with the rise of positive thoughts. To summarize, 54% of the users showed positive 
feelings and 46% of the users showed negative feelings.
The scheme proposed within the methodology has a universal approach and changing the data source 
allows to analyse and obtain the frequency of the desired results within the examined text, guaranteeing 
the replication of this methodology in different works with similar characteristics","Twitter Sentiment Analysis on Coronavirus: Machine Learning Approachsentiment analysis, COVID-19, social media, microblogging, NLP, Logistic Regression, public opinion, positive feelings, negative feelings, universal approach, data source.In machine learning, a fundamental challenge is the analysis of data to identify 
feelings using algorithms that allow us to determine the positive or negative emotions that people 
have regarding a topic. Social networks and microblogging are a valuable source of information, 
being mostly used to express personal points of view and thoughts. Based on this knowledge we 
propose a sentiment analysis of English tweets during the pandemic COVID-19 in 2020. The 
tweets were classified as positive or negative by applying the Logistic Regression algorithm,
using this method we got a classification accuracy of 78.5%.Taking into account that the COVID-19 disease is global health problem and has affected most countries 
and their economies, this paper focuses on analysing people’s reaction to the pandemic. The main goal 
of the research is to deduce whether the sentiment of the public opinion is positive or negative by 
applying machine learning algorithms and NLP techniques. Despite the fact that the analysis found
variation of opinions, it seems that people mostly remain positive about the pandemic, January is the 
only month in which negative thoughts predominated, March is the month when the COVID-19 disease 
was declared as a pandemic and many countries started to apply care measures and safety protocols, 
which coincides with the rise of positive thoughts. To summarize, 54% of the users showed positive 
feelings and 46% of the users showed negative feelings.
The scheme proposed within the methodology has a universal approach and changing the data source 
allows to analyse and obtain the frequency of the desired results within the examined text, guaranteeing 
the replication of this methodology in different works with similar characteristicsFig2. Sentiment Analysis using Supervised Machine Leaming Algorithms.Fig 1 Sophia is known for her human ike appearance, ability ‘tomake facial expressions, nitate human gestures and _answer certain questions on specific topics. According to hercreato, she uses artificial intelizence, visual data processing and fecal recognition 2. “How can you demonstrate your empathy and ‘compassionin 2 useful manner? Robots like me will be able toleam important things about our fiends sowe can ‘provide custom personal assistance, like reminders to sive medicine.” Sophia said. Is A shall be replace aman? Documents Sophia is known for her human ike appearance, ality to make facial expressions iitate human ‘gestures and answer certain questions on specific topics. According to her creator, she uses artificial inteligence, visual data processing andfacal recognition, “How can you demonstrate your enpathy and compassionin a useful manne? Robots ike me vil beable toleam important things about our fiiends so we can provide custom personal assistance tke reminders to give medicine,” Sophia said. 1. Negative Documents. Is AI shall replace human? Example of Document-based Opinion Mining.ply, tidyr Visualization by ggplot Summarized Text dplyr, tidyr summarize Text ‘TOKINIZATION Tidy Text COUNT, dplyr INNERJOIN, dplyr Sentiment Lexicon Fig3. The Estimation Computation Procedure [44]",sentiment analysis,"The article discusses the use of machine learning and natural language processing techniques for sentiment analysis of English tweets during the COVID-19 pandemic in 2020. The study applies the Logistic Regression algorithm to classify tweets as positive or negative and achieves a classification accuracy of 78.5%. The analysis found that people mostly remained positive about the pandemic, with 54% of users showing positive feelings and 46% showing negative feelings. The proposed methodology has a universal approach that can be replicated in similar works with different data sources.",Object and Sentiment Recognition,"Fig2. Sentiment Analysis using Supervised Machine Leaming Algorithms.Fig 1 Sophia is known for her human ike appearance, ability ‘tomake facial expressions, nitate human gestures and _answer certain questions on specific topics. According to hercreato, she uses artificial intelizence, visual data processing and fecal recognition 2. “How can you demonstrate your empathy and ‘compassionin 2 useful manner? Robots like me will be able toleam important things about our fiends sowe can ‘provide custom personal assistance, like reminders to sive medicine.” Sophia said. Is A shall be replace aman? Documents Sophia is known for her human ike appearance, ality to make facial expressions iitate human ‘gestures and answer certain questions on specific topics. According to her creator, she uses artificial inteligence, visual data processing andfacal recognition, “How can you demonstrate your enpathy and compassionin a useful manne? Robots ike me vil beable toleam important things about our fiiends so we can provide custom personal assistance tke reminders to give medicine,” Sophia said. 1. Negative Documents. Is AI shall replace human? Example of Document-based Opinion Mining.ply, tidyr Visualization by ggplot Summarized Text dplyr, tidyr summarize Text ‘TOKINIZATION Tidy Text COUNT, dplyr INNERJOIN, dplyr Sentiment Lexicon Fig3. The Estimation Computation Procedure [44]",Sentiment Analysis
338,Sentiment Analysis Techniques and Approaches,"Sentiment analysis; Naïve Bayes; K-Nearest 
Neighbour; Random Forest; Maximum Entropy; SVM; Voted 
Perceptron.","Sentiment analysis or opinion mining is the 
extraction and detailed examination of opinions and attitudes 
from any form of text. Sentiment analysis is a very useful 
method widely used to express the opinion of a large group or 
mass. This sentiment can be based on the attitude of the author 
or his/her affective state at the moment of writing the text. 
Social media and other online platforms contain a huge 
amount of unstructured data in the form of tweets, blogs, posts, 
etc.This paper aims at analyzing a solution for the sentiment 
classification at a fined grained level, namely the sentence level 
in which the polarity of the sentence can be given by the three 
categories as positive, negative or neutral. In this paper, we 
have analyzed the popular techniques adopted in the classical 
Sentiment Analysis problem of analyzing Movie reviews like 
Naïve Bayes, K-Nearest Neighbour, Random Forest, 
Maximum Entropy, SVM, and Voted Perceptrons discussed in 
various papers with their advantages and disadvantages in 
detail and how many times have they provided researchers 
with satisfying results","From the above results, it can be concluded from the above given accuracies that the model implemented with the help 
of the Naive Bayesian has been much more promising than 
the SVM based approach with the given data and thus would 
be more promising to work on. The Core NLP based method 
not only carries a normal classification process but also does 
some important data preprocessing steps on the movie 
review. The data mostly used was accomplished using the 
available tools within the Stanford Core NLP library, such 
as Stanford Tokenizer and Stanford Lemmatizer,[9] 
performance improvement in various research papers. 
Additionally, research overlapping sentiment analysis and 
natural language processing has seen to be addressing many 
problems to the applicability of sentiment analysis such as 
irony detection [12] and multi-lingual support [13]. 
Moreover, the Sentiment Analyzer module significantly 
supported the model building process for these approaches 
which greatly increased the accuracy of the models which 
was built in various research papers. It was also found that 
while journals rarely change their title name and vary in 
issue and volume numbers, the conference proceedings' 
names are not reliable over the years. In order to overcome 
this issue, the author[18] cleaned the venues[11]. And thus 
it can be concluded that researching and verifying the 
methods that have benefited most, as well as research papers 
analysis and filtering, are very important in the success of 
the current model. This knowledge and experiences, when 
reused can lead to further advantages to current Sentiment 
Analysis papers and projects and is a must step to include.","Sentiment Analysis Techniques and ApproachesSentiment analysis; Naïve Bayes; K-Nearest 
Neighbour; Random Forest; Maximum Entropy; SVM; Voted 
Perceptron.Sentiment analysis or opinion mining is the 
extraction and detailed examination of opinions and attitudes 
from any form of text. Sentiment analysis is a very useful 
method widely used to express the opinion of a large group or 
mass. This sentiment can be based on the attitude of the author 
or his/her affective state at the moment of writing the text. 
Social media and other online platforms contain a huge 
amount of unstructured data in the form of tweets, blogs, posts, 
etc.This paper aims at analyzing a solution for the sentiment 
classification at a fined grained level, namely the sentence level 
in which the polarity of the sentence can be given by the three 
categories as positive, negative or neutral. In this paper, we 
have analyzed the popular techniques adopted in the classical 
Sentiment Analysis problem of analyzing Movie reviews like 
Naïve Bayes, K-Nearest Neighbour, Random Forest, 
Maximum Entropy, SVM, and Voted Perceptrons discussed in 
various papers with their advantages and disadvantages in 
detail and how many times have they provided researchers 
with satisfying resultsFrom the above results, it can be concluded from the above given accuracies that the model implemented with the help 
of the Naive Bayesian has been much more promising than 
the SVM based approach with the given data and thus would 
be more promising to work on. The Core NLP based method 
not only carries a normal classification process but also does 
some important data preprocessing steps on the movie 
review. The data mostly used was accomplished using the 
available tools within the Stanford Core NLP library, such 
as Stanford Tokenizer and Stanford Lemmatizer,[9] 
performance improvement in various research papers. 
Additionally, research overlapping sentiment analysis and 
natural language processing has seen to be addressing many 
problems to the applicability of sentiment analysis such as 
irony detection [12] and multi-lingual support [13]. 
Moreover, the Sentiment Analyzer module significantly 
supported the model building process for these approaches 
which greatly increased the accuracy of the models which 
was built in various research papers. It was also found that 
while journals rarely change their title name and vary in 
issue and volume numbers, the conference proceedings' 
names are not reliable over the years. In order to overcome 
this issue, the author[18] cleaned the venues[11]. And thus 
it can be concluded that researching and verifying the 
methods that have benefited most, as well as research papers 
analysis and filtering, are very important in the success of 
the current model. This knowledge and experiences, when 
reused can lead to further advantages to current Sentiment 
Analysis papers and projects and is a must step to include.(MV. Mantyld etal. / Computer Science Review 27 (2018) 16-32 Sz Fig. 2. Mindmup.com tool was used to collaborate in quality coding of the topics. ez ey =100 60 40 20 0 eons oe = 2S be eee oe SASSStSSsrssgnssgu a eee SERRSSRSSSSSESRSES ——sentiment analysis. ~———=customer feedback Fig. 1. Google Trends (www.google.com/trends) data showing the relative popu- larity of search strings ""sentiment analysis” and “customer feedback”.a emotes, rise EGG a Saree a soy features semantic framework oats learningessisas"" prong i ae sreVieWSez : § network SS rstra Cl Snewors twitter sstextS § i Myetection O Ine wee 575 5 rakein M iaWeDweets 2 9 extraction echngues nsuperyaes information cess Seri fecina autoat ee summorgaten ei b 2014-2016 i ges tecmaues fansystem, ret towards meesinas Enetworksmicrobloganalytics ¢_ leans tweets MECIa ersertle irees itte VBC pone | prediction soSige SO some, En RigSsublestivty eree od oa nepal 013°"" Fig. 8. (a) Word cloud of all the sentiment analysis papers (b) Word comparison cloud (2014-2016) papers on top and (-2013) on bottom.3000 2000 1000 ° 2000 2002 200-2008. -«2008-«2010«2012« 2018-2016 Fig. 3. Number of papers published per year in Scopus,Different Languages Data 2 fo Other other, Tools / Data Analysis _/ oe yA Techniques NUP [/ \C sentir Society / (secu | Travel ° Application Domain J’ Finance corporate Medical Entertainment ~ ° Othe er Expertise and Influence | ( toteraction bs Globe ° \ \ Humans J” truth <n ft \_ Language \_Behavior \ Emotions ° Fig. 9. Top level of our classification tree.",sentiment analysis,"Sentiment analysis is a method used to extract opinions and attitudes from text, especially social media and online platforms. It involves analyzing text at a fine-grained level, such as the sentence level, to determine its polarity as positive, negative, or neutral. Various techniques have been used to analyze movie reviews, including Naïve Bayes, K-Nearest Neighbour, Random Forest, Maximum Entropy, SVM, and Voted Perceptrons. From these techniques, Naive Bayesian has shown the most promising results. Additionally, sentiment analysis research overlaps with natural language processing, which addresses challenges such as irony detection and multi-lingual support. Using a Sentiment Analyzer module can significantly improve the accuracy of sentiment analysis models. To ensure the success of current models, it is important to research and verify the most beneficial methods and filter research papers. This knowledge and experience can lead to further advancements in sentiment analysis.",Object and Sentiment Recognition,"(MV. Mantyld etal. / Computer Science Review 27 (2018) 16-32 Sz Fig. 2. Mindmup.com tool was used to collaborate in quality coding of the topics. ez ey =100 60 40 20 0 eons oe = 2S be eee oe SASSStSSsrssgnssgu a eee SERRSSRSSSSSESRSES ——sentiment analysis. ~———=customer feedback Fig. 1. Google Trends (www.google.com/trends) data showing the relative popu- larity of search strings ""sentiment analysis” and “customer feedback”.a emotes, rise EGG a Saree a soy features semantic framework oats learningessisas"" prong i ae sreVieWSez : § network SS rstra Cl Snewors twitter sstextS § i Myetection O Ine wee 575 5 rakein M iaWeDweets 2 9 extraction echngues nsuperyaes information cess Seri fecina autoat ee summorgaten ei b 2014-2016 i ges tecmaues fansystem, ret towards meesinas Enetworksmicrobloganalytics ¢_ leans tweets MECIa ersertle irees itte VBC pone | prediction soSige SO some, En RigSsublestivty eree od oa nepal 013°"" Fig. 8. (a) Word cloud of all the sentiment analysis papers (b) Word comparison cloud (2014-2016) papers on top and (-2013) on bottom.3000 2000 1000 ° 2000 2002 200-2008. -«2008-«2010«2012« 2018-2016 Fig. 3. Number of papers published per year in Scopus,Different Languages Data 2 fo Other other, Tools / Data Analysis _/ oe yA Techniques NUP [/ \C sentir Society / (secu | Travel ° Application Domain J’ Finance corporate Medical Entertainment ~ ° Othe er Expertise and Influence | ( toteraction bs Globe ° \ \ Humans J” truth <n ft \_ Language \_Behavior \ Emotions ° Fig. 9. Top level of our classification tree.",Sentiment Analysis
339,Sentiment Analysis on Social Media Data Using Intelligent Techniques,"Convolutional Neural Networks (CNN), 
Emotions, Multi-layer Perceptron (MLP), 
Sentiment Analysis","Social media gives a simple method of communication 
technology for people to share their opinion, attraction and 
feeling. The aim of the paper is to extract various sentiment 
behaviour and will be used to make a strategic decision and 
also aids to categorize sentiment and affections of people as 
clear, contradictory or neutral. The data was preprocessed 
with the help of noise removal for removing the noise. The 
research work applied various techniques. After the noise 
removal, the popular classification methods were applied to 
extract the sentiment. The data were classified with the help of 
Multi-layer Perceptron (MLP), Convolutional Neural 
Networks (CNN). These two classification results were 
checked against the others classified such as Support Vector 
Machine (SVM), Random Forest, Decision tree, Naïve Bayes, 
etc., based on the sentiment classification from twitter data 
and consumer affairs website. The proposed work found that 
Multi-layer Perceptron and Convolutional Neural Networks 
performs better than another Machine Learning Classifier.","Sentiment analysis using intelligent techniques approach in 
this paper was proposed to deal with social media data. It has 
been observed that various techniques can be used to achieve 
a sentiment analysis on social media data and others. However, 
with the methods used shows that presence in the spare vector 
representation recorded a better performance than frequency. 
According to the experiment result of Twitter data and uber 
ride data from consumer affair website shows that Neural 
Networks methods such as Multi-layer Perceptron (MLP) and 
Convolutional Neural Network (CNN) performed better than 
others classifier in general. Whereas, its proposed system can 
be applied in the other internet community","Sentiment Analysis on Social Media Data Using Intelligent TechniquesConvolutional Neural Networks (CNN), 
Emotions, Multi-layer Perceptron (MLP), 
Sentiment AnalysisSocial media gives a simple method of communication 
technology for people to share their opinion, attraction and 
feeling. The aim of the paper is to extract various sentiment 
behaviour and will be used to make a strategic decision and 
also aids to categorize sentiment and affections of people as 
clear, contradictory or neutral. The data was preprocessed 
with the help of noise removal for removing the noise. The 
research work applied various techniques. After the noise 
removal, the popular classification methods were applied to 
extract the sentiment. The data were classified with the help of 
Multi-layer Perceptron (MLP), Convolutional Neural 
Networks (CNN). These two classification results were 
checked against the others classified such as Support Vector 
Machine (SVM), Random Forest, Decision tree, Naïve Bayes, 
etc., based on the sentiment classification from twitter data 
and consumer affairs website. The proposed work found that 
Multi-layer Perceptron and Convolutional Neural Networks 
performs better than another Machine Learning Classifier.Sentiment analysis using intelligent techniques approach in 
this paper was proposed to deal with social media data. It has 
been observed that various techniques can be used to achieve 
a sentiment analysis on social media data and others. However, 
with the methods used shows that presence in the spare vector 
representation recorded a better performance than frequency. 
According to the experiment result of Twitter data and uber 
ride data from consumer affair website shows that Neural 
Networks methods such as Multi-layer Perceptron (MLP) and 
Convolutional Neural Network (CNN) performed better than 
others classifier in general. Whereas, its proposed system can 
be applied in the other internet communityNeutral mPositive m Negative ‘August July June May April ‘March 500 1000 1500-2000 »«2500 «30003500 Figure 5. Number of comments across months.mmecwtral mm Pestive mm Negatve Ave ‘comments accros categories Figure 4. Length of comments among sentiment classes.(None, 18, 512) Globalviax Layer (None, 9,512) (©) Hybrid 1D-CNN + BiLSTM Architecture Figure 6. Deep neural networks used for sentiment classification.Facebooklcomments (“sentiment } ‘Annotators _Unlabeladjoomments Category Labeting Figure 1. High-level architecture diagram of the proposed ALBANA analyser. @ ‘©",sentiment analysis,"The paper discusses sentiment analysis of social media data to extract various sentiment behaviors for strategic decision making. The data was preprocessed to remove noise, and classification techniques such as Multi-layer Perceptron (MLP), Convolutional Neural Networks (CNN), SVM, Random Forest, Decision tree, and Naïve Bayes were used to extract sentiment from Twitter data and consumer affairs websites. The proposed work found that MLP and CNN performed better than other classifiers. The study concludes that various techniques can be used to achieve sentiment analysis on social media data, and presence in the spare vector representation recorded better performance than frequency. The proposed system can be applied in other internet communities.",Object and Sentiment Recognition,"Neutral mPositive m Negative ‘August July June May April ‘March 500 1000 1500-2000 »«2500 «30003500 Figure 5. Number of comments across months.mmecwtral mm Pestive mm Negatve Ave ‘comments accros categories Figure 4. Length of comments among sentiment classes.(None, 18, 512) Globalviax Layer (None, 9,512) (©) Hybrid 1D-CNN + BiLSTM Architecture Figure 6. Deep neural networks used for sentiment classification.Facebooklcomments (“sentiment } ‘Annotators _Unlabeladjoomments Category Labeting Figure 1. High-level architecture diagram of the proposed ALBANA analyser. @ ‘©",Sentiment Analysis
340,Person Recognition in Social Media Photos,"Computer vision, person recognition, social media.","People nowadays share large parts of their personal lives through social media. Being able to automatically recognise
people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task,
however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social
media photos sets new challenges for computer vision, including non-cooperative subjects (e.g. backward viewpoints, unusual poses)
and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet
features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance
gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time
and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA [1]
benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social
groups, and events.
Compared the conference version of the paper [2], this paper additionally presents (1) analysis of a face recogniser (DeepID2+ [3]), (2)
new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even
compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the
head viewpoint-wise breakdown of performance, and (5) results on the open-world setup.","We have analysed the problem of person recognition in social
media photos where people may appear with occluded faces, in
diverse poses, and in various social events. We have investigated
efficacy of various cues, including the face recogniser DeepID2+
[3], and their time and head viewpoint generalisability. For better
analysis, we have contributed additional splits on PIPA [1] that
simulate different amount of time gap between training and testing
samples.
We have made four major conclusions. (1) Cues based on face
and head are robust across time (§5.1). (2) Cues based on context
are robust across head viewpoints (§5.2). (3) The final model
naeil2, a combination of face and context cues, is robust across
both time and viewpoint and achieves a ∼9 pp improvement over
a recent state of the art on the challenging Day split (§4.5). (4)
Better convnet architectures and face recognisers will improve the
performance of the naeil and naeil2 frameworks in the future
§4.5).
The remaining challenges are mainly the large time gap and
occluded face scenarios (§5.2). One possible direction is to exploit
non-visual cues like GPS and time metadata, camera parameters,
or social media album/friendship graphs","Person Recognition in Social Media PhotosComputer vision, person recognition, social media.People nowadays share large parts of their personal lives through social media. Being able to automatically recognise
people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task,
however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social
media photos sets new challenges for computer vision, including non-cooperative subjects (e.g. backward viewpoints, unusual poses)
and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet
features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance
gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time
and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA [1]
benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social
groups, and events.
Compared the conference version of the paper [2], this paper additionally presents (1) analysis of a face recogniser (DeepID2+ [3]), (2)
new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even
compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the
head viewpoint-wise breakdown of performance, and (5) results on the open-world setup.We have analysed the problem of person recognition in social
media photos where people may appear with occluded faces, in
diverse poses, and in various social events. We have investigated
efficacy of various cues, including the face recogniser DeepID2+
[3], and their time and head viewpoint generalisability. For better
analysis, we have contributed additional splits on PIPA [1] that
simulate different amount of time gap between training and testing
samples.
We have made four major conclusions. (1) Cues based on face
and head are robust across time (§5.1). (2) Cues based on context
are robust across head viewpoints (§5.2). (3) The final model
naeil2, a combination of face and context cues, is robust across
both time and viewpoint and achieves a ∼9 pp improvement over
a recent state of the art on the challenging Day split (§4.5). (4)
Better convnet architectures and face recognisers will improve the
performance of the naeil and naeil2 frameworks in the future
§4.5).
The remaining challenges are mainly the large time gap and
occluded face scenarios (§5.2). One possible direction is to exploit
non-visual cues like GPS and time metadata, camera parameters,
or social media album/friendship graphs08 07 06 05 true label 04 03 02 predicted label Figure 2. Confusion matrix for Binary Logistic Regression model (overall accuracy 78.57%).oi Rare according tothe etm ‘nti Figure 1. Flowchart of the twitter sentiment analysis classifier.‘Twitter Sentiments 0 me Nagatve mm Fostive © b0 } fo 0 dn Feb Morr My, nd Figure 4. Stacked chart of the tweets sentiment analysis. Tweets sentiments Negative Twasts Figure 5. Pie chart of the overall tweets sentiment analysis.",Person recognition,"The paper discusses the challenges of person recognition in social media photos, which require recognizing people in non-cooperative scenarios and with changes in appearance. The authors propose a framework that uses convnet features from multiple image regions and analyze the importance of different features over time and viewpoint generalizability. They also introduce new splits on the PIPA dataset to simulate different time gaps between training and testing samples. The results show that cues based on face and context are robust across time and viewpoint, and the proposed naeil2 framework achieves state-of-the-art results. The authors suggest that future research could explore non-visual cues such as GPS and time metadata, camera parameters, or social media album/friendship graphs.",Object and Sentiment Recognition,"08 07 06 05 true label 04 03 02 predicted label Figure 2. Confusion matrix for Binary Logistic Regression model (overall accuracy 78.57%).oi Rare according tothe etm ‘nti Figure 1. Flowchart of the twitter sentiment analysis classifier.‘Twitter Sentiments 0 me Nagatve mm Fostive © b0 } fo 0 dn Feb Morr My, nd Figure 4. Stacked chart of the tweets sentiment analysis. Tweets sentiments Negative Twasts Figure 5. Pie chart of the overall tweets sentiment analysis.",Sentiment Analysis
341,Person Recognition in Personal Photo Collections,"Computer vision, Person recognition, Social media.","—People nowadays share large parts of their personal lives through social media. Being able to automatically recognise
people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task,
however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social
media photos sets new challenges for computer vision, including non-cooperative subjects (e.g. backward viewpoints, unusual poses)
and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet
features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance
gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time
and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA [1]
benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social
groups, and events.
Compared the conference version of the paper [2], this paper additionally presents (1) analysis of a face recogniser (DeepID2+ [3]), (2)
new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even
compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the
head viewpoint-wise breakdown of performance, and (5) results on the open-world setup","Adding a few times more examples per person will not push the
performance to 100%. Methodological advances are required to
fully solve the problem. On the other hand, the methods already
collect substantial amount of identity information only from single
sample per person (far above chance level).","Person Recognition in Personal Photo CollectionsComputer vision, Person recognition, Social media.—People nowadays share large parts of their personal lives through social media. Being able to automatically recognise
people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task,
however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social
media photos sets new challenges for computer vision, including non-cooperative subjects (e.g. backward viewpoints, unusual poses)
and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet
features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance
gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time
and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA [1]
benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social
groups, and events.
Compared the conference version of the paper [2], this paper additionally presents (1) analysis of a face recogniser (DeepID2+ [3]), (2)
new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even
compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the
head viewpoint-wise breakdown of performance, and (5) results on the open-world setupAdding a few times more examples per person will not push the
performance to 100%. Methodological advances are required to
fully solve the problem. On the other hand, the methods already
collect substantial amount of identity information only from single
sample per person (far above chance level).Hold out method and 10-fold cross validation I Hold out method [_10-fold cross validation ‘Support Vector Stochastic Gradient Logistic Regression SGD with logistic K Nearest Random Forest Naive Bayesian Machine Learning Approaches Maximum Entropy Ensemble classifier Figure 1.2: the accuracy obtained from the hold out method and 10 fold cross-validation method.z Tnput and tokentze data 2. Remove redundant word: and special characters Figure 1",Person recognition,"The paper discusses the challenges of person recognition in social media photos, including non-cooperative subjects and changes in appearance, and presents a simple approach that uses convnet features from multiple image regions. The authors propose new recognition scenarios that focus on time and appearance gaps between training and testing samples and achieve state-of-the-art results on the PIPA benchmark. They also analyze the importance of different features and present a new method called naeil2 that combines their previous approach with a face recognizer to improve performance. The paper concludes that methodological advances are needed to fully solve the problem of person recognition in social media photos but the methods presented in the paper already collect substantial identity information even from a single sample per person.",Object and Sentiment Recognition,Hold out method and 10-fold cross validation I Hold out method [_10-fold cross validation ‘Support Vector Stochastic Gradient Logistic Regression SGD with logistic K Nearest Random Forest Naive Bayesian Machine Learning Approaches Maximum Entropy Ensemble classifier Figure 1.2: the accuracy obtained from the hold out method and 10 fold cross-validation method.z Tnput and tokentze data 2. Remove redundant word: and special characters Figure 1,Sentiment Analysis
342,"The Impact of Social Media on the Brand Capital of
Famous People",famous people; personal brand; Internet users; social media,"The article is of a research nature. The aim of the article is to identify the role of social media
in shaping personal brand. To this end, the first part discusses the concept of personal brand, as well
as components of brand capital in the case of famous people, including consumer-based capital.
Attention was also paid to the great importance of social media and the growing role of their users in
the process of shaping personal brand. Based on the analysis of the source literature, a research gap
was identified, related to the lack of empirical verification of the relationship between users’ online
activity and and capital of famous people, also known as celebrities, associated with artistic and
cultural activities. The article uses the results of the direct research carried out in the period 2019–2020.
The second (empirical) part of the article presents research hypotheses, methodology, as well as results
and conclusions from the research. Based on 26 in-depth individual interviews that were conducted
with people famous in Poland (mainly engaged in artistic and cultural activities) and surveys of a
group of 324 social media users, it was shown, among others, that online activity of Internet users
stimulates the brand capital of famous people. Statistically significant relationships were observed for
such components of the personal brand as awareness/associations with the personal brand and for
the relationship regarding the perception of the quality of activities carried out by a famous person.","People famous in Poland who reach for social media in the process of building the capital of
their personal brand should remember that these media are governed by different laws than typical
marketing communication tools. Particular attention should be paid to openness, transparency,
informality and equality of users. An important implication of these features is readiness for an honest
dialogue with users. However, it is not enough to focus the attention of others on yourself, you also
need to maintain it, which is much more difficult. Therefore, to maintain the dynamics of this image,
you need to create interesting events. It should be remembered that situations appearing in the lives of
famous people in an uncontrolled manner also require adequate exposure because these types of events
are particularly attractive to a large number of Internet users. Famous people must also be aware of the
fact that interactions with social media users can be both positive and negative, which translates into
the capital of their personal brand in both positive and negative ways [42,43]. Famous people building
their personal brand using social media may have to face troublesome interactions, criticising their
activities, presented photos, videos, initiatives or topics. The manifestation of criticism (both more and
less justified) in the case of famous persons is inevitable, therefore it is necessary for such persons to
determine—in consultation with their agents—the appropriate guidelines for dealing with such cases.
Of course, disabling comments is always an option, but it closes a priceless feedback channel, thanks to
which famous people can not only improve their activities and adjust them better to the needs of their
recipients, but also build personal brand capital based on the activity of social media users [44–46].
The obtained results may be of use in the development of the strategy for building a personal
brand using social media to be used by famous people operating in a cultural and artistic environment.
Based on the conducted research, it is possible to indicate future research directions regarding the
factors influencing the involvement of Internet users in the process of building celebrities’ brand capital.
In the future, in-depth qualitative and quantitative research is planned on a much larger sample of
celebrities and online consumers of famous people’s services in countries of Central Europe.","The Impact of Social Media on the Brand Capital of
Famous Peoplefamous people; personal brand; Internet users; social mediaThe article is of a research nature. The aim of the article is to identify the role of social media
in shaping personal brand. To this end, the first part discusses the concept of personal brand, as well
as components of brand capital in the case of famous people, including consumer-based capital.
Attention was also paid to the great importance of social media and the growing role of their users in
the process of shaping personal brand. Based on the analysis of the source literature, a research gap
was identified, related to the lack of empirical verification of the relationship between users’ online
activity and and capital of famous people, also known as celebrities, associated with artistic and
cultural activities. The article uses the results of the direct research carried out in the period 2019–2020.
The second (empirical) part of the article presents research hypotheses, methodology, as well as results
and conclusions from the research. Based on 26 in-depth individual interviews that were conducted
with people famous in Poland (mainly engaged in artistic and cultural activities) and surveys of a
group of 324 social media users, it was shown, among others, that online activity of Internet users
stimulates the brand capital of famous people. Statistically significant relationships were observed for
such components of the personal brand as awareness/associations with the personal brand and for
the relationship regarding the perception of the quality of activities carried out by a famous person.People famous in Poland who reach for social media in the process of building the capital of
their personal brand should remember that these media are governed by different laws than typical
marketing communication tools. Particular attention should be paid to openness, transparency,
informality and equality of users. An important implication of these features is readiness for an honest
dialogue with users. However, it is not enough to focus the attention of others on yourself, you also
need to maintain it, which is much more difficult. Therefore, to maintain the dynamics of this image,
you need to create interesting events. It should be remembered that situations appearing in the lives of
famous people in an uncontrolled manner also require adequate exposure because these types of events
are particularly attractive to a large number of Internet users. Famous people must also be aware of the
fact that interactions with social media users can be both positive and negative, which translates into
the capital of their personal brand in both positive and negative ways [42,43]. Famous people building
their personal brand using social media may have to face troublesome interactions, criticising their
activities, presented photos, videos, initiatives or topics. The manifestation of criticism (both more and
less justified) in the case of famous persons is inevitable, therefore it is necessary for such persons to
determine—in consultation with their agents—the appropriate guidelines for dealing with such cases.
Of course, disabling comments is always an option, but it closes a priceless feedback channel, thanks to
which famous people can not only improve their activities and adjust them better to the needs of their
recipients, but also build personal brand capital based on the activity of social media users [44–46].
The obtained results may be of use in the development of the strategy for building a personal
brand using social media to be used by famous people operating in a cultural and artistic environment.
Based on the conducted research, it is possible to indicate future research directions regarding the
factors influencing the involvement of Internet users in the process of building celebrities’ brand capital.
In the future, in-depth qualitative and quantitative research is planned on a much larger sample of
celebrities and online consumers of famous people’s services in countries of Central Europe.CNN MLP ‘SVM Random Forest Decision Tree Logistic Regression Maxént Naive Bayes Figure 4, Performance Evaluation on Uber Ride Review CNN MLP svm Random Forest Decision Tree Logistic Regression MaxEnt Naive Bayes Figure 5. Performance Evaluation on Famous PersonmNegative m Positive mNeutral Figure 2. Sentiment Analysis of Uber Ride Review Negative mPositive Neutral Figure 3. Sentiment Analysis of Famous PersonTwitter and Social Media Data Extraction Preprocessing Features Extraction Sentiment and Classification analysis Result Visualization and Interpretation Figure 1. Proposed System",Others,"The article discusses the role of social media in shaping personal brand, with a focus on celebrities associated with artistic and cultural activities. The first part of the article defines personal brand and discusses the importance of social media in building brand capital. The second part presents empirical research, which shows that online activity of Internet users stimulates the brand capital of famous people. However, celebrities must also be aware of the potential negative impacts of interactions with social media users, such as criticism. The article concludes with implications for celebrities building their personal brand using social media, and suggests future research directions.",Object and Sentiment Recognition,"CNN MLP ‘SVM Random Forest Decision Tree Logistic Regression Maxént Naive Bayes Figure 4, Performance Evaluation on Uber Ride Review CNN MLP svm Random Forest Decision Tree Logistic Regression MaxEnt Naive Bayes Figure 5. Performance Evaluation on Famous PersonmNegative m Positive mNeutral Figure 2. Sentiment Analysis of Uber Ride Review Negative mPositive Neutral Figure 3. Sentiment Analysis of Famous PersonTwitter and Social Media Data Extraction Preprocessing Features Extraction Sentiment and Classification analysis Result Visualization and Interpretation Figure 1. Proposed System",Sentiment Analysis
343,"A review of the studies on social media images from the perspective of
information interaction","Social media
Images
Information interaction
Interactive behavior","As the development of social media and the rise of visual culture, image in the social media
has received more and more attention from scholars and sorting out its content is beneficial to clarifying the
related research of images in social media and provide a new research perspective. [Method/procedure] This
article takes the image literature in social media as the research object, carries on the keyword analysis to the
literature and summarizes the typical research methods. Then this paper adopts analytic induction method to
summarize the research progress of images in social media in recent ten years from the perspective of “publisherinformation-receiver” in information interaction. [Results/conclusions] Through the analysis of research topics, it
is found that the feature of images on social media and user's information interaction behavior based on image are
the key content that researchers pay attention to. Studies on images in social media can be divided into three
parts:the characteristics of images in social media, image publishing behavior in social media, and image
perception and acquisition behavior of social media users.[Innovation/value]This paper constructs a systematic
cognitive frame for image research in social media, summarizes the existing research results, and predicts the
future research from three aspects: the impact of image-based information interaction on users' social relationships in social media environments, the problem of user privacy disclosure in image social communication and the
advancement of computer vision technique in image research in social media environment","In this paper, the research progress of images in social media is
summarized by comprehensive induction. Through the extraction and
analysis of relevant literature and key words, this paper reveals the
research hotspots and development trends of visual information interaction in image social interaction. By combing the research methods and
topics in this field and combining the transmission process of images in
social media, this paper starts from three different perspectives: the
characteristics of images in social media, the image-based information
publishing behavior in social media, and the image-based information
perception and acquisition behavior in social media. It is found that the
research on image characteristics in social media mainly analyzes the
content of images from different perspectives; The research on the information publishing behavior of social media based on images mainly
focuses on the influencing factors of user behavior and the reflection of
the information carried by images on the personal characteristics of
image publishers; the research on information perception and acquisition
behavior of social media based on images includes two aspects: behavioral motivation and influencing factors.
This paper looks into the future research from three aspects: the
impact of image-based information interaction in social media on users'
social relationships and privacy, and the advancement of computer vision
technique on image research in social media. In terms of users' social
relationships, future studies can focus on the factors at different levels of
image social behavior affecting weak social ties and compare the effects
of image social behavior on strong and weak social ties in social media. In
terms of user privacy, on the one hand, from the perspective of users, we
can explore the influence of users’ image use behavior on privacy
disclosure; On the other hand, from the perspective of social media
platform, the influence of platform mechanism on the privacy disclosure caused by the use of images on social media can be explored. At the same
time, the future research on images in social media can break through
some limitations of existing research with the help of computer vision
technique, and increase the scientific and practical contribution of the
research.","A review of the studies on social media images from the perspective of
information interactionSocial media
Images
Information interaction
Interactive behaviorAs the development of social media and the rise of visual culture, image in the social media
has received more and more attention from scholars and sorting out its content is beneficial to clarifying the
related research of images in social media and provide a new research perspective. [Method/procedure] This
article takes the image literature in social media as the research object, carries on the keyword analysis to the
literature and summarizes the typical research methods. Then this paper adopts analytic induction method to
summarize the research progress of images in social media in recent ten years from the perspective of “publisherinformation-receiver” in information interaction. [Results/conclusions] Through the analysis of research topics, it
is found that the feature of images on social media and user's information interaction behavior based on image are
the key content that researchers pay attention to. Studies on images in social media can be divided into three
parts:the characteristics of images in social media, image publishing behavior in social media, and image
perception and acquisition behavior of social media users.[Innovation/value]This paper constructs a systematic
cognitive frame for image research in social media, summarizes the existing research results, and predicts the
future research from three aspects: the impact of image-based information interaction on users' social relationships in social media environments, the problem of user privacy disclosure in image social communication and the
advancement of computer vision technique in image research in social media environmentIn this paper, the research progress of images in social media is
summarized by comprehensive induction. Through the extraction and
analysis of relevant literature and key words, this paper reveals the
research hotspots and development trends of visual information interaction in image social interaction. By combing the research methods and
topics in this field and combining the transmission process of images in
social media, this paper starts from three different perspectives: the
characteristics of images in social media, the image-based information
publishing behavior in social media, and the image-based information
perception and acquisition behavior in social media. It is found that the
research on image characteristics in social media mainly analyzes the
content of images from different perspectives; The research on the information publishing behavior of social media based on images mainly
focuses on the influencing factors of user behavior and the reflection of
the information carried by images on the personal characteristics of
image publishers; the research on information perception and acquisition
behavior of social media based on images includes two aspects: behavioral motivation and influencing factors.
This paper looks into the future research from three aspects: the
impact of image-based information interaction in social media on users'
social relationships and privacy, and the advancement of computer vision
technique on image research in social media. In terms of users' social
relationships, future studies can focus on the factors at different levels of
image social behavior affecting weak social ties and compare the effects
of image social behavior on strong and weak social ties in social media. In
terms of user privacy, on the one hand, from the perspective of users, we
can explore the influence of users’ image use behavior on privacy
disclosure; On the other hand, from the perspective of social media
platform, the influence of platform mechanism on the privacy disclosure caused by the use of images on social media can be explored. At the same
time, the future research on images in social media can break through
some limitations of existing research with the help of computer vision
technique, and increase the scientific and practical contribution of the
research.",Person recognition,"This article summarizes the research progress of images in social media in the past ten years. The research is divided into three parts: the characteristics of images in social media, image publishing behavior, and image perception and acquisition behavior of social media users. The article also identifies the research hotspots and development trends of visual information interaction in image social interaction. The paper concludes by predicting future research in three aspects: the impact of image-based information interaction on users' social relationships, the problem of user privacy disclosure, and the advancement of computer vision technique in image research in social media.",Object and Sentiment Recognition,,Object Recognition
344,"Fake profile recognition using big data analytics in 
social media platforms",fake profile; social media; big data; spark,"Online social media platforms today have many more users than ever before. This 
increased fake profiles trends which is harming both social and business entities as fraudsters use 
images of people for creating new fake profiles. However, most of those proposed methods are 
out-dated and aren’t accurate enough with an average accuracy of 83%. Our proposed solution, 
for this problem, is a Spark ML-based project that can predict fake profiles with higher accuracy 
than other present methods of profile recognition. Our project consists of Spark ML libraries 
including Random Forest Classifier and other plotting tools. We have described our proposed 
model diagram and tried to depict our results in graphical representations like confusion matrix, 
learning curve and ROC plot for better understanding. Research findings through this project 
illustrate that this proposed system has accuracy of 93% in finding fake profiles over social 
media platforms. While there is 7% false positive rate in which our system fails to correctly 
identify a fake profile. ","Trend of social media is increasing on daily basis. Risk of 
security issues and also it is noticed that fake profiles on 
social media is increasing. Proposed solution for this issue 
aims to provide better and secure platform to user of social 
media. By using face recognition library our proposed 
solution for this problem is to train 70% of our profiles data 
on machine learning algorithms using Spark ML lib and 
then we will test remaining 30% data to find accuracy and 
predictions. Our prediction model will be depending on 
steps such as reading data set from CSV feature 
engineering, training data using Random Forest, plotting 
learning curve, plotting confusion matrix, plotting ROC 
curve. Accuracy for our proposed solution is 94%. 
Limitations of this project include some false positive 
results that can effect result performance by up to 6%. 
Future work for this project can be an improved method to 
identify fake profiles and reduce this error ratio from 6% to as low as possible. We will also try to implement with deep 
learning and bi-models (Maksimov and Koiranen, 2020; 
Mujahid et al., 2021; Nagi et al., 2021; AbdelAziz et al., 
2020; Aftab et al., 2021; Javed et al., 2021; Mubashar at al., 
2021) in our model to enhance fake profile recognition on 
bases on images and profile pictures. We will also work for 
a user friendly interface where a non tech user will also be 
able to identify fake profiles and avoid scams.","Fake profile recognition using big data analytics in 
social media platformsfake profile; social media; big data; sparkOnline social media platforms today have many more users than ever before. This 
increased fake profiles trends which is harming both social and business entities as fraudsters use 
images of people for creating new fake profiles. However, most of those proposed methods are 
out-dated and aren’t accurate enough with an average accuracy of 83%. Our proposed solution, 
for this problem, is a Spark ML-based project that can predict fake profiles with higher accuracy 
than other present methods of profile recognition. Our project consists of Spark ML libraries 
including Random Forest Classifier and other plotting tools. We have described our proposed 
model diagram and tried to depict our results in graphical representations like confusion matrix, 
learning curve and ROC plot for better understanding. Research findings through this project 
illustrate that this proposed system has accuracy of 93% in finding fake profiles over social 
media platforms. While there is 7% false positive rate in which our system fails to correctly 
identify a fake profile. Trend of social media is increasing on daily basis. Risk of 
security issues and also it is noticed that fake profiles on 
social media is increasing. Proposed solution for this issue 
aims to provide better and secure platform to user of social 
media. By using face recognition library our proposed 
solution for this problem is to train 70% of our profiles data 
on machine learning algorithms using Spark ML lib and 
then we will test remaining 30% data to find accuracy and 
predictions. Our prediction model will be depending on 
steps such as reading data set from CSV feature 
engineering, training data using Random Forest, plotting 
learning curve, plotting confusion matrix, plotting ROC 
curve. Accuracy for our proposed solution is 94%. 
Limitations of this project include some false positive 
results that can effect result performance by up to 6%. 
Future work for this project can be an improved method to 
identify fake profiles and reduce this error ratio from 6% to as low as possible. We will also try to implement with deep 
learning and bi-models (Maksimov and Koiranen, 2020; 
Mujahid et al., 2021; Nagi et al., 2021; AbdelAziz et al., 
2020; Aftab et al., 2021; Javed et al., 2021; Mubashar at al., 
2021) in our model to enhance fake profile recognition on 
bases on images and profile pictures. We will also work for 
a user friendly interface where a non tech user will also be 
able to identify fake profiles and avoid scams.",Others,"The trend of social media platforms has led to an increase in fake profiles, which harms social and business entities. Existing methods for identifying fake profiles have an average accuracy of 83%, which is not accurate enough. The proposed solution is a Spark ML-based project that can predict fake profiles with higher accuracy than existing methods, with an accuracy of 93% and a 7% false positive rate. The project uses face recognition libraries and trains 70% of the profiles data on machine learning algorithms using Spark ML lib, then tests the remaining 30% data to find accuracy and predictions. The proposed solution's limitations include a false positive rate of up to 6%. Future work includes improving the method to identify fake profiles and reducing the error ratio, implementing with deep learning and bi-models to enhance fake profile recognition, and developing a user-friendly interface for non-tech users to identify fake profiles and avoid scams.",Object and Sentiment Recognition,,Object Recognition
345,"A Review Paper on Face Recognition 
Methodologies","face recognition, literature review, algorithms, neural networks, line edge mapping.","In the previous few years, the procedures of face 
recognition have been researched thoroughly. Well-versed 
reviews, for various human face recognition methodologies, are 
provided in this paper. Initially, we proffer a summary of face 
recognition with its application. Followed by a literature review 
of various face recognition techniques. Several face recognition
algorithms are analyzed and elaborated with their limitations as 
well. It also includes brief overviews regarding various modern 
approaches like neural networks, line edge mapping, and many 
others, which are widely used nowadays to make the process of 
face recognition more efficient. Conclusively, the research 
results are reviewed and are summarized.","So in this paper, we've wholly reviewed some of the 
methodologies and we've also learned that way face 
recognition and different approaches are researched it will be 
one of the major machine learning applications in the coming 
future. We've also found that there are various practical 
methods and approaches to achieve this and to add some 
greater scope regarding face recognition.","A Review Paper on Face Recognition 
Methodologiesface recognition, literature review, algorithms, neural networks, line edge mapping.In the previous few years, the procedures of face 
recognition have been researched thoroughly. Well-versed 
reviews, for various human face recognition methodologies, are 
provided in this paper. Initially, we proffer a summary of face 
recognition with its application. Followed by a literature review 
of various face recognition techniques. Several face recognition
algorithms are analyzed and elaborated with their limitations as 
well. It also includes brief overviews regarding various modern 
approaches like neural networks, line edge mapping, and many 
others, which are widely used nowadays to make the process of 
face recognition more efficient. Conclusively, the research 
results are reviewed and are summarized.So in this paper, we've wholly reviewed some of the 
methodologies and we've also learned that way face 
recognition and different approaches are researched it will be 
one of the major machine learning applications in the coming 
future. We've also found that there are various practical 
methods and approaches to achieve this and to add some 
greater scope regarding face recognition.",Facial Emotion Recognition,"This paper provides a comprehensive review of various face recognition techniques, including a summary of face recognition and its applications, literature reviews of different methods, analysis of algorithms, and modern approaches such as neural networks and line edge mapping. The paper concludes by summarizing the research results and suggesting that face recognition will be one of the major machine learning applications in the future, with various practical methods and approaches to achieve greater scope in this field.",Object and Sentiment Recognition,,Deep Learning and Machine Learning
346,"Effective use of social media platforms 
for promotion of mental health 
awareness","Health promotion, mental health, social media"," Social media platforms are progressively developing as a rich source of mass 
communication. Increasing mental health awareness with the help of social media can be a good 
initiative to reach out to a large number of people in a short time frame. This study was conducted 
to understand the usefulness of social media platforms for health promotion.It was a qualitative study to evaluate the effectiveness of social 
media platforms in hosting health promotion campaigns in the field of mental health, which was 
observed over 5 months from May to September 2019 to reach more people for effective information 
dissemination. The campaigns were as follows (1) The Buddies for Suicide Prevention: an online 
campaign to create awareness about suicide prevention. The campaign included script writing, 
slogan writing, poster making, and short films making, organized for the general public who were 
interested to take part; (2) The #Iquitobacco was a 21‑day campaign with an idea of tobacco cessation 
in the community, conducted among social media viewers who were willing to participate; and (3) 
#Migrainethepainfultruth was yet another campaign conducted among the social media viewers who 
were interested to participate. All the campaigns were conducted using two famous social media 
platforms commonly used by young adults. Descriptive statistics such as frequency and proportions 
were computed for the number of likes and shares. The Facebook and Instagram posts concerning all the campaigns brought about a 
considerable amount of reach to the targeted population. After the campaigns, the page reached to 
around 10.3 k people (both fans and nonfans).","Using social media platforms to conduct mental health 
campaigns is an effective initiative as one can reach out 
to many people over a short time frame. With the help of 
technology, participants could virtually be a part of the 
campaign easily. There is an increasing trend in mental 
health awareness by effectively using digital media as 
an information dissemination platform.","Effective use of social media platforms 
for promotion of mental health 
awarenessHealth promotion, mental health, social media Social media platforms are progressively developing as a rich source of mass 
communication. Increasing mental health awareness with the help of social media can be a good 
initiative to reach out to a large number of people in a short time frame. This study was conducted 
to understand the usefulness of social media platforms for health promotion.It was a qualitative study to evaluate the effectiveness of social 
media platforms in hosting health promotion campaigns in the field of mental health, which was 
observed over 5 months from May to September 2019 to reach more people for effective information 
dissemination. The campaigns were as follows (1) The Buddies for Suicide Prevention: an online 
campaign to create awareness about suicide prevention. The campaign included script writing, 
slogan writing, poster making, and short films making, organized for the general public who were 
interested to take part; (2) The #Iquitobacco was a 21‑day campaign with an idea of tobacco cessation 
in the community, conducted among social media viewers who were willing to participate; and (3) 
#Migrainethepainfultruth was yet another campaign conducted among the social media viewers who 
were interested to participate. All the campaigns were conducted using two famous social media 
platforms commonly used by young adults. Descriptive statistics such as frequency and proportions 
were computed for the number of likes and shares. The Facebook and Instagram posts concerning all the campaigns brought about a 
considerable amount of reach to the targeted population. After the campaigns, the page reached to 
around 10.3 k people (both fans and nonfans).Using social media platforms to conduct mental health 
campaigns is an effective initiative as one can reach out 
to many people over a short time frame. With the help of 
technology, participants could virtually be a part of the 
campaign easily. There is an increasing trend in mental 
health awareness by effectively using digital media as 
an information dissemination platform.",Others,"Social media platforms have become a valuable tool for mass communication, including in the promotion of mental health awareness. This qualitative study evaluated the effectiveness of three health promotion campaigns conducted on Facebook and Instagram over a 5-month period in 2019. The campaigns focused on suicide prevention, tobacco cessation, and migraines, and involved script writing, slogan writing, poster making, and short film making. Descriptive statistics showed that the campaigns reached a considerable number of people, with the Facebook and Instagram posts reaching around 10.3k people. The study concludes that using social media platforms for mental health campaigns is an effective way to reach a large number of people in a short time frame and that digital media is increasingly being used for mental health awareness initiatives",Object and Sentiment Recognition,,Object Recognition
347,"Human face recognition based on convolutional
neural network and augmented dataset","Face recognition;
convolutional neural
network; augmented
dataset; CNN","To deal with the issue of human face recognition on small original dataset, a new approach combining convolutional neural network (CNN) with augmented dataset is developed in this paper. The
original small dataset is augmented to be a large dataset via several transformations of the face
images. Based on the augmented face image dataset, the feature of the faces can be effectively
extracted and higher face recognition accuracy can be achieved by using the ingenious CNN. The
effectiveness and superiority of the proposed approach can be verified by several experiments and
comparisons with some frequently used face recognition methods.","In this paper, a new approach has been developed to
deal with the problem of human face recognition on small
original dataset. The original small dataset is augmented
to be a large dataset by using the transformations of
the face images, such as flip, shift, scaling, and rotation.
Based on the remarkably augmented face dataset, the
face recognition can be effectively implemented via an
ingenious CNN. Several experiments are carried out to
verify the effectiveness of the augmented dataset, and
the superiority of the new approach can also be confirmed in comparison with some of the frequently used
face recognition methods.
Actually, the proposed strategy is an economic
method to augment the dataset, and can be applied to a
variety of fields related to data-based training and learning. Our future work will focus on the application of the
data augmentation approach on some more complex
problems, e.g. signal processing, image recognition, and
image-based fault detection","Human face recognition based on convolutional
neural network and augmented datasetFace recognition;
convolutional neural
network; augmented
dataset; CNNTo deal with the issue of human face recognition on small original dataset, a new approach combining convolutional neural network (CNN) with augmented dataset is developed in this paper. The
original small dataset is augmented to be a large dataset via several transformations of the face
images. Based on the augmented face image dataset, the feature of the faces can be effectively
extracted and higher face recognition accuracy can be achieved by using the ingenious CNN. The
effectiveness and superiority of the proposed approach can be verified by several experiments and
comparisons with some frequently used face recognition methods.In this paper, a new approach has been developed to
deal with the problem of human face recognition on small
original dataset. The original small dataset is augmented
to be a large dataset by using the transformations of
the face images, such as flip, shift, scaling, and rotation.
Based on the remarkably augmented face dataset, the
face recognition can be effectively implemented via an
ingenious CNN. Several experiments are carried out to
verify the effectiveness of the augmented dataset, and
the superiority of the new approach can also be confirmed in comparison with some of the frequently used
face recognition methods.
Actually, the proposed strategy is an economic
method to augment the dataset, and can be applied to a
variety of fields related to data-based training and learning. Our future work will focus on the application of the
data augmentation approach on some more complex
problems, e.g. signal processing, image recognition, and
image-based fault detection",Facial Emotion Recognition,"This paper presents a new approach to address the challenge of human face recognition on small original datasets. The approach combines a convolutional neural network (CNN) with an augmented dataset, which is created by transforming the face images through flipping, shifting, scaling, and rotation. The augmented dataset allows for effective feature extraction and achieves higher face recognition accuracy. The proposed approach is compared to other face recognition methods and shown to be superior. The paper suggests that this approach could be applied to other fields related to data-based training and learning, such as signal processing, image recognition, and image-based fault detectioon.",Object and Sentiment Recognition,,Object Recognition
348,"A Review of Face Recognition Technology
","Face recognition, image processing, neural network, artificial intelligence.","Face recognition technology is a biometric technology, which is based on the identification of
facial features of a person. People collect the face images, and the recognition equipment automatically
processes the images. The paper introduces the related researches of face recognition from different
perspectives. The paper describes the development stages and the related technologies of face recognition.
We introduce the research of face recognition for real conditions, and we introduce the general evaluation
standards and the general databases of face recognition. We give a forward-looking view of face recognition.
Face recognition has become the future development direction and has many potential application prospects","With the development of science and technology, the face
recognition technology has made great achievements, but
there is still room for its improvement in practical application.
In the future, there may be a special camera for face recognition, which can improve the image quality and solve the problems of image filtering, image reconstruction ,
denoising  etc. We can also use 3D technology
to supplement 2D images to solve some problems such as
rotation and occlusion.
","A Review of Face Recognition Technology
Face recognition, image processing, neural network, artificial intelligence.Face recognition technology is a biometric technology, which is based on the identification of
facial features of a person. People collect the face images, and the recognition equipment automatically
processes the images. The paper introduces the related researches of face recognition from different
perspectives. The paper describes the development stages and the related technologies of face recognition.
We introduce the research of face recognition for real conditions, and we introduce the general evaluation
standards and the general databases of face recognition. We give a forward-looking view of face recognition.
Face recognition has become the future development direction and has many potential application prospectsWith the development of science and technology, the face
recognition technology has made great achievements, but
there is still room for its improvement in practical application.
In the future, there may be a special camera for face recognition, which can improve the image quality and solve the problems of image filtering, image reconstruction ,
denoising  etc. We can also use 3D technology
to supplement 2D images to solve some problems such as
rotation and occlusion.
",Facial Emotion Recognition,"This paper provides an overview of face recognition technology, which is a biometric technology based on identifying facial features. The paper covers the development stages and related technologies of face recognition, research for real conditions, evaluation standards, and general databases. The paper also offers a forward-looking view of the technology, including potential application prospects and areas for improvement. The paper suggests that future improvements could include the use of a special camera for face recognition, 3D technology to supplement 2D images, and solutions to problems such as image filtering, reconstruction, denoising, rotation, and occlusion.",Object and Sentiment Recognition,,Object Recognition
349,"A Review Paper on Sign Language Recognition 
System For Deaf And Dumb People using Image 
Processing","Sign language identification, Hidden Morkov 
Model,Artificial Neural Network, Data glove, Leap motion 
controller, Kinectic Sensor.","Communications between deaf-mute and a normal 
person have always been a challenging task. This paper reviews
a different methods adopted to reduce barrier of communication 
by developing an assistive device for deaf-mute persons. The 
advancement in embedded systems, provides a space to design 
and develop a sign language translator system to assist the dumb 
people, there exist a number of assistant tools. The main 
objective is to develop a real time embedded device for 
physically challenged to aid their communication in effective 
means.","In this review paper, different techniques of sign language 
recognition are reviewed on the basis of sign acquiring 
methods and sign identification methods. For sign acquiring 
methods, vision based methods and for sign identification 
methods, artificial neuron network proves a strong 
candidature","A Review Paper on Sign Language Recognition 
System For Deaf And Dumb People using Image 
ProcessingSign language identification, Hidden Morkov 
Model,Artificial Neural Network, Data glove, Leap motion 
controller, Kinectic Sensor.Communications between deaf-mute and a normal 
person have always been a challenging task. This paper reviews
a different methods adopted to reduce barrier of communication 
by developing an assistive device for deaf-mute persons. The 
advancement in embedded systems, provides a space to design 
and develop a sign language translator system to assist the dumb 
people, there exist a number of assistant tools. The main 
objective is to develop a real time embedded device for 
physically challenged to aid their communication in effective 
means.In this review paper, different techniques of sign language 
recognition are reviewed on the basis of sign acquiring 
methods and sign identification methods. For sign acquiring 
methods, vision based methods and for sign identification 
methods, artificial neuron network proves a strong 
candidature",Person recognition,"In this review paper, different techniques of sign language 
recognition are reviewed on the basis of sign acquiring 
methods and sign identification methods. For sign acquiring 
methods, vision based methods and for sign identification 
methods, artificial neuron network proves a strong 
candidature",Object and Sentiment Recognition,,Medical Data Analysis
350,"Survey on Data Analysis in Social Media:
A Practical Application Aspect","social media; topic analysis; time series analysis; sentiment analysis; network analysis; disaster
management; bio-surveillance; business intelligence","Social media has more than three billion users sharing events, comments, and feelings throughout the
world. It serves as a critical information source with large volumes, high velocity, and a wide variety of data. The
previous studies on information spreading, relationship analyzing, and individual modeling, etc., have been heavily
conducted to explore the tremendous social and commercial values of social media data. This survey studies the
previous literature and the existing applications from a practical perspective. We outline a commonly used pipeline
in building social media-based applications and focus on discussing available analysis techniques, such as topic
analysis, time series analysis, sentiment analysis, and network analysis. After that, we present the impacts of such
applications in three different areas, including disaster management, healthcare, and business. Finally, we list
existing challenges and suggest promising future research directions in terms of data privacy, 5G wireless network,
and multilingual support","Different from the traditional information source, social
media provides user-generated content with large
volume, high velocity, and a wide variety. In this survey,
we conduct a comprehensive review of the previous
literature which discusses extracting values and insights
from social media data. We systematically compare
the existing applications and categorize those based on
their analysis techniques and impacting areas. From
practical perspectives, we outline a commonly used
pipeline in building such applications and focus on
discussing popular analysis techniques.
After that, we emphasize three fields in terms of their
impacts, which are healthcare, disaster management, and
business, in the hope to provide a broader exploration
of such applications. Finally, we point out the existing
challenges and outlook the future research directions in
the field of social media-based applications in terms of
data privacy, the 5G wireless network, and multilingual
support.","Survey on Data Analysis in Social Media:
A Practical Application Aspectsocial media; topic analysis; time series analysis; sentiment analysis; network analysis; disaster
management; bio-surveillance; business intelligenceSocial media has more than three billion users sharing events, comments, and feelings throughout the
world. It serves as a critical information source with large volumes, high velocity, and a wide variety of data. The
previous studies on information spreading, relationship analyzing, and individual modeling, etc., have been heavily
conducted to explore the tremendous social and commercial values of social media data. This survey studies the
previous literature and the existing applications from a practical perspective. We outline a commonly used pipeline
in building social media-based applications and focus on discussing available analysis techniques, such as topic
analysis, time series analysis, sentiment analysis, and network analysis. After that, we present the impacts of such
applications in three different areas, including disaster management, healthcare, and business. Finally, we list
existing challenges and suggest promising future research directions in terms of data privacy, 5G wireless network,
and multilingual supportDifferent from the traditional information source, social
media provides user-generated content with large
volume, high velocity, and a wide variety. In this survey,
we conduct a comprehensive review of the previous
literature which discusses extracting values and insights
from social media data. We systematically compare
the existing applications and categorize those based on
their analysis techniques and impacting areas. From
practical perspectives, we outline a commonly used
pipeline in building such applications and focus on
discussing popular analysis techniques.
After that, we emphasize three fields in terms of their
impacts, which are healthcare, disaster management, and
business, in the hope to provide a broader exploration
of such applications. Finally, we point out the existing
challenges and outlook the future research directions in
the field of social media-based applications in terms of
data privacy, the 5G wireless network, and multilingual
support.",Others,"This survey explores the practical applications of social media data, which serves as a critical information source with large volumes, high velocity, and a wide variety of data. The survey outlines a commonly used pipeline in building social media-based applications and discusses available analysis techniques such as topic analysis, time series analysis, sentiment analysis, and network analysis. The impacts of social media-based applications in disaster management, healthcare, and business are also presented. Existing challenges are listed, and future research directions are suggested in terms of data privacy, 5G wireless network, and multilingual support.",Deep Learning and Machine Learning,,Object Recognition
351,"A systematic review of social media-based sentiment analysis: Emerging
trends and challenges","Sentiment analysis
Social media
Challenges
Evaluation metrics
Systematic review","In the present information age, a wide and significant variety of social media platforms have been developed
and become an important part of modern life. Massive amounts of user-generated data sourced from various
social networking platforms also provide new insights for businesses and governments. However, it has become
difficult to extract useful information from the vast amount of information effectively. Sentiment analysis
provides an automated method of analyzing sentiment, emotion and opinion in written language to address
this issue. In the existing literature, a large number of scholars have worked on improving the performance
of various sentiment classifiers or applying them to various domains using data from social networking
platforms. This paper explores the challenges that scholars have encountered and other potential problems in
studying sentiment analysis in social media. It gives insights into the goals of the sentiment analysis task, the
implementation process, and the ways in which it is utilized in various application domains. It also provides
a comparison of different studies and highlights several challenges related to the datasets, text languages,
analysis methods and evaluation metrics. The paper contributes to the research on sentiment analysis and can
help practitioners select a suitable methodology for their applications","This paper provides a systematic review and analysis of the literature on sentiment analysis in social media. In addition to gaining a
comprehensive understanding of the application of sentiment analysis
to user-generated data, the paper identifies the challenges and issues in
the existing sentiment analysis research. Using the PRISMA framework,
the paper reports the objectives of sentiment analysis tasks, the general
implementation process, the algorithms adopted and how they are
used in different domains. Afterward, by comparing aspects of different
studies, the paper presents several challenges and issues related to
datasets, languages of the review text, analysis methods and evaluation
metrics in the existing literature.
Our work also has several limitations. As our aim is to investigate
the most recent studies on sentiment analysis in social media, the
publication period was set to be between 2018 to 2021. In addition,
this review paper only included the publications written in English,
which may lead to an inadequate understanding of the sentiment
analysis of non-English texts, for example, the development of lexicons of non-English texts, as well as methods for non-English data
extraction, cleaning, tokenization and analysis. In our future work,
the timeframe will be expanded to capture more literature varieties
and understand more sentiment analysis techniques. Researchers from
non-English speaking countries or multilingual scholars will be invited
to work together to understand the current state of global sentiment
analysis research better.","A systematic review of social media-based sentiment analysis: Emerging
trends and challengesSentiment analysis
Social media
Challenges
Evaluation metrics
Systematic reviewIn the present information age, a wide and significant variety of social media platforms have been developed
and become an important part of modern life. Massive amounts of user-generated data sourced from various
social networking platforms also provide new insights for businesses and governments. However, it has become
difficult to extract useful information from the vast amount of information effectively. Sentiment analysis
provides an automated method of analyzing sentiment, emotion and opinion in written language to address
this issue. In the existing literature, a large number of scholars have worked on improving the performance
of various sentiment classifiers or applying them to various domains using data from social networking
platforms. This paper explores the challenges that scholars have encountered and other potential problems in
studying sentiment analysis in social media. It gives insights into the goals of the sentiment analysis task, the
implementation process, and the ways in which it is utilized in various application domains. It also provides
a comparison of different studies and highlights several challenges related to the datasets, text languages,
analysis methods and evaluation metrics. The paper contributes to the research on sentiment analysis and can
help practitioners select a suitable methodology for their applicationsThis paper provides a systematic review and analysis of the literature on sentiment analysis in social media. In addition to gaining a
comprehensive understanding of the application of sentiment analysis
to user-generated data, the paper identifies the challenges and issues in
the existing sentiment analysis research. Using the PRISMA framework,
the paper reports the objectives of sentiment analysis tasks, the general
implementation process, the algorithms adopted and how they are
used in different domains. Afterward, by comparing aspects of different
studies, the paper presents several challenges and issues related to
datasets, languages of the review text, analysis methods and evaluation
metrics in the existing literature.
Our work also has several limitations. As our aim is to investigate
the most recent studies on sentiment analysis in social media, the
publication period was set to be between 2018 to 2021. In addition,
this review paper only included the publications written in English,
which may lead to an inadequate understanding of the sentiment
analysis of non-English texts, for example, the development of lexicons of non-English texts, as well as methods for non-English data
extraction, cleaning, tokenization and analysis. In our future work,
the timeframe will be expanded to capture more literature varieties
and understand more sentiment analysis techniques. Researchers from
non-English speaking countries or multilingual scholars will be invited
to work together to understand the current state of global sentiment
analysis research better.",sentiment analysis,"The paper explores the challenges and potential problems in studying sentiment analysis in social media. It provides insights into the sentiment analysis task's goals, implementation process, and its use in various application domains. The paper also compares different studies and highlights several challenges related to datasets, text languages, analysis methods, and evaluation metrics. The paper aims to help practitioners select a suitable methodology for their applications. However, the review is limited to studies published between 2018 and 2021 and only includes publications written in English, which may lead to an inadequate understanding of sentiment analysis in non-English texts. Future work will expand the timeframe and involve researchers from non-English speaking countries or multilingual scholars.",Object and Sentiment Recognition,,Sentiment Analysis
352,Analytics of social media data – State of characteristics and application ,"Keywords: 
Social media analytics 
Social media analysis 
Social media data 
Social media monitoring 
Social media listening 
Literature review","The spread and use of social networks provide a rich data source that can be used to answer a wide range of 
research questions from various disciplines. However, the nature of social media data poses a challenge to the 
analysis. The aim of this study is to provide an in-depth overview of the research that analyzes social media data 
since 2017. An extensive literature review based on 94 papers led to the findings that clear definitions are neither 
established nor commonly applied. Predominant research domains include marketing, hospitality and tourism, 
disaster management, and disruptive technology. The majority of analyzed social media data are taken from 
Twitter. Sentiment and content analysis are the current prevailing methods. Half of the studies include practical 
implications. Based on the literature review, clear definitions are provided, and future avenues for high-quality 
research are suggested","The large amount of user-generated data, the technological possibilities, and the knowhow to use this type of data are still growing. Clear 
and distinguishable definitions are rare, which is why this review proposes such definitions to move the research area forward. In social 
media, the timeliness of data is crucial. Studies using perished platforms 
such as Google+ or Foursquare, which did not penetrate the European 
market, cannot be replicated or validated. Furthermore, the replication 
of studies is only possible if researchers provide a clear description of 
their processes that are indeed repeatable. Moreover, even though it was 
possible to identify the area of interest of the studies, not all authors 
have a clear idea of which purpose they have developed a new method 
for or which framework to use on social media data. Future research is 
needed to advance the still young field of social media analytics. Future 
research avenues also include finding suitable procedures that allow a 
transparent process of identifying, collecting, processing, and analyzing 
social media data. Additionally, we suggest a stronger focus on the 
ethical and legal dimensions of using social media data. 
To conclude, several limitations in this literature review must be 
addressed. Although the four chosen major academic databases (Taylor 
& Francis Journals, Elsevier, ProQuest, Web of Science) surely have a 
very high level of quality, and other academic databases such as EconLit 
or Google Scholar could have been used as well. The material discussed 
in this study was collected from January 2017 to July 2020. Therefore, 
the results only give a picture of this specific period, and it is expected 
that publications that analyze social media data will grow considerably. 
Finally, the study does not exhaust the subject matter, nor does it present 
an ultimate list of publications that are currently available.","Analytics of social media data – State of characteristics and application Keywords: 
Social media analytics 
Social media analysis 
Social media data 
Social media monitoring 
Social media listening 
Literature reviewThe spread and use of social networks provide a rich data source that can be used to answer a wide range of 
research questions from various disciplines. However, the nature of social media data poses a challenge to the 
analysis. The aim of this study is to provide an in-depth overview of the research that analyzes social media data 
since 2017. An extensive literature review based on 94 papers led to the findings that clear definitions are neither 
established nor commonly applied. Predominant research domains include marketing, hospitality and tourism, 
disaster management, and disruptive technology. The majority of analyzed social media data are taken from 
Twitter. Sentiment and content analysis are the current prevailing methods. Half of the studies include practical 
implications. Based on the literature review, clear definitions are provided, and future avenues for high-quality 
research are suggestedThe large amount of user-generated data, the technological possibilities, and the knowhow to use this type of data are still growing. Clear 
and distinguishable definitions are rare, which is why this review proposes such definitions to move the research area forward. In social 
media, the timeliness of data is crucial. Studies using perished platforms 
such as Google+ or Foursquare, which did not penetrate the European 
market, cannot be replicated or validated. Furthermore, the replication 
of studies is only possible if researchers provide a clear description of 
their processes that are indeed repeatable. Moreover, even though it was 
possible to identify the area of interest of the studies, not all authors 
have a clear idea of which purpose they have developed a new method 
for or which framework to use on social media data. Future research is 
needed to advance the still young field of social media analytics. Future 
research avenues also include finding suitable procedures that allow a 
transparent process of identifying, collecting, processing, and analyzing 
social media data. Additionally, we suggest a stronger focus on the 
ethical and legal dimensions of using social media data. 
To conclude, several limitations in this literature review must be 
addressed. Although the four chosen major academic databases (Taylor 
& Francis Journals, Elsevier, ProQuest, Web of Science) surely have a 
very high level of quality, and other academic databases such as EconLit 
or Google Scholar could have been used as well. The material discussed 
in this study was collected from January 2017 to July 2020. Therefore, 
the results only give a picture of this specific period, and it is expected 
that publications that analyze social media data will grow considerably. 
Finally, the study does not exhaust the subject matter, nor does it present 
an ultimate list of publications that are currently available.",Others,"This study aims to provide an overview of research analyzing social media data since 2017. The study identifies a lack of clear definitions in the field and identifies predominant research domains, including marketing, hospitality, and tourism. Twitter is the most commonly analyzed platform, and sentiment and content analysis are the prevailing methods. The study suggests future research avenues, including finding suitable procedures for transparently identifying, collecting, processing, and analyzing social media data and focusing on the ethical and legal dimensions of using such data. Limitations include a limited time frame and a focus on only four academic databases.",Deep Learning and Machine Learning,,Deep Learning and Machine Learning
353,"SENTIMENT ANALYSIS OF SOCIAL MEDIA 
DATA: PERSPECTIVES AND AVENUES
","Sentiment Analysis, Social Media, Technology, Perspectives, Avenues","Because of enhancement of technology and its growth, there is a huge volume of data present in the web for 
internet users and a lot of data is generated too. Internet has become a platform for online learning, exchanging
ideas and sharing opinions. Social networking sites like Twitter, Facebook, Google+ are rapidly gaining popularity
as they allow people to share and express their views about topics, have discussion with different communities, or 
post messages across the world. There hasn’t been lot of work in the field of sentiment analysis of social media. 
This survey focuses mainly on sentiment analysis of social media which is helpful to analyze the information in
the comments or tweets where opinions are highly unstructured, heterogeneous and are either positive or negative, 
or neutral in some cases. In this paper, it is provided a survey and comparative analyses of existing techniques for 
opinion mining","In this paper, it is provided a survey and comparative study of fundamental techniques for opinion mining including
machine learning and lexicon-based approaches, together with cross domain and cross-lingual methods and some
evaluation metrics. Research results show that machine learning methods, such as SVM and naive Bayes have the 
highest accuracy and can be regarded as the baseline learning methods, while lexicon-based methods are very 
effective in some cases, which require little effort in human-labeled document","SENTIMENT ANALYSIS OF SOCIAL MEDIA 
DATA: PERSPECTIVES AND AVENUES
Sentiment Analysis, Social Media, Technology, Perspectives, AvenuesBecause of enhancement of technology and its growth, there is a huge volume of data present in the web for 
internet users and a lot of data is generated too. Internet has become a platform for online learning, exchanging
ideas and sharing opinions. Social networking sites like Twitter, Facebook, Google+ are rapidly gaining popularity
as they allow people to share and express their views about topics, have discussion with different communities, or 
post messages across the world. There hasn’t been lot of work in the field of sentiment analysis of social media. 
This survey focuses mainly on sentiment analysis of social media which is helpful to analyze the information in
the comments or tweets where opinions are highly unstructured, heterogeneous and are either positive or negative, 
or neutral in some cases. In this paper, it is provided a survey and comparative analyses of existing techniques for 
opinion miningIn this paper, it is provided a survey and comparative study of fundamental techniques for opinion mining including
machine learning and lexicon-based approaches, together with cross domain and cross-lingual methods and some
evaluation metrics. Research results show that machine learning methods, such as SVM and naive Bayes have the 
highest accuracy and can be regarded as the baseline learning methods, while lexicon-based methods are very 
effective in some cases, which require little effort in human-labeled document",sentiment analysis,"The paper discusses the growth of social media and the need for sentiment analysis due to the unstructured nature of opinions expressed online. The survey provides a comparative analysis of existing techniques for opinion mining, including machine learning and lexicon-based approaches, cross-domain and cross-lingual methods, and evaluation metrics. The study finds that machine learning methods such as SVM and naive Bayes have the highest accuracy and can be considered as baseline learning methods, while lexicon-based methods are effective in some cases requiring minimal human-labeled documents.",Object and Sentiment Recognition,,Object Recognition
354,"Towards large-scale case-finding: training and validation of 
residual networks for detection of chronic obstructive 
pulmonary disease using low-dose CT","Residual networks,Neural Networks, Medical Data,COPD,ECLIPSE,Computed Tomography.","Accurate detection of chronic obstructive pulmonary 
disease (COPD) is critical to the timely initiation of 
therapies that reduce the risk of future exacerbations and 
hospitalisations, delay disease progression, and improve 
the overall prognosis of patients.1,2 However, a large body 
of literature indicates that a considerable proportion of 
COPD patients are undiagnosed.3,4 For instance, 
Lamprecht and colleagues3
 pooled data from more than 
30 800 participants (from 44 countries on six continents) 
and found that 81·4% of individuals with chronic airflow 
limitation did not have a clinical diagnosis of COPD 
before spirometric screening. Consequently, costeffective strategies for case-finding are urgently needed.
With the growing use of CT imaging for pulmonary 
nodule assessment, as well as for diagnosis and screening 
of lung cancer in smokers,5,6 there is an opportunity to 
use these scans to identify those with COPD, with 
subsequent confirmation using spirometry. We 
hypothesised that deep learning of chest CT data using neural networks could be a valuable assistive tool for COPD case-finding in this setting. By presenting a 
sufficiently large training data set of CT scans acquired 
from a pool of individuals of known disease categories 
(eg, COPD or non-COPD), deep neural networks have the potential to recognise, without any human guidance, 
recurring but subtle image patterns of various spatial 
scales that can best discriminate between the categories  Accordingly, we posited that a specialisedtype of neural network, known as a residual network,8could be deployed in large-scale clinical settings for 
computerised COPD detection. We developed three 
variants of such a network using training data collected 
from the PanCan study,5
 which evaluated current 
smokers and ex-smokers at high risk of lung cancer with 
screening of low-dose thoracic CT scans. We subsequently 
performed external validation using data collected from 
the ECLIPSE study,9
 which largely evaluated clinically 
","our results support the hypothesis that a 
deep-learning approach based on residual networks 
could be used for case-finding of COPD subjects from 
Articles
e266 www.thelancet.com/digital-health Vol 2 May 2020
CT scans, with accuracy of more than 88%. This accuracy 
is within the clinically acceptable range, especially 
considering the variation inherent in spirometry tests 
alone. We believe that this approach is a powerful 
technique to identify patients within the general 
population that have not been previously identified as 
having COPD. Once deployed in a clinical setting, our 
proposed pipeline could operate in the background such 
that scans identified by the pipeline as COPD would be 
flagged. Activation maps highlighting contributive voxels 
could be computed a priori and used in conjunction with 
the original CT scan when radiologists proceed with 
visual evaluation as part of current clinical practice. Our 
next step is to investigate whether our approach could 
also be used to detect changes in the lung due to 
pathogenesis or to evaluate novel therapies for this lung 
disease. Eventually, our overall approach could provide 
useful indications to radiologists and clinicians about 
clinically relevant findings that could improve the 
diagnosis and follow-up of specific patients.
","Towards large-scale case-finding: training and validation of 
residual networks for detection of chronic obstructive 
pulmonary disease using low-dose CTResidual networks,Neural Networks, Medical Data,COPD,ECLIPSE,Computed Tomography.Accurate detection of chronic obstructive pulmonary 
disease (COPD) is critical to the timely initiation of 
therapies that reduce the risk of future exacerbations and 
hospitalisations, delay disease progression, and improve 
the overall prognosis of patients.1,2 However, a large body 
of literature indicates that a considerable proportion of 
COPD patients are undiagnosed.3,4 For instance, 
Lamprecht and colleagues3
 pooled data from more than 
30 800 participants (from 44 countries on six continents) 
and found that 81·4% of individuals with chronic airflow 
limitation did not have a clinical diagnosis of COPD 
before spirometric screening. Consequently, costeffective strategies for case-finding are urgently needed.
With the growing use of CT imaging for pulmonary 
nodule assessment, as well as for diagnosis and screening 
of lung cancer in smokers,5,6 there is an opportunity to 
use these scans to identify those with COPD, with 
subsequent confirmation using spirometry. We 
hypothesised that deep learning of chest CT data using neural networks could be a valuable assistive tool for COPD case-finding in this setting. By presenting a 
sufficiently large training data set of CT scans acquired 
from a pool of individuals of known disease categories 
(eg, COPD or non-COPD), deep neural networks have the potential to recognise, without any human guidance, 
recurring but subtle image patterns of various spatial 
scales that can best discriminate between the categories  Accordingly, we posited that a specialisedtype of neural network, known as a residual network,8could be deployed in large-scale clinical settings for 
computerised COPD detection. We developed three 
variants of such a network using training data collected 
from the PanCan study,5
 which evaluated current 
smokers and ex-smokers at high risk of lung cancer with 
screening of low-dose thoracic CT scans. We subsequently 
performed external validation using data collected from 
the ECLIPSE study,9
 which largely evaluated clinically 
our results support the hypothesis that a 
deep-learning approach based on residual networks 
could be used for case-finding of COPD subjects from 
Articles
e266 www.thelancet.com/digital-health Vol 2 May 2020
CT scans, with accuracy of more than 88%. This accuracy 
is within the clinically acceptable range, especially 
considering the variation inherent in spirometry tests 
alone. We believe that this approach is a powerful 
technique to identify patients within the general 
population that have not been previously identified as 
having COPD. Once deployed in a clinical setting, our 
proposed pipeline could operate in the background such 
that scans identified by the pipeline as COPD would be 
flagged. Activation maps highlighting contributive voxels 
could be computed a priori and used in conjunction with 
the original CT scan when radiologists proceed with 
visual evaluation as part of current clinical practice. Our 
next step is to investigate whether our approach could 
also be used to detect changes in the lung due to 
pathogenesis or to evaluate novel therapies for this lung 
disease. Eventually, our overall approach could provide 
useful indications to radiologists and clinicians about 
clinically relevant findings that could improve the 
diagnosis and follow-up of specific patients.
",Medical Data Analysis,"The article discusses the potential use of deep-learning approaches based on residual networks for case-finding of chronic obstructive pulmonary disease (COPD) from CT scans. The authors present results indicating an accuracy of more than 88% for identifying COPD subjects from CT scans, which they consider within the clinically acceptable range. They suggest that this approach could be a powerful technique to identify patients with COPD within the general population who have not been previously diagnosed. The proposed pipeline could operate in the background and flag scans identified by the pipeline as COPD. The authors suggest that their overall approach could provide useful indications to radiologists and clinicians about clinically relevant findings that could improve the diagnosis and follow-up of specific patients. The article highlights the urgent need for cost-effective strategies for case-finding of COPD, given the large proportion of undiagnosed patients. With the growing use of CT imaging for pulmonary nodule assessment and lung cancer screening, the authors suggest that deep learning of chest CT data using neural networks could be a valuable assistive tool for COPD case-finding in this setting.",Medical Data Analysis,,Deep Learning and Machine Learning
355,"Emotion recognition using deep learning approach from audio–visual
emotional big data.","CNN, Facial Emotion Recognition,Support Vector Machine.","This paper proposes an emotion recognition system using a deep learning approach from emotional Big Data. The Big Data comprises of speech and video. In the
proposed system, a speech signal is first processed in the frequency domain to obtain a Mel-spectrogram, which can be treated as an image. Then this Mel-spectrogram
is fed to a convolutional neural network (CNN). For video signals, some representative frames from a video segment are extracted and fed to the CNN. The outputs
of the two CNNs are fused using two consecutive extreme learning machines (ELMs). The output of the fusion is given to a support vector machine (SVM) for final
classification of the emotions. The proposed system is evaluated using two audio–visual emotional databases, one of which is Big Data. Experimental results confirm
the effectiveness of the proposed system involving the CNNs and the ELMs.","An audio-visual emotion recognition system was proposed. The 2D
CNN for the speech signal and the 3D CNN for the video signal were
used. Different fusion strategies including the proposed ELM-based fusion were investigated. The proposed system was evaluated using Big
Data of emotion and the eNTERFACE database. In both the databases,
the proposed system outperformed other similar systems. The ELMbased fusion performed better than the classifiers’ combination. One of
the reasons for this good performance is that the ELMs add a high degree of non-linearity in the features’ fusion. The proposed system can be
extended to be a noise-robust system by using a sophisticated processing of speech signals instead of using the conventional MFCC features,
and by using some noise-removal techniques in key frames of video signals. In case of the failure to capture either speech or face, an intelligent
weighting scheme in the fusion can be adopted to the proposed system
for a seamless execution.
The proposed system can be integrated in to any emotion-aware
intelligent systems for a better service to the users or customers
Using edge technology, the weights of the deep network
parameters can easily be stored for a fast processing 
In a future study, we will evaluate the proposed system in an edgeand-cloud computing framework. We also want to investigate other deep
architectures to improve the performance of the system using the eNTERFACE database and emotion in the wild challenge databases","Emotion recognition using deep learning approach from audio–visual
emotional big data.CNN, Facial Emotion Recognition,Support Vector Machine.This paper proposes an emotion recognition system using a deep learning approach from emotional Big Data. The Big Data comprises of speech and video. In the
proposed system, a speech signal is first processed in the frequency domain to obtain a Mel-spectrogram, which can be treated as an image. Then this Mel-spectrogram
is fed to a convolutional neural network (CNN). For video signals, some representative frames from a video segment are extracted and fed to the CNN. The outputs
of the two CNNs are fused using two consecutive extreme learning machines (ELMs). The output of the fusion is given to a support vector machine (SVM) for final
classification of the emotions. The proposed system is evaluated using two audio–visual emotional databases, one of which is Big Data. Experimental results confirm
the effectiveness of the proposed system involving the CNNs and the ELMs.An audio-visual emotion recognition system was proposed. The 2D
CNN for the speech signal and the 3D CNN for the video signal were
used. Different fusion strategies including the proposed ELM-based fusion were investigated. The proposed system was evaluated using Big
Data of emotion and the eNTERFACE database. In both the databases,
the proposed system outperformed other similar systems. The ELMbased fusion performed better than the classifiers’ combination. One of
the reasons for this good performance is that the ELMs add a high degree of non-linearity in the features’ fusion. The proposed system can be
extended to be a noise-robust system by using a sophisticated processing of speech signals instead of using the conventional MFCC features,
and by using some noise-removal techniques in key frames of video signals. In case of the failure to capture either speech or face, an intelligent
weighting scheme in the fusion can be adopted to the proposed system
for a seamless execution.
The proposed system can be integrated in to any emotion-aware
intelligent systems for a better service to the users or customers
Using edge technology, the weights of the deep network
parameters can easily be stored for a fast processing 
In a future study, we will evaluate the proposed system in an edgeand-cloud computing framework. We also want to investigate other deep
architectures to improve the performance of the system using the eNTERFACE database and emotion in the wild challenge databases",Emotion detection using normal face,"The paper proposes an emotion recognition system that uses a deep learning approach from emotional Big Data consisting of speech and video. The system involves processing the speech signal to obtain a Mel-spectrogram and treating it as an image, which is fed to a convolutional neural network (CNN), while for video signals, some representative frames are extracted and fed to a 3D CNN. The outputs of the two CNNs are fused using two consecutive extreme learning machines (ELMs), and the output of the fusion is given to a support vector machine (SVM) for final classification of emotions. The proposed system is evaluated using two audio–visual emotional databases, one of which is Big Data. The experimental results show that the proposed system outperforms other similar systems, with the ELM-based fusion performing better than the classifiers’ combination. The proposed system can be extended to be a noise-robust system and can be integrated into any emotion-aware intelligent systems for better service to users or customers. Future work includes evaluating the proposed system in an edge-and-cloud computing framework and investigating other deep architectures to improve its performance.",Object and Sentiment Recognition,,Sentiment Analysis
356,Extended deep neural network for facial emotion recognition.,"Facial emotion recognition,
Fully convolution network.","Humans use facial expressions to show their emotional states. However, facial expression recognition has
remained a challenging and interesting problem in computer vision. In this paper we present our approach which is the extension of our previous work for facial emotion recognition [1]. The aim of this
work is to classify each image into one of six facial emotion classes. The proposed model is based on
single Deep Convolutional Neural Networks (DNNs), which contain convolution layers and deep residual
blocks. In the proposed model, firstly the image label to all faces has been set for the training. Secondly,
the images go through proposed DNN model. This model trained on two datasets Extended Cohn–Kanade
(CK+) and Japanese Female Facial Expression (JAFFE) Dataset. The overall results show that, the proposed
DNN model can outperform the recent state-of-the-art approaches for emotion recognition. Even the proposed model has accuracy improvement in comparison with our previous model.","In this paper, we present a fully deep neural network model for
facial emotion recognition and the model has been tested on two
public datasets to assess the performance of the proposed model.
Particularly, it has been found that the combination of FCN and
residual block cloud considerably improve the overall result, which
verified the efficiency of the proposed model.","Extended deep neural network for facial emotion recognition.Facial emotion recognition,
Fully convolution network.Humans use facial expressions to show their emotional states. However, facial expression recognition has
remained a challenging and interesting problem in computer vision. In this paper we present our approach which is the extension of our previous work for facial emotion recognition [1]. The aim of this
work is to classify each image into one of six facial emotion classes. The proposed model is based on
single Deep Convolutional Neural Networks (DNNs), which contain convolution layers and deep residual
blocks. In the proposed model, firstly the image label to all faces has been set for the training. Secondly,
the images go through proposed DNN model. This model trained on two datasets Extended Cohn–Kanade
(CK+) and Japanese Female Facial Expression (JAFFE) Dataset. The overall results show that, the proposed
DNN model can outperform the recent state-of-the-art approaches for emotion recognition. Even the proposed model has accuracy improvement in comparison with our previous model.In this paper, we present a fully deep neural network model for
facial emotion recognition and the model has been tested on two
public datasets to assess the performance of the proposed model.
Particularly, it has been found that the combination of FCN and
residual block cloud considerably improve the overall result, which
verified the efficiency of the proposed model.",Emotion detection using normal face,"This paper presents an approach for facial emotion recognition using a deep convolutional neural network model. The model is an extension of the authors' previous work and is trained on two datasets, Extended Cohn-Kanade and Japanese Female Facial Expression. The model outperforms recent state-of-the-art approaches for emotion recognition and shows improved accuracy compared to the authors' previous model. The combination of fully connected networks and residual blocks is found to improve the overall performance of the model. Overall, the proposed deep neural network model is effective for facial emotion recognition and has been tested on public datasets.",Object and Sentiment Recognition,,Object Recognition
357,"A probability and integrated learning based classification algorithm for
high-level human emotion recognition problems
.","Emotion analysis problem,
Classification probability,
Integrated learning.","In this paper, a probability and integrated learning (PIL) based classification algorithm is proposed for
solving high-level human emotion recognition problems. Firstly, by simulating human thinking mode
and construction, a novel topology of integrated learning is proposed to obtain the essential material
basis for analyzing the complex human emotions. Secondly, classification algorithm based on PIL is presented to adapt the emotion classification fuzziness caused by the emotional uncertainty, which is realized by calculating the confidence interval of the classification probability. This paper also presented
three new analyses methods based on classification probability including the emotional sensitivity, emotional decision preference and emotional tube. Our study expects that the proposed method could be
used in the affective computing for video, and may play a reference role in artificial emotion established
for robot with a natural and humanized way.","Classification probability algorithm based on integrated learning (PIL) is probably applicable to the problem similar to emotion
classification which is coexistence of fuzziness and complexity.
However, it needs to be verified through massive data or more
new problems to its generalization ability. In the above mentioned
experimental process, we found some interesting phenomena. For
instance, compared with the results tagged by the testers and
results obtained by the classifiers, it could be found that sometesters’ tagged sequence is very similar to some specific classifiers’
judgment. According to the label sequence of each tester, we added
up the output probability of each classifier and figured out different sensitivity distribution for the different testers. Finally, our
algorithm performed reasonably well and described the emotional
tube for music videos. However, there is significant scope in
extending the present study in context of methodology and more
complex applications. There are several problems and unanswered
questions which are worth to be explored and discussed in the
future. (1) The factors for the preference and emotional sensitivity;
(2) PIL should expend to valence-arousal space; (3) the optimization of parameters. Further, the sophisticated algorithms based
on advanced genetic programming principles [54–57] considering
uncertainties for improving classification accuracy can be applied.","A probability and integrated learning based classification algorithm for
high-level human emotion recognition problems
.Emotion analysis problem,
Classification probability,
Integrated learning.In this paper, a probability and integrated learning (PIL) based classification algorithm is proposed for
solving high-level human emotion recognition problems. Firstly, by simulating human thinking mode
and construction, a novel topology of integrated learning is proposed to obtain the essential material
basis for analyzing the complex human emotions. Secondly, classification algorithm based on PIL is presented to adapt the emotion classification fuzziness caused by the emotional uncertainty, which is realized by calculating the confidence interval of the classification probability. This paper also presented
three new analyses methods based on classification probability including the emotional sensitivity, emotional decision preference and emotional tube. Our study expects that the proposed method could be
used in the affective computing for video, and may play a reference role in artificial emotion established
for robot with a natural and humanized way.Classification probability algorithm based on integrated learning (PIL) is probably applicable to the problem similar to emotion
classification which is coexistence of fuzziness and complexity.
However, it needs to be verified through massive data or more
new problems to its generalization ability. In the above mentioned
experimental process, we found some interesting phenomena. For
instance, compared with the results tagged by the testers and
results obtained by the classifiers, it could be found that sometesters’ tagged sequence is very similar to some specific classifiers’
judgment. According to the label sequence of each tester, we added
up the output probability of each classifier and figured out different sensitivity distribution for the different testers. Finally, our
algorithm performed reasonably well and described the emotional
tube for music videos. However, there is significant scope in
extending the present study in context of methodology and more
complex applications. There are several problems and unanswered
questions which are worth to be explored and discussed in the
future. (1) The factors for the preference and emotional sensitivity;
(2) PIL should expend to valence-arousal space; (3) the optimization of parameters. Further, the sophisticated algorithms based
on advanced genetic programming principles [54–57] considering
uncertainties for improving classification accuracy can be applied.",Emotion detection using normal face,"The paper proposes a classification algorithm called probability and integrated learning (PIL) for recognizing human emotions in complex situations with fuzziness. The algorithm is based on a novel topology of integrated learning, and it adapts to emotional uncertainty by calculating the confidence interval of the classification probability. The paper also presents three new analyses methods based on classification probability, including emotional sensitivity, emotional decision preference, and emotional tube. The proposed method has potential applications in affective computing for videos and may be useful in artificial emotion for robots. The study suggests exploring factors for preference and emotional sensitivity, expanding PIL to valence-arousal space, and optimizing parameters in future research. The paper also mentions the possibility of applying sophisticated algorithms based on advanced genetic programming principles to improve classification accuracy.",Object and Sentiment Recognition,,Sentiment Analysis
358,"Video facial emotion recognition based on local enhanced motion history
image and CNN-CTSLSTM networks.","Video emotion recognition,
Motion history image,
LSTM,
Facial landmarks.","This paper focuses on the issue of recognition of facial emotion expressions in video sequences and proposes an integrated framework of two networks: a local network, and a global network, which are based
on local enhanced motion history image (LEMHI) and CNN-LSTM cascaded networks respectively. In the
local network, frames from unrecognized video are aggregated into a single frame by a novel method,
LEMHI. This approach improves MHI by using detected human facial landmarks as attention areas to
boost local value in difference image calculation, so that the action of crucial facial unit can be captured
effectively. Then this single frame will be fed into a CNN network for prediction. On the other hand, an
improved CNN-LSTM model is used as a global feature extractor and classifier for video facial emotion
recognition in the global network. Finally, a random search weighted summation strategy is conducted
as late-fusion fashion to final predication. Our work also offers an insight into networks and visible feature maps from each layer of CNN to decipher which portions of the face influence the networks’ predictions. Experiments on the AFEW, CK+ and MMI datasets using subject-independent validation scheme
demonstrate that the integrated framework of two networks achieves a better performance than using
individual network separately. Compared with state-of-the-arts methods, the proposed framework
demonstrates a superior performance","This paper presents a facial expression recognition framework
using LEMHI-CNN and CNN-RNN. The integrated framework incorporates facial landmarks to enable attention-aware facial motion
capturing and utilize neural networks to extract spatial-temporal
features and classify them, which achieves better performance
than most of the state-of-the-art methods on CK+, MMI and AFEW
dataset. Our main contributions are threefold. Firstly, we proposed
an attention-aware facial motion features based on MHI. Secondly,
we introduced temporal segment LSTM to video emotion recognition and improve it. Thirdly, we integrated two models with late
fusion based on random weight search.
Although the accuracy we achieved on CK+ and MMI dataset is
comparably good, the accuracy on AFEW dataset is still unsatisfactory. That is to say, current methods can be employed in real world.
However, how to improve the performance on wild expression
dataset, such as AFEW, needs to be further explored.","Video facial emotion recognition based on local enhanced motion history
image and CNN-CTSLSTM networks.Video emotion recognition,
Motion history image,
LSTM,
Facial landmarks.This paper focuses on the issue of recognition of facial emotion expressions in video sequences and proposes an integrated framework of two networks: a local network, and a global network, which are based
on local enhanced motion history image (LEMHI) and CNN-LSTM cascaded networks respectively. In the
local network, frames from unrecognized video are aggregated into a single frame by a novel method,
LEMHI. This approach improves MHI by using detected human facial landmarks as attention areas to
boost local value in difference image calculation, so that the action of crucial facial unit can be captured
effectively. Then this single frame will be fed into a CNN network for prediction. On the other hand, an
improved CNN-LSTM model is used as a global feature extractor and classifier for video facial emotion
recognition in the global network. Finally, a random search weighted summation strategy is conducted
as late-fusion fashion to final predication. Our work also offers an insight into networks and visible feature maps from each layer of CNN to decipher which portions of the face influence the networks’ predictions. Experiments on the AFEW, CK+ and MMI datasets using subject-independent validation scheme
demonstrate that the integrated framework of two networks achieves a better performance than using
individual network separately. Compared with state-of-the-arts methods, the proposed framework
demonstrates a superior performanceThis paper presents a facial expression recognition framework
using LEMHI-CNN and CNN-RNN. The integrated framework incorporates facial landmarks to enable attention-aware facial motion
capturing and utilize neural networks to extract spatial-temporal
features and classify them, which achieves better performance
than most of the state-of-the-art methods on CK+, MMI and AFEW
dataset. Our main contributions are threefold. Firstly, we proposed
an attention-aware facial motion features based on MHI. Secondly,
we introduced temporal segment LSTM to video emotion recognition and improve it. Thirdly, we integrated two models with late
fusion based on random weight search.
Although the accuracy we achieved on CK+ and MMI dataset is
comparably good, the accuracy on AFEW dataset is still unsatisfactory. That is to say, current methods can be employed in real world.
However, how to improve the performance on wild expression
dataset, such as AFEW, needs to be further explored.Non-COPD or | coro? “Tran model using random subsets N Evaluate trained model using all ECLIPSE samples",Emotion detection using normal face,"This paper proposes a framework for facial expression recognition in video sequences, which combines two networks: a local network and a global network. The local network uses a novel approach called LEMHI to aggregate frames into a single frame, which is then fed into a CNN network for prediction. The global network uses an improved CNN-LSTM model for feature extraction and classification. The two networks are integrated using a random search weighted summation strategy. Experiments on the AFEW, CK+, and MMI datasets demonstrate that the integrated framework achieves better performance than using individual networks separately and outperforms state-of-the-art methods. However, the accuracy on the AFEW dataset is still unsatisfactory, and further research is needed to improve performance on wild expression datasets.",Object and Sentiment Recognition,Non-COPD or | coro? “Tran model using random subsets N Evaluate trained model using all ECLIPSE samples,Medical Data Analysis
359,"Facial expression recognition with Convolutional Neural Networks:
Coping with few data and the training sample order.",Facial expression recognition Convolutional Neural Networks Computer vision Machine learning Expression specific features,"Facial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromarketing and sociable robots. The recognition of facial
expressions is not an easy problem for machine learning methods, since people can vary significantly in
the way they show their expressions. Even images of the same person in the same facial expression can
vary in brightness, background and pose, and these variations are emphasized if considering different
subjects (because of variations in shape, ethnicity among others). Although facial expression recognition
is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while
training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging
problem in computer vision. In this work, we propose a simple solution for facial expression recognition
that uses a combination of Convolutional Neural Network and specific image pre-processing steps.
Convolutional Neural Networks achieve better accuracy with big data. However, there are no publicly
available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific
features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CKþ, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the
accuracy rate is presented. The proposed method: achieves competitive results when compared with
other facial expression recognition methods – 96.76% of accuracy in the CKþ database – it is fast to train,
and it allows for real time facial expression recognition with standard computers.","Based on the paper, the authors proposed a facial expression recognition system that combines Convolutional Neural Network (CNN) and specific image pre-processing steps to improve accuracy. The CNN is used to learn the set of features that best models the desired classification, which decreases the need for hand-coded features. However, the CNN requires a large amount of data for training, which is a constraint of deep architectures.

To address this problem, the authors applied pre-processing operations to the images to decrease variations between images and to select a subset of the features to be learned. This reduces the need for a large amount of data. The experiments showed that the combination of normalization procedures significantly improved the method's accuracy.

The proposed approach achieved competitive results compared to recent methods in the literature that use the same facial expression database and experimental methodology. It also presents a simpler solution, takes less time to train, and performs recognition in real-time. The cross-database experiments showed that the proposed approach works in unknown environments, where the testing image acquisition conditions and subjects vary from the training images, but there is still room for improvement.

Preliminary experiments were performed with deeper architectures, trained with a large amount of data. These experiments used a deep CNN composed of 38 layers and trained with about 982,800 images from 2662 subjects to recognize faces. The results achieved were promising, indicating that a deep learning approach can be a better way to produce a discriminative model for facial expression recognition, allowing it to work in uncontrolled scenarios.","Facial expression recognition with Convolutional Neural Networks:
Coping with few data and the training sample order.Facial expression recognition Convolutional Neural Networks Computer vision Machine learning Expression specific featuresFacial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromarketing and sociable robots. The recognition of facial
expressions is not an easy problem for machine learning methods, since people can vary significantly in
the way they show their expressions. Even images of the same person in the same facial expression can
vary in brightness, background and pose, and these variations are emphasized if considering different
subjects (because of variations in shape, ethnicity among others). Although facial expression recognition
is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while
training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging
problem in computer vision. In this work, we propose a simple solution for facial expression recognition
that uses a combination of Convolutional Neural Network and specific image pre-processing steps.
Convolutional Neural Networks achieve better accuracy with big data. However, there are no publicly
available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific
features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CKþ, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the
accuracy rate is presented. The proposed method: achieves competitive results when compared with
other facial expression recognition methods – 96.76% of accuracy in the CKþ database – it is fast to train,
and it allows for real time facial expression recognition with standard computers.Based on the paper, the authors proposed a facial expression recognition system that combines Convolutional Neural Network (CNN) and specific image pre-processing steps to improve accuracy. The CNN is used to learn the set of features that best models the desired classification, which decreases the need for hand-coded features. However, the CNN requires a large amount of data for training, which is a constraint of deep architectures.

To address this problem, the authors applied pre-processing operations to the images to decrease variations between images and to select a subset of the features to be learned. This reduces the need for a large amount of data. The experiments showed that the combination of normalization procedures significantly improved the method's accuracy.

The proposed approach achieved competitive results compared to recent methods in the literature that use the same facial expression database and experimental methodology. It also presents a simpler solution, takes less time to train, and performs recognition in real-time. The cross-database experiments showed that the proposed approach works in unknown environments, where the testing image acquisition conditions and subjects vary from the training images, but there is still room for improvement.

Preliminary experiments were performed with deeper architectures, trained with a large amount of data. These experiments used a deep CNN composed of 38 layers and trained with about 982,800 images from 2662 subjects to recognize faces. The results achieved were promising, indicating that a deep learning approach can be a better way to produce a discriminative model for facial expression recognition, allowing it to work in uncontrolled scenarios.Frame sequence in a video 2i+1 frames —_| Calculate Calculate Calculate Histogram | “| Histogram |” | Histogram Choose the frame with the least change | Selected frame<li Mel-spectrogram Speech—> Preprocessing -—> 2D CNN > >| ""| 5 3 svM Recognized ron, Frame ~ Video —>| Selection and 3D CNN * | Processing Normalized Gray Images, LBP and TX IDP Images Fig. 1. An overall block diagram of the proposed emotion recognition system. ‘Speech Video —> Eee Fourier a Log | sMelspectrogram, _, To 2D Windowing Transform delta, double delta CNN (a) Speech Processing Mean a a Normalization | ae + & ale > To 3D CNN Selection Conversion (b) Video ProcessingCNN¢+ fully connected network except the final output layer Decision — CNN+ fully connected network except the final output layer",Emotion detection using normal face,"The paper proposes a simple solution for facial expression recognition using a combination of Convolutional Neural Network and specific image pre-processing steps. The method achieves competitive results compared to other facial expression recognition methods, with 96.76% accuracy on the CK+ database, and allows for real-time recognition. The study evaluates the impact of each pre-processing operation on accuracy and shows that the combination of normalization procedures significantly improves accuracy. The proposed method works in unknown environments, but there is room for improvement. Future work includes investigating other learning methods to increase the method's robustness and fine-tuning a pre-trained deep neural network to focus on more specific features.



",Object and Sentiment Recognition,"Frame sequence in a video 2i+1 frames —_| Calculate Calculate Calculate Histogram | “| Histogram |” | Histogram Choose the frame with the least change | Selected frame<li Mel-spectrogram Speech—> Preprocessing -—> 2D CNN > >| ""| 5 3 svM Recognized ron, Frame ~ Video —>| Selection and 3D CNN * | Processing Normalized Gray Images, LBP and TX IDP Images Fig. 1. An overall block diagram of the proposed emotion recognition system. ‘Speech Video —> Eee Fourier a Log | sMelspectrogram, _, To 2D Windowing Transform delta, double delta CNN (a) Speech Processing Mean a a Normalization | ae + & ale > To 3D CNN Selection Conversion (b) Video ProcessingCNN¢+ fully connected network except the final output layer Decision — CNN+ fully connected network except the final output layer",Object Recognition
360,"Deep multi-path convolutional neural network joint with salient
region attention for facial expression recognition.","Attention,
Convolutional neural network,
Facial expression recognition,
Multi-Path variation-suppressing network,
Salient expressional region descriptor.","Facial Expression Recognition (FER) has long been a challenging task in the field of computer vision. In
this paper, we present a novel model, named Deep Attentive Multi-path Convolutional Neural Network
(DAM-CNN), for FER. Different from most existing models, DAM-CNN can automatically locate expressionrelated regions in an expressional image and yield a robust image representation for FER. The proposed
model contains two novel modules: an attention-based Salient Expressional Region Descriptor (SERD) and
the Multi-Path Variation-Suppressing Network (MPVS-Net). SERD can adaptively estimate the importance
of different image regions for FER task, while MPVS-Net disentangles expressional information from irrelevant variations. By jointly combining SERD and MPVS-Net, DAM-CNN is able to highlight expressionrelevant features and generate a variation-robust representation for expression classification. Extensive
experimental results on both constrained datasets (CK+, JAFFE, TFEID) and unconstrained datasets (SFEW,
FER2013, BAUM-2i) demonstrate the effectiveness of our DAM-CNN model.","In this paper, we propose a novel model named DAM-CNN for FER. The proposed model consists of three modules, i.e., a feature extraction module (VGG-Face), the SERD and the MPVS-Net. In our method, features are extracted by the VGG-Face and fed into the SERD module, which adaptively highlights the features that are highly-relevant to FER task and helps locate expression-sensitive regions. Visualizations of the located regions show that SERD is able to automatically focus on some expression-related regions such as the neighborhood of eyes and mouth. The other module, MPVS-Net, is proposed to handle different variations. Based on the encoder-decoder architecture, the high-level representations yielded by MPVS-Net are robust to multiple variations, which is effective for FER task. By jointly combining the SERD and MPVSNet, DAM-CNN can be more discriminative to different expressions Experimental results on both constrained and unconstrained
datasets demonstrate the effectiveness of our model.
The proposed DAM-CNN can still be improved in some aspects. In our future work, we intend to modify our training strategy: converting from the current two-stage training to an end-toend manner. By end-to-end training, we expect to better develop
the advantages of SERD and MPVS-Net. Also, we plan to optimize
the network structure following the approaches we mentioned in
Section 4.5 in order to improve the robustness and efficiency of our
model. In addition, DAM-CNN is a generic model for classification.
It can be extended to other recognition task such as face recognition, which can be one of our future work to further investigate
the effectiveness of DAM-CNN.
","Deep multi-path convolutional neural network joint with salient
region attention for facial expression recognition.Attention,
Convolutional neural network,
Facial expression recognition,
Multi-Path variation-suppressing network,
Salient expressional region descriptor.Facial Expression Recognition (FER) has long been a challenging task in the field of computer vision. In
this paper, we present a novel model, named Deep Attentive Multi-path Convolutional Neural Network
(DAM-CNN), for FER. Different from most existing models, DAM-CNN can automatically locate expressionrelated regions in an expressional image and yield a robust image representation for FER. The proposed
model contains two novel modules: an attention-based Salient Expressional Region Descriptor (SERD) and
the Multi-Path Variation-Suppressing Network (MPVS-Net). SERD can adaptively estimate the importance
of different image regions for FER task, while MPVS-Net disentangles expressional information from irrelevant variations. By jointly combining SERD and MPVS-Net, DAM-CNN is able to highlight expressionrelevant features and generate a variation-robust representation for expression classification. Extensive
experimental results on both constrained datasets (CK+, JAFFE, TFEID) and unconstrained datasets (SFEW,
FER2013, BAUM-2i) demonstrate the effectiveness of our DAM-CNN model.In this paper, we propose a novel model named DAM-CNN for FER. The proposed model consists of three modules, i.e., a feature extraction module (VGG-Face), the SERD and the MPVS-Net. In our method, features are extracted by the VGG-Face and fed into the SERD module, which adaptively highlights the features that are highly-relevant to FER task and helps locate expression-sensitive regions. Visualizations of the located regions show that SERD is able to automatically focus on some expression-related regions such as the neighborhood of eyes and mouth. The other module, MPVS-Net, is proposed to handle different variations. Based on the encoder-decoder architecture, the high-level representations yielded by MPVS-Net are robust to multiple variations, which is effective for FER task. By jointly combining the SERD and MPVSNet, DAM-CNN can be more discriminative to different expressions Experimental results on both constrained and unconstrained
datasets demonstrate the effectiveness of our model.
The proposed DAM-CNN can still be improved in some aspects. In our future work, we intend to modify our training strategy: converting from the current two-stage training to an end-toend manner. By end-to-end training, we expect to better develop
the advantages of SERD and MPVS-Net. Also, we plan to optimize
the network structure following the approaches we mentioned in
Section 4.5 in order to improve the robustness and efficiency of our
model. In addition, DAM-CNN is a generic model for classification.
It can be extended to other recognition task such as face recognition, which can be one of our future work to further investigate
the effectiveness of DAM-CNN.
#ERROR!",Emotion detection using normal face,"The paper presents a novel model called Deep Attentive Multi-path Convolutional Neural Network (DAM-CNN) for Facial Expression Recognition (FER). The model includes two modules, the attention-based Salient Expressional Region Descriptor (SERD) and the Multi-Path Variation-Suppressing Network (MPVS-Net). SERD identifies expression-related regions in an image, while MPVS-Net disentangles expression information from irrelevant variations. By combining SERD and MPVS-Net, DAM-CNN generates a variation-robust representation for expression classification. The model's effectiveness is demonstrated through experimental results on both constrained and unconstrained datasets. Future work includes modifying the training strategy, optimizing the network structure, and extending the model to other recognition tasks such as face recognition.",Object and Sentiment Recognition,#ERROR!,Object Recognition
361,"A probabilistic topic model using deep        visual word representation for
simultaneous image classification and annotation.","Image classification and annotation,
Topic models,
Probabilistic model,
Deep learning,
Convolutional neural network,
LLC.
","Researches have shown that holistic examination of an image provides better understanding of the image
compared to separate processes each devoted to a single task like annotation, classification or segmentation. During the past decades, there have been several efforts for simultaneous image classification
and annotation using probabilistic or neural network based topic models. Despite their relative success,
most of these models suffer from the poor visual word representation and the imbalance between the
number of visual and annotation words in the training data. This paper proposes a novel model for simultaneous image classification and annotation model based on SupDocNADE, a neural network based topic
model for image classification and annotation. The proposed model, named wSupDocNADE, addresses the
above shortcomings by using a new coding and introducing a weighting mechanism for the SupDocNADE
model. In the coding step of the model, several patches extracted from the input image are first fed to a
deep convolutional neural network and the feature vectors obtained from this network are coded using
the LLC coding. These vectors are then aggregated in a final descriptor through sum pooling. To overcome
the imbalance between the visual and annotation words, a weighting factor is considered for each visual
or annotation word. The weights of the visual words are set based on their frequencies obtained from the
pooling method and the weights of the annotation words are learned from the training data. The experimental results on three benchmark datasets show the superiority of the proposed model in both image
classification and annotation tasks over state-of-the-art models.","In this paper, we introduced wSupdocNADE model for simultaneous image classification and annotation using a CNN-based and
LLC-coded image descriptor and a neural auto-regressive network
based topic model. Two main advantages of wSupDocNADE over
the previous models of this field are (1) the use of more informative representation of the input image and (2) addressing the issue
of imbalance between the number of visual words extracted from
the input image and the annotation words provided for each image
at the training time. Experimental results on three image annotation and classification benchmark datasets clearly showed the
superiority of the model. However, the performance of the model
can be further improved in many ways; for example, by using an
attention framework in which the image patches that encompass
more important content are assigned higher weight values.","A probabilistic topic model using deep        visual word representation for
simultaneous image classification and annotation.Image classification and annotation,
Topic models,
Probabilistic model,
Deep learning,
Convolutional neural network,
LLC.
Researches have shown that holistic examination of an image provides better understanding of the image
compared to separate processes each devoted to a single task like annotation, classification or segmentation. During the past decades, there have been several efforts for simultaneous image classification
and annotation using probabilistic or neural network based topic models. Despite their relative success,
most of these models suffer from the poor visual word representation and the imbalance between the
number of visual and annotation words in the training data. This paper proposes a novel model for simultaneous image classification and annotation model based on SupDocNADE, a neural network based topic
model for image classification and annotation. The proposed model, named wSupDocNADE, addresses the
above shortcomings by using a new coding and introducing a weighting mechanism for the SupDocNADE
model. In the coding step of the model, several patches extracted from the input image are first fed to a
deep convolutional neural network and the feature vectors obtained from this network are coded using
the LLC coding. These vectors are then aggregated in a final descriptor through sum pooling. To overcome
the imbalance between the visual and annotation words, a weighting factor is considered for each visual
or annotation word. The weights of the visual words are set based on their frequencies obtained from the
pooling method and the weights of the annotation words are learned from the training data. The experimental results on three benchmark datasets show the superiority of the proposed model in both image
classification and annotation tasks over state-of-the-art models.In this paper, we introduced wSupdocNADE model for simultaneous image classification and annotation using a CNN-based and
LLC-coded image descriptor and a neural auto-regressive network
based topic model. Two main advantages of wSupDocNADE over
the previous models of this field are (1) the use of more informative representation of the input image and (2) addressing the issue
of imbalance between the number of visual words extracted from
the input image and the annotation words provided for each image
at the training time. Experimental results on three image annotation and classification benchmark datasets clearly showed the
superiority of the model. However, the performance of the model
can be further improved in many ways; for example, by using an
attention framework in which the image patches that encompass
more important content are assigned higher weight values.D. Jiang et al/ Measurement 150 (2020) 107049 Extract /\\ a Audio Features Audio classifier 14 inputs — 1 output BP network Extract a al ese Visual classifier Training set Save into 16 inputs — | output BP network XXX Positive and Negative databases K-Nearest NeighborTest set o4 Features Input I VY Oo Audio classitie Lyries classifier sual classifies I Classifiers om J | AVL av ‘ . AL . e VL ‘ eP(negative)=0 Max(neutral emotion) ~ Min(t) | neutral emotion|-0.5 al | positive emotion| Ppostive)= -Min(neutral emotion) + Max(4:) P(neutral) = | neutral emotion|-0.5 ul P(postive) = inegarive emorion| P(neutral P(negative) = P(negative) = 0 (P(negative), ¢,-1) 8(P (postive), ¢,1) @(P(neutral), €,0) Fig. 7. Overvipw of the confidence interval decision framework.",Emotion detection using normal face,"The paper presents a novel model, wSupDocNADE, for simultaneous image classification and annotation, which addresses the shortcomings of previous models related to poor visual word representation and imbalance between visual and annotation words. The proposed model uses a deep convolutional neural network and the LLC coding to generate a more informative representation of the input image, and introduces a weighting mechanism to overcome the imbalance issue. Experimental results on three benchmark datasets show that wSupDocNADE outperforms state-of-the-art models in image classification and annotation tasks. However, the model can still be improved by incorporating attention mechanisms to assign higher weights to more important image patches.",Object and Sentiment Recognition,"D. Jiang et al/ Measurement 150 (2020) 107049 Extract /\\ a Audio Features Audio classifier 14 inputs — 1 output BP network Extract a al ese Visual classifier Training set Save into 16 inputs — | output BP network XXX Positive and Negative databases K-Nearest NeighborTest set o4 Features Input I VY Oo Audio classitie Lyries classifier sual classifies I Classifiers om J | AVL av ‘ . AL . e VL ‘ eP(negative)=0 Max(neutral emotion) ~ Min(t) | neutral emotion|-0.5 al | positive emotion| Ppostive)= -Min(neutral emotion) + Max(4:) P(neutral) = | neutral emotion|-0.5 ul P(postive) = inegarive emorion| P(neutral P(negative) = P(negative) = 0 (P(negative), ¢,-1) 8(P (postive), ¢,1) @(P(neutral), €,0) Fig. 7. Overvipw of the confidence interval decision framework.",Object Recognition
362,"A dynamic framework based on local Zernike moment and motion history
image for facial expression recognition.","Zernike moment,
Facial expression,
Motion history image,
Entropy,
Feature extraction.","A dynamic descriptor facilitates robust recognition of facial expressions in video sequences. The current two
main approaches to the recognition are basic emotion recognition and recognition based on facial action coding
system (FACS) action units. In this paper we focus on basic emotion recognition and propose a spatio-temporal
feature based on local Zernike moment in the spatial domain using motion change frequency. We also design a
dynamic feature comprising motion history image and entropy. To recognise a facial expression, a weighting
strategy based on the latter feature and sub-division of the image frame is applied to the former to enhance the
dynamic information of facial expression, and followed by the application of the classical support vector
machine. Experiments on the CK+ and MMI datasets using leave-one-out cross validation scheme demonstrate
that the integrated framework achieves a better performance than using individual descriptor separately.
Compared with six state-of-arts methods, the proposed framework demonstrates a superior performance.","This paper presents a facial expression recognition framework using enMHI_OF and QLZM_MCF. The framework which comprises
pre-processing, feature extraction followed by 2D PCA and SVM
classification achieves better performance than most of the state-ofart methods on CK+ dataset and MMI dataset. Our main contributions
are three folds. First, we proposed a spatio-temporal feature based on
QLZM. Second, we applied optical flow in MHI to obtain MHI_OF
feature which incorporates velocity information. Third, we introduced
entropy to employ the spatial relation of different facial parts, and
designed a strategy based on entropy to integrate enMHI_OF and
QLZM_MCF. The proposed framework performs slightly worse in
distinguishing the three expressions of Fear, Sadness and Contempt,
thus how to design a better feature to represent these expressions will
be part of our feature work. Also, since an expression usually occurs
along with the movement of shoulder and hands, it might be useful to
exploit these information in our recognition system.
When applying a facial expression recognition framework in real
situations, computation speed might be a factor to be considered. In
some case, the increase in speed may result in a decrease in recognition
performance. How to design a framework for facial expression recognition which increases the computational speed without any degradation
in the recognition rate remains a challenge.","A dynamic framework based on local Zernike moment and motion history
image for facial expression recognition.Zernike moment,
Facial expression,
Motion history image,
Entropy,
Feature extraction.A dynamic descriptor facilitates robust recognition of facial expressions in video sequences. The current two
main approaches to the recognition are basic emotion recognition and recognition based on facial action coding
system (FACS) action units. In this paper we focus on basic emotion recognition and propose a spatio-temporal
feature based on local Zernike moment in the spatial domain using motion change frequency. We also design a
dynamic feature comprising motion history image and entropy. To recognise a facial expression, a weighting
strategy based on the latter feature and sub-division of the image frame is applied to the former to enhance the
dynamic information of facial expression, and followed by the application of the classical support vector
machine. Experiments on the CK+ and MMI datasets using leave-one-out cross validation scheme demonstrate
that the integrated framework achieves a better performance than using individual descriptor separately.
Compared with six state-of-arts methods, the proposed framework demonstrates a superior performance.This paper presents a facial expression recognition framework using enMHI_OF and QLZM_MCF. The framework which comprises
pre-processing, feature extraction followed by 2D PCA and SVM
classification achieves better performance than most of the state-ofart methods on CK+ dataset and MMI dataset. Our main contributions
are three folds. First, we proposed a spatio-temporal feature based on
QLZM. Second, we applied optical flow in MHI to obtain MHI_OF
feature which incorporates velocity information. Third, we introduced
entropy to employ the spatial relation of different facial parts, and
designed a strategy based on entropy to integrate enMHI_OF and
QLZM_MCF. The proposed framework performs slightly worse in
distinguishing the three expressions of Fear, Sadness and Contempt,
thus how to design a better feature to represent these expressions will
be part of our feature work. Also, since an expression usually occurs
along with the movement of shoulder and hands, it might be useful to
exploit these information in our recognition system.
When applying a facial expression recognition framework in real
situations, computation speed might be a factor to be considered. In
some case, the increase in speed may result in a decrease in recognition
performance. How to design a framework for facial expression recognition which increases the computational speed without any degradation
in the recognition rate remains a challenge.Global network Cross Temporal Segment VGG16 (part) Fig. 4. The overview of the integrated framework.Segment | oe Cross Segment feature segment Segment II { Segment N Fig. 3. Cross temporal segment LSTM architecture.",Emotion detection using normal face,"This paper proposes a framework for recognizing facial expressions in video sequences using a dynamic descriptor. The framework utilizes a spatio-temporal feature based on local Zernike moment in the spatial domain and motion change frequency, as well as a dynamic feature comprising motion history image and entropy. To recognize a facial expression, a weighting strategy based on the dynamic feature and sub-division of the image frame is applied to the spatio-temporal feature, followed by support vector machine classification. The proposed framework outperforms six state-of-the-art methods on the CK+ and MMI datasets. However, the framework performs slightly worse in distinguishing fear, sadness, and contempt expressions, and computation speed is a factor that needs to be considered when applying the framework in real situations. Further work is needed to design better features to represent these expressions and to increase the computational speed of the framework without degrading the recognition rate.",Object and Sentiment Recognition,Global network Cross Temporal Segment VGG16 (part) Fig. 4. The overview of the integrated framework.Segment | oe Cross Segment feature segment Segment II { Segment N Fig. 3. Cross temporal segment LSTM architecture.,Object Recognition
363,"Deep spatial-temporal feature fusion for facial expression recognition
in static images.","Facial expression recognition,
Optical flow,
Spatial-temporal feature fusion,
Transfer learning.","Traditional methods of performing facial expression recognition commonly use hand-crafted spatial features. This paper proposes a multi-channel deep neural network that learns and fuses the spatialtemporal features for recognizing facial expressions in static images. The essential idea of this method
is to extract optical flow from the changes between the peak expression face image (emotional-face) and
the neutral face image (neutral-face) as the temporal information of a certain facial expression, and use
the gray-level image of emotional-face as the spatial information. A Multi-channel Deep Spatial-Temporal
feature Fusion neural Network (MDSTFN) is presented to perform the deep spatial-temporal feature extraction and fusion from static images. Each channel of the proposed method is fine-tuned from a pretrained deep convolutional neural networks (CNN) instead of training a new CNN from scratch. In addition, average-face is used as a substitute for neutral-face in real-world applications. Extensive experiments
are conducted to evaluate the proposed method on benchmarks databases including CK+, MMI, and RaFD.
The results show that the optical flow information from emotional-face and neutral-face is a useful complement to spatial feature and can effectively improve the performance of facial expression recognition
from static images. Compared with state-of-the-art methods, the proposed method can achieve better
recognition accuracy, with rates of 98.38% on the CK+ database, 99.17% on the RaFD database, and 99.59%
on the MMI database, respectively..","This paper presented a deep neural network architecture with
multi-channels to extract and fuse the spatial-temporal features of
static image for FER. The optical flow computed from the changes
between emotional-face and neutral-face is used to represent
the temporal changes of expression, while the gray-level image
of emotional-face is used to provide the spatial information of
expression. The feature extraction channels of the MDSTFN (Multichannel Deep Spatial-Temporal feature Fusion neural Network) are
fine-tuned from a pre-trained CNN model. This transfer learning
scheme can not only obtain a better feature extractor for FER on
a small number of image samples but can also reduce the risk
of overfitting. Three kinds of strategies were investigated to fuse
the temporal and spatial features obtained by multiple feature
channels. On three benchmark databases (CK+, RaFD, and MMI),
extensive experiments were conducted to evaluate the proposed
method under various parameters such as channel combination,
fusion strategy, cross-database, and pre-trained CNN models.
The results show that the MDSTFN-based method is a feasible
deep spatial-temporal feature extraction architecture for facial
expression recognition. Replacing neutral-face with average-face
improves the practicality of the proposed method, while the
results of a comparison show that the MDSTFN-based method can
achieve better accuracy than state-of-the-art methods.","Deep spatial-temporal feature fusion for facial expression recognition
in static images.Facial expression recognition,
Optical flow,
Spatial-temporal feature fusion,
Transfer learning.Traditional methods of performing facial expression recognition commonly use hand-crafted spatial features. This paper proposes a multi-channel deep neural network that learns and fuses the spatialtemporal features for recognizing facial expressions in static images. The essential idea of this method
is to extract optical flow from the changes between the peak expression face image (emotional-face) and
the neutral face image (neutral-face) as the temporal information of a certain facial expression, and use
the gray-level image of emotional-face as the spatial information. A Multi-channel Deep Spatial-Temporal
feature Fusion neural Network (MDSTFN) is presented to perform the deep spatial-temporal feature extraction and fusion from static images. Each channel of the proposed method is fine-tuned from a pretrained deep convolutional neural networks (CNN) instead of training a new CNN from scratch. In addition, average-face is used as a substitute for neutral-face in real-world applications. Extensive experiments
are conducted to evaluate the proposed method on benchmarks databases including CK+, MMI, and RaFD.
The results show that the optical flow information from emotional-face and neutral-face is a useful complement to spatial feature and can effectively improve the performance of facial expression recognition
from static images. Compared with state-of-the-art methods, the proposed method can achieve better
recognition accuracy, with rates of 98.38% on the CK+ database, 99.17% on the RaFD database, and 99.59%
on the MMI database, respectively..This paper presented a deep neural network architecture with
multi-channels to extract and fuse the spatial-temporal features of
static image for FER. The optical flow computed from the changes
between emotional-face and neutral-face is used to represent
the temporal changes of expression, while the gray-level image
of emotional-face is used to provide the spatial information of
expression. The feature extraction channels of the MDSTFN (Multichannel Deep Spatial-Temporal feature Fusion neural Network) are
fine-tuned from a pre-trained CNN model. This transfer learning
scheme can not only obtain a better feature extractor for FER on
a small number of image samples but can also reduce the risk
of overfitting. Three kinds of strategies were investigated to fuse
the temporal and spatial features obtained by multiple feature
channels. On three benchmark databases (CK+, RaFD, and MMI),
extensive experiments were conducted to evaluate the proposed
method under various parameters such as channel combination,
fusion strategy, cross-database, and pre-trained CNN models.
The results show that the MDSTFN-based method is a feasible
deep spatial-temporal feature extraction architecture for facial
expression recognition. Replacing neutral-face with average-face
improves the practicality of the proposed method, while the
results of a comparison show that the MDSTFN-based method can
achieve better accuracy than state-of-the-art methods.fi CNN Input Convolution Subsampling Convolution Subsampling Fully Connected Layer Output Image Size saaz 32328128 saxtani 6x88 eee 256 N Kernel size 3S 2x2 77 22 Fig. 7. Architecture of the proposed Convolutional Neutral Network. It comprises of five layers: two convolutional layers, two sub-sampling layers and one fully connected layer,Input Pre-processing Rotation Correction Rotation Corrected, Image as Cropping eo A Cropped Image Output es Intensity Normalization =* =, Down-sampling aS Inesty Normalized sox image Image Training ontine — Recognition oa Network eradent | [0 Best 009 inna oetcadins ———[vtseton] —]_ nee 08 09 aq Confidence Tomes tevel Input Labels Ouput Label",Emotion detection using normal face,"This paper proposes a Multi-channel Deep Spatial-Temporal feature Fusion neural Network (MDSTFN) for facial expression recognition (FER) from static images. The proposed method extracts optical flow from changes between peak expression and neutral face images as temporal information, and gray-level images of emotional-face as spatial information. The feature extraction channels of the MDSTFN are fine-tuned from pre-trained CNN models. Three strategies are investigated to fuse temporal and spatial features. The proposed method achieves better accuracy than state-of-the-art methods on benchmark databases including CK+, RaFD, and MMI. The use of average-face in place of neutral-face improves practicality.",Object and Sentiment Recognition,"fi CNN Input Convolution Subsampling Convolution Subsampling Fully Connected Layer Output Image Size saaz 32328128 saxtani 6x88 eee 256 N Kernel size 3S 2x2 77 22 Fig. 7. Architecture of the proposed Convolutional Neutral Network. It comprises of five layers: two convolutional layers, two sub-sampling layers and one fully connected layer,Input Pre-processing Rotation Correction Rotation Corrected, Image as Cropping eo A Cropped Image Output es Intensity Normalization =* =, Down-sampling aS Inesty Normalized sox image Image Training ontine — Recognition oa Network eradent | [0 Best 009 inna oetcadins ———[vtseton] —]_ nee 08 09 aq Confidence Tomes tevel Input Labels Ouput Label",Object Recognition
364,"Semi-supervised facial expression recognition using reduced spatial
features and Deep Belief Networks.","Emotion recognition,
Semi-supervised learning,
Dimensionality reduction,
Contrastive divergence,
Backpropagation,
K-Fold cross-validation.","A semi-supervised emotion recognition algorithm using reduced features as well as a novel feature selection approach is proposed. The proposed algorithm consists of a cascaded structure where first a feature
extraction is applied to the facial images, followed by a feature reduction. A semi-supervised training
with all the available labeled and unlabeled data is applied to a Deep Belief Network (DBN). Feature selection is performed to eliminate those features that do not provide information, using a reconstruction
error-based ranking. Results show that HOG features of mouth provide the best performance. The performance evaluation has been done between the semi-supervised approach using DBN and other supervised
strategies such as Support Vector Machine (SVM) and Convolutional Neural Network (CNN). The results
show that the semi-supervised approach has improved efficiency using the information contained in both
labeled and unlabeled data. Different databases were used to validate the experiments and the application of Linear Discriminant Analysis (LDA) on the HOG features of mouth gave the highest recognition
rate.","A semi-supervised approach for facial ER utilizing reduced
facial features with most of the data being unlabeled is introduced
with a four-layered neural network. They are convenient to use
due to their easy training. Since we use CD and BP, training can
be done sequentially. Semi-supervised learning was achieved by
combining CD and BP, as CD is unsupervised, and BP is supervised.
The facial features used were mouth and eye HOG, 2D-DWT of
eyes and 2D-DWT of mouth. Further, the analysis was done with different dimensionality reduction algorithm on each of the feature
sets. The test accuracies were compared for the semi-supervised
training using DBN and supervised training using SVM and CNN. It
was observed that the semi-supervised training showed the best
performance with a test accuracy of 98.57% and outperformed
SVM in terms of accuracy and CNN in terms of computational
complexity. DBN used the information in unlabeled data to give
better performance and the most accurate model required 60%
of labeled data and 40% of unlabeled data. The semi-supervised
training with HOG features of mouth also showed a consistent
performance even when there was just 10% labeled data and rest
unlabeled data. Furthermore, the DBN was trained in a supervised,
semi-supervised and unsupervised manner using the reduced
features to examine the difference in performance and again semisupervised training was able to give better accuracy compared to
the supervised and unsupervised training. Feature analysis with
the reduced-dimensional features (LDA) was performed using the
reconstruction error technique. Based on the experiment using
reconstruction error it was found that reduced HOG features of
mouth contained the most relevant information to classify facial
expression. Analysis based on training run-time and test accuracies
for features of different dimension was also carried out. It was
observed that the best performance, that is minimum run-time
and high accuracy was given by the dimensionally reduced features (LDA) with 2-dimensions. The test accuracies improved
significantly after using dimensionality reduction technique. Future
work aims at the use of ER technology in videos, in particular in
emergency response situational awareness systems with thermal
imaging, to detect emotions in civilians in the emergency scenario.
Declaration of Competing Interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Acknowledgments
This work has been supported by National Science Foundation
S&CC EAGER grant 1637092. Authors would like to thank UNM
Center for Advanced Research Computing, supported in part by the
NSF, for providing high performance computing, large-scale storage
and visualization resources.

","Semi-supervised facial expression recognition using reduced spatial
features and Deep Belief Networks.Emotion recognition,
Semi-supervised learning,
Dimensionality reduction,
Contrastive divergence,
Backpropagation,
K-Fold cross-validation.A semi-supervised emotion recognition algorithm using reduced features as well as a novel feature selection approach is proposed. The proposed algorithm consists of a cascaded structure where first a feature
extraction is applied to the facial images, followed by a feature reduction. A semi-supervised training
with all the available labeled and unlabeled data is applied to a Deep Belief Network (DBN). Feature selection is performed to eliminate those features that do not provide information, using a reconstruction
error-based ranking. Results show that HOG features of mouth provide the best performance. The performance evaluation has been done between the semi-supervised approach using DBN and other supervised
strategies such as Support Vector Machine (SVM) and Convolutional Neural Network (CNN). The results
show that the semi-supervised approach has improved efficiency using the information contained in both
labeled and unlabeled data. Different databases were used to validate the experiments and the application of Linear Discriminant Analysis (LDA) on the HOG features of mouth gave the highest recognition
rate.A semi-supervised approach for facial ER utilizing reduced
facial features with most of the data being unlabeled is introduced
with a four-layered neural network. They are convenient to use
due to their easy training. Since we use CD and BP, training can
be done sequentially. Semi-supervised learning was achieved by
combining CD and BP, as CD is unsupervised, and BP is supervised.
The facial features used were mouth and eye HOG, 2D-DWT of
eyes and 2D-DWT of mouth. Further, the analysis was done with different dimensionality reduction algorithm on each of the feature
sets. The test accuracies were compared for the semi-supervised
training using DBN and supervised training using SVM and CNN. It
was observed that the semi-supervised training showed the best
performance with a test accuracy of 98.57% and outperformed
SVM in terms of accuracy and CNN in terms of computational
complexity. DBN used the information in unlabeled data to give
better performance and the most accurate model required 60%
of labeled data and 40% of unlabeled data. The semi-supervised
training with HOG features of mouth also showed a consistent
performance even when there was just 10% labeled data and rest
unlabeled data. Furthermore, the DBN was trained in a supervised,
semi-supervised and unsupervised manner using the reduced
features to examine the difference in performance and again semisupervised training was able to give better accuracy compared to
the supervised and unsupervised training. Feature analysis with
the reduced-dimensional features (LDA) was performed using the
reconstruction error technique. Based on the experiment using
reconstruction error it was found that reduced HOG features of
mouth contained the most relevant information to classify facial
expression. Analysis based on training run-time and test accuracies
for features of different dimension was also carried out. It was
observed that the best performance, that is minimum run-time
and high accuracy was given by the dimensionally reduced features (LDA) with 2-dimensions. The test accuracies improved
significantly after using dimensionality reduction technique. Future
work aims at the use of ER technology in videos, in particular in
emergency response situational awareness systems with thermal
imaging, to detect emotions in civilians in the emergency scenario.
Declaration of Competing Interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Acknowledgments
This work has been supported by National Science Foundation
S&CC EAGER grant 1637092. Authors would like to thank UNM
Center for Advanced Research Computing, supported in part by the
NSF, for providing high performance computing, large-scale storage
and visualization resources.

Salient Features Salient Expression| Regions Attentive Mask CNN FeaturesFeature Extraction ‘SERD MPVS-Net (VGG-Face) —_——_.Visualied Salient fxprenional Reson Feature Extraction Module Salient Features Movs net Fig. 1 The architecture of DAM-CNN.",Emotion detection using normal face,"The article presents a novel approach for emotion recognition (ER) using a semi-supervised algorithm with reduced features and a reconstruction error-based feature selection method. The proposed algorithm involves a cascaded structure that first extracts features from facial images, reduces them, and then trains a Deep Belief Network (DBN) using both labeled and unlabeled data. HOG features of the mouth were found to be the most effective, and the semi-supervised approach outperformed SVM and CNN methods in terms of accuracy and computational complexity. The use of reduced-dimensional features with LDA further improved the performance. The authors declare no competing interests and acknowledge funding from the NSF and support from UNM Center for Advanced Research Computing. The future work aims to extend ER technology to videos, especially in emergency response scenarios.",Object and Sentiment Recognition,Salient Features Salient Expression| Regions Attentive Mask CNN FeaturesFeature Extraction ‘SERD MPVS-Net (VGG-Face) —_——_.Visualied Salient fxprenional Reson Feature Extraction Module Salient Features Movs net Fig. 1 The architecture of DAM-CNN.,Object Recognition
365,A survey on opinion summarization techniques for social media,"Natural language processing, Opinion summarization, Opinion mining, Tweet summarization.","The volume of data on the social media is huge and even keeps increasing. The need for efficient processing of this extensive information resulted in increasing research interest in knowledge engineering tasks such as Opinion Summarization. This survey shows the current opinion summarization challenges for social media, then the necessary pre-summarization steps like preprocessing, features extraction, noise elimination, and handling of synonym features. Next, it covers the various approaches used in opinion summarization like Visualization, Abstractive, Aspect based, Query-focused, Real Time, Update Summarization, and highlight other Opinion Summarization approaches such as Contrastive, Concept-based, Community Detection, Domain Specific, Bilingual, Social Bookmarking, and Social Media Sampling. It covers the different datasets used in opinion summarization and future work suggested in each technique. Finally, it provides different ways for evaluating opinion summarization.","Regardless of numerous research efforts, current opinion summarization studies still have numerous impediments for enhancements . Moreover, there is a real requirement for building a summarization corpus specifically from social media for quickening the advancement in this area .
We see that recently there is more research interest in opinion summarization, especially in Abstractive Summarization category. And the current trend among many researchers is the use of deep learning techniques and utilizing the GPUs for training from large-scale data. So, we expect the following three keywords to dominate in the near future for the field of opinion summarization (Abstractive Summarization + Deep Learning + GPUs). And we also expect that more attention will be gained for the deep learning software tools such as TensorFlow, Microsoft Cognitive Toolkit.

","A survey on opinion summarization techniques for social mediaNatural language processing, Opinion summarization, Opinion mining, Tweet summarization.The volume of data on the social media is huge and even keeps increasing. The need for efficient processing of this extensive information resulted in increasing research interest in knowledge engineering tasks such as Opinion Summarization. This survey shows the current opinion summarization challenges for social media, then the necessary pre-summarization steps like preprocessing, features extraction, noise elimination, and handling of synonym features. Next, it covers the various approaches used in opinion summarization like Visualization, Abstractive, Aspect based, Query-focused, Real Time, Update Summarization, and highlight other Opinion Summarization approaches such as Contrastive, Concept-based, Community Detection, Domain Specific, Bilingual, Social Bookmarking, and Social Media Sampling. It covers the different datasets used in opinion summarization and future work suggested in each technique. Finally, it provides different ways for evaluating opinion summarization.Regardless of numerous research efforts, current opinion summarization studies still have numerous impediments for enhancements . Moreover, there is a real requirement for building a summarization corpus specifically from social media for quickening the advancement in this area .
We see that recently there is more research interest in opinion summarization, especially in Abstractive Summarization category. And the current trend among many researchers is the use of deep learning techniques and utilizing the GPUs for training from large-scale data. So, we expect the following three keywords to dominate in the near future for the field of opinion summarization (Abstractive Summarization + Deep Learning + GPUs). And we also expect that more attention will be gained for the deep learning software tools such as TensorFlow, Microsoft Cognitive Toolkit.

LLC LLC LLC Fig. 1. Block-diagram of the proposed model (wSupDocNADE) for simultaneous image classification and annotation.",Text summarization,"The field of opinion summarization faces obstacles in enhancing current techniques, and there is a need for a summarization corpus from social media to advance the field. Abstractive summarization, deep learning, and GPUs are expected to dominate future research in opinion summarization, with more attention given to software tools like TensorFlow and Microsoft Cognitive Toolkit.",Natural Language Processing,LLC LLC LLC Fig. 1. Block-diagram of the proposed model (wSupDocNADE) for simultaneous image classification and annotation.,Object Recognition
366,Automatic summarization of scientific articles: A survey,"AutomaticText summarization, Scientific article, Single-document, Multi-document","The scientific research process generally starts with the examination of the state of the art, which may involve a vast number of publications. Automatically summarizing scientific articles would help researchers in their investigation by speeding up the research process. The automatic summarization of scientific articles differs from the summarization of generic texts due to their specific structure and inclusion of citation sentences. Most of the valuable information in scientific articles is presented in tables, figures, and algorithm pseudocode. These elements, however, do not usually appear in a generic text. Therefore, several approaches that consider the particularity of a scientific article structure were proposed to enhance the quality of the generated summary, resulting in ad hoc automatic summarizers. This paper provides a comprehensive study of the state of the art in this field and discusses some future research directions. It particularly presents a review of approaches developed during the last decade, the corpora used, and their evaluation methods. It also discusses their limitations and points out some open problems. The conclusions of this study highlight the prevalence of extractive techniques for the automatic summarization of single monolingual articles using a combination of statistical, natural language processing, and machine learning techniques. The absence of benchmark corpora and gold standard summaries for scientific articles remains the main issue for this task.","This paper presented a critical review of the state-of-the-art systems of summarizing scientific articles. It covered various aspects of the summarization task, including the solutions, evaluation, and corpora used in the evaluation process. It also highlighted some advantages and limitations in the literature. Our study found a high dominance of extractive techniques, single-article summarization, combinations of statistical (TF-IDF) and machine learning approaches (SVM, naïve Bayes, and clustering), and intrinsic evaluation methods (largely ROUGE metrics). The main related issues include the unavailability of training and test benchmark corpora, gold standard summaries for comparison, suitable evaluation metrics, and baseline systems needed for comparison purposes. Although graph-based methods have been successful in solving the task of multi-document summarization, they have received less attention in the field of automatic summarization of scientific articles. Most of the surveyed studies are centered around citation-based approaches and targeted a single article. Further research is needed to advance knowledge in this field by shifting from single-article to multi-article summarization and from extractive to abstractive in order to enhance the coherence and readability of the output. Deep learning approaches are also worth investigating due to their success in addressing other difficult NLP tasks.","Automatic summarization of scientific articles: A surveyAutomaticText summarization, Scientific article, Single-document, Multi-documentThe scientific research process generally starts with the examination of the state of the art, which may involve a vast number of publications. Automatically summarizing scientific articles would help researchers in their investigation by speeding up the research process. The automatic summarization of scientific articles differs from the summarization of generic texts due to their specific structure and inclusion of citation sentences. Most of the valuable information in scientific articles is presented in tables, figures, and algorithm pseudocode. These elements, however, do not usually appear in a generic text. Therefore, several approaches that consider the particularity of a scientific article structure were proposed to enhance the quality of the generated summary, resulting in ad hoc automatic summarizers. This paper provides a comprehensive study of the state of the art in this field and discusses some future research directions. It particularly presents a review of approaches developed during the last decade, the corpora used, and their evaluation methods. It also discusses their limitations and points out some open problems. The conclusions of this study highlight the prevalence of extractive techniques for the automatic summarization of single monolingual articles using a combination of statistical, natural language processing, and machine learning techniques. The absence of benchmark corpora and gold standard summaries for scientific articles remains the main issue for this task.This paper presented a critical review of the state-of-the-art systems of summarizing scientific articles. It covered various aspects of the summarization task, including the solutions, evaluation, and corpora used in the evaluation process. It also highlighted some advantages and limitations in the literature. Our study found a high dominance of extractive techniques, single-article summarization, combinations of statistical (TF-IDF) and machine learning approaches (SVM, naïve Bayes, and clustering), and intrinsic evaluation methods (largely ROUGE metrics). The main related issues include the unavailability of training and test benchmark corpora, gold standard summaries for comparison, suitable evaluation metrics, and baseline systems needed for comparison purposes. Although graph-based methods have been successful in solving the task of multi-document summarization, they have received less attention in the field of automatic summarization of scientific articles. Most of the surveyed studies are centered around citation-based approaches and targeted a single article. Further research is needed to advance knowledge in this field by shifting from single-article to multi-article summarization and from extractive to abstractive in order to enhance the coherence and readability of the output. Deep learning approaches are also worth investigating due to their success in addressing other difficult NLP tasks.Facial landmarks | i Detection T Face Alignment fQLZM_MCF Feature Feature Feature Integration Facial Expression Classification Fig. 5. The proposed framework. BUISSe001d-Ol4 uoHeoyisse|Q uol9eXQ, aunyead",Text summarization,"This paper reviews the current state of scientific article summarization, focusing on solutions, evaluation, and corpora used. Extractive techniques, single-article summarization, and statistical/machine learning approaches are dominant, with intrinsic evaluation methods (ROUGE metrics) used largely. Challenges include lack of benchmark corpora, gold standard summaries, evaluation metrics, and baseline systems. Graph-based methods are successful in multi-document summarization but less studied for scientific articles. More research is needed to improve coherence and readability, shift to multi-article and abstractive summarization, and explore deep learning approaches.",Natural Language Processing,"Facial landmarks | i Detection T Face Alignment fQLZM_MCF Feature Feature Feature Integration Facial Expression Classification Fig. 5. The proposed framework. BUISSe001d-Ol4 uoHeoyisse|Q uol9eXQ, aunyead",Object Recognition
367,Person recognition and the brain: Merging evidence from patients and healthy individuals,"Person recognition, Anterior temporal lobe, Neuroimaging, Meta-analysis, Patients, Familiarity.","Recognizing other persons is a key skill in social interaction, whether it is with our family at home or with our colleagues at work. Due to brain lesions such as stroke, or neurodegenerative disease, or due to psychiatric conditions, abilities in recognizing even personally familiar persons can be impaired. The underlying causes in the human brain have not yet been well understood. Here, we provide a comprehensive overview of studies reporting locations of brain damage in patients impaired in person-identity recognition, and relate the results to a quantitative meta-analysis based on functional imaging studies investigating person-identity recognition in healthy individuals. We identify modality-specific brain areas involved in recognition from different person characteristics, and potential multimodal hubs for person processing in the anterior temporal, frontal, and parietal lobes and posterior cingulate. Our combined review is built on cognitive and neuroscientific models of face- and voice-identity recognition and revises them within the multimodal context of person-identity recognition. These results provide a novel framework for future research in person-identity recognition both in the clinical as well as basic neurosciences.",The present review localized a network of modality-specific as well as multimodal/modality-free regions in the human brain that enable recognition of person identity. It highlighted the importance of investigating person-identity recognition from multiple modalities in both patients and healthy individuals. The results revised several assumptions of traditional cognitive and neuroscientific models of person-identity recognition and provide a model-driven framework for testing patients in clinical settings as well as basic experimental research to further advance our understanding of person-identity recognition.,"Person recognition and the brain: Merging evidence from patients and healthy individualsPerson recognition, Anterior temporal lobe, Neuroimaging, Meta-analysis, Patients, Familiarity.Recognizing other persons is a key skill in social interaction, whether it is with our family at home or with our colleagues at work. Due to brain lesions such as stroke, or neurodegenerative disease, or due to psychiatric conditions, abilities in recognizing even personally familiar persons can be impaired. The underlying causes in the human brain have not yet been well understood. Here, we provide a comprehensive overview of studies reporting locations of brain damage in patients impaired in person-identity recognition, and relate the results to a quantitative meta-analysis based on functional imaging studies investigating person-identity recognition in healthy individuals. We identify modality-specific brain areas involved in recognition from different person characteristics, and potential multimodal hubs for person processing in the anterior temporal, frontal, and parietal lobes and posterior cingulate. Our combined review is built on cognitive and neuroscientific models of face- and voice-identity recognition and revises them within the multimodal context of person-identity recognition. These results provide a novel framework for future research in person-identity recognition both in the clinical as well as basic neurosciences.The present review localized a network of modality-specific as well as multimodal/modality-free regions in the human brain that enable recognition of person identity. It highlighted the importance of investigating person-identity recognition from multiple modalities in both patients and healthy individuals. The results revised several assumptions of traditional cognitive and neuroscientific models of person-identity recognition and provide a model-driven framework for testing patients in clinical settings as well as basic experimental research to further advance our understanding of person-identity recognition.Fig. 3. The architecture of GoogleNetv2. Fig. 4. The detail of Inception module,1. Sun et al. /Pattern Recognition Letters 119 (2019) 49-61 51 7 7 ewetonsbiace vt ul Image pre-processing | Feature fusion Feature extraction Fig. 1. The architecture of the MDSTFN-based FER method.1, Sum et al. /Pattemn Recognition Letters 119 (2019) 49-61 ‘SVM —__ ft concatenation Score averaging fusion SVM based fusion",Person recognition,This review focuses on the identification of modality-specific and multimodal/modality-free regions in the brain that facilitate recognition of person identity. It emphasizes the need to study person-identity recognition from multiple modalities in both healthy individuals and patients. The findings challenge traditional cognitive and neuroscientific models of person-identity recognition and offer a model-driven approach for further research in clinical and experimental settings.,Object and Sentiment Recognition,"Fig. 3. The architecture of GoogleNetv2. Fig. 4. The detail of Inception module,1. Sun et al. /Pattern Recognition Letters 119 (2019) 49-61 51 7 7 ewetonsbiace vt ul Image pre-processing | Feature fusion Feature extraction Fig. 1. The architecture of the MDSTFN-based FER method.1, Sum et al. /Pattemn Recognition Letters 119 (2019) 49-61 ‘SVM —__ ft concatenation Score averaging fusion SVM based fusion",Object Recognition
368,Object semantics sentiment correlation analysis enhanced image sentiment classification,"Image sentiment classification, Object semantics, Bayesian network, Object semantics sentiment correlation model, Convolutional Neural Network","With the development of artificial intelligence and deep learning, image sentiment analysis has become a hotspot in computer vision and attracts more attention. Most of the existing methods focus on identifying the emotions by studying complex models or robust features from the whole image, which neglects the influence of object semantics on image sentiment analysis. In this paper, we propose a novel object semantics sentiment correlation model (OSSCM), which is based on Bayesian network, to guide the image sentiment classification. OSSCM is constructed by exploring the relationships between image emotions and the object semantics combination in the images, which can fully consider the effect of object semantics for image emotions. Then, a convolutional neural networks (CNN) based visual sentiment analysis model is proposed to analyze image sentiment from visual aspect. Finally, three fusion strategies are proposed to realize OSSCM enhanced image sentiment classification. Experiments on public emotion datasets FI and Flickr_LDL demonstrate that our proposed image sentiment classification method can achieve good performance on image emotion analysis, and outperform state of the art methods.","Emotions are states of feeling triggered by specific factors. Visual sentiment analysis studies the impact of images on human emotions, which has become a hotspot in computer vision, and been widely used in image retrieval, machine recognition and robotics. In this paper, we propose a novel image sentiment classification method, which applies the correlation between object semantics and image sentiment to enhance the visual sentiment analysis model for more accurate image emotion prediction. An object semantics sentiment correlation model based on Bayesian networks is proposed to represent the correlation between image emotions and object semantics combination, and obtain the homologous emotion probability distribution for object semantics combination. Experiments on the popular affective datasets FI and Flickr_LDL show that our proposed image sentiment analysis method can achieve good performance on image emotion prediction and outperform state of the art methods. In the future work, attention mechanism will be studied for concentrating the sentiment analysis on specific visual emotion region.","Object semantics sentiment correlation analysis enhanced image sentiment classificationImage sentiment classification, Object semantics, Bayesian network, Object semantics sentiment correlation model, Convolutional Neural NetworkWith the development of artificial intelligence and deep learning, image sentiment analysis has become a hotspot in computer vision and attracts more attention. Most of the existing methods focus on identifying the emotions by studying complex models or robust features from the whole image, which neglects the influence of object semantics on image sentiment analysis. In this paper, we propose a novel object semantics sentiment correlation model (OSSCM), which is based on Bayesian network, to guide the image sentiment classification. OSSCM is constructed by exploring the relationships between image emotions and the object semantics combination in the images, which can fully consider the effect of object semantics for image emotions. Then, a convolutional neural networks (CNN) based visual sentiment analysis model is proposed to analyze image sentiment from visual aspect. Finally, three fusion strategies are proposed to realize OSSCM enhanced image sentiment classification. Experiments on public emotion datasets FI and Flickr_LDL demonstrate that our proposed image sentiment classification method can achieve good performance on image emotion analysis, and outperform state of the art methods.Emotions are states of feeling triggered by specific factors. Visual sentiment analysis studies the impact of images on human emotions, which has become a hotspot in computer vision, and been widely used in image retrieval, machine recognition and robotics. In this paper, we propose a novel image sentiment classification method, which applies the correlation between object semantics and image sentiment to enhance the visual sentiment analysis model for more accurate image emotion prediction. An object semantics sentiment correlation model based on Bayesian networks is proposed to represent the correlation between image emotions and object semantics combination, and obtain the homologous emotion probability distribution for object semantics combination. Experiments on the popular affective datasets FI and Flickr_LDL show that our proposed image sentiment analysis method can achieve good performance on image emotion prediction and outperform state of the art methods. In the future work, attention mechanism will be studied for concentrating the sentiment analysis on specific visual emotion region.“EX Facedetection EXTRACT go Extraction mouth patel ae Extraction of eve ate ON Featureot Mouth patch of woe a reno Eyepaten 2ppwr Featureof| Eye patch Dimensionality ‘reduction Dimensionality ‘reduction Fig. 1. Block diagram of the proposed method. facial expressionClassification Output layer < e@.--e@ Supervised Backpropagation of error Hidden Layer Input layer < ‘Unsupervised DBN",sentiment analysis,"The text proposes a new method for image sentiment analysis that uses the correlation between object semantics and image sentiment to improve accuracy. The proposed method involves a Bayesian network model that represents the correlation between image emotions and object semantics, resulting in a probability distribution for object semantics combinations. Experiments on popular datasets show that the proposed method outperforms existing methods. Future work will include studying attention mechanisms to focus on specific visual emotion regions.



",Object and Sentiment Recognition,“EX Facedetection EXTRACT go Extraction mouth patel ae Extraction of eve ate ON Featureot Mouth patch of woe a reno Eyepaten 2ppwr Featureof| Eye patch Dimensionality ‘reduction Dimensionality ‘reduction Fig. 1. Block diagram of the proposed method. facial expressionClassification Output layer < e@.--e@ Supervised Backpropagation of error Hidden Layer Input layer < ‘Unsupervised DBN,Object Recognition
369,Identification of fact-implied implicit sentiment based on multi-level semantic fused representation,"Fact-implied implicit sentiment, Multi-level feature fusion, Representation learning, Sentiment analysis, Tree convolution","Sentiment can be expressed in an explicit or implicit manner. Most of the current studies on sentiment analysis focus on the identification of explicit sentiment but ignore the implicit. According to our statistics during data labeling in previous work, nearly a third of subjective sentences contain implicit sentiment, and 72% of the implicit sentiment sentences are fact-implied ones. We analyze the characteristics of the sentences that express fact-implied implicit sentiment and consider that fact-implied implicit sentiment is usually affected by its sentiment target, context semantic background and its own sentence structure. This paper focuses on the recognition of fact-implied implicit sentiment at the sentence level. A multi-level semantic fusion method is proposed to learn the features for identification based on representation learning. Three features in different levels are learned from the corpus, namely, sentiment target representation at the word level, structure embedded representation at the sentence level and context semantic background representation at the document level. We manually construct a fact-implied implicit sentiment corpus in Chinese, and experiments on the datasets show that the proposed method can effectively recognize fact-implied implicit sentiment sentences.

","This paper focuses on the identification and polarity classification of fact-implied implicit sentiment sentences. We analyze in detail the characteristics of fact-implied implicit sentiment expression and give the formal definition. A multi-level semantic feature fusion model is proposed to learn the representation of implicit sentiment. The multi-level features mainly include the sentiment target at word level, implicit sentiment expression at sentence level, and context semantic background at document level. Experiments on two manually labeled fact-implied implicit sentiment datasets show that our proposed method achieved 74.2% and 70.3% in identification accuracy and gained 78.3% and 80.5% in polarity classification accuracy in the two datasets, respectively.","Identification of fact-implied implicit sentiment based on multi-level semantic fused representationFact-implied implicit sentiment, Multi-level feature fusion, Representation learning, Sentiment analysis, Tree convolutionSentiment can be expressed in an explicit or implicit manner. Most of the current studies on sentiment analysis focus on the identification of explicit sentiment but ignore the implicit. According to our statistics during data labeling in previous work, nearly a third of subjective sentences contain implicit sentiment, and 72% of the implicit sentiment sentences are fact-implied ones. We analyze the characteristics of the sentences that express fact-implied implicit sentiment and consider that fact-implied implicit sentiment is usually affected by its sentiment target, context semantic background and its own sentence structure. This paper focuses on the recognition of fact-implied implicit sentiment at the sentence level. A multi-level semantic fusion method is proposed to learn the features for identification based on representation learning. Three features in different levels are learned from the corpus, namely, sentiment target representation at the word level, structure embedded representation at the sentence level and context semantic background representation at the document level. We manually construct a fact-implied implicit sentiment corpus in Chinese, and experiments on the datasets show that the proposed method can effectively recognize fact-implied implicit sentiment sentences.

This paper focuses on the identification and polarity classification of fact-implied implicit sentiment sentences. We analyze in detail the characteristics of fact-implied implicit sentiment expression and give the formal definition. A multi-level semantic feature fusion model is proposed to learn the representation of implicit sentiment. The multi-level features mainly include the sentiment target at word level, implicit sentiment expression at sentence level, and context semantic background at document level. Experiments on two manually labeled fact-implied implicit sentiment datasets show that our proposed method achieved 74.2% and 70.3% in identification accuracy and gained 78.3% and 80.5% in polarity classification accuracy in the two datasets, respectively.(™ 10.20% Error = 43.80% Crash 2.40% Freeze : 2.10% MB Lag MB Redoot MM Freeze ME(a) Sentiment propagation graph. The root node is the original tweet, while others are retweeting comments. (b) Sentiment fluctuation chart. The horizontal axis shows the time line, and the vertical axis shows the number of retweeting ‘comments with a certain sentiment orientation.Document (or tweet) overall sentiment score using the unsupervised polarity detection algorithm 2 | Number of positive words £3 | Number of negative words 4 | Number of negation words f5 Number of negation words followed by a positive word {6 | Number of negation words followed by a negative word f7 Inverse sentiment 18 | Number of positive words followed by target 19 | Number of negative words followed by target f10 | Number of negation words followed by target fil_| Number of positive words followed by a negative word f12 | Number of negative words followed by a positive word f13_| Number of target words followed by a positive word f14 | Number of target words followed by a negative word f15 | Number of negation words followed by a positive word which is followed by target f16 | Number of negation words followed by a negative word which is followed by target‘Input: Labeled examples ‘Output: Ciass labels of U Extract SL from U using constraints; Learn an initial naive Bayesian classifier # using LU SL and Equations 1 and 2; t inlabeled examples U 1 2 3 4 5 E-Step 6 foreach example din U (including SL) 7 Compute P(qjid) using fand Equation 3. 8 /M-Step 9 Learn a new naive Bayesian classifier ffrom LU U 10 by computing Plwjq) and P(c) using Equations 1 and 2. 11 until the clasitier parameters stabilize 42 Classify examples in Uto Cusing the final classifier f.‘Camera Gaoge-omete aay Googe ares wom ee a io = ‘Seotiment dstebution Perenige of mentionsRemoval of URt's, HTML Determining tags, Special symbols, abbreviations boundary of stop & Repeated words sentences Feature identification Feature extraction Feature Refinement 1 POS Tagging 1 Baced on frequency '¢ More Dependency Rule Generation count Rulesener ST eaey Preprocessing & Feature Extraction ‘Noise Elemination ‘Synonym features ‘sNon-Textual based Summarization ‘*Abstractive Text Summarization ‘*Aspect-based Opinion Summari *Query-Focused Summarization ‘*Real-Time Event Summarization ‘Update based Summarization ‘Miscellaneous ‘intrinsic Evaluation ‘Extrinsic EvaluationPreprocessing (Sentence ‘segmentation, Tokenization) Term frequency calculation Redundancy and relevance detectioncoment O00 100 8 §©000oD Oooo Ss si ws = Update Summarisation =]; Sy See AcordngtDunyanew || acorn Dunya news || Repose hs ttreerptedine soe | etre erptedine shoe || erptaina shoe factory Update aowonan wan Wi] saveeda pests Ol senor paiceoiowr Summaries | wcscnne | homtnaiontre ""stan enna Theactoryislocatedon™ | Fresafety isa cts Smapeainaeaton i Bandrandinasan. | concn rose = ss =| ‘ezrdngtoOunyanend |! senaro fire oruptedina shoe tan nnan says 0 0 Sentence | svoutoworters werein Sonn esse dao Updates | b's Tetcarieaaen™| :Cc Keon ae Extraction Wordnet Clustering all Summarization Summarized post related to . >) Poss topmost drug family Grouping of Te 5 ature sassifer Classification Ls} posts as per ol tate y,| Cc author id |Source Documents + Sentences U ‘Semantic Role Labeling (SRL) Semantic Similarity Matrix Semantic Clustering of Predicate Argument Structures (PASS) Selection of PASs from each Cluster based on Optimized Features tT Abstractive Summary Generationtales fem i Sentence Compression Abstractive Summation Extnctive ‘SummarizationSyntactic text ; Concept simplification Hectenelyes representation Concept analysis additional +———>|_ summarisation techniques \ x ‘Surface representation sentence generation <——+| simplified sentence final sentence selectionPolaritymy phone calls d {2:1} {2:2} (2:3, 3:6) {2:4,8:7) O—-O— SID:PID pairs —*9:4} Bs Input: SID:1. The iPhone is a great device. SID:2. My phone calls drop frequently with the iPhone. SID:3. Great device, but the calls drop too frequently. ‘SID:4. The iPhone is worth the price.The ° 396.23 @ National 310.10 _@ Hurricane 88.01."" | 261.69 Gilbert ¢* @ Center j9L74 | 338.6 swept » @ in 5 227.16 273.89 toward @ ° ; 147.9 235.62 Ll? ] i Sunday Bs 78.18 at ¢ and 124.73 | ne latitude » rises 174,39 | - [..] e © Civil $7.79 @ (C.JI love tl camera Tam amazed at ‘that I have took the quality of simply by using the photo auto mode photos. auto mode quality (a) DTI (©) ADTI Elaboration Contrast great camera It gives tons of/ Elaboration control for photo buffs x control but still has an for the novice auto mode to use auto mode (b) DT2 (d) ADT2‘Types Example Tweets Statement | Lily? Releases. 4 Times Journalists = hutp:/www photozz.com/? 104k #sincewebeinghonest why u so obsessed with what Question | me n her do?? Don't u got ya own man??? Oh wail..... RT @NaonkaMixon: 1 will donate 10 Sto the Red Suggestion | Cross Japan Earthquake fund for every person that retweets this! #PRAYFORJAPAN is enjoying this new season of Comment | sCetebrityApprentice.... Nikki Taylor = Yumt! 5 68. I want to get married 10 someone i meet in Miscellaneous highschool. #100factsaboume‘Opinion of the First Teacher abo 1 student is very high (Opinion value 8.5) ‘Opinion of the Second Teacher about the student is high (Opinion vale 6.0) Opinion of the Third Teacher about the student is low (Opinion vale :3.3383335) The collaborated opinion about the student is high (Opinion vale :5.9444447)MajorSA Tweetext aaa Salient Noise & Stop HashTag | Phrase! <a an! batt Topie Keyword Phrase Tweet Splissing aise. Ngram Selection alge. Template Based Topic Word] Seu Summaryvies! i: ‘ae: cr mewig ee aeCones ken chs) bere ae Me Lee a be Peet peg SON Fass eg 3002 Man con ca ex ry Vh-en ht oe 5m naa ens We spats wl ja ch a siatekatnes “rule gehen ancae vecenalce 2 twee ecco 94 (uc er 8 ohm py py Am oman tet HH faacram, nds hey He = oe Nee Ry eer sup Ae aegis baa oe: ype ch fmrca seams casey wm tym eRe ve ea ME MICS IRC Hoe by 2p. le fa oay ivadeh Weave {, hiporsd 24° ae caewe actos ell seen Te oudine vosw ashes 2: satya ww ogc hs | tome, reba = 4 we cane enn ab sp. U Drown level Cink Sammy Fees Raed Oplaen Samay ood tems: spcee here = we te wee been ee ae2000/03/20: Election DayFor “<topic words>"", people <verb frame> “<ngrams>""{, (and) <verb frame> “<ngrams>™)",sentiment analysis,"The paper presents a method for identifying and classifying the polarity of implicit sentiment sentences that are conveyed through facts. The authors define implicit sentiment and propose a multi-level semantic feature fusion model that considers word-level sentiment, sentence-level implicit sentiment, and document-level context. The proposed method achieved high accuracy in identifying and classifying implicit sentiment polarity in two manually labeled datasets.",Object and Sentiment Recognition,"(™ 10.20% Error = 43.80% Crash 2.40% Freeze : 2.10% MB Lag MB Redoot MM Freeze ME(a) Sentiment propagation graph. The root node is the original tweet, while others are retweeting comments. (b) Sentiment fluctuation chart. The horizontal axis shows the time line, and the vertical axis shows the number of retweeting ‘comments with a certain sentiment orientation.Document (or tweet) overall sentiment score using the unsupervised polarity detection algorithm 2 | Number of positive words £3 | Number of negative words 4 | Number of negation words f5 Number of negation words followed by a positive word {6 | Number of negation words followed by a negative word f7 Inverse sentiment 18 | Number of positive words followed by target 19 | Number of negative words followed by target f10 | Number of negation words followed by target fil_| Number of positive words followed by a negative word f12 | Number of negative words followed by a positive word f13_| Number of target words followed by a positive word f14 | Number of target words followed by a negative word f15 | Number of negation words followed by a positive word which is followed by target f16 | Number of negation words followed by a negative word which is followed by target‘Input: Labeled examples ‘Output: Ciass labels of U Extract SL from U using constraints; Learn an initial naive Bayesian classifier # using LU SL and Equations 1 and 2; t inlabeled examples U 1 2 3 4 5 E-Step 6 foreach example din U (including SL) 7 Compute P(qjid) using fand Equation 3. 8 /M-Step 9 Learn a new naive Bayesian classifier ffrom LU U 10 by computing Plwjq) and P(c) using Equations 1 and 2. 11 until the clasitier parameters stabilize 42 Classify examples in Uto Cusing the final classifier f.‘Camera Gaoge-omete aay Googe ares wom ee a io = ‘Seotiment dstebution Perenige of mentionsRemoval of URt's, HTML Determining tags, Special symbols, abbreviations boundary of stop & Repeated words sentences Feature identification Feature extraction Feature Refinement 1 POS Tagging 1 Baced on frequency '¢ More Dependency Rule Generation count Rulesener ST eaey Preprocessing & Feature Extraction ‘Noise Elemination ‘Synonym features ‘sNon-Textual based Summarization ‘*Abstractive Text Summarization ‘*Aspect-based Opinion Summari *Query-Focused Summarization ‘*Real-Time Event Summarization ‘Update based Summarization ‘Miscellaneous ‘intrinsic Evaluation ‘Extrinsic EvaluationPreprocessing (Sentence ‘segmentation, Tokenization) Term frequency calculation Redundancy and relevance detectioncoment O00 100 8 §©000oD Oooo Ss si ws = Update Summarisation =]; Sy See AcordngtDunyanew || acorn Dunya news || Repose hs ttreerptedine soe | etre erptedine shoe || erptaina shoe factory Update aowonan wan Wi] saveeda pests Ol senor paiceoiowr Summaries | wcscnne | homtnaiontre ""stan enna Theactoryislocatedon™ | Fresafety isa cts Smapeainaeaton i Bandrandinasan. | concn rose = ss =| ‘ezrdngtoOunyanend |! senaro fire oruptedina shoe tan nnan says 0 0 Sentence | svoutoworters werein Sonn esse dao Updates | b's Tetcarieaaen™| :Cc Keon ae Extraction Wordnet Clustering all Summarization Summarized post related to . >) Poss topmost drug family Grouping of Te 5 ature sassifer Classification Ls} posts as per ol tate y,| Cc author id |Source Documents + Sentences U ‘Semantic Role Labeling (SRL) Semantic Similarity Matrix Semantic Clustering of Predicate Argument Structures (PASS) Selection of PASs from each Cluster based on Optimized Features tT Abstractive Summary Generationtales fem i Sentence Compression Abstractive Summation Extnctive ‘SummarizationSyntactic text ; Concept simplification Hectenelyes representation Concept analysis additional +———>|_ summarisation techniques \ x ‘Surface representation sentence generation <——+| simplified sentence final sentence selectionPolaritymy phone calls d {2:1} {2:2} (2:3, 3:6) {2:4,8:7) O—-O— SID:PID pairs —*9:4} Bs Input: SID:1. The iPhone is a great device. SID:2. My phone calls drop frequently with the iPhone. SID:3. Great device, but the calls drop too frequently. ‘SID:4. The iPhone is worth the price.The ° 396.23 @ National 310.10 _@ Hurricane 88.01."" | 261.69 Gilbert ¢* @ Center j9L74 | 338.6 swept » @ in 5 227.16 273.89 toward @ ° ; 147.9 235.62 Ll? ] i Sunday Bs 78.18 at ¢ and 124.73 | ne latitude » rises 174,39 | - [..] e © Civil $7.79 @ (C.JI love tl camera Tam amazed at ‘that I have took the quality of simply by using the photo auto mode photos. auto mode quality (a) DTI (©) ADTI Elaboration Contrast great camera It gives tons of/ Elaboration control for photo buffs x control but still has an for the novice auto mode to use auto mode (b) DT2 (d) ADT2‘Types Example Tweets Statement | Lily? Releases. 4 Times Journalists = hutp:/www photozz.com/? 104k #sincewebeinghonest why u so obsessed with what Question | me n her do?? Don't u got ya own man??? Oh wail..... RT @NaonkaMixon: 1 will donate 10 Sto the Red Suggestion | Cross Japan Earthquake fund for every person that retweets this! #PRAYFORJAPAN is enjoying this new season of Comment | sCetebrityApprentice.... Nikki Taylor = Yumt! 5 68. I want to get married 10 someone i meet in Miscellaneous highschool. #100factsaboume‘Opinion of the First Teacher abo 1 student is very high (Opinion value 8.5) ‘Opinion of the Second Teacher about the student is high (Opinion vale 6.0) Opinion of the Third Teacher about the student is low (Opinion vale :3.3383335) The collaborated opinion about the student is high (Opinion vale :5.9444447)MajorSA Tweetext aaa Salient Noise & Stop HashTag | Phrase! <a an! batt Topie Keyword Phrase Tweet Splissing aise. Ngram Selection alge. Template Based Topic Word] Seu Summaryvies! i: ‘ae: cr mewig ee aeCones ken chs) bere ae Me Lee a be Peet peg SON Fass eg 3002 Man con ca ex ry Vh-en ht oe 5m naa ens We spats wl ja ch a siatekatnes “rule gehen ancae vecenalce 2 twee ecco 94 (uc er 8 ohm py py Am oman tet HH faacram, nds hey He = oe Nee Ry eer sup Ae aegis baa oe: ype ch fmrca seams casey wm tym eRe ve ea ME MICS IRC Hoe by 2p. le fa oay ivadeh Weave {, hiporsd 24° ae caewe actos ell seen Te oudine vosw ashes 2: satya ww ogc hs | tome, reba = 4 we cane enn ab sp. U Drown level Cink Sammy Fees Raed Oplaen Samay ood tems: spcee here = we te wee been ee ae2000/03/20: Election DayFor “<topic words>"", people <verb frame> “<ngrams>""{, (and) <verb frame> “<ngrams>™)",Deep Learning and Machine Learning
370,Salient object based visual sentiment analysis by combining deep features and handcrafted features,"Salient object detection, sentiment analysis, feature fusion, Convolutional Neural Network ","With the rapid growth of social networks, the visual sentiment analysis has quickly emerged for opinion mining. Recent study reveals that the sentiments conveyed by some images are related to salient objects in them, we propose a scheme for visual sentiment analysis that combines deep and handcrafted features. First, the salient objects are identified from the entire images. Then a pre-trained model such as VGG16 is used to extract deep features from the salient objects. In addition, hand-crafted features such as Visual texture, Colourfulness, Complexity and Fourier Sigma are extracted from all the salient objects. Deep features are combined individually with all the handcrafted features and the performance is measured. The sentiment is predicted using Convolutional Neural Network Classifier. The proposed method is tested on ArtPhoto, Emotion6, Abstract, IAPS datasets, Flickr and Flickr & Instagram datasets. The experimental results substantially proved that the proposed method achieves higher accuracy than other methods.","Based on the proposed framework, the sentiment classification of a given image can be summarized as follows. For a given image, salient objects are first generated. In order to reduce redundancy of salient objects for a single image, the candidate selection method is applied based on their sentiment scores and the best candidates are kept. Deep features are extracted using pretrained model and handcrafted features such as visual texture, complexity, colourfulness and Fourier Sigma are extracted by the described equations. All the features are classified individually and also combined with consistent weights. The feature vectors are classified using CNN classifier.","Salient object based visual sentiment analysis by combining deep features and handcrafted featuresSalient object detection, sentiment analysis, feature fusion, Convolutional Neural Network With the rapid growth of social networks, the visual sentiment analysis has quickly emerged for opinion mining. Recent study reveals that the sentiments conveyed by some images are related to salient objects in them, we propose a scheme for visual sentiment analysis that combines deep and handcrafted features. First, the salient objects are identified from the entire images. Then a pre-trained model such as VGG16 is used to extract deep features from the salient objects. In addition, hand-crafted features such as Visual texture, Colourfulness, Complexity and Fourier Sigma are extracted from all the salient objects. Deep features are combined individually with all the handcrafted features and the performance is measured. The sentiment is predicted using Convolutional Neural Network Classifier. The proposed method is tested on ArtPhoto, Emotion6, Abstract, IAPS datasets, Flickr and Flickr & Instagram datasets. The experimental results substantially proved that the proposed method achieves higher accuracy than other methods.Based on the proposed framework, the sentiment classification of a given image can be summarized as follows. For a given image, salient objects are first generated. In order to reduce redundancy of salient objects for a single image, the candidate selection method is applied based on their sentiment scores and the best candidates are kept. Deep features are extracted using pretrained model and handcrafted features such as visual texture, complexity, colourfulness and Fourier Sigma are extracted by the described equations. All the features are classified individually and also combined with consistent weights. The feature vectors are classified using CNN classifier.Anse Geertion Based Semen ition Bae Smnestcn Other Approaches",sentiment analysis,"Based on the proposed framework, the sentiment classification of a given image can be summarized as follows. For a given image, salient objects are first generated. In order to reduce redundancy of salient objects for a single image, the candidate selection method is applied based on their sentiment scores and the best candidates are kept. Deep features are extracted using pretrained model and handcrafted features such as visual texture, complexity, colourfulness and Fourier Sigma are extracted by the described equations. All the features are classified individually and also combined with consistent weights. The feature vectors are classified using CNN classifier.  summarize the above text",Object and Sentiment Recognition,Anse Geertion Based Semen ition Bae Smnestcn Other Approaches,Deep Learning and Machine Learning
371,Sentiment-oriented query-focused text summarization addressed with a multi-objective optimization approach,"Query-focused summarization, Sentiment-oriented summarization, Sentiment analysis, Multi-objective optimization, Crow search algorithm","Nowadays, the automatic text summarization is a highly relevant task in many contexts. In particular, query-focused summarization consists of generating a summary from one or multiple documents according to a query given by the user. Additionally, sentiment analysis and opinion mining analyze the polarity of the opinions contained in texts. These two issues are integrated in an approach to produce an opinionated summary according to the user’s query. Thereby, the query-focused sentiment-oriented extractive multi-document text summarization problem entails the optimization of different criteria, specifically, query relevance, redundancy reduction, and sentiment relevance. An adaptation of the metaheuristic population-based crow search algorithm has been designed, implemented, and tested to solve this multi-objective problem. Experiments have been carried out by using datasets from the Text Analysis Conference (TAC) datasets. Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics and the Pearson correlation coefficient have been used for the performance assessment. The results have reported that the proposed approach outperforms the existing methods in the scientific literature, with a percentage improvement of 75.5% for ROUGE-1 score and 441.3% for ROUGE-2 score. It also has been obtained a Pearson correlation coefficient of +0.841, reporting a strong linear positive correlation between the sentiment scores of the generated summaries and the sentiment scores of the queries of the topics.","The query-focused extractive multi-document text summarization task consists of generating a summary automatically according to a determined user information, that is given as a query. Additionally, the sentiment analysis and opinion mining task focuses on the analysis of the polarities of the sentences from a document collection, also considering their sentiment scores. Therefore, joining both issues in a single one, it is possible to produce a summary that includes the most relevant sentences for the user’s query, also having a similar sentiment orientation. In this regard, query-focused sentiment-oriented extractive multi-document text summarization involves the simultaneous optimization of the criteria of the query relevance, the redundancy reduction, and the sentiment relevance.","Sentiment-oriented query-focused text summarization addressed with a multi-objective optimization approachQuery-focused summarization, Sentiment-oriented summarization, Sentiment analysis, Multi-objective optimization, Crow search algorithmNowadays, the automatic text summarization is a highly relevant task in many contexts. In particular, query-focused summarization consists of generating a summary from one or multiple documents according to a query given by the user. Additionally, sentiment analysis and opinion mining analyze the polarity of the opinions contained in texts. These two issues are integrated in an approach to produce an opinionated summary according to the user’s query. Thereby, the query-focused sentiment-oriented extractive multi-document text summarization problem entails the optimization of different criteria, specifically, query relevance, redundancy reduction, and sentiment relevance. An adaptation of the metaheuristic population-based crow search algorithm has been designed, implemented, and tested to solve this multi-objective problem. Experiments have been carried out by using datasets from the Text Analysis Conference (TAC) datasets. Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics and the Pearson correlation coefficient have been used for the performance assessment. The results have reported that the proposed approach outperforms the existing methods in the scientific literature, with a percentage improvement of 75.5% for ROUGE-1 score and 441.3% for ROUGE-2 score. It also has been obtained a Pearson correlation coefficient of +0.841, reporting a strong linear positive correlation between the sentiment scores of the generated summaries and the sentiment scores of the queries of the topics.The query-focused extractive multi-document text summarization task consists of generating a summary automatically according to a determined user information, that is given as a query. Additionally, the sentiment analysis and opinion mining task focuses on the analysis of the polarities of the sentences from a document collection, also considering their sentiment scores. Therefore, joining both issues in a single one, it is possible to produce a summary that includes the most relevant sentences for the user’s query, also having a similar sentiment orientation. In this regard, query-focused sentiment-oriented extractive multi-document text summarization involves the simultaneous optimization of the criteria of the query relevance, the redundancy reduction, and the sentiment relevance.roe =o specific knowledge (a eee ne iz -verbal zi verbal info non-verbal info feelings of coco Ed Ea oo early on Es es name voice face name voice face name voice face left right left right left right hemisphere hemisphere hemisphere",sentiment analysis,"The task of query-focused extractive multi-document text summarization involves generating a summary based on a user's query, while sentiment analysis and opinion mining involves analyzing the polarity and sentiment scores of sentences in a document collection. Combining these tasks results in a summary that includes the most relevant sentences for the user's query, with a similar sentiment orientation. This is known as query-focused sentiment-oriented extractive multi-document text summarization, which optimizes the criteria of query relevance, redundancy reduction, and sentiment relevance.",Object and Sentiment Recognition,roe =o specific knowledge (a eee ne iz -verbal zi verbal info non-verbal info feelings of coco Ed Ea oo early on Es es name voice face name voice face name voice face left right left right left right hemisphere hemisphere hemisphere,Object Recognition
